Understanding Digital
Signal Processing
Third Edition

This page intentionally left blank

Understanding Digital
Signal Processing
Third Edition
Richard G. Lyons
Upper Saddle River, NJ • Boston • Indianapolis • San Francisco
New York • Toronto • Montreal • London • Munich • Paris • Madrid
Capetown • Sydney • Tokyo • Singapore • Mexico City

Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trade-
marks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the
designations have been printed with initial capital letters or in all capitals.
The author and publisher have taken care in the preparation of this book, but make no expressed or implied war-
ranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or con-
sequential damages in connection with or arising out of the use of the information or programs contained herein.
The publisher offers excellent discounts on this book when ordered in quantity for bulk purchases or special
sales, which may include electronic versions and/or custom covers and content particular to your business,
training goals, marketing focus, and branding interests. For more information, please contact:
U.S. Corporate and Government Sales
(800) 382-3419
corpsales@pearsontechgroup.com
For sales outside the United States please contact:
International Sales
international@pearson.com
Visit us on the Web: informit.com/ph
Library of Congress Cataloging-in-Publication Data
Lyons, Richard G., 1948-
Understanding digital signal processing / Richard G. Lyons.—3rd ed.
p. cm.
Includes bibliographical references and index.
ISBN 0-13-702741-9 (hardcover : alk. paper)
1. Signal processing—Digital techniques. I. Title.
TK5102.9.L96 2011
621.382'2—dc22 2010035407
Copyright © 2011 Pearson Education, Inc.
All rights reserved. Printed in the United States of America. This publication is protected by copyright, and per-
mission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system,
or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For
information regarding permissions, write to:
Pearson Education, Inc.
Rights and Contracts Department
501 Boylston Street, Suite 900
Boston, MA02116
Fax: (617) 671-3447
ISBN-13: 978-0-13-702741-5
ISBN-10: 0-13-702741-9
Text printed in the United States on recycled paper at Edwards Brothers in Ann Arbor, Michigan.
First printing, November 2010

I dedicate this book to my daughters, Julie and Meredith—I wish I
could go with you; to my mother, Ruth, for making me finish my
homework; to my father, Grady, who didn’t know what he started
when he built that workbench in the basement; to my brother Ray
for improving us all; to my brother Ken who succeeded where I
failed; to my sister Nancy for running interference for us; and to the
skilled folks on the USENET newsgroup comp.dsp. They ask the good
questions and provide the best answers. Finally, to Sigi Pardula (Bat-
girl): Without your keeping the place running, this book wouldn’t
exist.

This page intentionally left blank

Contents
PREFACE xv
ABOUT THE AUTHOR xxiii
1 DISCRETE SEQUENCES AND SYSTEMS 1
1.1 Discrete Sequences and Their Notation 2
1.2 Signal Amplitude, Magnitude, Power 8
1.3 Signal Processing Operational Symbols 10
1.4 Introduction to Discrete Linear Time-Invariant Systems 12
1.5 Discrete Linear Systems 12
1.6 Time-Invariant Systems 17
1.7 The Commutative Property of Linear Time-Invariant Systems 18
1.8 Analyzing Linear Time-Invariant Systems 19
References 21
Chapter 1 Problems 23
2 PERIODIC SAMPLING 33
2.1 Aliasing: Signal Ambiguity in the Frequency Domain 33
2.2 Sampling Lowpass Signals 38
2.3 Sampling Bandpass Signals 42
2.4 Practical Aspects of Bandpass Sampling 45
References 49
Chapter 2 Problems 50
3 THE DISCRETE FOURIER TRANSFORM 59
3.1 Understanding the DFT Equation 60
3.2 DFT Symmetry 73
vii

viii Contents
3.3 DFT Linearity 75
3.4 DFT Magnitudes 75
3.5 DFT Frequency Axis 77
3.6 DFT Shifting Theorem 77
3.7 Inverse DFT 80
3.8 DFT Leakage 81
3.9 Windows 89
3.10 DFT Scalloping Loss 96
3.11 DFT Resolution, Zero Padding, and Frequency-Domain
Sampling 98
3.12 DFT Processing Gain 102
3.13 The DFT of Rectangular Functions 105
3.14 Interpreting the DFT Using the Discrete-Time
Fourier Transform 120
References 124
Chapter 3 Problems 125
4 THE FAST FOURIER TRANSFORM 135
4.1 Relationship of the FFT to the DFT 136
4.2 Hints on Using FFTs in Practice 137
4.3 Derivation of the Radix-2 FFT Algorithm 141
4.4 FFT Input/Output Data Index Bit Reversal 149
4.5 Radix-2 FFT Butterfly Structures 151
4.6 Alternate Single-Butterfly Structures 154
References 158
Chapter 4 Problems 160
5 FINITE IMPULSE RESPONSE FILTERS 169
5.1 An Introduction to Finite Impulse Response (FIR) Filters 170
5.2 Convolution in FIR Filters 175
5.3 Lowpass FIR Filter Design 186
5.4 Bandpass FIR Filter Design 201
5.5 Highpass FIR Filter Design 203
5.6 Parks-McClellan Exchange FIR Filter Design Method 204
5.7 Half-band FIR Filters 207
5.8 Phase Response of FIR Filters 209
5.9 AGeneric Description of Discrete Convolution 214

Contents ix
5.10 Analyzing FIR Filters 226
References 235
Chapter 5 Problems 238
6 INFINITE IMPULSE RESPONSE FILTERS 253
6.1 An Introduction to Infinite Impulse Response Filters 254
6.2 The Laplace Transform 257
6.3 The z-Transform 270
6.4 Using the z-Transform to Analyze IIR Filters 274
6.5 Using Poles and Zeros to Analyze IIR Filters 282
6.6 Alternate IIR Filter Structures 289
6.7 Pitfalls in Building IIR Filters 292
6.8 Improving IIR Filters with Cascaded Structures 295
6.9 Scaling the Gain of IIR Filters 300
6.10 Impulse Invariance IIR Filter Design Method 303
6.11 Bilinear Transform IIR Filter Design Method 319
6.12 Optimized IIR Filter Design Method 330
6.13 ABrief Comparison of IIR and FIR Filters 332
References 333
Chapter 6 Problems 336
7 SPECIALIZED DIGITAL NETWORKS AND FILTERS 361
7.1 Differentiators 361
7.2 Integrators 370
7.3 Matched Filters 376
7.4 Interpolated Lowpass FIR Filters 381
7.5 Frequency Sampling Filters: The Lost Art 392
References 426
Chapter 7 Problems 429
8 QUADRATURE SIGNALS 439
8.1 Why Care about Quadrature Signals? 440
8.2 The Notation of Complex Numbers 440
8.3 Representing Real Signals Using Complex Phasors 446
8.4 AFew Thoughts on Negative Frequency 450

x Contents
8.5 Quadrature Signals in the Frequency Domain 451
8.6 Bandpass Quadrature Signals in the Frequency Domain 454
8.7 Complex Down-Conversion 456
8.8 AComplex Down-Conversion Example 458
8.9 An Alternate Down-Conversion Method 462
References 464
Chapter 8 Problems 465
9 THE DISCRETE HILBERT TRANSFORM 479
9.1 Hilbert Transform Definition 480
9.2 Why Care about the Hilbert Transform? 482
9.3 Impulse Response of a Hilbert Transformer 487
9.4 Designing a Discrete Hilbert Transformer 489
9.5 Time-Domain Analytic Signal Generation 495
9.6 Comparing Analytical Signal Generation Methods 497
References 498
Chapter 9 Problems 499
10 SAMPLE RATE CONVERSION 507
10.1 Decimation 508
10.2 Two-Stage Decimation 510
10.3 Properties of Downsampling 514
10.4 Interpolation 516
10.5 Properties of Interpolation 518
10.6 Combining Decimation and Interpolation 521
10.7 Polyphase Filters 522
10.8 Two-Stage Interpolation 528
10.9 z-Transform Analysis of Multirate Systems 533
10.10 Polyphase Filter Implementations 535
10.11 Sample Rate Conversion by Rational Factors 540
10.12 Sample Rate Conversion with Half-band Filters 543
10.13 Sample Rate Conversion with IFIR Filters 548
10.14 Cascaded Integrator-Comb Filters 550
References 566
Chapter 10 Problems 568

Contents xi
11 SIGNAL AVERAGING 589
11.1 Coherent Averaging 590
11.2 Incoherent Averaging 597
11.3 Averaging Multiple Fast Fourier Transforms 600
11.4 Averaging Phase Angles 603
11.5 Filtering Aspects of Time-Domain Averaging 604
11.6 Exponential Averaging 608
References 615
Chapter 11 Problems 617
12 DIGITAL DATA FORMATS AND THEIR EFFECTS 623
12.1 Fixed-Point Binary Formats 623
12.2 Binary Number Precision and Dynamic Range 632
12.3 Effects of Finite Fixed-Point Binary Word Length 634
12.4 Floating-Point Binary Formats 652
12.5 Block Floating-Point Binary Format 658
References 658
Chapter 12 Problems 661
13 DIGITAL SIGNAL PROCESSING TRICKS 671
13.1 Frequency Translation without Multiplication 671
13.2 High-Speed Vector Magnitude Approximation 679
13.3 Frequency-Domain Windowing 683
13.4 Fast Multiplication of Complex Numbers 686
13.5 Efficiently Performing the FFT of Real Sequences 687
13.6 Computing the Inverse FFT Using the Forward FFT 699
13.7 Simplified FIR Filter Structure 702
13.8 Reducing A/D Converter Quantization Noise 704
13.9 A/D Converter Testing Techniques 709
13.10 Fast FIR Filtering Using the FFT 716
13.11 Generating Normally Distributed Random Data 722
13.12 Zero-Phase Filtering 725
13.13 Sharpened FIR Filters 726
13.14 Interpolating a Bandpass Signal 728
13.15 Spectral Peak Location Algorithm 730

xii Contents
13.16 Computing FFT Twiddle Factors 734
13.17 Single Tone Detection 737
13.18 The Sliding DFT 741
13.19 The Zoom FFT 749
13.20 APractical Spectrum Analyzer 753
13.21 An Efficient Arctangent Approximation 756
13.22 Frequency Demodulation Algorithms 758
13.23 DC Removal 761
13.24 Improving Traditional CIC Filters 765
13.25 Smoothing Impulsive Noise 770
13.26 Efficient Polynomial Evaluation 772
13.27 Designing Very High-Order FIR Filters 775
13.28 Time-Domain Interpolation Using the FFT 778
13.29 Frequency Translation Using Decimation 781
13.30 Automatic Gain Control (AGC) 783
13.31 Approximate Envelope Detection 784
13.32 AQuadrature Oscillator 786
13.33 Specialized Exponential Averaging 789
13.34 Filtering Narrowband Noise Using Filter Nulls 792
13.35 Efficient Computation of Signal Variance 797
13.36 Real-time Computation of Signal Averages and Variances 799
13.37 Building Hilbert Transformers from Half-band Filters 802
13.38 Complex Vector Rotation with Arctangents 805
13.39 An Efficient Differentiating Network 810
13.40 Linear-Phase DC-Removal Filter 812
13.41 Avoiding Overflow in Magnitude Computations 815
13.42 Efficient Linear Interpolation 815
13.43 Alternate Complex Down-conversion Schemes 816
13.44 Signal Transition Detection 820
13.45 Spectral Flipping around Signal Center Frequency 821
13.46 Computing Missing Signal Samples 823
13.47 Computing Large DFTs Using Small FFTs 826
13.48 Computing Filter Group Delay without Arctangents 830
13.49 Computing a Forward and Inverse FFT Using a Single FFT 831
13.50 Improved Narrowband Lowpass IIR Filters 833
13.51 AStable Goertzel Algorithm 838
References 840

Contents xiii
A THE ARITHMETIC OF COMPLEX NUMBERS 847
A.1 Graphical Representation of Real and Complex Numbers 847
A.2 Arithmetic Representation of Complex Numbers 848
A.3 Arithmetic Operations of Complex Numbers 850
A.4 Some Practical Implications of Using Complex Numbers 856
B CLOSED FORM OF A GEOMETRIC SERIES 859
C TIME REVERSAL AND THE DFT 863
D MEAN,VARIANCE,AND STANDARD DEVIATION 867
D.1 Statistical Measures 867
D.2 Statistics of Short Sequences 870
D.3 Statistics of Summed Sequences 872
D.4 Standard Deviation (RMS) of a Continuous Sinewave 874
D.5 Estimating Signal-to-Noise Ratios 875
D.6 The Mean and Variance of Random Functions 879
D.7 The Normal Probability Density Function 882
E DECIBELS (DB AND DBM) 885
E.1 Using Logarithms to Determine Relative Signal Power 885
E.2 Some Useful Decibel Numbers 889
E.3 Absolute Power Using Decibels 891
F DIGITAL FILTER TERMINOLOGY 893
G FREQUENCY SAMPLING FILTER DERIVATIONS 903
G.1 Frequency Response of a Comb Filter 903
G.2 Single Complex FSF Frequency Response 904
G.3 Multisection Complex FSF Phase 905
G.4 Multisection Complex FSF Frequency Response 906

xiv Contents
G.5 Real FSF Transfer Function 908
G.6 Type-IV FSF Frequency Response 910
H FREQUENCY SAMPLING FILTER DESIGN TABLES 913
I COMPUTING CHEBYSHEV WINDOW SEQUENCES 927
I.1 Chebyshev Windows for FIR Filter Design 927
I.2 Chebyshev Windows for Spectrum Analysis 929
INDEX 931

Preface
This book is an expansion of previous editions of Understanding Digital Signal
Processing. Like those earlier editions, its goals are (1) to help beginning stu-
dents understand the theory of digital signal processing (DSP) and (2) to pro-
vide practical DSP information, not found in other books, to help working
engineers/scientists design and test their signal processing systems. Each
chapter of this book contains new information beyond that provided in ear-
lier editions.
It’s traditional at this point in the preface of a DSP textbook for the au-
thor to tell readers why they should learn DSP. I don’t need to tell you how
important DSP is in our modern engineering world. You already know that.
I’ll just say that the future of electronics is DSP, and with this book you will
not be left behind.
FOR INSTRUCTORS
This third edition is appropriate as the text for a one- or two-semester under-
graduate course in DSP. It follows the DSP material I cover in my corporate
training activities and a signal processing course I taught at the University of
California Santa Cruz Extension. To aid students in their efforts to learn DSP,
this third edition provides additional explanations and examples to increase
its tutorial value. To test a student’s understanding of the material, home-
work problems have been included at the end of each chapter. (For qualified
instructors, a Solutions Manual is available from Prentice Hall.)
xv

xvi Preface
FOR PRACTICING ENGINEERS
To help working DSPengineers, the changes in this third edition include, but
are not limited to, the following:
• Practical guidance in building discrete differentiators, integrators, and
matched filters
• Descriptions of statistical measures of signals, variance reduction by
way of averaging, and techniques for computing real-world signal-to-
noise ratios (SNRs)
• A significantly expanded chapter on sample rate conversion (multirate
systems) and its associated filtering
• Implementing fast convolution (FIR filtering in the frequency domain)
• IIR filter scaling
• Enhanced material covering techniques for analyzing the behavior and
performance of digital filters
• Expanded descriptions of industry-standard binary number formats
used in modern processing systems
• Numerous additions to the popular “Digital Signal Processing Tricks”
chapter
FOR STUDENTS
Learning the fundamentals, and how to speak the language, of digital signal
processing does not require profound analytical skills or an extensive back-
ground in mathematics. All you need is a little experience with elementary al-
gebra, knowledge of what a sinewave is, this book, and enthusiasm. This may
sound hard to believe, particularly if you’ve just flipped through the pages of
this book and seen figures and equations that look rather complicated. The
content here, you say, looks suspiciously like material in technical journals
and textbooks whose meaning has eluded you in the past. Well, this is not just
another book on digital signal processing.
In this book I provide a gentle, but thorough, explanation of the theory
and practice of DSP. The text is not written so that you mayunderstand the ma-
terial, but so that you mustunderstand the material. I’ve attempted to avoid the
traditional instructor–student relationship and have tried to make reading this
book seem like talking to a friend while walking in the park. I’ve used just
enough mathematics to help you develop a fundamental understanding of DSP
theory and have illustrated that theory with practical examples.
I have designed the homework problems to be more than mere exercises
that assign values to variables for the student to plug into some equation in
order to compute a result. Instead, the homework problems are designed to

Preface xvii
be as educational as possible in the sense of expanding on and enabling fur-
ther investigation of specific aspects of DSPtopics covered in the text. Stated
differently, the homework problems are not designed to induce “death by al-
gebra,” but rather to improve your understanding of DSP. Solving the prob-
lems helps you become proactive in your own DSP education instead of
merely being an inactive recipient of DSPinformation.
THE JOURNEY
Learning digital signal processing is not something you accomplish; it’s a
journey you take. When you gain an understanding of one topic, questions
arise that cause you to investigate some other facet of digital signal process-
ing.† Armed with more knowledge, you’re likely to begin exploring further
aspects of digital signal processing much like those shown in the diagram on
page xviii. This book is your tour guide during the first steps of your journey.
You don’t need a computer to learn the material in this book, but it
would certainly help. DSP simulation software allows the beginner to verify
signal processing theory through the time-tested trial and error process.‡ In
particular, software routines that plot signal data, perform the fast Fourier
transforms, and analyze digital filters would be very useful.
As you go through the material in this book, don’t be discouraged if
your understanding comes slowly. As the Greek mathematician Menaechmus
curtly remarked to Alexander the Great, when asked for a quick explanation
of mathematics, “There is no royal road to mathematics.” Menaechmus was
confident in telling Alexander the only way to learn mathematics is through
careful study. The same applies to digital signal processing. Also, don’t worry
if you need to read some of the material twice. While the concepts in this
book are not as complicated as quantum physics, as mysterious as the lyrics
of the song “Louie Louie,” or as puzzling as the assembly instructions of a
metal shed, they can become a little involved. They deserve your thoughtful
attention. So, go slowly and read the material twice if necessary; you’ll be
glad you did. If you show persistence, to quote Susan B. Anthony, “Failure is
impossible.”
†“You see I went on with this research just the way it led me. This is the only way I ever heard of
research going. I asked a question, devised some method of getting an answer, and got—a fresh
question. Was this possible, or that possible? You cannot imagine what this means to an investi-
gator, what an intellectual passion grows upon him. You cannot imagine the strange colourless
delight of these intellectual desires” (Dr. Moreau—infamous physician and vivisectionist from
H.G. Wells’ Island of Dr. Moreau,1896).
‡“One must learn by doing the thing; for though you think you know it, you have no certainty
until you try it” (Sophocles, 496–406 B.C.).

xviii Preface
Periodic
Sampling
How can the spectra of sampled How can the sample rates of
signals be analyzed? discrete signals be changed?
Window
Functions
How can DFT How can digital filter
How does
measurement accuracy frequency responses
windowing
be improved? be improved?
work?
How can
Discrete Fourier
spectra be Digital Filters
Transform
modified?
Why are discrete spectra What causes
periodic, and what causes passband ripple in
DFT leakage? digital filters?
Convolution
How can spectral noise be reduced How can the noise reduction
to enhance signal detection? effects of averaging be improved?
Signal
Averaging
COMING ATTRACTIONS
Chapter 1 begins by establishing the notation used throughout the remainder
of the book. In that chapter we introduce the concept of discrete signal se-
quences, show how they relate to continuous signals, and illustrate how those
sequences can be depicted in both the time and frequency domains. In addi-
tion, Chapter 1 defines the operational symbols we’ll use to build our signal
processing system block diagrams. We conclude that chapter with a brief in-
troduction to the idea of linear systems and see why linearity enables us to
use a number of powerful mathematical tools in our analysis.
Chapter 2 introduces the most frequently misunderstood process in dig-
ital signal processing, periodic sampling. Although the concept of sampling a

Preface xix
continuous signal is not complicated, there are mathematical subtleties in the
process that require thoughtful attention. Beginning gradually with simple
examples of lowpass sampling, we then proceed to the interesting subject of
bandpass sampling. Chapter 2 explains and quantifies the frequency-domain
ambiguity (aliasing) associated with these important topics.
Chapter 3 is devoted to one of the foremost topics in digital signal pro-
cessing, the discrete Fourier transform (DFT) used for spectrum analysis.
Coverage begins with detailed examples illustrating the important properties
of the DFT and how to interpret DFT spectral results, progresses to the topic
of windows used to reduce DFT leakage, and discusses the processing gain
afforded by the DFT. The chapter concludes with a detailed discussion of the
various forms of the transform of rectangular functions that the reader is
likely to encounter in the literature.
Chapter 4 covers the innovation that made the most profound impact on
the field of digital signal processing, the fast Fourier transform (FFT). There
we show the relationship of the popular radix 2 FFT to the DFT, quantify the
powerful processing advantages gained by using the FFT, demonstrate why
the FFT functions as it does, and present various FFT implementation struc-
tures. Chapter 4 also includes a list of recommendations to help the reader
use the FFT in practice.
Chapter 5 ushers in the subject of digital filtering. Beginning with a sim-
ple lowpass finite impulse response (FIR) filter example, we carefully
progress through the analysis of that filter’s frequency-domain magnitude
and phase response. Next, we learn how window functions affect, and can be
used to design, FIR filters. The methods for converting lowpass FIR filter de-
signs to bandpass and highpass digital filters are presented, and the popular
Parks-McClellan (Remez) Exchange FIR filter design technique is introduced
and illustrated by example. In that chapter we acquaint the reader with, and
take the mystery out of, the process called convolution. Proceeding through
several simple convolution examples, we conclude Chapter 5 with a discus-
sion of the powerful convolution theorem and show why it’s so useful as a
qualitative tool in understanding digital signal processing.
Chapter 6 is devoted to a second class of digital filters, infinite impulse
response (IIR) filters. In discussing several methods for the design of IIR fil-
ters, the reader is introduced to the powerful digital signal processing analy-
sis tool called the z-transform. Because the z-transform is so closely related to
the continuous Laplace transform, Chapter 6 starts by gently guiding the
reader from the origin, through the properties, and on to the utility of the
Laplace transform in preparation for learning the z-transform. We’ll see how
IIR filters are designed and implemented, and why their performance is so
different from that of FIR filters. To indicate under what conditions these fil-
ters should be used, the chapter concludes with a qualitative comparison of
the key properties of FIR and IIR filters.

xx Preface
Chapter 7 introduces specialized networks known as digital differentia-
tors, integrators, and matched filters. In addition, this chapter covers two spe-
cialized digital filter types that have not received their deserved exposure in
traditional DSP textbooks. Called interpolated FIR and frequency sampling fil-
ters, providing improved lowpass filtering computational efficiency, they be-
long in our arsenal of filter design techniques. Although these are FIR filters,
their introduction is delayed to this chapter because familiarity with the
z-transform (in Chapter 6) makes the properties of these filters easier to un-
derstand.
Chapter 8 presents a detailed description of quadrature signals (also
called complex signals). Because quadrature signal theory has become so im-
portant in recent years, in both signal analysis and digital communications
implementations, it deserves its own chapter. Using three-dimensional illus-
trations, this chapter gives solid physical meaning to the mathematical nota-
tion, processing advantages, and use of quadrature signals. Special emphasis
is given to quadrature sampling (also called complex down-conversion).
Chapter 9 provides a mathematically gentle, but technically thorough,
description of the Hilbert transform—a process used to generate a quadrature
(complex) signal from a real signal. In this chapter we describe the properties,
behavior, and design of practical Hilbert transformers.
Chapter 10 presents an introduction to the fascinating and useful
process of sample rate conversion (changing the effective sample rate of dis-
crete data sequences through decimation or interpolation). Sample rate con-
version—so useful in improving the performance and reducing the
computational complexity of many signal processing operations—is essen-
tially an exercise in lowpass filter design. As such, polyphase and cascaded
integrator-comb filters are described in detail in this chapter.
Chapter 11 covers the important topic of signal averaging. There we
learn how averaging increases the accuracy of signal measurement schemes
by reducing measurement background noise. This accuracy enhancement is
called processing gain, and the chapter shows how to predict the processing
gain associated with averaging signals in both the time and frequency do-
mains. In addition, the key differences between coherent and incoherent aver-
aging techniques are explained and demonstrated with examples. To
complete that chapter the popular scheme known as exponential averaging is
covered in some detail.
Chapter 12 presents an introduction to the various binary number for-
mats the reader is likely to encounter in modern digital signal processing. We
establish the precision and dynamic range afforded by these formats along
with the inherent pitfalls associated with their use. Our exploration of the
critical subject of binary data word width (in bits) naturally leads to a discus-
sion of the numerical resolution limitations of analog-to-digital (A/D) con-
verters and how to determine the optimum A/D converter word size for a

Preface xxi
given application. The problems of data value overflow roundoff errors are
covered along with a statistical introduction to the two most popular reme-
dies for overflow, truncation and rounding. We end that chapter by covering
the interesting subject of floating-point binary formats that allow us to over-
come most of the limitations induced by fixed-point binary formats, particu-
larly in reducing the ill effects of data overflow.
Chapter 13 provides the literature’s most comprehensive collection of
tricks of the trade used by DSP professionals to make their processing algo-
rithms more efficient. These techniques are compiled into a chapter at the end
of the book for two reasons. First, it seems wise to keep our collection of tricks
in one chapter so that we’ll know where to find them in the future. Second,
many of these clever schemes require an understanding of the material from
the previous chapters, making the last chapter an appropriate place to keep
our arsenal of clever tricks. Exploring these techniques in detail verifies and
reiterates many of the important ideas covered in previous chapters.
The appendices include a number of topics to help the beginner under-
stand the nature and mathematics of digital signal processing. Acomprehen-
sive description of the arithmetic of complex numbers is covered in Appendix
A, and Appendix B derives the often used, but seldom explained, closed form
of a geometric series. The subtle aspects and two forms of time reversal in dis-
crete systems (of which zero-phase digital filtering is an application) are ex-
plained in Appendix C. The statistical concepts of mean, variance, and
standard deviation are introduced and illustrated in Appendix D, and Ap-
pendix E provides a discussion of the origin and utility of the logarithmic
decibel scale used to improve the magnitude resolution of spectral represen-
tations. Appendix F, in a slightly different vein, provides a glossary of the ter-
minology used in the field of digital filters. Appendices G and H provide
supplementary information for designing and analyzing specialized digital
filters. Appendix I explains the computation of Chebyshev window se-
quences.
ACKNOWLEDGMENTS
Much of the new material in this edition is a result of what I’ve learned from
those clever folk on the USENET newsgroup comp.dsp. (I could list a dozen
names, but in doing so I’d make 12 friends and 500 enemies.) So, I say thanks to
my DSPpals on comp.dsp for teaching me so much signal processing theory.
In addition to the reviewers of previous editions of this book, I thank
Randy Yates, Clay Turner, and Ryan Groulx for their time and efforts to help
me improve the content of this book. I am especially indebted to my eagle-
eyed mathematician friend Antoine Trux for his relentless hard work to both
enhance this DSPmaterial and create a homework Solutions Manual.

xxii Preface
As before, I thank my acquisitions editor, Bernard Goodwin, for his pa-
tience and guidance, and his skilled team of production people, project editor
Elizabeth Ryan in particular, at Prentice Hall.
If you’re still with me this far in this Preface, I end by saying I had a ball
writing this book and sincerely hope you benefit from reading it. If you have
any comments or suggestions regarding this material, or detect any errors no
matter how trivial, please send them to me at R.Lyons@ieee.org. I promise I
will reply to your e-mail.

About the Author
Richard Lyons is a consulting systems engineer and lecturer with Besser Associates in
Mountain View, California. He has been the lead hardware engineer for numerous
signal processing systems for both the National Security Agency (NSA) and Northrop
Grumman Corp. Lyons has taught DSPat the University of California Santa Cruz Ex-
tension and authored numerous articles on DSP. As associate editor for the IEEE Sig-
nal Processing Magazinehe created, edits, and contributes to the magazine’s “DSPTips
& Tricks” column.
xxiii

This page intentionally left blank

CHAPTER ONE
Discrete
Sequences
and Systems
Digital signal processing has never been more prevalent or easier to perform.
It wasn’t that long ago when the fast Fourier transform (FFT), a topic we’ll
discuss in Chapter 4, was a mysterious mathematical process used only in in-
dustrial research centers and universities. Now, amazingly, the FFT is readily
available to us all. It’s even a built-in function provided by inexpensive
spreadsheet software for home computers. The availability of more sophisti-
cated commercial signal processing software now allows us to analyze and
develop complicated signal processing applications rapidly and reliably. We
can perform spectral analysis, design digital filters, develop voice recogni-
tion, data communication, and image compression processes using software
that’s interactive both in the way algorithms are defined and how the result-
ing data are graphically displayed. Since the mid-1980s the same integrated
circuit technology that led to affordable home computers has produced pow-
erful and inexpensive hardware development systems on which to imple-
ment our digital signal processing designs.† Regardless, though, of the ease
with which these new digital signal processing development systems and
software can be applied, we still need a solid foundation in understanding
the basics of digital signal processing. The purpose of this book is to build
that foundation.
In this chapter we’ll set the stage for the topics we’ll study throughout the re-
mainder of this book by defining the terminology used in digital signal process-
†During a television interview in the early 1990s, a leading computer scientist stated that had
automobile technology made the same strides as the computer industry, we’d all have a car that
would go a half million miles per hour and get a half million miles per gallon. The cost of that
car would be so low that it would be cheaper to throw it away than pay for one day’s parking in
San Francisco.
1

2 Discrete Sequences and Systems
ing, illustrating the various ways of graphically representing discrete signals, es-
tablishing the notation used to describe sequences of data values, presenting the
symbols used to depict signal processing operations, and briefly introducing the
concept of a linear discrete system.
1.1 DISCRETE SEQUENCES AND THEIR NOTATION
In general, the term signal processing refers to the science of analyzing time-
varying physical processes. As such, signal processing is divided into two cat-
egories, analog signal processing and digital signal processing. The term
analog is used to describe a waveform that’s continuous in time and can take
on a continuous range of amplitude values. An example of an analog signal is
some voltage that can be applied to an oscilloscope, resulting in a continuous
display as a function of time. Analog signals can also be applied to a conven-
tional spectrum analyzer to determine their frequency content. The term ana-
log appears to have stemmed from the analog computers used prior to 1980.
These computers solved linear differential equations by means of connecting
physical (electronic) differentiators and integrators using old-style telephone
operator patch cords. That way, a continuous voltage or current in the actual
circuit was analogous to some variable in a differential equation, such as
speed, temperature, air pressure, etc. (Although the flexibility and speed of
modern-day digital computers have since made analog computers obsolete, a
good description of the short-lived utility of analog computers can be found
in reference [1].) Because present-day signal processing of continuous radio-
type signals using resistors, capacitors, operational amplifiers, etc., has noth-
ing to do with analogies, the term analog is actually a misnomer. The more
correct term is continuous signal processing for what is today so commonly
called analog signal processing. As such, in this book we’ll minimize the use
of the term analog signals and substitute the phrase continuous signals when-
ever appropriate.
The term discrete-time signal is used to describe a signal whose indepen-
dent time variable is quantized so that we know only the value of the signal
at discrete instants in time. Thus a discrete-time signal is not represented by a
continuous waveform but, instead, a sequence of values. In addition to quan-
tizing time, a discrete-time signal quantizes the signal amplitude. We can il-
lustrate this concept with an example. Think of a continuous sinewave with a
peak amplitude of 1 at a frequency f described by the equation
o
x(t) = sin(2πf t). (1–1)
o
The frequency f is measured in hertz (Hz). (In physical systems, we usually
o
measure frequency in units of hertz. One Hz is a single oscillation, or cycle,
per second. One kilohertz (kHz) is a thousand Hz, and a megahertz (MHz) is

1.1 Discrete Sequences and Their Notation 3
Continousx(t)
1
0.5
(a) 0
Continuous-time
variable, t
–0.5
–1
Discretex(n)
1 x(7) at time 7t s seconds
0.5
11 13 15 17 19 31 33 35 37 39
(b) 0
1 3 5 7 9 21 23 25 27 29 Discrete-time
index, n
–0.5
–1
ts
Discretex(n)
1
0.5
11 13 15 17 19 31 33 35 37 39
(c) 0
1 3 5 7 9 21 23 25 27 29 Discrete-time
index, n
–0.5
–1
Figure 1–1 A time-domain sinewave: (a) continuous waveform representa-
tion; (b) discrete sample representation; (c) discrete samples with
connecting lines.
one million Hz.†)Withtin Eq. 1–1 representing time in seconds, the f tfactor
o
has dimensions of cycles, and the complete 2πf tterm is an angle measured in
o
radians.
Plotting Eq. (1–1), we get the venerable continuous sinewave curve
shown in Figure 1–1(a).If our continuous sinewave represents a physical volt-
†The dimension for frequency used to be cycles/second; that’s why the tuning dials of old radios
indicate frequency as kilocycles/second (kcps) or megacycles/second (Mcps). In 1960 the scien-
tific community adopted hertz as the unit of measure for frequency in honor of the German
physicist Heinrich Hertz, who first demonstrated radio wave transmission and reception in
1887.

4 Discrete Sequences and Systems
age, we could sample it once every t seconds using an analog-to-digital con-
s
verter and represent the sinewave as a sequence of discrete values. Plotting
those individual values as dots would give us the discrete waveform in Fig-
ure 1–1(b). We say that Figure 1–1(b) is the “discrete-time” version of the con-
tinuous signal in Figure 1–1(a). The independent variable t in Eq. (1–1) and
Figure 1–1(a) is continuous. The independent indexvariablenin Figure 1–1(b)
is discrete and can have only integer values. That is, index n is used to iden-
tify the individual elements of the discrete sequence in Figure 1–1(b).
Do not be tempted to draw lines between the dots in Figure 1–1(b). For
some reason, people (particularly those engineers experienced in working
with continuous signals) want to connect the dots with straight lines, or the
stair-step lines shown in Figure 1–1(c). Don’t fall into this innocent-looking
trap. Connecting the dots can mislead the beginner into forgetting that the
x(n) sequence is nothing more than a list of numbers. Remember, x(n) is a
discrete-time sequence of individual values, and each value in that sequence
plots as a single dot. It’s not that we’re ignorant of what lies between the dots
ofx(n); there isnothing between those dots.
We can reinforce this discrete-time sequence concept by listing those Fig-
ure 1–1(b) sampled values as follows:
x(0) =0 (1st sequence value, index n= 0)
x(1) =0.31 (2nd sequence value, index n= 1)
x(2) = 0.59 (3rd sequence value, index n= 2)
x(3) = 0.81 (4th sequence value, index n= 3)
. . . . . .
and so on, (1–2)
where n represents the time index integer sequence 0, 1, 2, 3, etc., and t is
s
some constant time period between samples. Those sample values can be rep-
resented collectively, and concisely, by the discrete-time expression
x(n) = sin(2πf nt). (1–3)
o s
(Here again, the 2πf nt term is an angle measured in radians.) Notice that the
o s
indexnin Eq. (1–2) started with a value of 0, instead of 1. There’s nothing sa-
cred about this; the first value ofncould just as well have been 1, but we start
the index n at zero out of habit because doing so allows us to describe the
sinewave starting at time zero. The variablex(n) in Eq. (1–3) is read as “the se-
quencexof n.” Equations (1–1) and (1–3) describe what are also referred to as
time-domain signals because the independent variables, the continuous time t
in Eq. (1–1), and the discrete-time nt values used in Eq. (1–3) are measures of
s
time.
With this notion of a discrete-time signal in mind, let’s say that a discrete
system is a collection of hardware components, or software routines, that op-
erate on a discrete-time signal sequence. For example, a discrete system could

1.1 Discrete Sequences and Their Notation 5
be a process that gives us a discrete output sequence y(0), y(1), y(2), etc., when
a discrete input sequence ofx(0),x(1),x(2), etc., is applied to the system input
as shown in Figure 1–2(a). Again, to keep the notation concise and still keep
track of individual elements of the input and output sequences, an abbrevi-
ated notation is used as shown in Figure 1–2(b) wherenrepresents the integer
sequence 0, 1, 2, 3, etc. Thus,x(n) and y(n) are general variables that represent
two separate sequences of numbers. Figure 1–2(b) allows us to describe a sys-
tem’s output with a simple expression such as
y(n) = 2x(n) – 1. (1–4)
Illustrating Eq. (1–4), if x(n) is the five-element sequence x(0) = 1, x(1) = 3,
x(2) = 5,x(3) = 7, and x(4) = 9, then y(n) is the five-element sequence y(0) = 1,
y(1)=5, y(2)=9, y(3)=13, and y(4)=17.
Equation (1–4) is formally called a difference equation. (In this book we
won’t be working with differential equations or partial differential equations.
However, we will, now and then, work with partially difficult equations.)
The fundamental difference between the way time is represented in con-
tinuous and discrete systems leads to a very important difference in how we
characterize frequency in continuous and discrete systems. To illustrate, let’s
reconsider the continuous sinewave in Figure 1–1(a). If it represented a volt-
age at the end of a cable, we could measure its frequency by applying it to an
oscilloscope, a spectrum analyzer, or a frequency counter. We’d have a prob-
lem, however, if we were merely given the list of values from Eq. (1–2) and
asked to determine the frequency of the waveform they represent. We’d
graph those discrete values, and, sure enough, we’d recognize a single
sinewave as in Figure 1–1(b). We can say that the sinewave repeats every 20
samples, but there’s no way to determine the exact sinewave frequency from
the discrete sequence values alone. You can probably see the point we’re lead-
ing to here. If we knew the time between samples—the sample period t—
s
we’d be able to determine the absolute frequency of the discrete sinewave.
x(0),x(1),x(2),x(3), . . . Discrete y(0),y(1),y(2),y(3), . . .
(a) System
Discrete
(b) x(n) y(n)
System
Figure 1–2 With an input applied, a discrete system provides an output: (a) the
input and output are sequences of individual values; (b) input and
output using the abbreviated notation ofx(n) and y(n).

6 Discrete Sequences and Systems
Given that the t sample period is, say, 0.05 milliseconds/sample, the period
s
of the sinewave is
sinewave period=
20 samples⋅0.05 milliseconds
=1 millisecond. (1–5)
period sample
Because the frequency of a sinewave is the reciprocal of its period, we now know
that the sinewave’s absolute frequency is 1/(1 ms), or 1 kHz. On the other hand, if
we found that the sample period was, in fact, 2 milliseconds, the discrete samples
in Figure 1–1(b) would represent a sinewave whose period is 40 milliseconds and
whose frequency is 25 Hz. The point here is that when dealing with discrete
systems, absolute frequency determination in Hz is dependent on the sam-
pling frequency
f =1/t. (1–5’)
s s
We’ll be reminded of this dependence throughout the remainder of this book.
In digital signal processing, we often find it necessary to characterize the
frequency content of discrete time-domain signals. When we do so, this fre-
quency representation takes place in what’s called the frequency domain. By
x 1 (n) in the time domain
1 X 1 (m) amplitude in the
frequency domain
1
0.5
20 25 30 0.5
(a) 0
5 10 15 Time (n) 0
–0.5 0 fo 2fo 3fo 4fo 5fo Frequency
–1
x 2 (n) in the time domain 1 X 2 ( m ) a m p fr l e it q u u d e e n in cy t h d e omain
0.5
10 15 25 30 0.5
(b) 0 0.4
5 20 Time(n)
0
–0.5 0 f 2f 3f 4f 5f Frequency
o o o o o
x s u m (n) in the time domain
1.5 X s u m (m) amplitude in the
frequency domain
1
1
0.5 20 25 30 0.5
(c) 0
5 10 15 Time (n) 0
–0.5 0 fo 2fo 3fo 4fo 5fo Frequency
–1
–1.5
Figure 1–3 Time- and frequency-domain graphical representations: (a) sinewave
of frequency f; (b) reduced amplitude sinewave of frequency 2f;
o o
(c)sum of the two sinewaves.

1.1 Discrete Sequences and Their Notation 7
way of example, let’s say we have a discrete sinewave sequencex (n) with an
1
arbitrary frequency f Hz as shown on the left side of Figure 1–3(a). We can
o
also characterize x (n)by showing its spectral content, the X (m)sequence on
1 1
the right side of Figure 1-3(a), indicating that it has a single spectral compo-
nent, and no other frequency content. Although we won’t dwell on it just
now, notice that the frequency-domain representations in Figure 1–3 are
themselves discrete.
To illustrate our time- and frequency-domain representations further,
Figure 1–3(b) shows another discrete sinewave x (n), whose peak amplitude
2
is 0.4, with a frequency of 2f . The discrete sample values of x (n) are ex-
o 2
pressed by the equation
⋅
x (n) = 0.4 sin(2π2f nt). (1–6)
2 o s
When the two sinewaves, x (n) and x (n), are added to produce a new
1 2
waveformx (n), its time-domain equation is
sum
x (n) =x (n) +x (n) = sin(2πf nt) + 0.4 ⋅ sin(2π2f nt), (1–7)
sum 1 2 o s o s
and its time- and frequency-domain representations are those given in Figure
1–3(c). We interpret theX (m) frequency-domain depiction, the spectrum, in
sum
Figure 1–3(c) to indicate thatx (n) has a frequency component of f Hz and
sum o
a reduced-amplitude frequency component of 2f Hz.
o
Notice three things in Figure 1–3. First, time sequences use lowercase
variable names like the “x” in x (n), and uppercase symbols for frequency-
1
domain variables such as the “X” inX (m). The termX (m) is read as “the spec-
1 1
tral sequence X sub one of m.” Second, because the X (m) frequency-domain
1
representation of the x (n) time sequence is itself a sequence (a list of num-
1
bers), we use the index “m” to keep track of individual elements inX (m). We
1
can list frequency-domain sequences just as we did with the time sequence in
Eq. (1–2). For example,X (m) is listed as
sum
X (0) =0 (1stX (m) value, index m= 0)
sum sum
X (1) = 1.0 (2ndX (m) value, index m= 1)
sum sum
X (2) = 0.4 (3rdX (m) value, index m= 2)
sum sum
X (3) = 0 (4thX (m) value, index m= 3)
sum sum
. . . . . .
and so on,
where the frequency index m is the integer sequence 0, 1, 2, 3, etc. Third, be-
cause thex (n) +x (n) sinewaves have a phase shift of zero degrees relative to
1 2
each other, we didn’t really need to bother depicting this phase relationship
in X (m) in Figure 1–3(c). In general, however, phase relationships in
sum
frequency-domain sequences are important, and we’ll cover that subject in
Chapters 3 and 5.

8 Discrete Sequences and Systems
Akey point to keep in mind here is that we now know three equivalent
ways to describe a discrete-time waveform. Mathematically, we can use a
time-domain equation like Eq. (1–6). We can also represent a time-domain
waveform graphically as we did on the left side of Figure 1–3, and we can de-
pict its corresponding, discrete, frequency-domain equivalent as that on the
right side of Figure 1–3.
As it turns out, the discrete time-domain signals we’re concerned with
are not only quantized in time; their amplitude values are also quantized. Be-
cause we represent all digital quantities with binary numbers, there’s a limit
to the resolution, or granularity, that we have in representing the values of
discrete numbers. Although signal amplitude quantization can be an impor-
tant consideration—we cover that particular topic in Chapter 12—we won’t
worry about it just now.
1.2 SIGNAL AMPLITUDE, MAGNITUDE, POWER
Let’s define two important terms that we’ll be using throughout this book:
amplitude and magnitude. It’s not surprising that, to the layman, these terms
are typically used interchangeably. When we check our thesaurus, we find
that they are synonymous.†In engineering, however, they mean two different
things, and we must keep that difference clear in our discussions. The ampli-
tude of a variable is the measure of how far, and in what direction, that vari-
able differs from zero. Thus, signal amplitudes can be either positive or
negative. The time-domain sequences in Figure 1–3 presented the sample
value amplitudes of three different waveforms. Notice how some of the indi-
vidual discrete amplitude values were positive and others were negative.
|x (n)|
1
1
0.5
0
5 10 15 20 25 30 Time (n)
–0.5
Figure 1–4 Magnitude samples, |x(n)|, of the time waveform in Figure 1–3(a).
1
†Of course, laymen are “other people.” To the engineer, the brain surgeon is the layman. To the
brain surgeon, the engineer is the layman.

1.2 Signal Amplitude, Magnitude, Power 9
X s u m (m)amplitudein the X s u m (m)power in the
frequency domain frequency domain
1 1
0.5 0.5
0.4
0.16
0 0
0 f 2f 3f 4f 5f Frequency 0 f 2f 3f 4f 5f Frequency
o o o o o o o o o o
Figure 1–5 Frequency-domain amplitude and frequency-domain power of the
x (n) time waveform in Figure 1–3(c).
sum
The magnitude of a variable, on the other hand, is the measure of how
far, regardless of direction, its quantity differs from zero. So magnitudes are
always positive values. Figure 1–4 illustrates how the magnitude of the x (n)
1
time sequence in Figure 1–3(a) is equal to the amplitude, but with the sign al-
ways being positive for the magnitude. We use the modulus symbol (||) to
represent the magnitude of x (n). Occasionally, in the literature of digital sig-
1
nal processing, we’ll find the term magnitudereferred to as the absolute value.
When we examine signals in the frequency domain, we’ll often be inter-
ested in the power level of those signals. The power of a signal is proportional
to its amplitude (or magnitude) squared. If we assume that the proportional-
ity constant is one, we can express the power of a sequence in the time or fre-
quency domains as
x (n)=|x(n)|2, (1–8)
pwr
or
X (m)=|X(m)|2. (1–8’)
pwr
Very often we’ll want to know the difference in power levels of two signals in
the frequency domain. Because of the squared nature of power, two signals
with moderately different amplitudes will have a much larger difference in
their relative powers. In Figure 1–3, for example, signal x (n)’s amplitude is
1
2.5 times the amplitude of signal x (n), but its power level is 6.25 that of
2
x (n)’s power level. This is illustrated in Figure 1–5 where both the amplitude
2
and power ofX (m) are shown.
sum
Because of their squared nature, plots of power values often involve
showing both very large and very small values on the same graph. To make
these plots easier to generate and evaluate, practitioners usually employ the
decibel scale as described in Appendix E.

10 Discrete Sequences and Systems
1.3 SIGNAL PROCESSING OPERATIONAL SYMBOLS
We’ll be using block diagrams to graphically depict the way digital signal
processing operations are implemented. Those block diagrams will comprise
an assortment of fundamental processing symbols, the most common of
which are illustrated and mathematically defined in Figure 1–6.
Figure 1–6(a) shows the addition, element for element, of two discrete
sequences to provide a new sequence. If our sequence indexnbegins at 0, we
say that the first output sequence value is equal to the sum of the first element
of the b sequence and the first element of the c sequence, or a(0) = b(0) + c(0).
Likewise, the second output sequence value is equal to the sum of the second
Addition:
b(n) + a(n) a(n) = b(n) + c(n)
(a)
c(n)
Subtraction:
+
b(n) + a(n) a(n) = b(n) – c(n)
(b) –
c(n)
b(n) Summation:
n+3
b(n+1) a(n) = b(k)
(c) + a(n)
b(n+2) k = n
= b(n) + b(n+1) + b(n+2) + b(n+3)
b(n+3)
Multiplication:
b(n) a(n) .
a(n) = b(n)c(n) = b(n) c(n)
(d) [Sometimes we use a " . "
c(n) to signify multiplication.]
Unit delay:
b(n) Delay a(n)
a(n) = b(n-1)
(e)
b(n) z-1 a(n)
Figure 1–6 Terminology and symbols used in digital signal processing block
diagrams.

1.3 Signal Processing Operational Symbols 11
element of the b sequence and the second element of the c sequence, or
a(1) = b(1) + c(1). Equation (1–7) is an example of adding two sequences. The
subtraction process in Figure 1–6(b) generates an output sequence that’s the
element-for-element difference of the two input sequences. There are times
when we must calculate a sequence whose elements are the sum of more than
two values. This operation, illustrated in Figure 1–6(c), is called summation
and is very common in digital signal processing. Notice how the lower and
upper limits of the summation index k in the expression in Figure 1–6(c) tell
us exactly which elements of the b sequence to sum to obtain a given a(n)
value. Because we’ll encounter summation operations so often, let’s make
sure we understand their notation. If we repeat the summation equation from
Figure 1–6(c) here, we have
∑n+3
a(n)= b(k). (1–9)
k=n
This means that
whenn=0, indexkgoes from 0 to 3, so a(0) = b(0) + b(1) + b(2) + b(3)
whenn=1, indexkgoes from 1 to 4, so a(1) = b(1) + b(2) + b(3) + b(4)
whenn=2, indexkgoes from 2 to 5, so a(2) = b(2) + b(3) + b(4) + b(5) (1–10)
whenn=3, indexkgoes from 3 to 6, so a(3) = b(3) + b(4) + b(5) + b(6)
. . . . . .
and so on.
We’ll begin using summation operations in earnest when we discuss digital
filters in Chapter 5.
The multiplication of two sequences is symbolized in Figure 1–6(d).
Multiplication generates an output sequence that’s the element-for-element
product of two input sequences: a(0) = b(0)c(0), a(1) = b(1)c(1), and so on. The
last fundamental operation that we’ll be using is called the unit delayin Figure
1–6(e). While we don’t need to appreciate its importance at this point, we’ll
merely state that the unit delay symbol signifies an operation where the out-
put sequence a(n) is equal to a delayed version of the b(n) sequence. For ex-
ample, a(5)=b(4), a(6)=b(5), a(7)=b(6), etc. As we’ll see in Chapter 6, due to
the mathematical techniques used to analyze digital filters, the unit delay is
very often depicted using the term z–1.
The symbols in Figure 1–6 remind us of two important aspects of digital
signal processing. First, our processing operations are always performed on
sequences of individual discrete values, and second, the elementary opera-
tions themselves are very simple. It’s interesting that, regardless of how com-
plicated they appear to be, the vast majority of digital signal processing
algorithms can be performed using combinations of these simple operations.
If we think of a digital signal processing algorithm as a recipe, then the sym-
bols in Figure 1–6 are the ingredients.

12 Discrete Sequences and Systems
1.4 INTRODUCTION TO DISCRETE LINEAR TIME-INVARIANT SYSTEMS
In keeping with tradition, we’ll introduce the subject of linear time-invariant
(LTI) systems at this early point in our text. Although an appreciation for LTI
systems is not essential in studying the next three chapters of this book, when
we begin exploring digital filters, we’ll build on the strict definitions of linear-
ity and time invariance. We need to recognize and understand the notions of
linearity and time invariance not just because the vast majority of discrete
systems used in practice are LTI systems, but because LTI systems are very ac-
commodating when it comes to their analysis. That’s good news for us be-
cause we can use straightforward methods to predict the performance of any
digital signal processing scheme as long as it’s linear and time invariant. Be-
cause linearity and time invariance are two important system characteristics
having very special properties, we’ll discuss them now.
1.5 DISCRETE LINEAR SYSTEMS
The term linear defines a special class of systems where the output is the su-
perposition, or sum, of the individual outputs had the individual inputs been
applied separately to the system. For example, we can say that the application
of an inputx (n) to a system results in an output y (n). We symbolize this situ-
1 1
ation with the following expression:
x (n)
⎯r⎯esu⎯lts⎯ ⎯in→
y (n). (1–11)
1 1
Given a different inputx (n), the system has a y (n) output as
2 2
x (n) ⎯r⎯esu⎯lts⎯ in→ y (n). (1–12)
2 2
For the system to be linear, when its input is the sum x (n) + x (n), its output
1 2
must be the sum of the individual outputs so that
x (n)+x (n) ⎯r⎯esu⎯lts⎯ in→ y (n)+y (n). (1–13)
1 2 1 2
One way to paraphrase expression (1–13) is to state that a linear system’s out-
put is the sum of the outputs of its parts. Also, part of this description of lin-
earity is a proportionality characteristic. This means that if the inputs are
scaled by constant factors c and c , then the output sequence parts are also
1 2
scaled by those factors as
c x (n)+c x (n) ⎯r⎯esu⎯lts⎯ in→ c y (n)+c y (n). (1–14)
1 1 2 2 1 1 2 2
In the literature, this proportionality attribute of linear systems in expression
(1–14) is sometimes called the homogeneity property. With these thoughts in
mind, then, let’s demonstrate the concept of system linearity.

1.5 Discrete Linear Systems 13
1.5.1 Example of a Linear System
To illustrate system linearity, let’s say we have the discrete system shown in
Figure 1–7(a) whose output is defined as
−x(n)
y(n)= , (1–15)
2
that is, the output sequence is equal to the negative of the input sequence
with the amplitude reduced by a factor of two. If we apply an x (n) input se-
1
quence representing a 1 Hz sinewave sampled at a rate of 32 samples per
cycle, we’ll have a y (n) output as shown in the center of Figure 1–7(b). The
1
frequency-domain spectral amplitude of the y (n) output is the plot on the
1
Linear
(a) Inputx(n) Discrete Outputy(n) = –x(n)/2
System
1 x 1 (n) 1 y 1 (n) Y 1 (m)
0.5
0.5 0.5
1
(b) 0 0 0
Time Time 0 2 4 6 8 10 12 14 Freq
–0.5 –0.5 (Hz)
–0.5
–1 –1
x 2 (n) y 2 (n) Y (m)
1 1 2
0.5
0.5 0.5
3
(c) 0 0 0
Time Time 0 2 4 6 8 10 12 14 Freq
–0.5 –0.5 (Hz)
–0.5
–1 –1
x (n) = x (n) + x (n) y (n)
2
3 1 2
2
3 Y3 (m)
0.5
1 1
1 3
(d) 0 0 0
Time Time 0 2 4 6 8 10 12 14 Freq
–1 –1 (Hz)
–0.5
–2 –2
Figure 1–7 Linear system input-to-output relationships: (a) system block diagram
where y(n) = –x(n)/2; (b) system input and output with a 1 Hz
sinewave applied; (c) with a 3 Hz sinewave applied; (d) with the sum
of 1 Hz and 3 Hz sinewaves applied.

14 Discrete Sequences and Systems
right side of Figure 1–7(b), indicating that the output comprises a single tone
of peak amplitude equal to –0.5 whose frequency is 1 Hz. Next, applying an
x (n) input sequence representing a 3 Hz sinewave, the system provides a
2
y (n) output sequence, as shown in the center of Figure 1–7(c). The spectrum
2
of the y (n) output, Y (m), confirming a single 3 Hz sinewave output is shown
2 2
on the right side of Figure 1–7(c). Finally—here’s where the linearity comes
in—if we apply an x (n) input sequence that’s the sum of a 1 Hz sinewave
3
and a 3 Hz sinewave, the y (n) output is as shown in the center of Figure
3
1–7(d). Notice how y (n) is the sample-for-sample sum of y (n) and y (n). Fig-
3 1 2
ure 1–7(d) also shows that the output spectrum Y (m) is the sum of Y (m) and
3 1
Y (m). That’s linearity.
2
1.5.2 Example of a Nonlinear System
It’s easy to demonstrate how a nonlinear system yields an output that is not
equal to the sum of y (n) and y (n) when its input isx (n)+x (n). Asimple ex-
1 2 1 2
ample of a nonlinear discrete system is that in Figure 1–8(a) where the output
is the square of the input described by
y(n) = [x(n)]2. (1–16)
We’ll use a well-known trigonometric identity and a little algebra to predict
the output of this nonlinear system when the input comprises simple
sinewaves. Following the form of Eq. (1–3), let’s describe a sinusoidal se-
quence, whose frequency f =1 Hz, by
o
⋅ ⋅
x (n) = sin(2πf nt) = sin(2π 1 nt). (1–17)
1 o s s
Equation (1–17) describes the x (n) sequence on the left side of Figure 1–8(b).
1
Given this x (n) input sequence, the y (n) output of the nonlinear system is
1 1
the square of a 1 Hz sinewave, or
⋅ ⋅ ⋅ ⋅ ⋅
y (n) = [x (n)]2= sin(2π 1 nt) sin(2π 1 nt). (1–18)
1 1 s s
We can simplify our expression for y (n) in Eq. (1–18) by using the following
1
trigonometric identity:
sin(α) ⋅ sin(β)=
cos(α−β)
−
cos(α+β)
. (1–19)
2 2
Using Eq. (1–19), we can express y (n) as
1
⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
cos(2π 1 nt −2π 1 nt ) cos(2π 1 nt +2π 1 nt )
y (n)= s s − s s
1 2 2
(1–20)
⋅ ⋅ ⋅ ⋅
cos(0) cos(4π 1 nt ) 1 cos(2π 2 nt )
= − s = − s ,
2 2 2 2

1.5 Discrete Linear Systems 15
Nonlinear
(a) Inputx(n) Discrete Outputy(n) = [x(n)]2
System
x (n)
1 1 1 y 1 (n) Y 1 (m)
0.5 0.8 1 zero Hz component
0.6 0.5 2
(b) –0.5 0 Time 0 0 . . 2 4 –0.5 0 0 4 6 8 10 12 14 F (H re z q )
–1 0 Time –1
x (n)
1 2 1 y 2 (n) Y 2 (m)
0.5 0.8 1
(c) –0.5 0 Time 0 0 0 . . . 2 4 6 –0 0 . . 5 0 5 0 2 4 6 8 10 12 14 F (H re z q )
0 –1
–1 Time
x (n)
2 3 2.5 y 3 (n) Y 3 (m)
1
1 2
1.5 0.5 4 6
(d) – 0 1 Time 0.5 1 –0. 0 5 0 2 8 10 12 14 F (H re z q )
0 –1
–2 Time
Figure 1–8 Nonlinear system input-to-output relationships: (a) system block dia-
gram where y(n) = [x(n)]2; (b) system input and output with a 1 Hz
sinewave applied; (c) with a 3 Hz sinewave applied; (d) with the sum
of 1 Hz and 3 Hz sinewaves applied.
which is shown as the all-positive sequence in the center of Figure 1–8(b). Be-
cause Eq. (1–19) results in a frequency sum (α + β) and frequency difference
(α – β) effect when multiplying two sinusoids, the y (n) output sequence will
1
be a cosine wave of 2 Hz and a peak amplitude of –0.5, added to a constant
value of 1/2. The constant value of 1/2 in Eq. (1–20) is interpreted as a zero
Hz frequency component, as shown in the Y (m) spectrum in Figure 1–8(b).
1
We could go through the same algebraic exercise to determine that when a
3 Hz sinewave x (n) sequence is applied to this nonlinear system, the output
2
y (n) would contain a zero Hz component and a 6 Hz component, as shown
2
in Figure 1–8(c).
System nonlinearity is evident if we apply anx (n) sequence comprising
3
the sum of a 1 Hz and a 3 Hz sinewave as shown in Figure 1–8(d). We can

16 Discrete Sequences and Systems
predict the frequency content of the y (n) output sequence by using the alge-
3
braic relationship
(a+b)2=a2+2ab+b2, (1–21)
where a and b represent the 1 Hz and 3 Hz sinewaves, respectively. From Eq.
(1–19), the a2 term in Eq. (1–21) generates the zero Hz and 2 Hz output sinu-
soids in Figure 1–8(b). Likewise, the b2term produces in y (n) another zero Hz
3
and the 6 Hz sinusoid in Figure 1–8(c). However, the 2ab term yields addi-
tional 2 Hz and 4 Hz sinusoids in y (n). We can show this algebraically by
3
using Eq. (1–19) and expressing the 2abterm in Eq. (1–21) as
⋅ ⋅ ⋅ ⋅ ⋅ ⋅
2ab=2 sin(2π 1 nt ) sin(2π 3 nt )
s s
2cos(2π⋅
1
⋅
nt
−2π⋅
3
⋅
nt )
2cos(2π⋅
1
⋅
nt
+2π⋅
3
⋅
nt )
= s s − s s (1–22)
2 2
⋅ ⋅ ⋅ ⋅
=cos(2π 2 nt )−cos(2π 4 nt ). †
s s
Equation (1–22) tells us that two additional sinusoidal components will be
present in y (n) because of the system’s nonlinearity, a 2 Hz cosine wave
3
whose amplitude is +1 and a 4 Hz cosine wave having an amplitude of –1.
These spectral components are illustrated in Y (m) on the right side of Figure
3
1–8(d).
Notice that when the sum of the two sinewaves is applied to the nonlin-
ear system, the output contained sinusoids, Eq. (1–22), that were not present
in either of the outputs when the individual sinewaves alone were applied.
Those extra sinusoids were generated by an interaction of the two input sinu-
soids due to the squaring operation. That’s nonlinearity; expression (1–13)
was not satisfied. (Electrical engineers recognize this effect of internally gen-
erated sinusoids as intermodulation distortion.) Although nonlinear systems are
usually difficult to analyze, they are occasionally used in practice. References
[2], [3], and [4], for example, describe their application in nonlinear digital fil-
ters.Again, expressions (1–13) and (1–14) state that a linear system’s output
resulting from a sum of individual inputs is the superposition (sum) of the in-
dividual outputs. They also stipulate that the output sequence y (n) depends
1
only on x (n) combined with the system characteristics, and not on the other
1
input x (n); i.e., there’s no interaction between inputs x (n) and x (n) at the
2 1 2
output of a linear system.
⋅ ⋅ ⋅ ⋅ ⋅
†The first term in Eq. (1–22) is cos(2π nt – 6π nt) = cos(–4π nt) = cos(–2π 2 nt). However, be-
s s s s ⋅ ⋅
cause the cosine function is even, cos(–α)=cos(α), we can express that first term as cos(2π 2 nt).
s

1.6 Time-Invariant Systems 17
1.6 TIME-INVARIANT SYSTEMS
Atime-invariant system is one where a time delay (or shift) in the input se-
quence causes an equivalent time delay in the system’s output sequence.
Keeping in mind that n is just an indexing variable we use to keep track of
our input and output samples, let’s say a system provides an output y(n)
given an input ofx(n), or
x(n) ⎯r⎯esu⎯lts⎯ in→ y(n). (1–23)
For a system to be time invariant, with a shifted version of the original x(n)
input applied,x’(n), the following applies:
x'(n)=x(n+k) ⎯r⎯esu⎯lts⎯ in→ y'(n)=y(n+k), (1–24)
where k is some integer representing k sample period time delays. For a sys-
tem to be time invariant, Eq. (1–24) must hold true for any integer value of k
and any input sequence.
1.6.1 Example of a Time-Invariant System
Let’s look at a simple example of time invariance illustrated in Figure 1–9. As-
sume that our initialx(n) input is a unity-amplitude 1 Hz sinewave sequence
with a y(n) output, as shown in Figure 1–9(b). Consider a different input se-
quencex’(n), where
x’(n) =x(n–4). (1–25)
Equation (1–25) tells us that the input sequence x’(n) is equal to sequence x(n)
shifted to the right by k= –4 samples. That is, x’(4) = x(0), x’(5) = x(1), x’(6) =
x(2), and so on as shown in Figure 1–9(c). The discrete system is time invari-
ant because the y’(n) output sequence is equal to the y(n) sequence shifted to
the right by four samples, or y’(n) = y(n–4). We can see that y’(4) = y(0), y’(5) =
y(1), y’(6) = y(2), and so on as shown in Figure 1–9(c). For time-invariant sys-
tems, the time shifts in x’(n) and y’(n) are equal. Take careful notice of the
minus sign in Eq. (1–25). In later chapters, that is the notation we’ll use to al-
gebraically describe a time-delayed discrete sequence.
Some authors succumb to the urge to define a time-invariant system as
one whose parameters do not change with time. That definition is incomplete
and can get us in trouble if we’re not careful. We’ll just stick with the formal
definition that a time-invariant system is one where a time shift in an input se-
quence results in an equal time shift in the output sequence. By the way, time-
invariant systems in the literature are often called shift-invariantsystems.†
†An example of a discrete process that’s not time invariant is the downsampling, or decimation,
process described in Chapter 10.

18 Discrete Sequences and Systems
Linear
Time-Invariant
(a) Inputx(n) Output y(n) = –x(n)/2
Discrete
System
x(n) y(n)
1 0.5
4 6 8 10 2 12 14
(b) 0 0
0 2 12 14 4 6 8 10
–1 –0.5
Time Time
x'(n)
y'(n)
1 0.5
8 10 12 14 4 6
(c) 0 0
0 2 4 6 0 2 8 10 12 14
–1 –0.5 Time
Time
Figure 1–9 Time-invariant system input/output relationships: (a) system block dia-
gram, y(n) = –x(n)/2; (b) system input/output with a sinewave input;
(c)input/outputwhenasinewave,delayedbyfoursamples,istheinput.
1.7 THE COMMUTATIVE PROPERTY OF LINEAR TIME-INVARIANT SYSTEMS
Although we don’t substantiate this fact until we reach Section 6.11, it’s not
too early to realize that LTI systems have a useful commutative property by
which their sequential order can be rearranged with no change in their final
output. This situation is shown in Figure 1–10 where two different LTI
systems are configured in series. Swapping the order of two cascaded systems
does not alter the final output. Although the intermediate data sequences f(n)
and g(n) will usually not be equal, the two pairs of LTI systems will have iden-
Inputx(n) LTI f(n) LTI Output y(n)
(a) System #1 System #2
Inputx(n) LTI g(n) LTI Output y(n)
(b) System #2 System #1
Figure 1–10 Linear time-invariant (LTI) systems in series: (a) block diagram of two
LTI systems; (b) swapping the order of the two systems does not
change the resultant output y(n).

1.8 Analyzing Linear Time-Invariant Systems 19
tical y(n) output sequences. This commutative characteristic comes in handy
for designers of digital filters, as we’ll see in Chapters 5 and 6.
1.8 ANALYZING LINEAR TIME-INVARIANT SYSTEMS
As previously stated, LTI systems can be analyzed to predict their perfor-
mance. Specifically, if we know the unit impulse responseof an LTI system, we
can calculate everything there is to know about the system; that is, the sys-
tem’s unit impulse response completely characterizes the system. By “unit
impulse response” we mean the system’s time-domain output sequence
when the input is a single unity-valued sample (unit impulse) preceded and
followed by zero-valued samples as shown in Figure 1–11(b).
Knowing the (unit) impulse response of an LTI system, we can deter-
mine the system’s output sequence for any input sequence because the out-
put is equal to the convolutionof the input sequence and the system’s impulse
response. Moreover, given an LTI system’s time-domain impulse response,
we can find the system’s frequency responseby taking the Fourier transform in
the form of a discrete Fourier transform of that impulse response[5]. The con-
cepts in the two previous sentences are among the most important principles
in all of digital signal processing!
Don’t be alarmed if you’re not exactly sure what is meant by convolu-
tion, frequency response, or the discrete Fourier transform. We’ll introduce
these subjects and define them slowly and carefully as we need them in
later chapters. The point to keep in mind here is that LTI systems can be de-
signed and analyzed using a number of straightforward and powerful
analysis techniques. These techniques will become tools that we’ll add to
Linear
Time-Invariant
(a) Inputx(n) Output y(n)
Discrete
System
x(n) impulse input y(n) impulse response
1
unity-valued sample
(b)
0 Time 0 Time
Figure 1–11 LTI system unit impulse response sequences: (a) system block dia-
gram; (b) impulse input sequencex(n) and impulse response output
sequencey(n).

20 Discrete Sequences and Systems
our signal processing toolboxes as we journey through the subject of digital
signal processing.
In the testing (analyzing) of continuous linear systems, engineers often
use a narrow-in-time impulsive signal as an input signal to their systems. Me-
chanical engineers give their systems a little whack with a hammer, and elec-
trical engineers working with analog-voltage systems generate a very narrow
voltage spike as an impulsive input. Audio engineers, who need an impulsive
acoustic test signal, sometimes generate an audio impulse by firing a starter
pistol.
In the world of DSP, an impulse sequence called a unit impulse takes the
form
x(n) = . . . 0, 0, 0, 0, 0, A, 0, 0, 0, 0, 0, . . . (1–26)
The value A is often set equal to one. The leading sequence of zero-valued
samples, before the A-valued sample, must be a bit longer than the length of
the transient response of the system under test in order to initialize the sys-
tem to its zero state. The trailing sequence of zero-valued samples, following
the A-valued sample, must be a bit longer than the transient response of the
system under test in order to capture the system’s entire y(n) impulse re-
sponse output sequence.
Let’s further explore this notion of impulse response testing to deter-
mine the frequency response of a discrete system (and take an opportunity to
start using the operational symbols introduced in Section 1.3). Consider the
block diagram of a 4-point moving averager shown in Figure 1–12(a). As the
x(n) input samples march their way through the system, at each time index n
four successive input samples are averaged to compute a single y(n) output.
As we’ll learn in subsequent chapters, a moving averagerbehaves like a digital
lowpass filter. However, we can quickly illustrate that fact now.
If we apply an impulse input sequence to the system, we’ll obtain its
y(n) impulse response output shown in Figure 1–12(b). The y(n) output is
computed using the following difference equation:
1 1 ∑n
y(n)= [x(n)+x(n−1)+x(n−2)+ x(n−3)]= x(k). (1–27)
4 4
k=n−3
If we then perform a discrete Fourier transform (a process we cover in much
detail in Chapter 3) on y(n), we obtain the Y(m) frequency-domain informa-
tion, allowing us to plot the frequency magnitude response of the 4-point
moving averager as shown in Figure 1–12(c). So we see that a moving aver-
ager indeed has the characteristic of a lowpass filter. That is, the averager at-
tenuates (reduces the amplitude of) high-frequency signal content applied to
its input.

1.8 Analyzing Linear Time-Invariant Systems 21
x(n) x(n-1) x(n-2) x(n-3)
Delay Delay Delay
(a)
y(n)
1/4
x(n) impulse input y(n) impulse response
1 1/4
unity-valued sample
(b)
0 n (Time) 0 n (Time)
Discrete Fourier
transform
|Y(m)|
1
Frequency
0.75
magnitude
(c) 0.5 response
0.25
0
0 3 6 9 12 15 18 m (Freq)
Figure 1–12 Analyzing a moving averager: (a) averager block diagram; (b)
impulse input and impulse response; (c) averager frequency mag-
nitude response.
OK, this concludes our brief introduction to discrete sequences and sys-
tems. In later chapters we’ll learn the details of discrete Fourier transforms,
discrete system impulse responses, and digital filters.
REFERENCES
[1] Karplus, W. J., and Soroka, W. W. Analog Methods, 2nd ed., McGraw-Hill, New York, 1959,
p. 117.
[2] Mikami, N., Kobayashi, M., and Yokoyama, Y. “ANew DSP-Oriented Algorithm for Cal-
culation of the Square Root Using a Nonlinear Digital Filter,” IEEE Trans. on Signal Process-
ing, Vol. 40, No. 7, July 1992.

22 Discrete Sequences and Systems
[3] Heinen, P., and Neuvo, Y. “FIR-Median Hybrid Filters,” IEEE Trans. on Acoust. Speech, and
Signal Proc., Vol. ASSP-35, No. 6, June 1987.
[4] Oppenheim, A., Schafer, R., and Stockham, T. “Nonlinear Filtering of Multiplied and Con-
volved Signals,” Proc. IEEE, Vol. 56, August 1968.
[5] Pickerd, John. “Impulse-Response Testing Lets a Single Test Do the Work of Thousands,”
EDN, April 27, 1995.

Chapter 1 Problems 23
CHAPTER 1 PROBLEMS
1.1 This problem gives us practice in thinking about sequences of numbers. For
centuries mathematicians have developed clever ways of computing π. In
1671 the Scottish mathematician James Gregory proposed the following very
simple series for calculating π:
⎛ 1 1 1 1 1 ⎞
π≈4⋅ 1− + − + − ... .
⎝ ⎠
3 5 7 9 11
Thinking of the terms inside the parentheses as a sequence indexed by the
variable n, where n=0, 1, 2, 3, . . ., 100, write Gregory’s algorithm in the form
π ≈
4⋅∑100
(−1 )?⋅?
n=0
replacing the “?” characters with expressions in terms of index n.
1.2 One of the ways to obtain discrete sequences, for follow-on processing, is to
digitize a continuous (analog) signal with an analog-to-digital (A/D) con-
verter. A 6-bit A/D converter’s output words (6-bit binary words) can only
represent 26=64 different numbers. (We cover this digitization, sampling, and
A/D converters in detail in upcoming chapters.) Thus we say the A/D con-
verter’s “digital” output can only represent a finite number of amplitude val-
ues. Can you think of a continuous time-domain electrical signal that only has
a finite number of amplitude values? If so, draw a graph of that continuous-
time signal.
1.3 On the Internet, the author once encountered the following line of
C-language code
PI = 2*asin(1.0);
whose purpose was to define the constant π. In standard mathematical nota-
tion, that line of code can be described by
π= 2 · sin–1(1).
Under what assumption does the above expression correctly define the con-
stant π?

24 Discrete Sequences and Systems
1.4 Many times in the literature of signal processing you will encounter the identity
x0= 1.
That is, x raised to the zero power is equal to one. Using the Laws of Expo-
nents, prove the above expression to be true.
1.5 Recall that for discrete sequences the t sample period (the time period be-
s
tween samples) is the reciprocal of the sample frequency f. Write the equa-
s
tions, as we did in the text’s Eq. (1–3), describing time-domain sequences for
unity-amplitude cosine waves whose f frequencies are
o
(a) f = f/2, one-half the sample rate,
o s
(b) f = f/4, one-fourth the sample rate,
o s
(c) f = 0 (zero) Hz.
o
1.6 Draw the three time-domain cosine wave sequences, where a sample value is
represented by a dot, described in Problem 1.5. The correct solution to Part (a)
of this problem is a useful sequence used to convert some lowpass digital fil-
ters into highpass filters. (Chapter 5 discusses that topic.) The correct solution
to Part (b) of this problem is an important discrete sequence used for frequency
translation (both for signal down-conversion and up-conversion) in modern-day
wireless communications systems. The correct solution to Part (c) of this
problem should convince us that it’s perfectly valid to describe a cosine se-
quence whose frequency is zero Hz.
1.7 Draw the three time-domain sequences of unity-amplitude sinewaves (not
cosine waves) whose frequencies are
(a) f = f/2, one-half the sample rate,
o s
(b) f = f/4, one-fourth the sample rate,
o s
(c) f = 0 (zero) Hz.
o
The correct solutions to Parts (a) and (c) show us that the two frequencies, 0
Hz and f/2 Hz, are special frequencies in the world of discrete signal pro-
s
cessing. What is special about the sinewave sequences obtained from the
above Parts (a) and (c)?
1.8 Consider the infinite-length time-domain sequence x(n) in Figure P1–8. Draw
the first eight samples of a shifted time sequence defined by
x (n) = x(n+1).
shift

Chapter 1 Problems 25
x(n)
1
. . .
2 6 7
0
0 1 3 4 5 n
Figure P1–8
1.9 Assume, during your reading of the literature of DSP, you encounter the
process shown in Figure P1–9. The x(n) input sequence, whose f sample rate
s
is 2500 Hz, is multiplied by a sinusoidal m(n) sequence to produce the y(n)
output sequence. What is the frequency, measured in Hz, of the sinusoidal
m(n) sequence?
x(n) y(n)
m(n) = sin(0.8 n)
f = 2500 Hz
s
Figure P1–9
1.10 There is a process in DSPcalled an “N-point running sum” (a kind of digital
lowpass filter, actually) that is described by the following equation:
N−1
y(n)= ∑ x(n− p).
p=0
Write out, giving the indices of all the x() terms, the algebraic expression that
describes the computations needed to compute y(9) when N=6.
1.11 A5-point moving averager can be described by the following difference equa-
tion:
n
1 1 ∑
y(n)= [x(n)+x(n−1)+x(n−2)+x(n−3)+x(n−4)]= x(k). (P1–1)
5 5 k=n−4
The averager’s signal-flow block diagram is shown in Figure P1–11, where
the x(n) input samples flow through the averager from left to right.

26 Discrete Sequences and Systems
x(n) x(n-1) x(n-2) x(n-3) x(n-4)
Delay Delay Delay Delay
y(n)
1/5
Figure P1–11
Equation (P1–1) is equivalent to
(P1–2)
x(n) x(n−1) x(n−2) x(n−3) x(n−4)
y(n)= + + + +
5 5 5 5 5
n
x(k)
= ∑ .
k=n−4 5
(a) Draw the block diagram of the discrete system described by Eq. (P1–2).
(b) The moving average processes described by Eqs. (P1–1) and (P1–2) have
identical impulse responses. Draw that impulse response.
(c) If you had to implement (using programmable hardware or assembling
discrete hardware components) either Eq. (P1–1) or Eq. (P1–2), which
would you choose? Explain why.
1.12 In this book we will look at many two-dimensional drawings showing the value
of one variable (y) plotted as a function of another variable (x). Stated in different
words, we’ll graphically display what are the values of a yaxis variable for vari-
ous values of an xaxis variable. For example, Figure P1–12(a) plots the weight of
a male child as a function of the child’s age. The dimension of the xaxis is years
70
60
g) 50
K 40
(a) ht ( 30
g
ei 20
W
2 4 6 8 10 12 14 16 18
Age (years)
(b) xi s
a
y
x axis
Figure P1–12

Chapter 1 Problems 27
and the dimension of the y axis is kilograms. What are the dimensions of the x
and yaxes of the familiar two-dimensional plot given in Figure P1–12(b)?
1.13 Let’s say you are writing software code to generate an x(n) test sequence com-
posed of the sum of two equal-amplitude discrete cosine waves, as
x(n) = cos(2πf nt +φ) + cos(2πf nt)
o s o s
where t is the time between your x(n) samples, and φis a constant phase shift
s
measured in radians. An example x(n) when φ=π/2 is shown in Figure P1–13
where the x(n) sequence, represented by the circular dots, is a single sinusoid
whose frequency is f Hz.
o
x(n) sequence
cos(2 fnt)
o s
sequence
1
0
cos(2 fnt + /2)
o s
sequence
0 10 20 30
n
Figure P1–13
Using the trigonometric identity cos(α+β) + cos(α–β) = 2cos(α)cos(β), derive
an equation for x(n) that is of the form
x(n) = 2cos(α)cos(β)
where variables αand βare in terms of 2πf nt andφ.
o s
1.14 In your engineering education you’ll often read in some mathematical deriva-
tion, or hear someone say, “For small α, sin(α)=α.” (In fact, you’ll encounter
that statement a few times in this book.) Draw two curves defined by
x= α, and y= sin(α)
over the range of α = –π/2 to α = π/2, and discuss why that venerable “For
small α, sin(α)=α” statement is valid.
1.15 Considering two continuous (analog) sinusoids, having initial phase angles of
α radians at time t = 0, replace the following “?” characters with the correct
angle arguments:
(a) sin(2πf t+ α) = cos(?).
o
(b) cos(2πf t+ α) = sin(?).
o

28 Discrete Sequences and Systems
1.16 National Instruments Corp. manufactures an A/D converter, Model #NI USB-
5133, that is capable of sampling an analog signal at an f sample rate of 100
s
megasamples per second (100 MHz). The A/D converter has internal mem-
ory that can store up to 4x106discrete samples. What is the maximum number
of cycles of a 25 MHz analog sinewave that can be stored in the A/D con-
verter’s memory? Show your work.
1.17 In the first part of the text’s Section 1.5 we stated that for a process (or system)
to be linear it must satisfy a scaling property that we called the proportionality
characteristic in the text’s Eq. (1–14). Determine if the following processes
have that proportionality characteristic:
(a)y (n) = x(n–1)/6,
a
(b)y (n) = 3 + x(n),
b
(c)y (n) = sin[x(n)].
c
This problem is not “busy work.” Knowing if a process (or system) is linear
tells us what signal processing principles, and algorithms, can be applied in
the analysis of that process (or system).
1.18 There is an often-used process in DSP called decimation, and in that process
we retain some samples of an x(n) input sequence and discard other x(n) sam-
ples. Decimation by a factor of two can be described algebraically by
y(m) = x(2n) (P1–3)
where index m=0,1,2,3,. . . The decimation defined by Eq. (P1–3) means that
y(m) is equal to alternate samples (every other sample) of x(n). For example:
y(0) = x(0), y(1) = x(2), y(2) = x(4), y(3) = x(6), . . .
and so on. Here is the question: Is that decimation process time invariant? Il-
lustrate your answer by decimating a simple sinusoidal x(n) time-domain se-
quence by a factor of two to obtain y(m). Next, create a shifted-by-one-sample
version of x(n) and call it x (n). That new sequence is defined by
shift
x (n) = x(n+1). (P1–4)
shift
Finally, decimate x (n) according to Eq. (P1–3) to obtain y (m). The deci-
shift shift
mation process is time invariant if y (m) is equal to a time-shifted version of
shift
y(m). That is, decimation is time invariant if
y (m) = y(m+1).
shift
1.19 In Section 1.7 of the text we discussed the commutative property of linear
time-invariant systems. The two networks in Figure P1–19 exhibit that prop-

Chapter 1 Problems 29
erty. Prove this to be true by showing that, given the same x(n) input se-
quence, outputs y (n) and y (n) will be equal.
1 2
x(n) y(n) x(n) y(n)
1 2
Delay Delay
A A
B B
(a) (b)
Figure P1–19
1.20 Here we investigate several simple discrete processes that turn out to be useful
in a number of DSPapplications. Draw the block diagrams, showing their in-
puts as x(n), of the processes described by the following difference equations:
(a) a 4th-ordercomb filter: y (n) = x(n) – x(n–4),
C
(b) an integrator: y(n) = x(n) + y(n–1),
I I
(c) a leaky integrator: y (n) = Ax(n) + (1–A)y (n–1) [the scalar value A is a
LI LI
real-valued constant in the range 0 <A<1],
(d) a differentiator: y (n) = 0.5x(n) – 0.5x(n-2).
D
1.21 Draw the unit impulse responses (the output sequences when the input is a
unit sample impulse applied at time n=0) of the four processes listed in Prob-
lem 1.20. Let A = 0.5 for the leaky integrator. Assume that all sample values
within the systems are zero at time n= 0.
1.22 DSPengineers involved in building control systems often need to know what
is the step response of a discrete system. The step response, y (n), can be de-
step
fined in two equivalent ways. One way is to say that y (n) is a system’s re-
step
sponse to an input sequence of all unity-valued samples. Asecond definition
is that y (n) is the cumulative sum (the accumulation, discrete integration)
step
of that system’s unit impulse response y (n). Algebraically, this second defi-
imp
nition of step response is expressed as
n
∑
y (n)= y (k).
step imp
k=−∞
In words, the above y (n) expression tells us: “The step response at time
step
index n is equal to the sum of all the previous impulse response samples up
to and including y (n).” With that said, what are the step responses of the
imp

30 Discrete Sequences and Systems
four processes listed in Problem 1.20? (Let A = 0.5 for the leaky integrator.)
Assume that all sample values within the system are zero at time n=0.
1.23 Thinking about the spectra of signals, the ideal continuous (analog) square-
wave s(t) in Figure P1–23, whose fundamental frequency is f Hz, is equal to
o
the sum of an f Hz sinewave and all sinewaves whose frequencies are odd
o
multiples of f Hz. We call s(t) “ideal” because we assume the amplitude tran-
o
sitions from plus and minus Aoccur instantaneously (zero seconds!). Contin-
uous Fourier analysis of the s(t) squarewave allows us to describe this sum of
frequencies as the following infinite sum:
4A⎡ sin(6πf t) sin(10πf t) sin(14πf t) ⎤
s(t)= ⎢sin(2πf t)+ o + o + o + ...⎥.
π ⎣ o 3 5 7 ⎦
s(t)
A
. . . . . .
t = 0 t = 1/f t (Time)
o
A
Figure P1–23
Using a summation symbol, we can express squarewave s(t) algebraically as
∞
4A∑
s(t)= sin(2πnf t)/n,
π o
n=1
for n= odd integers only, showing s(t) to be an infinite sum of sinusoids.
(a) Imagine applying s(t) to a filter that completely removes s(t)’s lowest-
frequency spectral component. Draw the time-domain waveform at the
output of such a filter.
(b) Assume s(t) represents a voltage whose f fundamental frequency is 1 Hz,
o
and we wish to amplify that voltage to peak amplitudes of ±2A. Over
what frequency range must an amplifier operate (that is, what must be
the amplifier’s passband width) in order to exactly double the ideal 1 Hz
squarewave’s peak-peak amplitude?
1.24 This interesting problem illustrates an illegal mathematical operation that we
must learn to avoid in our future algebraic activities. The following claims to

Chapter 1 Problems 31
be a mathematical proof that 4 = 5. Which of the following steps is illegal? Ex-
plain why.
Proof that 4 = 5:
Step 1: 16 – 36 = 25 – 45
Step 2: 42– 9 · 4 = 52– 9 · 5
Step 3: 42– 9 · 4 + 81/4 = 52– 9 · 5 + 81/4
Step 4: (4 – 9/2)2= (5 – 9/2)2
Step 5: 4 – 9/2 = 5 – 9/2
Step 6: 4 = 5

This page intentionally left blank

CHAPTER TWO
. . . . . .
0
Periodic
. . . . . .
Sampling
0
Periodic sampling, the process of representing a continuous signal with a se-
quence of discrete data values, pervades the field of digital signal processing.
In practice, sampling is performed by applying a continuous signal to an
analog-to-digital (A/D) converter whose output is a series of digital values.
Because sampling theory plays an important role in determining the accuracy
and feasibility of any digital signal processing scheme, we need a solid appre-
ciation for the often misunderstood effects of periodic sampling. With regard
to sampling, the primary concern is just how fast a given continuous signal
must be sampled in order to preserve its information content. We can sample
a continuous signal at any sample rate we wish, and we’ll obtain a series of
discrete values—but the question is, how well do these values represent the
original signal? Let’s learn the answer to that question and, in doing so, ex-
plore the various sampling techniques used in digital signal processing.
2.1 ALIASING: SIGNAL AMBIGUITY IN THE FREQUENCY DOMAIN
There is a frequency-domain ambiguity associated with discrete-time signal
samples that does not exist in the continuous signal world, and we can appre-
ciate the effects of this uncertainty by understanding the sampled nature of
discrete data. By way of example, suppose you were given the following se-
quence of values,
x(0) =0
x(1) =0.866
x(2) = 0.866
x(3) =0
33

34 Periodic Sampling
x(4) =–0.866
x(5) =–0.866
x(6) =0,
and were told that they represent instantaneous values of a time-domain
sinewave taken at periodic intervals. Next, you were asked to draw that
sinewave. You’d start by plotting the sequence of values shown by the dots in
Figure 2–1(a). Next, you’d be likely to draw the sinewave, illustrated by the
solid line in Figure 2–1(b), that passes through the points representing the
original sequence.
Another person, however, might draw the sinewave shown by the
shaded line in Figure 2–1(b). We see that the original sequence of values
could, with equal validity, represent sampled values of both sinewaves. The
key issue is that if the data sequence represents periodic samples of a
sinewave, we cannot unambiguously determine the frequency of the
sinewave from those sample values alone.
Reviewing the mathematical origin of this frequency ambiguity enables
us not only to deal with it, but to use it to our advantage. Let’s derive an ex-
pression for this frequency-domain ambiguity and, then, look at a few specific
examples. Consider the continuous time-domain sinusoidal signal defined as
0.866
(a) 0
Time
–0.866
(b)
Time
Figure 2–1 Frequency ambiguity: (a) discrete-time sequence of values; (b) two
different sinewaves that pass through the points of the discrete
sequence.

2.1 Aliasing: Signal Ambiguity in the Frequency Domain 35
x(t)=sin(2πf t). (2–1)
o
This x(t) signal is a garden-variety sinewave whose frequency is f Hz. Now
o
let’s sample x(t) at a rate of f samples/second, i.e., at regular periods of t sec-
s s
onds where t =1/f. If we start sampling at time t=0, we will obtain samples
s s
at times 0t, 1t, 2t, and so on. So, from Eq. (2–1), the first n successive sam-
s s s
ples have the values
0th sample: x(0) = sin(2πf 0t)
o s
1st sample: x(1) = sin(2πf 1t)
o s
2nd sample: x(2) = sin(2πf 2t)
o s (2–2)
. . . . . .
. . . . . .
nth sample: x(n) = sin(2πf nt).
o s
Equation (2–2) defines the value of the nth sample of our x(n) sequence to
be equal to the original sinewave at the time instant nt. Because two values of
s
a sinewave are identical if they’re separated by an integer multiple of 2πradi-
ans, i.e., sin(ø)=sin(ø+2πm) wheremis any integer, we can modify Eq. (2–2) as
m
x(n)=sin(2πf o nt s )=sin(2πf o nt s +2πm)=sin(2π(f o + nt )nt s ). (2–3)
s
If we letmbe an integer multiple of n,m=kn, we can replace the m/nratio in
Eq. (2–3) withkso that
k
x(n)=sin(2π(f + )nt ). (2–4)
o t s
s
Becausef =1/t , we can equate the x(n) sequences in Eqs. (2–2) and (2–4) as
s s
x(n) =sin(2πf nt) = sin(2π(f +kf)nt). (2–5)
o s o s s
The f and (f +kf) factors in Eq. (2–5) are therefore equal. The implication of
o o s
Eq. (2–5) is critical. It means that an x(n) sequence of digital sample values,
representing a sinewave of f Hz, also exactly represents sinewaves at other
o
frequencies, namely, f +kf. This is one of the most important relationships in
o s
the field of digital signal processing. It’s the thread with which all sampling
schemes are woven. In words, Eq. (2–5) states:
When sampling at a rate of f samples/second, ifkis any positive or negative
s
integer, we cannot distinguish between the sampled values of a sinewave of
f Hz and a sinewave of (f +kf) Hz.
o o s
It’s true. No sequence of values stored in a computer, for example, can
unambiguously represent one and only one sinusoid without additional

36 Periodic Sampling
information. This fact applies equally to A/D-converter output samples as
well as signal samples generated by computer software routines. The sam-
pled nature of any sequence of discrete values makes that sequence also rep-
resent an infinite number of different sinusoids.
Equation (2–5) influences all digital signal processing schemes. It’s the
reason that, although we’ve only shown it for sinewaves, we’ll see in Chap-
ter3 that the spectrum of any discrete series of sampled values contains peri-
odic replications of the original continuous spectrum. The period between
these replicated spectra in the frequency domain will always be f, and the
s
spectral replications repeat all the way from DC to daylight in both directions
of the frequency spectrum. That’s becausekin Eq. (2–5) can be any positive or
negative integer. (In Chapters 5 and 6, we’ll learn that Eq. (2–5) is the reason
that all digital filter frequency responses are periodic in the frequency domain
and is crucial to analyzing and designing a popular type of digital filter
known as the infinite impulse response filter.)
To illustrate the effects of Eq. (2–5), let’s build on Figure 2–1 and con-
sider the sampling of a 7 kHz sinewave at a sample rate of 6 kHz. Anew sam-
ple is determined every 1/6000 seconds, or once every 167 microseconds, and
their values are shown as the dots in Figure 2–2(a).
Notice that the sample values would not change at all if, instead, we
were sampling a 1 kHz sinewave. In this example f = 7 kHz, f = 6 kHz, and
⋅ o s
k=–1 in Eq. (2–5), such that f +kf =[7+(–1 6)]=1 kHz. Our problem is that no
o s
processing scheme can determine if the sequence of sampled values, whose
amplitudes are represented by the dots, came from a 7 kHz or a 1 kHz sinu-
soid. If these amplitude values are applied to a digital process that detects en-
ergy at 1 kHz, the detector output would indicate energy at 1 kHz. But we
know that there is no 1 kHz tone there—our input is a spectrally pure 7 kHz
tone. Equation (2–5) is causing a sinusoid, whose name is 7 kHz, to go by the
alias of 1 kHz. Asking someone to determine which sinewave frequency ac-
counts for the sample values in Figure 2–2(a) is like asking, “When I add two
numbers I get a sum of four. What are the two numbers?” The answer is that
there is an infinite number of number pairs that can add up to four.
Figure 2–2(b) shows another example of frequency ambiguity that we’ll
call aliasing, where a 4 kHz sinewave could be mistaken for a –2 kHz
sinewave. In Figure 2–2(b), f = 4 kHz, f = 6 kHz, and k = –1 in Eq. (2–5), so
⋅ o s
thatf +kf =[4+(–1 6)]=–2 kHz. Again, if we examine a sequence of numbers
o s
representing the dots in Figure 2–2(b), we could not determine if the sampled
sinewave was a 4 kHz tone or a –2 kHz tone. (Although the concept of nega-
tive frequencies might seem a bit strange, it provides a beautifully consistent
methodology for predicting the spectral effects of sampling. Chapter 8 dis-
cusses negative frequencies and how they relate to real and complex signals.)
Now, if we restrict our spectral band of interest to the frequency range of
±f/2, the previous two examples take on a special significance. The frequency
s

2.1 Aliasing: Signal Ambiguity in the Frequency Domain 37
7 kHz 1 kHz
(a)
Time
–2kHz
4 kHz
(b)
Time
Spectralband
of interest
(c)
Freq
–3 –2 0 1 3 4 6 7
(kHz)
(–f /2) (f /2) (f )
s s s
Figure 2–2 Frequency ambiguity effects of Eq. (2–5): (a) sampling a 7 kHz
sinewave at a sample rate of 6 kHz; (b) sampling a 4 kHz sinewave at
a sample rate of 6 kHz; (c) spectral relationships showing aliasing of
the 7 and 4 kHz sinewaves.
f/2 is an important quantity in sampling theory and is referred to by differ-
s
ent names in the literature, such as critical Nyquist, half Nyquist, and folding
frequency. A graphical depiction of our two frequency aliasing examples is
provided in Figure 2–2(c). We’re interested in signal components that are
aliased into the frequency band between –f/2 and +f/2. Notice in Figure
s s
2–2(c) that within the spectral band of interest (±3 kHz, because f = 6 kHz),
s
there is energy at –2 kHz and +1 kHz, aliased from 4 kHz and 7 kHz, respec-
tively. Note also that the vertical positions of the dots in Figure 2–2(c) have no

38 Periodic Sampling
Band of
Replica Replica Replica
interest
(a)
–fs /2 0 f s /2 f s 2f s 3f s Freq
f = 6 kHz
s
(b)
–3 0 1 3 6 7 12 13 18 19 Freq
(kHz)
Figure 2–3 Shark’s tooth pattern: (a) aliasing at multiples of the sampling fre-
quency; (b) aliasing of the 7 kHz sinewave to 1 kHz, 13 kHz, and 19 kHz.
amplitude significance but that their horizontal positions indicate which fre-
quencies are related through aliasing.
Ageneral illustration of aliasing is provided in the shark’s toothpattern in
Figure 2–3(a). Note how the peaks of the pattern are located at integer multi-
ples of f Hz. The pattern shows how signals residing at the intersection of a
s
horizontal line and a sloped line will be aliased to all of the intersections of
that horizontal line and all other lines with like slopes. For example, the pat-
tern in Figure 2–3(b) shows that our sampling of a 7 kHz sinewave at a sam-
ple rate of 6 kHz will provide a discrete sequence of numbers whose
spectrum ambiguously represents tones at 1 kHz, 7 kHz, 13 kHz, 19 kHz, etc.
Let’s pause for a moment and let these very important concepts soak in a bit.
Again, discrete sequence representations of a continuous signal have un-
avoidable ambiguities in their frequency domains. These ambiguities must be
taken into account in all practical digital signal processing algorithms.
OK, let’s review the effects of sampling signals that are more interesting
than just simple sinusoids.
2.2 SAMPLING LOWPASS SIGNALS
Consider the situation of sampling a signal such as a continuous real-valued
lowpass x(t) signal whose spectrum is shown in Figure 2–4(a). Notice that the
spectrum is symmetrical around zero Hz, and the spectral amplitude is zero
above +BHz and below –BHz; i.e., the signal is band-limited. (From a practical
standpoint, the term band-limited signal merely implies that any signal energy
outside the range of ±BHz is below the sensitivity of our system.) The x(t) time
signal is called a lowpass signalbecause its spectral energy is low in frequency.

2.2 Sampling Lowpass Signals 39
Continuous spectrum
(a)
–B 0 B Freq
Discrete spectrum
. . . . . .
(b)
–2f s –f s –f s /2 0 f s /2 f s 2fs Freq
B
. . . Aliasing Aliasing . . .
(c)
–2f s –fs –f s /2 0 f s /2 f s =1.5B 2fs Freq
–B B
–B/2 B/2
Figure 2–4 Spectral replications: (a) original continuous lowpass signal spectrum;
(b) spectral replications of the sampled lowpass signal when f/2>B;
s
(c) frequency overlap and aliasing when the sampling rate is too low
becausef/2<B.
s
Pausing for a moment, if the continuous x(t) signal were a voltage on a
coax cable applied to the input of an analog spectrum analyzer, we would
only see the spectral energy over the positive-frequency range of 0 to +B Hz
on the analyzer’s screen. However, in our world of discrete signals (DSP)
we show the spectrum of real-valued signals as having both positive- and
negative-frequency spectral energy. Throughout this book we’ll repeatedly
see why such spectral representations are often useful, and sometimes
mandatory in our work. The mathematical justification for two-sided spectral
diagrams is provided in both Chapters 3 and 8. For now, we request the
reader’s acceptance that Figure 2–4(a) is a valid representation of the spec-
trum of the continuous x(t) signal.
Given that the continuous x(t) signal, whose spectrum is shown in Figure
2–4(a), is sampled at a rate of f samples/second, we can see the spectral repli-
s
cation effects of sampling in Figure 2–4(b) showing the original spectrum in ad-
dition to an infinite number of replications. The period of spectral replication is
f Hz. Figure 2–4(b) is the spectrum of the sequence of x(n) sampled values of
s

40 Periodic Sampling
the continuous x(t) signal. (Although we stated in Section 1.1 that frequency-
domain representations of discrete time-domain sequences are themselves dis-
crete, the replicated spectra in Figure 2–4(b) are shown as continuous lines,
instead of discrete dots, merely to keep the figure from looking too cluttered.
We’ll cover the full implications of discrete frequency spectra in Chapter 3.)
Let’s step back a moment and understand Figure 2–4 for all it’s worth.
Figure 2–4(a) is the spectrum of a continuous signal, a signal that can only
exist in one of two forms. Either it’s a continuous signal that can be sampled,
through A/D conversion, or it is merely an abstract concept such as a mathe-
matical expression for a signal. It cannotbe represented in a digital machine in
its current band-limited form. Once the signal is represented by a sequence of
discrete sample values, its spectrum takes the replicated form of Figure 2–4(b).
The replicated spectra are not just figments of the mathematics; they
exist and have a profound effect on subsequent digital signal processing.†The
replications may appear harmless, and it’s natural to ask, “Why care about
spectral replications? We’re only interested in the frequency band within
±f/2.” Well, if we perform a frequency translation operation or induce a
s
change in sampling rate through decimation or interpolation, the spectral
replications will shift up or down right in the middle of the frequency range
of interest ±f/2 and could cause problems[1]. Let’s see how we can control
s
the locations of those spectral replications.
In practical A/D conversion schemes, f is always greater than 2Bto sep-
s
arate spectral replications at the folding frequencies of ±f/2. This very impor-
s
tant relationship of f ≥ 2B is known as the Nyquist criterion. To illustrate why
s
the term folding frequency is used, let’s lower our sampling frequency to
f = 1.5B Hz. The spectral result of this undersampling is illustrated in Figure
s
2–4(c). The spectral replications are now overlapping the original baseband
spectrum centered at zero Hz. Limiting our attention to the band ±f/2 Hz, we
s
see two very interesting effects. First, the lower edge and upper edge of the
spectral replications centered at +f and –f now lie in our band of interest.
s s
This situation is equivalent to the original spectrum folding to the left at +f/2
s
and folding to the right at –f/2. Portions of the spectral replications now
s
combine with the original spectrum, and the result is aliasing errors. The dis-
crete sampled values associated with the spectrum of Figure 2–4(c) no longer
truly represent the original input signal. The spectral information in the
bands of –Bto –B/2 and B/2 to BHz has been corrupted. We show the ampli-
tude of the aliased regions in Figure 2–4(c) as shaded lines because we don’t
really know what the amplitudes will be if aliasing occurs.
†Toward the end of Section 5.9, as an example of using the convolution theorem, another de-
rivation of periodic sampling’s replicated spectra will be presented.

2.2 Sampling Lowpass Signals 41
The second effect illustrated by Figure 2–4(c) is that the entire spec-
tral content of the original continuous signal is now residing in the band
of interest between –f /2 and +f /2. This key property was true in Figure
s s
2–4(b) and will always be true, regardless of the original signal or the
sample rate. This effect is particularly important when we’re digitizing
(A/D converting) continuous signals. It warns us that any signal energy
located above +B Hz and below –B Hz in the original continuous spec-
trum of Figure 2–4(a) will always end up in the band of interest after
sampling, regardless of the sample rate. For this reason, continuous (ana-
log) lowpass filters are necessary in practice.
We illustrate this notion by showing a continuous signal of bandwidth B
accompanied by noise energy in Figure 2–5(a). Sampling this composite con-
tinuous signal at a rate that’s greater than 2B prevents replications of the sig-
nal of interest from overlapping each other, but all of the noise energy still
ends up in the range between –f/2 and +f/2 of our discrete spectrum shown
s s
in Figure 2–5(b). This problem is solved in practice by using an analog low-
pass anti-aliasing filter prior to A/D conversion to attenuate any unwanted
signal energy above +Band below –BHz as shown in Figure 2–6. An example
lowpass filter response shape is shown as the dotted line superimposed on
the original continuous signal spectrum in Figure 2–6. Notice how the output
spectrum of the lowpass filter has been band-limited, and spectral aliasing is
avoided at the output of the A/D converter.
As a historical note, the notion of periodic sampling was studied by var-
ious engineers, scientists, and mathematicians such as the Russian V. Kotel-
nikov, the Swedish-born H. Nyquist, the Scottish E. Whittaker, and the
Noise Signal of Noise
interest
(a)
–-B 0 B Freq
(b)
–fs –fs/2 0 f s /2 fs Freq
Figure 2–5 Spectral replications: (a) original continuous signal-plus-noise spec-
trum; (b) discrete spectrum with noise contaminating the signal of in-
terest.

42 Periodic Sampling
Noise Noise
. . . . . .
–B 0 B Freq –fs –fs/2 0 fs /2 fs 2fs Freq
Original Filtered Discrete
continuous Analog Lowpass continuous signal A/D samples
signal Filter (cutoff Converter
frequency = B Hz)
–B 0 B Freq
Figure 2–6 Lowpass analog filtering prior to sampling at a rate of f Hz.
s
Japanese I. Someya[2]. But it was the American Claude Shannon, acknowl-
edging the work of others, that formalized the concept of periodic sampling
as we know it today and brought it to the broad attention of communications
engineers[3]. That was in 1948—the birth year of the transistor, marshmal-
lows, and this author.
This completes the discussion of simple lowpass sampling. Now let’s go
on to a more advanced sampling topic that’s proven so useful in practice.
2.3 SAMPLING BANDPASS SIGNALS
Although satisfying the majority of sampling requirements, the sampling of
lowpass signals, as in Figure 2–6, is not the only sampling scheme used in
practice. We can use a technique known as bandpass samplingto sample a con-
tinuous bandpass signal that is centered about some frequency other than
zero Hz. When a continuous input signal’s bandwidth and center frequency
permit us to do so, bandpass sampling not only reduces the speed require-
ment of A/D converters below that necessary with traditional lowpass sam-
pling; it also reduces the amount of digital memory necessary to capture a
given time interval of a continuous signal.
By way of example, consider sampling the band-limited signal shown in
Figure 2–7(a) centered at f = 20 MHz, with a bandwidth B = 5 MHz. We use
c
the term bandpass sampling for the process of sampling continuous signals
whose center frequencies have been translated up from zero Hz. What we’re
calling bandpass sampling goes by various other names in the literature, such
as IF sampling, harmonic sampling[4], sub-Nyquist sampling, and under-
sampling[5]. In bandpass sampling, we’re more concerned with a signal’s
bandwidth than its highest-frequency component. Note that the negative fre-
quency portion of the signal, centered at –f, is the mirror image of the positive
c

2.3 Sampling Bandpass Signals 43
B =
Bandpass signal 5 MHz
spectrum
(continuous)
(a)
–fc 0 fc = Freq
20 MHz
Bandpass signal
f = f – B/2 = 17.5 MHz spectrum (discrete)
s c
(b)
–fc –2.5MHz 0 2.5 MHz fc Freq
fs fs
f
s
fs
Figure 2–7 Bandpass signal sampling: (a) original continuous signal spectrum; (b)
sampled signal spectrum replications when sample rate is 17.5 MHz.
frequency portion—as it must be for real signals. Our bandpass signal’s highest-
frequency component is 22.5 MHz. Conforming to the Nyquist criterion (sam-
pling at twice the highest-frequency content of the signal) implies that the
sampling frequency must be a minimum of 45 MHz. Consider the effect if the
sample rate is 17.5 MHz shown in Figure 2–7(b). Note that the original spectral
components remain located at ±f, and spectral replications are located exactly
c
at baseband, i.e., butting up against each other at zero Hz. Figure 2–7(b) shows
that sampling at 45 MHz was unnecessary to avoid aliasing—instead we’ve
used the spectral replicating effects of Eq. (2–5) to our advantage.
Bandpass sampling performs digitization and frequency translation in a
single process, often called sampling translation. The processes of sampling
and frequency translation are intimately bound together in the world of digi-
tal signal processing, and every sampling operation inherently results in spec-
tral replications. The inquisitive reader may ask, “Can we sample at some still
lower rate and avoid aliasing?” The answer is yes, but, to find out how, we
have to grind through the derivation of an important bandpass sampling re-
lationship. Our reward, however, will be worth the trouble because here’s
where bandpass sampling really gets interesting.
Let’s assume we have a continuous input bandpass signal of bandwidth
B. Its carrier frequency is f Hz, i.e., the bandpass signal is centered at f Hz, and
c c
its sampled value spectrum is that shown in Figure 2–8(a). We can sample that
continuous signal at a rate, say f Hz, so the spectral replications of the positive
s’
and negative bands, Q and P, just butt up against each other exactly at zero Hz.
This situation, depicted in Figure 2–8(a), is reminiscent of Figure 2–7(b). With
an arbitrary number of replications, say m,in the range of 2f –B, we see that
c
2f −B
mf =2f −B or f = c . (2–6)
s' c s' m

44 Periodic Sampling
2f -B B
c
f -B/2 f
c s'
(a) P Q P'
-f -2f -f 0 f 2f f Freq
c s' s' s' s' c
2f -B
c
(b) P Q P'
-f 0 f Freq
c c
2f + B
c
(c) P Q P'
-f -2f -f 0 f 2f f Freq
c s'' s'' s'' s'' c
Figure 2–8 Bandpass sampling frequency limits: (a) sample rate f = (2f – B)/6;
s’ c
(b) sample rate is less than f ;(c)minimum sample rate f <f .
s’ s” s’
In Figure 2–8(a), m = 6 for illustrative purposes only. Of course m can be any
positive integer so long as f is never less than 2B. If the sample rate f is in-
s’ s’
creased, the original spectra (bold) do not shift, but all the replications will
shift. At zero Hz, the Pband will shift to the right, and the Q band will shift to
the left. These replications will overlap and aliasing occurs. Thus, from Eq.
(2–6), for an arbitrary m,there is a frequency that the sample rate must not ex-
ceed, or
2f −B 2f −B
f ≤ c or c ≥ f . (2–7)
s' m m s'
If we reduce the sample rate below the f value shown in Figure 2–8(a), the
s’
spacing between replications will decrease in the direction of the arrows in
Figure 2–8(b). Again, the original spectra do not shift when the sample rate is
changed. At some new sample rate f , where f < f , the replication P’ will
s” s’’ s’
just butt up against the positive original spectrum centered at f as shown in
c
Figure 2–8(c). In this condition, we know that
2f +B
(m+1)f =2f +B or f = c . (2–8)
s" c s" m+1
Should f be decreased in value, P’ will shift further down in frequency and
s”
start to overlap with the positive original spectrum at f and aliasing occurs.
c
Therefore, from Eq. (2–8) and for m+1, there is a frequency that the sample
rate must always exceed, or
2f +B
f ≥ c . (2–9)
s" m+1

2.4 Practical Aspects of Bandpass Sampling 45
Table 2–1 Equation (2–10) Applied to the Bandpass Signal Example
m (2f –B)/m (2f +B)/(m+1) Optimum sampling rate
c c
1 35.0 MHz 22.5 MHz 22.5 MHz
2 17.5 MHz 15.0 MHz 17.5 MHz
3 11.66 MHz 11.25 MHz 11.25 MHz
4 8.75 MHz 9.0 MHz —
5 7.0 MHz 7.5 MHz —
We can now combine Eqs. (2–7) and (2–9) to say that f may be chosen any-
s
where in the range between f andf to avoid aliasing, or
s” s’
2f −B 2f +B
c ≥ f ≥ c , (2–10)
m s m+1
wheremis an arbitrary, positive integer ensuring that f ≥2B. (For this type of
s
periodic sampling of real signals, known as real or 1st-order sampling, the
Nyquist criterion f ≥2Bmust still be satisfied.)
s
To appreciate the important relationships in Eq. (2–10), let’s return to our
bandpass signal example, where Eq. (2–10) enables the generation of Table 2–1.
This table tells us that our sample rate can be anywhere in the range of 22.5 to
35 MHz, anywhere in the range of 15 to 17.5 MHz, or anywhere in the range of
11.25 to 11.66 MHz. Any sample rate below 11.25 MHz is unacceptable because
it will not satisfy Eq. (2–10) as well as f ≥2B. The spectra resulting from several
s
of the sampling rates from Table 2–1 are shown in Figure 2–9 for our bandpass
signal example. Notice in Figure 2–9(f) that when f equals 7.5 MHz (m=5), we
s
have aliasing problems because neither the greater-than relationships in Eq.
(2–10) nor f ≥2Bhave been satisfied. Them=4 condition is also unacceptable
s
because f ≥ 2B is not satisfied. The last column in Table 2–1 gives the optimum
s
sampling frequency for each acceptablemvalue. Optimum sampling frequency
is defined here as that frequency where spectral replications butt up against
each other at zero Hz. For example, in them=1 range of permissible sampling
frequencies, it is much easier to perform subsequent digital filtering or other
processing on the signal samples whose spectrum is that of Figure 2–9(b), as
opposed to the spectrum in Figure 2–9(a).
2.4 PRACTICAL ASPECTS OF BANDPASS SAMPLING
Now that we’re familiar with the theory of bandpass sampling, let’s discuss a
few aspects of bandpass sampling in practical applications.

46 Periodic Sampling
Original spectrum f s /2 Original spectrum
f s = 35.0 MHz
m = 1
(a)
MHz
–25.0 –20.0 –15.0 –10.0 –5.0 0 5.0 10.0 15.0 20.0 25.0
f s = 22.5 MHz f s /2 m = 1
(b)
MHz
–25.0 –20.0 –15.0 –10.0 –5.0 0 5.0 10.0 15.0 20.0 25.0
f s = 17.5 MHz
f s /2 m = 2
(c)
MHz
–25.0 –20.0 –15.0 –10.0 –5.0 0 5.0 10.0 15.0 20.0 25.0
f s /2
f s = 15.0 MHz m = 2
(d)
MHz
–25.0 –20.0 –15.0 –10.0 –5.0 0 5.0 10.0 15.0 20.0 25.0
f s = 11.25 MHz f s /2 m = 3
(e)
MHz
–25.0 –20.0 –15.0 –10.0 –5.0 0 5.0 10.0 15.0 20.0 25.0
Aliasing Aliasing
f = 7.5 MHz
s m = 5
(f)
MHz
–25.0 –20.0 –15.0 –10.0 –5.0 0 5.0 10.0 15.0 20.0 25.0
Figure 2–9 Various spectral replications from Table 2–1: (a) f = 35 MHz;
s
(b)f = 22.5 MHz; (c) f = 17.5 MHz; (d) f = 15 MHz; (e) f = 11.25 MHz;
s s s s
(f) f =7.5 MHz.
s
2.4.1 Spectral Inversion in Bandpass Sampling
Some of the permissible f values from Eq. (2–10) will, although avoiding
s
aliasing problems, provide a sampled baseband spectrum (located near zero
Hz) that is inverted from the original analog signal’s positive and negative
spectral shapes. That is, the positive-frequency sampled baseband will have
the inverted shape of the negative half from the original analog spectrum.
This spectral inversion happens whenever m, in Eq. (2–10), is an odd integer,

2.4 Practical Aspects of Bandpass Sampling 47
as illustrated in Figures 2–9(c) and 2–9(d). When the original positive spectral
bandpass components are symmetrical about the f frequency, spectral inver-
c
sion presents no problem and any nonaliasing value for f from Eq. (2–10)
s
may be chosen.
However, if spectral inversion is something to be avoided, for example,
when single sideband signals are being processed, the applicable sample rates
to avoid spectral inversion are defined by Eq. (2–10) with the restriction that
mis an even integer and f > 2Bis satisfied.
s
Now here’s some good news. With a little additional digital processing we
can sample at rates defined by Eq. (2–10) with odd m, with their spectral inver-
sion, and easily reinvert the spectrum back to its original orientation. The dis-
crete spectrum of any digital signal can be inverted by multiplying the signal’s
discrete-time samples by a sequence of alternating plus ones and minus ones
(1, –1, 1, –1, etc.), indicated in the literature by the succinct expression (–1)n.
Although multiplying time samples by (–1)nis explored in detail in Sec-
tion 13.1, all we need to remember at this point is the simple rule that multi-
plication of real signal samples by (–1)n flips the positive-frequency band of
interest, from zero to +f/2 Hz, where the center of the flipping is f/4 Hz.
s s
Likewise, the multiplication flips the negative frequency band of interest,
from –f/2 to zero Hz, where the center of the flipping is –f/4 Hz as shown in
s s
Figure 2–10. In the literature of DSP, occasionally you’ll see the (–1)nsequence
expressed by the equivalent expression cos(πn).
Spectral band
of interest Spectrum
ofx(n)
(a)
ñ3f/4 ñf/2 0 f/2 3f/4 Freq
s s ñf/4 f/4 s s
s s
Spectrum of
.
(ñ1)n x(n)
(b)
ñ3f/4 ñf/2 0 f/2 3f/4 Freq
s s ñf/4 f/4 s s
s s
Figure 2–10 Spectral inversion through multiplication by (–1)n: (a) spectrum of
originalx(n); (b) spectrum of (–1)n·x(n).

48 Periodic Sampling
2.4.2 Positioning Sampled Spectra at f/4
s
In many signal processing applications we’ll find it useful to use an f band-
s
pass sampling rate that forces the sampled spectra to be centered exactly at
±f/4 as shown in Figure 2–10(a). As we’ll see in later chapters, this scenario
s
greatly simplifies certain common operations such as digital filtering, com-
plex down-conversion, and Hilbert transformations.
To ensure that sampled spectra reside at ±f/4, we select f using
s s
4f
f = c , where k = 1, 2, 3 ,... (2–11)
s 2k−1
where f is the center frequency of the original analog signal’s bandpass signal.
c
2.4.3 Noise in Bandpass-Sampled Signals
We have painted a rosy picture of bandpass sampling, with its analog signal
capture capabilities at reduced sample rates. However, there is a negative as-
pect associated with bandpass sampling. The signal-to-noise ratio (SNR), the
ratio of the power of a signal over the total background noise power, of our
digitized signal is degraded when we perform bandpass sampling. (Ageneral
discussion of SNR is provided in Appendix D.)
Here’s the story. The spectrum of an analog lowpass signal, output from
an analog anti-aliasing lowpass filter, is that shown in Figure 2–11(a). That
lowpass signal contains some amount of background noise power. Now if an
analog bandpass signal is likewise contaminated with background noise, as
shown by the spectral plot in Figure 2–11(b), the bandpass-sampled signal will
have an increased level of background noise as shown in Figure 2–11(c). That’s
because all of the background spectral noise in Figure 2–11(b) must now reside
in the range of –f/2 to f/2 in Figure 2–11(c). As such, the bandpass-sampled
s s
signal’s SNR is reduced (degraded).
As detailed in reference [6], if the analog bandpass signal’s background
noise spectral power level is relatively flat, as in Figure 2–11(b), the bandpass-
sampled background noise power increases by a factor of m + 1 (the denomi-
nator of the right-side ratio in Eq. (2–10)) while the desired signal power P
remains unchanged. As such, the bandpass-sampled signal’s SNR, measured
in decibels, is reduced by
D = 10 · log (m+ 1) dB (2–12)
SNR 10
below the SNR of the original analog signal. So for the Figure 2–11 example,
whenm= 1, the bandpass-sampled signal’s background noise power doubles,
and the total bandpass-sampled signal’s SNR is D = 3 dB less than the ana-
SNR
log bandpass signal’s SNR.

References 49
Analog lowpass
P signal spectral
Background power
noise power
(a)
Freq
0
Analog bandpass
signal spectral P
Background
power
noise power
(b)
−f f Freq
c 0 c
Increased
background P m = 1
noise power
(c)
−f s −f c −f s /2 0 f s /2 f c f s Freq
Figure 2–11 Sampling SNR degradation: (a) analog lowpass signal spectral
power; (b) analog bandpass signal spectral power; (c) bandpass-
sampled signal spectral power when m= 1.
The notion of using decibels, a very convenient method of comparing the
power of two signals (the two signals, in this case, are our bandpass signal and
the background noise signal), is discussed in Appendix E.
REFERENCES
[1] Crochiere, R., and Rabiner, L. “Optimum FIR Digital Implementations for Decimation, In-
terpolation, and Narrow-band Filtering,” IEEE Trans. on Acoust. Speech, and Signal Proc.,
Vol. ASSP-23, No. 5, October 1975.
[2] Luke, H. “The Origins of the Sampling Theorem,” IEEE Communications Magazine, April
1999, pp. 106–109.
[3] Shannon, C. “AMathematical Theory of Communication,” Bell Sys. Tech. Journal, Vol. 27,
1948, pp. 379–423, 623–656.
[4] Steyskal, H. “Digital Beamforming Antennas,” Microwave Journal, January 1987.
[5] Hill, G. “The Benefits of Undersampling,” Electronic Design, July 11, 1994.
[6] Vaughan, R., et al., “The Theory of Bandpass Sampling,” IEEE Trans. on Signal Processing,
Vol. 39, No. 9, September 1991, pp. 1973–1984.

50 Periodic Sampling
CHAPTER 2 PROBLEMS
2.1 Suppose you have a mechanical clock that has a minute hand, but no hour
hand. Next, suppose you took a photograph of the clock when the minute
hand was pointed at 12:00 noon and then took additional photos every 55
minutes. Upon showing those photos, in time order, to someone:
(a) What would that person think about the direction of motion of the
minute hand as time advances?
(b) With the idea of lowpass sampling in mind, how often would you need to
take photos, measured in photos/hour, so that the successive photos
show proper (true) clockwise minute-hand rotation?
2.2 Assume we sampled a continuous x(t) signal and obtained 100 x(n) time-
domain samples. What important information (parameter that we need to
know in order to analyze x(t)) is missing from the x(n) sequence?
2.3 National Instruments Corporation produces an analog-to-digital (A/D) con-
verter (Model #NI-5154) that can sample (digitize) an analog signal at a sam-
ple rate of f=2.0 GHz (gigahertz).
s
(a) What is the t period of the output samples of such a device?
s
(b) Each A/D output sample is an 8-bit binary word (one byte), and the con-
verter is able to store 256 million samples. What is the maximum time in-
terval over which the converter can continuously sample an analog
signal?
2.4 Consider a continuous time-domain sinewave, whose cyclic frequency is 500
Hz, defined by
x(t) = cos[2π(500)t+π/7].
Write the equation for the discrete x(n) sinewave sequence that results from
samplingx(t) at an f sample rate of 4000 Hz.
s
Note:This problem is not “busy work.” If you ever want to model the x(t) sig-
nal using software (MathCAD, MATLAB, Octave, etc.), then it is the desired
x(n) equation that you program into your software.
2.5 If we sampled a single continuous sinewave whose frequency is f Hz, over
o
what range must t (the time between digital samples) be to satisfy the
s
Nyquist criterion? Express that t range in terms of f .
s o

Chapter 2 Problems 51
2.6 Suppose we used the following statement to describe the Nyquist criterion
for lowpass sampling: “When sampling a single continuous sinusoid (a single
analog tone), we must obtain no fewer than N discrete samples per continu-
ous sinewave cycle.” What is the value of this integer N?
2.7 The Nyquist criterion, regarding the sampling of lowpass signals, is some-
times stated as “The sampling rate f must be equal to, or greater than, twice
s
the highest spectral component of the continuous signal being sampled.” Can
you think of how a continuous sinusoidal signal can be sampled in accor-
dance with that Nyquist criterion definition to yield all zero-valued discrete
samples?
2.8 Stock market analysts study time-domain charts (plots) of the closing price of
stock shares. A typical plot takes the form of that in Figure P2–8, where in-
stead of plotting discrete closing price sample values as dots, they draw
straight lines connecting the closing price value samples. What is the t period
s
for such stock market charts?
$
Stock
share
closing
price
Time
Figure P2–8
2.9 Consider a continuous time-domain sinewave defined by
x(t) = cos(4000πt)
that was sampled to produce the discrete sinewave sequence defined by
x(n)= cos(nπ/2).
What is the f sample rate, measured in Hz, that would result in sequence
s
x(n)?

52 Periodic Sampling
2.10 Consider the two continuous signals defined by
a(t)= cos(4000πt) and b(t)= cos(200πt)
whose product yields the x(t) signal shown in Figure P2–10. What is the mini-
mum f sample rate, measured in Hz, that would result in a sequence x(n)
s
with no aliasing errors (no spectral replication overlap)?
a(t)
x(t) Analog-to-digital x(n)
converter
b(t)
f
s
Figure P2–10
2.11 Consider a discrete time-domain sinewave sequence defined by
x(n)= sin(nπ/4)
that was obtained by sampling an analog x(t) = sin(2πf t) sinewave signal
o
whose frequency is f Hz. If the sample rate of x(n) is f = 160 Hz, what are
o s
three possible positive frequency values, measured in Hz, for f that would re-
o
sult in sequence x(n)?
2.12 Inthetextwediscussedthenotionofspectralfoldingthatcantakeplacewhenan
x (t) analog signal is sampled to produce a discrete x (n) sequence. We also
a d
statedthatalloftheanalogspectralenergycontainedinX (f)willresidewithin
a
thefrequencyrangeof±f/2oftheX (f)spectrumofthesampledx (n)sequence.
s d d
Giventhoseconcepts,considerthespectrumofananalogsignalshowninFigure
P2–12(a)whosespectrumisdividedintothesixsegmentsmarkedas1 to 6.Fill
in the following table showing which of the A-to-F spectral segments of X (f),
d
showninFigureP2–12(b),arealiasesofthe1-to-6spectralsegmentsofX (f).
a
Aliased Spectral Segments
X (f) spectral Associated
d
segment X (f) spectral segment
a
A 3
B 4
C
D
E
F

Chapter 2 Problems 53
X(f)
a
3 4
2 5
(a)
1 6
f f/2 0 f/2 f f
s s s s
X(f)
d
A B
(b) C D
E F
f f/2 0 f/2 f f
s s s s
Figure P2–12
2.13 Consider the simple analog signal defined by x(t)=sin(2π700t) shown in Fig-
ure P2–13. Draw the spectrum of x(n) showing all spectral components, label-
ing their frequency locations, in the frequency range –2f to +2f.
s s
x(t) Analog-to-digital x(n)
converter
f = 1000 Hz
s
Clock
generator
Figure P2–13
2.14 The Nançay Observatory, in France, uses a radio astronomy receiver that gen-
erates a wideband analog s(t) signal whose spectral magnitude is represented
in Figure P2–14. The Nançay scientists bandpass sample the analog s(t) sig-
nal, using an analog-to-digital (A/D) converter to produce an x(n) discrete se-
quence, at a sample rate of f=56 MHz.
s

54 Periodic Sampling
|S(f)| 14
Mhz
0 70 Freq
MHz
Figure P2–14
(a) Draw the spectrum of the x(n) sequence, X(f), showing its spectral energy
over the frequency range –70 MHz to 70 MHz.
(b) What is the center frequency of the first positive-frequency spectral repli-
cation in X(f)?
(c) How is your solution to Part (b) related to the f sample rate?
s
Hint:How is your solution to Part (b) related to f/2?
s
2.15 Think about the continuous (analog) signal x(t) that has the spectral magni-
tude shown in Figure P2–15. What is the minimum f sample rate for lowpass
s
sampling such that no spectral overlap occurs in the frequency range of 2 to 9
kHz in the spectrum of the discrete x(n) samples?
|X(f)|
0 2 9 10 Freq
1 (kHz)
Figure P2–15
2.16 If a person wants to be classified as a soprano in classical opera, she must be
able to sing notes in the frequency range of 247 Hz to 1175 Hz. What is the
minimum f sampling rate allowable for bandpass sampling of the full audio
s
spectrum of a singing soprano?
2.17 This problem requires the student to have some knowledge of electronics and
how a mixer operates inside a radio. (The definition of a bandpass filter is
given in Appendix F.) Consider the simplified version of what is called a su-
perheterodynedigital radio depicted in Figure P2–17.

Chapter 2 Problems 55
Antenna
Mixer
Analog w(t) u(t) Analog x(t) A/D x(n)
bandpass bandpass
converter
filter #1 filter #2
sin(2 f t) f (A/D clock)
LO s
Center frequency = 50 MHz, Center frequency, f = 15 MHz,
c
bandwidth = 10 MHz bandwidth = 5 MHz
Figure P2–17
(a) For what local oscillator frequency, f , would an image (a copy, or dupli-
LO
cation) of the w(t) signal’s spectrum be centered at 15 MHz (megahertz) in
signalu(t)?
(b) What is the purpose of the analog bandpass filter #2?
(c) Fill in the following table showing all ranges of acceptable f bandpass
s
sampling rates to avoid aliasing errors in the discrete x(n) sequence. Also
list, in the rightmost column, for which values of m the sampled spec-
trum, centered at 15 MHz, will be inverted.
Positive-frequency
sampled spectrum is
m (2f –B)/m (2f + B)/(m+1) inverted (Yes or No)
c c
1
2
3
(d) In digital receivers, to simplify AM and FM demodulation, it is advanta-
geous to have the spectrum of the discrete x(n) sequence be centered at one-
quarter of the sample rate. The text’s Eq. 2–11 describes how to achieve this
situation. If we were constrained to have f equal to 12 MHz, what would
s
be the maximum f local oscillator frequency such that the spectra of u(t),
LO
x(t), and x(n) are centered at f/4? (Note: In this scenario, the f center fre-
s c
quency of analog bandpass filter #2 will no longer be 15 MHz.)
2.18 Think about the analog anti-aliasing filter given in Figure P2–18(a), having a
one-sided bandwidth of BHz. Awideband analog signal passed through that
filter, and then sampled, would have an |X(m)| spectrum as shown in Figure
P2–18(b), where the dashed curves represent spectral replications.

56 Periodic Sampling
Suppose we desired that all aliased spectral components in |X(m)| over
our B Hz bandwidth of interest must be attenuated by at least 60 dB. Deter-
mine the equation, in terms of Band the f sampling rate, for the frequency at
s
which the anti-aliasing filter must have an attenuation value of –60 dB. The
solution to this problem gives us a useful rule of thumbwe can use in specify-
ing the desired performance of analog anti-aliasing filters.
|H(f)|
Analog filter
B B magnitude
response
(a)
0 Freq
|X(m)|
B
(b) 60 dB
f 0 f Freq
s s
Figure P2–18
2.19 This problem demonstrates a popular way of performing frequency down-
conversion (converting a bandpass signal into a lowpass signal) by way of
bandpass sampling. Consider the continuous 250-Hz-wide bandpass x(t) sig-
nal whose spectral magnitude is shown in Figure P2–19. Draw the spectrum,
over the frequency range of –1.3f to +1.3f, of the x(n) sampled sequence ob-
s s
tained when x(t) is sampled at f=1000 samples/second.
s
Spectrum of
analogx(t) 250 Hz
±1000 0 1000 Hz
Figure P2–19
2.20 Here’s a problem to test your understanding of bandpass sampling. Think
about the continuous (analog) signal x(t) that has the spectral magnitude
shown in Figure P2–20.

Chapter 2 Problems 57
|X(f)|
B
0 f Freq
c c
Figure P2–20
(a) What is the minimum f center frequency, in terms of x(t)’s bandwidth B,
c
that enables bandpass sampling of x(t)? Show your work.
(b) Given your results in Part (a) above, determine if it is possible to perform
bandpass sampling of the full spectrum of the commercial AM (ampli-
tude modulation) broadcast radio band in North America. Explain your
solution.
2.21 Suppose we want to perform bandpass sampling of a continuous 5 kHz-wide
bandpass signal whose spectral magnitude is shown in Figure P2–21.
|X(f)|
5
0 25 Freq
(kHz)
Figure P2–21
Fill in the following table showing the various ranges of acceptable f band-
s
pass sampling rates, similar to the text’s Table 2–1, to avoid aliasing errors.
Also list, in the rightmost column, for which values of m the sampled spec-
trum in the vicinity of zero Hz is inverted.

58 Periodic Sampling
Acceptable Bandpass Sample Rate Ranges
Positive-frequency
sampled spectrum is
m (2f –B)/m (2f +B)/(m+1) inverted (Yes or No)
c c
1
2
3
4
5
2.22 I recently encountered an Internet website that allegedly gave an algorithm
for the minimum f bandpass sampling rate for an analog bandpass signal
s
centered at f Hz, whose bandwidth is BHz. The algorithm is
c
4f
f = c
s,min 2Z−1
where
⎢4f + 2B⎥
Z= ⎢ c ⎥.
⎣ 4B ⎦
In the above notation, 1x2means the integer part of x. Here’s the problem: Is
the above f algorithm correct in computing the absolute minimum possible
s,min
nonaliasingf bandpass sampling rate for an analog bandpass signal centered
s
atf Hz, whose bandwidth is BHz? Verify your answer with an example.
c

CHAPTER THREE
The Discrete
Fourier
Transform
0
The discrete Fourier transform (DFT) is one of the two most common, and
powerful, procedures encountered in the field of digital signal processing.
(Digital filtering is the other.) The DFT enables us to analyze, manipulate, and
synthesize signals in ways not possible with continuous (analog) signal pro-
cessing. Even though it’s now used in almost every field of engineering, we’ll
see applications for DFT continue to flourish as its utility becomes more
widely understood. Because of this, a solid understanding of the DFT is
mandatory for anyone working in the field of digital signal processing.
The DFT is a mathematical procedure used to determine the harmonic,
or frequency, content of a discrete signal sequence. Although, for our pur-
poses, a discrete signal sequence is a set of values obtained by periodic sam-
pling of a continuous signal in the time domain, we’ll find that the DFT is
useful in analyzing any discrete sequence regardless of what that sequence
actually represents. The DFT’s origin, of course, is the continuous Fourier
transformX(f) defined as
+∞
X(f)= ∫ x(t)e −j2πftdt (3–1)
–∞
where x(t) is some continuous time-domain signal.†
In the field of continuous signal processing, Eq. (3–1) is used to trans-
form an expression of a continuous time-domain function x(t) into a continu-
ous frequency-domain function X(f). Subsequent evaluation of the X(f)
†Fourier is pronounced ‘for-ya¯. In engineering school, we called Eq. (3–1) the “four-year” trans-
form because it took about four years to do one homework problem.
59

60 The Discrete Fourier Transform
expression enables us to determine the frequency content of any practical
signal of interest and opens up a wide array of signal analysis and process-
ing possibilities in the fields of engineering and physics. One could argue
that the Fourier transform is the most dominant and widespread mathemati-
cal mechanism available for the analysis of physical systems. (A prominent
quote from Lord Kelvin better states this sentiment: “Fourier’s theorem is
not only one of the most beautiful results of modern analysis, but it may be
said to furnish an indispensable instrument in the treatment of nearly every
recondite question in modern physics.” By the way, the history of Fourier’s
original work in harmonic analysis, relating to the problem of heat conduc-
tion, is fascinating. References [1] and [2] are good places to start for those
interested in the subject.)
With the advent of the digital computer, the efforts of early digital pro-
cessing pioneers led to the development of the DFT defined as the discrete
frequency-domain sequence X(m), where
N∑ −1
DFT equation X(m)= x(n)e −j2πnm/N. (3–2)
(exponential form): →
n=0
For our discussion of Eq. (3–2), x(n) is a discrete sequence of time-domain
sampled values of the continuous variable x(t). The “e” in Eq. (3–2) is, of
course, the base of natural logarithms and j= –1.
3.1 UNDERSTANDING THE DFT EQUATION
Equation (3–2) has a tangled, almost unfriendly, look about it. Not to worry.
After studying this chapter, Eq. (3–2) will become one of our most familiar
and powerful tools in understanding digital signal processing. Let’s get
started by expressing Eq. (3–2) in a different way and examining it carefully.
From Euler’s relationship,e–jø=cos(ø)–jsin(ø), Eq. (3–2) is equivalent to
N∑–1
[ ]
DFT equation X(m)= x(n)cos(2πnm/N)– jsin(2πnm/N) . (3–3)
(rectangular form): →
n=0
We have separated the complex exponential of Eq. (3–2) into its real and
imaginary components where
X(m) =the mth DFT output component, i.e., X(0),X(1),X(2),X(3), etc.,
m =the index of the DFT output in the frequency domain,
m =0, 1, 2, 3, . . ., N–1,
x(n) =the sequence of input samples, x(0),x(1),x(2),x(3), etc.,

3.1 Understanding the DFT Equation 61
n= the time-domain index of the input samples, n=0, 1, 2, 3, . . ., N–1,
j= –1, and
N= the number of samples of the input sequence and the number of
frequency points in the DFT output.
Although it looks more complicated than Eq. (3–2), Eq. (3–3) turns out to
be easier to understand. (If you’re not too comfortable with it, don’t let the
j = –1 concept bother you too much. It’s merely a convenient abstraction
that helps us compare the phase relationship between various sinusoidal
components of a signal. Chapter 8 discusses the j operator in some detail.)†
The indices for the input samples (n) and the DFT output samples (m) always
go from 0 to N–1 in the standard DFT notation. This means that with Ninput
time-domain sample values, the DFT determines the spectral content of the
input at N equally spaced frequency points. The value N is an important pa-
rameter because it determines how many input samples are needed, the reso-
lution of the frequency-domain results, and the amount of processing time
necessary to calculate an N-point DFT.
It’s useful to see the structure of Eq. (3–3) by eliminating the summation
and writing out all the terms. For example, when N=4, nandmboth go from
0 to 3, and Eq. (3–3) becomes
∑3
X(m)= x(n)[cos(2πnm/4)– jsin(2πnm/4)].
(3–4a)
n=0
Writing out all the terms for the first DFT output term corresponding to m=0,
X(0)=x(0)cos(2π⋅
0
⋅
0/4)–
jx(0)sin(2π⋅
0
⋅
0/4)
+x(1)cos(2π⋅
1
⋅
0/4)–
jx(1)sin(2π⋅
1
⋅
0/4)
+x(2)cos(2π⋅
2
⋅
0/4)–
jx(2)sin(2π⋅
2
⋅
0/4)
(3–4b)
+x(3)cos(2π⋅
3
⋅
0/4)–
jx(3))sin(2π⋅
3
⋅
0/4).
For the second DFT output term corresponding to m=1, Eq. (3–4a) becomes
X(1)=x(0)cos(2π⋅
0
⋅
1/4)–
jx(0)sin(2π⋅
0
⋅
1/4)
+x(1)cos(2π⋅
1
⋅
1/4)–
jx(1)sin(2π⋅
1
⋅
1/4)
+x(2)cos(2π⋅
2
⋅
1/4)–
jx(2)sin(2π⋅
2
⋅
1/4)
(3–4c)
+x(3)cos(2π⋅
3
⋅
1/4)–
jx(3))sin(2π⋅
3
⋅
1/4).
†Instead of the letter j, be aware that mathematicians often use the letter ito represent the –1
operator.

62 The Discrete Fourier Transform
For the third output term corresponding to m=2, Eq. (3–4a) becomes
X(2)=x(0)cos(2π⋅
0
⋅
2/4)–
jx(0)sin(2π⋅
0
⋅
2/4)
+x(1)cos(2π⋅
1
⋅
2/4)–
jx(1)sin(2π⋅
1
⋅
2/4)
+x(2)cos(2π ⋅ 2 ⋅ 2/4)– jx(2)sin(2π⋅ 2 ⋅ 2/4) (3–4d)
+x(3)cos(2π⋅ 3 ⋅ 2/4)– jx(3))sin(2π ⋅ 3 ⋅ 2/4).
Finally, for the fourth and last output term corresponding to m=3, Eq. (3–4a)
becomes
X(3)=x(0)cos(2π⋅
0
⋅
3/4)–
jx(0)sin(2π⋅
0
⋅
3/4)
+x(1)cos(2π⋅
1
⋅
3/4)–
jx(1)sin(2π⋅
1
⋅
3/4)
+x(2)cos(2π⋅
2
⋅
3/4)–
jx(2)sin(2π⋅
2
⋅
3/4)
(3–4e)
+x(3)cos(2π⋅
3
⋅
3/4)–
jx(3))sin(2π⋅
3
⋅
3/4).
⋅
The above multiplication symbol “ “ in Eq. (3–4) is used merely to separate
the factors in the sine and cosine terms. The pattern in Eqs. (3–4b) through
(3–4e) is apparent now, and we can certainly see why it’s convenient to use
the summation sign in Eq. (3–3). Each X(m) DFT output term is the sum of the
point-for-pointproduct between an input sequence of signal values and a com-
plex sinusoid of the form cos(ø) – jsin(ø). The exact frequencies of the differ-
ent sinusoids depend on both the sampling rate f at which the original signal
s
was sampled, and the number of samples N. For example, if we are sampling
a continuous signal at a rate of 500 samples/second and, then, perform a 16-
point DFT on the sampled data, the fundamental frequency of the sinusoids is
f/N = 500/16 or 31.25 Hz. The other X(m) analysis frequencies are integral
s
multiples of the fundamental frequency, i.e.,
⋅
X(0) =1st frequency term, with analysis frequency=0 31.25=0 Hz,
⋅
X(1) =2nd frequency term, with analysis frequency=1 31.25=31.25 Hz,
⋅
X(2) =3rd frequency term, with analysis frequency=2 31.25=62.5 Hz,
⋅
X(3) =4th frequency term, with analysis frequency=3 31.25=93.75 Hz,
. . .
. . .
⋅
X(15) =16th frequency term, with analysis frequency=15 31.25=468.75 Hz.
TheNseparate DFT analysis frequencies are
mf
f (m)= s . (3–5)
analysis
N
So, in this example, the X(0) DFT term tells us the magnitude of any 0 Hz DC
(direct current) component contained in the input signal, the X(1) term speci-

3.1 Understanding the DFT Equation 63
Imaginary axis (j)
X (m)
imag
This point represents the
X (m)
mag complex number
X(m) = X (m) + jX (m).
real imag
ø
0 X r e a l (m) Real axis
Figure 3–1 Trigonometric relationships of an individual DFT X(m) complex output
value.
fies the magnitude of any 31.25 Hz component in the input signal, and the
X(2) term indicates the magnitude of any 62.5 Hz component in the input sig-
nal, etc. Moreover, as we’ll soon show by example, the DFT output terms also
determine the phase relationship between the various analysis frequencies
contained in an input signal.
Quite often we’re interested in both the magnitude and the power (mag-
nitude squared) contained in each X(m) term, and the standard definitions for
right triangles apply here as depicted in Figure 3–1.
If we represent an arbitrary DFT output value, X(m), by its real and
imaginary parts
X(m)=X (m)+ jX (m)=X (m) at an angle of X (m) , (3–6)
real imag mag ¯
the magnitude of X(m) is
X
mag
(m) = |X(m)| = X
real
((mm)) 2 +X
imag
(m)2 . (3–7)
By definition, the phase angle of X(m),X (m), is
ø
⎛X (m)⎞
X (m)=tan −1 ⎜ imag ⎟. (3–8)
ø ⎝X (m)⎠
real
The power of X(m), referred to as the power spectrum, is the magnitude
squared where
X (m)=X (m)2=X (m)2+X (m)2. (3–9)
PS mag real imag
3.1.1 DFT Example 1
The above Eqs. (3–2) and (3–3) will become more meaningful by way of an ex-
ample, so let’s go through a simple one step by step. Let’s say we want to

64 The Discrete Fourier Transform
sample and perform an 8-point DFT on a continuous input signal containing
components at 1 kHz and 2 kHz, expressed as
⋅ ⋅ ⋅ ⋅
x (t) =sin(2π 1000 t) + 0.5sin(2π 2000 t+3π/4. (3–10)
in
To make our example input signal x (t) a little more interesting, we have the
in
2 kHz term shifted in phase by 135° (3π/4 radians) relative to the 1 kHz
sinewave. With a sample rate of f, we sample the input every 1/f = t sec-
s s s
onds. Because N=8, we need 8 input sample values on which to perform the
DFT. So the 8-element sequence x(n) is equal to x (t) sampled at the nt in-
in s
stants in time so that
⋅ ⋅ ⋅ ⋅
x(n) = x (nt) = sin(2π 1000 nt) + 0.5sin(2π 2000 nt+3π/4). (3–11)
in s s s
If we choose to sample x (t) at a rate of f = 8000 samples/second from Eq.
in s
(3–5), our DFT results will indicate what signal amplitude exists in x(n) at the
analysis frequencies of mf/N, or 0 kHz, 1 kHz, 2 kHz, . . ., 7 kHz. With
s
f =8000 samples/second, our eight x(n) samples are
s
x(0)=0.3535, x(1)=0.3535,
x(2)=0.6464, x(3)=1.0607,
(3–11’)
x(4)=0.3535, x(5)=–1.0607,
x(6)=–1.3535, x(7)=–0.3535.
These x(n) sample values are the dots plotted on the solid continuous x (t)
in
curve in Figure 3–2(a). (Note that the sum of the sinusoidal terms in Eq.
(3–10), shown as the dashed curves in Figure 3–2(a), is equal to x (t).)
in
Now we’re ready to apply Eq. (3–3) to determine the DFT of our x(n)
input. We’ll start with m = 1 because the m = 0 case leads to a special result
⋅
that we’ll discuss shortly. So, for m = 1, or the 1 kHz (mf/N = 1 8000/8) DFT
s
frequency term, Eq. (3–3) for this example becomes
∑7
X(1)= x(n)cos(2πn/8)– jx(n)sin(2πn/8). (3–12)
n=0
Next we multiply x(n) by successive points on the cosine and sine curves of
the first analysis frequency that have a single cycle over our eight input sam-
ples. In our example, for m = 1, we’ll sum the products of the x(n) sequence
with a 1 kHz cosine wave and a 1 kHz sinewave evaluated at the angular val-
ues of 2πn/8. Those analysis sinusoids are shown as the dashed curves in Fig-
ure 3–2(b). Notice how the cosine and sinewaves have m = 1 complete cycles
in our sample interval.

3.1 Understanding the DFT Equation 65
1.5
sin(2π1000t) x
in
(t)
1
0.5
(a) 0
–0.5
–1
0.5sin(2π2000t+3π/4)
–1.5 0 1 2 3 4 5 6 7 n
1.5
m = 1
1
0.5
(b) 0
–0.5
–1
–1.5 0 1 2 3 4 5 6 7 n
1.5
m = 2
1
0.5
(c) 0
–0.5
–1
–1.5
0 1 2 3 4 5 6 7 n
1.5
m = 3
1
0.5
(d) 0
–0.5
–1
–1.5
0 1 2 3 4 5 6 7 n
Figure 3–2 DFT Example 1: (a) the input signal; (b) the input signal and the m=1
sinusoids; (c) the input signal and the m=2 sinusoids; (d) the input sig-
nal and the m=3 sinusoids.

66 The Discrete Fourier Transform
Substituting our x(n) sample values into Eq. (3–12) and listing the cosine
terms in the left column and the sine terms in the right column, we have
⋅ ⋅
X(1) = 0.3535 1.0 – j(0.3535 0.0) ←this is the n= 0 term
⋅ ⋅
+ 0.3535 0.707 – j(0.3535 0.707) ←this is the n= 1 term
⋅ ⋅
+ 0.6464 0.0 – j(0.6464 1.0) ←this is the n= 2 term
⋅ ⋅
+ 1.0607 –0.707 – j(1.0607 0.707) . . .
⋅ ⋅
+ 0.3535 –1.0 – j(0.3535 0.0) . . .
⋅ ⋅
– 1.0607 –0.707 – j(–1.0607 –0.707) . . .
⋅ ⋅
– 1.3535 0.0 – j(–1.3535 –1.0) . . .
⋅ ⋅
– 0.3535 0.707 – j(–0.3535 –0.707) ←this is the n= 7 term
= 0.3535 + j0.0
+ 0.250 – j0.250
+ 0.0 – j0.6464
– 0.750 – j0.750
– 0.3535 – j0.0
+ 0.750 – j0.750
+ 0.0 – j1.3535
– 0.250 – j0.250
= 0.0 – j4.0 = 4 (cid:2)–90° .
So we now see that the input x(n) contains a signal component at a frequency
of 1 kHz. Using Eqs. (3–7), (3–8), and (3–9) for our X(1) result, X (1) = 4,
mag
X (1)=16, and X(1)’s phase angle relative to a 1 kHz cosine is X (1)=–90°.
PS ø
For the m = 2 frequency term, we correlate x(n) with a 2 kHz cosine
wave and a 2 kHz sinewave. These waves are the dashed curves in Figure
3–2(c). Notice here that the cosine and sinewaves have m = 2 complete cycles
in our sample interval in Figure 3–2(c). Substituting our x(n) sample values in
Eq. (3–3) for m=2 gives
⋅ ⋅
X(2) = 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.3535 0.0 – j(0.3535 1.0)
⋅ ⋅
+ 0.6464 –1.0 – j(0.6464 0.0)
⋅ ⋅
+ 1.0607 0.0 – j(1.0607 –1.0)
⋅ ⋅
+ 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
–1.0607 0.0 – j(–1.0607 1.0)
⋅ ⋅
–1.3535 –1.0 – j(–1.3535 0.0)
⋅ ⋅
–0.3535 0.0 – j(–0.3535 –1.0)

3.1 Understanding the DFT Equation 67
= 0.3535 + j0.0
+ 0.0 – j0.3535
– 0.6464 – j0.0
– 0.0 + j1.0607
+ 0.3535 – j0.0
+ 0.0 + j1.0607
+ 1.3535 – j0.0
– 0.0 – j0.3535
= 1.414 + j1.414 = 2 (cid:2)45°.
Here our input x(n) contains a signal at a frequency of 2 kHz whose relative
amplitude is 2, and whose phase angle relative to a 2 kHz cosine is 45°. For
the m = 3 frequency term, we correlate x(n) with a 3 kHz cosine wave and a
3 kHz sinewave. These waves are the dashed curves in Figure 3–2(d). Again,
see how the cosine and sinewaves have m = 3 complete cycles in our sample
interval in Figure 3–2(d). Substituting our x(n) sample values in Eq. (3–3) for
m=3 gives
⋅ ⋅
X(3) = 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.3535 –0.707 – j(0.3535 0.707)
⋅ ⋅
+ 0.6464 0.0 – j(0.6464 –1.0)
⋅ ⋅
+ 1.0607 0.707 – j(1.0607 0.707)
⋅ ⋅
+ 0.3535 –1.0 – j(0.3535 0.0)
⋅ ⋅
– 1.0607 0.707 – j(–1.0607 –0.707)
⋅ ⋅
– 1.3535 0.0 – j(–1.3535 1.0)
⋅ ⋅
– 0.3535 –0.707 – j(–0.3535 –0.707)
= 0.3535 + j0.0
– 0.250 – j0.250
+ 0.0 + j0.6464
+ 0.750 – j0.750
– 0.3535 – j0.0
– 0.750 – j0.750
+ 0.0 + j1.3535
+ 0.250 – j0.250
= 0.0 – j0.0 = 0 (cid:2)0°.
Our DFT indicates that x(n) contained no signal at a frequency of 3 kHz. Let’s
continue our DFT for the m = 4 frequency term using the sinusoids in Figure
3–3(a).
So Eq. (3–3) is

68 The Discrete Fourier Transform
1.5
m = 4
1
0.5
(a) 0
–0.5
–1
–1.5
0 1 2 3 4 5 6 7 n
1.5
m = 5
1
0.5
(b) 0
–0.5
–1
–1.5
0 1 2 3 4 5 6 7 n
1.5
m = 6
1
0.5
(c) 0
–0.5
–1
–1.5
0 1 2 3 4 5 6 7 n
1.5
m = 7
1
0.5
(d) 0
–0.5
–1
–1.5
0 1 2 3 4 5 6 7 n
Figure 3–3 DFT Example 1: (a) the input signal and the m= 4 sinusoids; (b) the
input and the m=5 sinusoids; (c) the input and the m=6 sinusoids; (d)
the input and the m=7 sinusoids.

3.1 Understanding the DFT Equation 69
⋅ ⋅
X(4) = 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.3535 –1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.6464 1.0 – j(0.6464 0.0)
⋅ ⋅
+ 1.0607 –1.0 – j(1.0607 0.0)
⋅ ⋅
+ 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
– 1.0607 –1.0 – j(–1.0607 0.0)
⋅ ⋅
– 1.3535 1.0 – j(–1.3535 0.0)
⋅ ⋅
– 0.3535 –1.0 – j(–0.3535 0.0)
= 0.3535 – j0.0
– 0.3535 – j0.0
+ 0.6464 – j0.0
– 1.0607 – j0.0
+ 0.3535 – j0.0
+ 1.0607 – j0.0
– 1.3535 – j0.0
+ 0.3535 – j0.0
= 0.0 – j0.0 = 0 (cid:2)0°.
Our DFT for the m = 5 frequency term using the sinusoids in Figure 3–3(b)
yields
⋅ ⋅
X(5) = 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.3535 –0.707 – j(0.3535 –0.707)
⋅ ⋅
+ 0.6464 0.0 – j(0.6464 1.0)
⋅ ⋅
+ 1.0607 0.707 – j(1.0607 –0.707)
⋅ ⋅
+ 0.3535 –1.0 – j(0.3535 0.0)
⋅ ⋅
– 1.0607 0.707 – j(–1.0607 0.707)
⋅ ⋅
– 1.3535 0.0 – j(–1.3535 –1.0)
⋅ ⋅
– 0.3535 –0.707 – j(–0.3535 0.707)
= 0.3535 – j0.0
– 0.250 + j0.250
+ 0.0 – j0.6464
+ 0.750 + j0.750
– 0.3535 – j0.0
– 0.750 + j0.750
+ 0.0 – j1.3535
+ 0.250 + j0.250
= 0.0 – j.0 = 0 (cid:2) 0° .
For the m=6 frequency term using the sinusoids in Figure 3–3(c), Eq. (3–3) is

70 The Discrete Fourier Transform
⋅ ⋅
X(6) = 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.3535 0.0 – j(0.3535 –1.0)
⋅ ⋅
+ 0.6464 –1.0 – j(0.6464 0.0)
⋅ ⋅
+ 1.0607 0.0 – j(1.0607 1.0)
⋅ ⋅
+ 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
– 1.0607 0.0 – j(–1.0607 –1.0)
⋅ ⋅
– 1.3535 –1.0 – j(–1.3535 0.0)
⋅ ⋅
– 0.3535 0.0 – j(–0.3535 1.0)
= 0.3535 – j0.0
+ 0.0 + j0.3535
– 0.6464 – j0.0
+ 0.0 – j1.0607
+ 0.3535 – j0.0
+ 0.0 – j1.0607
+ 1.3535 – j0.0
+ 0.0 + j0.3535
= 1.414 – j1.414 = 2 (cid:2)–45° .
For the m=7 frequency term using the sinusoids in Figure 3–3(d), Eq. (3–3) is
⋅ ⋅
X(7) = 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.3535 0.707 – j(0.3535 –0.707)
⋅ ⋅
+ 0.6464 0.0 – j(0.6464 –1.0)
⋅ ⋅
+ 1.0607 –0.707 – j(1.0607 –0.707)
⋅ ⋅
+ 0.3535 –1.0 – j(0.3535 0.0)
⋅ ⋅
– 1.0607 –0.707 – j(–1.0607 0.707)
⋅ ⋅
– 1.3535 0.0 – j(–1.3535 1.0)
⋅ ⋅
– 0.3535 0.707 – j(–0.3535 0.707)
= 0.3535 + j0.0
+ 0.250 + j0.250
+ 0.0 + j0.6464
– 0.750 + j0.750
– 0.3535 – j0.0
+ 0.750 + j0.750
+ 0.0 + j1.3535
– 0.250 + j0.250
= 0.0 + j4.0 = 4 (cid:2)90° .
If we plot the X(m) output magnitudes as a function of frequency, we produce
the magnitude spectrum of the x(n) input sequence, shown in Figure 3–4(a).
The phase angles of the X(m) output terms are depicted in Figure 3–4(b).

3.1 Understanding the DFT Equation 71
Magnitude of X(m) Real part of X(m)
4 1.5
3
(a) (c) 1
2
0.5
1
0 0
m m
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
(kHz) (kHz)
Phase of X(m),X (m), in deg. Imag. part of X(m)
f
90 4
45 1 6 2 1 6
(b) 0 (d) 0
m m
–45 2 3 4 5 7 –2 2 3 4 5 7
(kHz) (kHz)
–90 –4
Indicates that the 1 kHz
input tone was a cosine
wave having an initial
phase of –90o.
Figure 3–4 DFT results from Example 1: (a) magnitude of X(m); (b) phase of X(m);
(c) real part of X(m); (d) imaginary part of X(m).
Hang in there; we’re almost finished with our example. We’ve saved the
calculation of the m=0 frequency term to the end because it has a special sig-
nificance. When m = 0, we correlate x(n) with cos(0) – jsin(0) so that Eq. (3–3)
becomes
N∑ −1
X(0)= x(n)[cos(0)– jsin(0)]. (3–13)
n=0
Because cos(0)=1, and sin(0)=0,
N∑ −1
X(0)= x(n). (3–13’)
n=0
We can see that Eq. (3–13’) is the sum of the x(n) samples. This sum is, of
course, proportional to the average of x(n). (Specifically, X(0) is equal to N
times x(n)’s average value.) This makes sense because the X(0) frequency
term is the non-time-varying (DC) component of x(n). If X(0) were nonzero,
this would tell us that the x(n) sequence is riding on a DC bias and has some
nonzero average value. For our specific example input from Eq. (3–10), the
sum, however, is zero. The input sequence has no DC component, so we

72 The Discrete Fourier Transform
know that X(0) will be zero. But let’s not be lazy—we’ll calculate X(0) anyway
just to be sure. Evaluating Eq. (3–3) or Eq. (3–13’) for m=0, we see that
⋅ ⋅
X(0) = 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
+ 0.6464 1.0 – j(0.6464 0.0)
⋅ ⋅
+ 1.0607 1.0 – j(1.0607 0.0)
⋅ ⋅
+ 0.3535 1.0 – j(0.3535 0.0)
⋅ ⋅
– 1.0607 1.0 – j(–1.0607 0.0)
⋅ ⋅
– 1.3535 1.0 – j(–1.3535 0.0)
⋅ ⋅
– 0.3535 1.0 – j(–0.3535 0.0)
X(0) = 0.3535 – j0.0
+ 0.3535 – j0.0
+ 0.6464 – j0.0
+ 1.0607 – j0.0
+ 0.3535 – j0.0
– 1.0607 – j0.0
– 1.3535 – j0.0
– 0.3535 – j0.0
= 0.0 – j0.0 = 0 (cid:2)0°.
So our x(n) had no DC component, and, thus, its average value is zero. Notice
that Figure 3–4 indicates that x (t), from Eq. (3–10), has signal components at
in
1 kHz (m = 1) and 2 kHz (m = 2). Moreover, the 1 kHz tone has a magnitude
twice that of the 2 kHz tone. The DFT results depicted in Figure 3–4 tell us ex-
actly the spectral content of the signal defined by Eqs. (3–10) and (3–11).
While looking at Figure 3–4(b), we might notice that the phase of X(1) is
–90 degrees and ask, “This –90 degrees phase is relative to what?” The answer
is: The DFT phase at the frequency mf/N is relative to a cosine wave at that
s
same frequency of mf/NHz where m= 1, 2, 3, ..., N–1. For example, the phase
s
ofX(1) is –90 degrees, so the input sinusoid whose frequency is 1 · f/N=1000
s
Hz was a cosine wave having an initial phase shift of –90 degrees. From the
trigonometric identity cos(α–90°) = sin(α), we see that the 1000 Hz input tone
was a sinewave having an initial phase of zero. This agrees with our Eq.(3–11).
The phase of X(2) is 45 degrees so the 2000 Hz input tone was a cosine wave
having an initial phase of 45 degrees, which is equivalent to a sinewave having
an initial phase of 135 degrees (3π/4 radians from Eq.(3–11)).
When the DFT input signals are real-valued, the DFT phase at 0 Hz (m=0,
DC) is always zero because X(0) is always real-only as shown by Eq. (3–13’).
The perceptive reader should be asking two questions at this point. First,
what do those nonzero magnitude values at m=6 and m=7 in Figure 3–4(a)
mean? Also, why do the magnitudes seem four times larger than we would

3.2 DFT Symmetry 73
expect? We’ll answer those good questions shortly. The above 8-point DFT ex-
ample, although admittedly simple, illustrates two very important character-
istics of the DFT that we should never forget. First, any individual X(m)
output value is nothing more than the sum of the term-by-term products, a
correlation, of an input signal sample sequence with a cosine and a sinewave
whose frequencies are m complete cycles in the total sample interval of N
samples. This is true no matter what the f sample rate is and no matter how
s
large Nis in an N-point DFT. The second important characteristic of the DFT
of real input samples is the symmetry of the DFT output terms.
3.2 DFT SYMMETRY
Looking at Figure 3–4(a) again, we see that there is an obvious symmetry in
the DFT results. Although the standard DFT is designed to accept complex
input sequences, most physical DFT inputs (such as digitized values of some
continuous signal) are referred to as real; that is, real inputs have nonzero real
sample values, and the imaginary sample values are assumed to be zero.
When the input sequence x(n) is real, as it will be for all of our examples, the
complex DFT outputs for m = 1 to m = (N/2) – 1 are redundant with fre-
quency output values for m>(N/2). The mth DFT output will have the same
magnitude as the (N–m)th DFT output. The phase angle of the DFT’s mth out-
put is the negative of the phase angle of the (N–m)th DFT output. So the mth
and (N–m)th outputs are related by the following
X(m)=|X(m)|at X ø (m) degrees (3–14)
=|X(N–m)|at –X (N–m) degrees
ø
for 1 ≤ m ≤ (N/2)–1. We can state that when the DFT input sequence is real,
X(m) is the complex conjugate of X(N–m), or
X(m)=X*(N–m),† (3–14’)
where the superscript “*” symbol denotes conjugation, and m = 1, 2, 3, . . . ,
N–1.
In our example above, notice in Figures 3–4(b) and 3–4(d) that X(5),
X(6), and X(7) are the complex conjugates of X(3),X(2), and X(1), respectively.
Like the DFT’s magnitude symmetry, the real part of X(m) has what is called
even symmetry,as shown in Figure 3–4(c), while the DFT’s imaginary part has
† Using our notation, the complex conjugate of x= a + jb is defined as x*= a – jb; that is, we
merely change the sign of the imaginary part of x. In an equivalent form, if x=ejø, then x*=e–jø.

74 The Discrete Fourier Transform
odd symmetry, as shown in Figure 3–4(d). This relationship is what is meant
when the DFT is called conjugate symmetric in the literature. It means that if
we perform an N-point DFT on a real input sequence, we’ll get N separate
complex DFT output terms, but only the first N/2+1 terms are independent.
So to obtain the DFT of x(n), we need only compute the first N/2+1 values of
X(m) where 0≤m≤(N/2); the X(N/2+1) to X(N–1) DFT output terms provide
no additional information about the spectrum of the real sequence x(n).
The above N-point DFT symmetry discussion applies to DFTs, whose in-
puts are real-valued, where N is an even number. If N happens to be an odd
number, then only the first (N+1)/2 samples of the DFT are independent. For
example, with a 9-point DFT only the first five DFT samples are independent.
Although Eqs. (3–2) and (3–3) are equivalent, expressing the DFT in the
exponential form of Eq. (3–2) has a terrific advantage over the form of Eq.
(3–3). Not only does Eq. (3–2) save pen and paper, but Eq. (3–2)’s exponentials
are much easier to manipulate when we’re trying to analyze DFT relation-
ships. Using Eq. (3–2), products of terms become the addition of exponents
and, with due respect to Euler, we don’t have all those trigonometric relation-
ships to memorize. Let’s demonstrate this by proving Eq. (3–14) to show the
symmetry of the DFT of real input sequences. Substituting N–m for m in Eq.
(3–2), we get the expression for the (N–m)th component of the DFT:
N∑–1 N∑–1
X(N–m)= x(n)e–j2πn(N−m)/N = x(n)e–j2πnN/Ne–j2πn(–m)/N
n=0 n=0
(3–15)
N∑–1
= x(n)e–j2πnej2πnm/N.
n=0
Becausee–j2πn=cos(2πn)–jsin(2πn)=1
for all integer values of n,
N∑–1
X(N–m)= x(n)e j2πnm/N. (3–15’)
n=0
We see that X(N–m) in Eq. (3–15’) is merely X(m) in Eq. (3–2) with the sign re-
versed on X(m)’s exponent—and that’s the definition of the complex conju-
gate. This is illustrated by the DFT output phase-angle plot in Figure 3–4(b)
for our DFT Example 1. Try deriving Eq. (3–15’) using the cosines and sines of
Eq. (3–3), and you’ll see why the exponential form of the DFT is so convenient
for analytical purposes.
There’s an additional symmetry property of the DFT that deserves men-
tion at this point. In practice, we’re occasionally required to determine the
DFT of real input functions where the input index nis defined over both posi-
tive and negative values. If that real input function is even, then X(m) is al-

3.4 DFT Magnitudes 75
ways real and even; that is, if the real x(n) =x(–n), then, X (m) is in general
real
nonzero and X (m) is zero. Conversely, if the real input function is odd,
imag
x(n)=–x(–n), then X (m) is always zero and X (m) is, in general, nonzero.
real imag
This characteristic of input function symmetry is a property that the DFT
shares with the continuous Fourier transform, and (don’t worry) we’ll cover
specific examples of it later in Section 3.13 and in Chapter 5.
3.3 DFT LINEARITY
The DFT has a very important property known as linearity. This property
states that the DFT of the sum of two signals is equal to the sum of the trans-
forms of each signal; that is, if an input sequence x (n) has a DFT X (m) and
1 1
another input sequence x (n) has a DFT X (m), then the DFT of the sum of
2 2
these sequences x (n)=x (n)+x (n) is
sum 1 2
X (m)=X (m) + X (m). (3–16)
sum 1 2
This is certainly easy enough to prove. If we plug x (n) into Eq. (3–2) to get
sum
X (m), then
sum
N∑–1
X (m)= [x (n)+x (n)]e–j2πnm/N
sum 1 2
n=0
N∑–1 N∑–1
= x (n)e–j2πnm/N + x (n)e–j2πnm/N =X (m)+X (m).
1 2 1 2
n=0 n=0
Without this property of linearity, the DFT would be useless as an analytical
tool because we could transform only those input signals that contain a single
sinewave. The real-world signals that we want to analyze are much more
complicated than a single sinewave.
3.4 DFT MAGNITUDES
The DFT Example 1 results of |X(1)|=4 and |X(2)|=2 may puzzle the reader
because our input x(n) signal, from Eq. (3–11), had peak amplitudes of 1.0 and
0.5, respectively. There’s an important point to keep in mind regarding DFTs
defined by Eq. (3–2). When a real input signal contains a sinewave component,
whose frequency is less than half the f sample rate, of peak amplitude A with
s o
an integral number of cycles over N input samples, the output magnitude of
the DFT for that particular sinewave is M where
r
M =A N/2. (3–17)
r o

76 The Discrete Fourier Transform
If the DFT input is a complex sinusoid of magnitude A (i.e.,A
ej2πfnts)
with an
o o
integer number of cycles over N samples, the M output magnitude of the
c
DFT for that particular sinewave is
M =A N. (3–17’)
c o
As stated in relation to Eq. (3–13’), if the DFT input was riding on a DC bias
value equal to D , the magnitude of the DFT’s X(0) output will be D N.
o o
Looking at the real input case for the 1000 Hz component of Eq. (3–11),
⋅
A = 1 and N = 8, so that M = 1 8/2 = 4, as our example shows. Equation
o real
(3–17) may not be so important when we’re using software or floating-point
hardware to perform DFTs, but if we’re implementing the DFT with fixed-point
hardware, we have to be aware that the output can be as large as N/2 times the
peak value of the input. This means that, for real inputs, hardware memory
registers must be able to hold values as large as N/2 times the maximum am-
plitude of the input sample values. We discuss DFT output magnitudes in fur-
ther detail later in this chapter. The DFT magnitude expressions in Eqs. (3–17)
and (3–17’) are why we occasionally see the DFT defined in the literature as
X'(m)= 1
N∑–1
x(n)e–j2πnm/N. (3–18)
N
n=0
The 1/Nscale factor in Eq. (3–18) makes the amplitudes of X’(m) equal to half
the time-domain input sinusoid’s peak value at the expense of the additional
division by N computation. Thus, hardware or software implementations of
the DFT typically use Eq. (3–2) as opposed to Eq. (3–18). Of course, there are
always exceptions. There are commercial software packages using
X"(m)= 1
N∑–1
x(n)e–j2πnm/N
N
n=0
and
x(n)= 1
N∑–1
X"(m)ej2πnm/N (3–18’)
N
m=0
for the forward and inverse DFTs. (In Section 3.7, we discuss the meaning and
significance of the inverse DFT.) The 1/ N scale factors in Eqs. (3–18’) seem
a little strange, but they’re used so that there’s no scale change when trans-
forming in either direction. When analyzing signal spectra in practice, we’re
normally more interested in the relative magnitudes rather than absolute
magnitudes of the individual DFT outputs, so scaling factors aren’t usually
that important to us.

3.6 DFT Shifting Theorem 77
3.5 DFT FREQUENCY AXIS
The frequency axis mof the DFT result in Figure 3–4 deserves our attention once
again. Suppose we hadn’t previously seen our DFT Example 1, were given the
eight input sample values, from Eq. (3–11’), and were asked to perform an 8-point
DFT on them. We’d grind through Eq. (3–2) and obtain the X(m) values shown in
Figure 3–4. Next we ask, “What’s the frequency of the highest magnitude compo-
nent in X(m) in Hz?” The answer is not “1” kHz. The answer depends on the orig-
inal sample rate f. Without prior knowledge, we have no idea over what time
s
interval the samples were taken, so we don’t know the absolute scale of the X(m)
frequency axis. The correct answer to the question is to take f and plug it into Eq.
s
(3–5) with m=1. Thus, if f =8000 samples/second, then the frequency associated
s
with the largest DFT magnitude term is
⋅
mf 1 8000
f (m)= s = f (1)= =1000 Hz.
analysis N analysis 8
If we said the sample rate f was 75 samples/second, we’d know, from Eq.
s
(3–5), that the frequency associated with the largest magnitude term is now
⋅
1 75
f (1)= =9.375Hz.
analysis
8
OK, enough of this—just remember that the DFT’s frequency spacing (resolu-
tion) is f/N.
s
To recap what we’ve learned so far:
• Each DFT output term is the sum of the term-by-term products of an
input time-domain sequence with sequences representing a sine and a
cosine wave.
• For real inputs, an N-point DFT’s output provides only N/2+1 indepen-
dent terms.
• The DFT is a linear operation.
• The magnitude of the DFT results is directly proportional to N.
• The DFT’s frequency resolution is f/N.
s
It’s also important to realize, from Eq. (3–5), that X(N/2), when m= N/2, corre-
sponds to half the sample rate, i.e., the folding (Nyquist) frequency f/2.
s
3.6 DFT SHIFTING THEOREM
There’s an important property of the DFT known as the shifting theorem. It
states that a shift in time of a periodic x(n) input sequence manifests itself as a

78 The Discrete Fourier Transform
constant phase shift in the angles associated with the DFT results. (We won’t
derive the shifting theorem equation here because its derivation is included in
just about every digital signal processing textbook in print.) If we decide to
sample x(n) starting at n equals some integer k, as opposed to n = 0, the DFT
of those time-shifted sample values is X (m) where
shifted
X
(m)=ej2πkm/NX(m).
(3–19)
shifted
Equation (3–19) tells us that if the point where we start sampling x(n) is
shifted to the right by k samples, the DFT output spectrum of X (m) is
shifted
X(m) with each of X(m)’s complex terms multiplied by the linear phase shift
ej2πkm/N, which is merely a phase shift of 2πkm/N radians or 360km/N de-
grees. Conversely, if the point where we start sampling x(n) is shifted to the
left by k samples, the spectrum of X (m) is X(m) multiplied by
e–j2πkm/N.
shifted
Let’s illustrate Eq. (3–19) with an example.
3.6.1 DFT Example 2
Suppose we sampled our DFT Example 1 input sequence later in time by k=3
samples. Figure 3–5 shows the original input time function,
x (t) = sin(2π1000t) + 0.5sin(2π2000t+3π/4).
in
We can see that Figure 3–5 is a continuation of Figure 3–2(a). Our new x(n) se-
quence becomes the values represented by the solid black dots in Figure 3–5
whose values are
The DFT in Example 1 was taken
over these eight sample values.
1.5
x (t)
in
1
0.5
0
–0.5
–1
–1.5
–3 –2 –1 0 1 2 3 4 5 6 7 n
The DFT in Example 2 is taken
over these eight sample values.
Figure 3–5 Comparison of sampling times between DFT Example 1 and DFT
Example 2.

3.6 DFT Shifting Theorem 79
x(0)=1.0607, x(1)=0.3535,
x(2)=–1.0607, x(3)=–1.3535,
x(4)=–0.3535, x(5)=0.3535,
x(6)=0.3535, x(7)=0.6464. (3–20)
Performing the DFT on Eq. (3–20), X (m) is
shifted
X (m) m X (m)’s X (m)’s X (m)’s X (m)’s
shifted shifted shifted shifted shifted
= magnitude phase real part imaginary part
0 0 0 0 0
1 4 +45 2.8284 2.8284
2 2 –45 1.4142 –1.414
3 0 0 0 0
4 0 0 0 0
5 0 0 0 0
6 2 +45 1.4142 1.4142
7 4 –45 2.8284 –2.828 (3–21)
The values in Eq. (3–21) are illustrated as the dots in Figure 3–6. Notice that
Figure 3–6(a) is identical to Figure 3–4(a). Equation (3–19) told us that the
magnitude of X (m) should be unchanged from that of X(m). That’s a
shifted
comforting thought, isn’t it? We wouldn’t expect the DFT magnitude of our
original periodic x (t) to change just because we sampled it over a different
in
Example 2: Magnitude of X s h i f te d (m) Example 2: Real part of X s h if t e d (m)
4 3
3 2
(a) 2 (c)
1
1
0 0
m m
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
(kHz) (kHz)
Example 2: Phase of X s h i ft e d (m) in degrees Example 2: Imaginary part of X s h if t e d (m)
45 4
2 7 2 2 7
(b) 0 (d) 0
m m
1 3 4 5 6 (kHz) –2 1 3 4 5 6 (kHz)
–45
–4
Figure 3–6 DFT results from Example 2: (a) magnitude of X (m); (b) phase of
shifted
X (m); (c) real part of X (m); (d) imaginary part of X (m).
shifted shifted shifted

80 The Discrete Fourier Transform
time interval. The phase of the DFT result does, however, change depending
on the instant at which we started to sample x (t).
in
By looking at the m = 1 component of X (m), for example, we can
shifted
double-check to see that phase values in Figure 3–6(b) are correct. Using
Eq. (3–19) and remembering that X(1) from DFT Example 1 had a magnitude
of 4 at a phase angle of –90º (or –π/2 radians), k=3 and N=8 so that
⋅ ⋅ ⋅
X (1) =ej2πkm/N X(1) =ej2π3 1/8 4e–jπ/2= 4ej(6π/8 – 4π/8)= 4ejπ/4. (3–22)
shifted
So X (1) has a magnitude of 4 and a phase angle of π/4 or +45°, which is
shifted
what we set out to prove using Eq. (3–19).
3.7 INVERSE DFT
Although the DFT is the major topic of this chapter, it’s appropriate, now, to
introduce the inverse discrete Fourier transform (IDFT). Typically we think of
the DFT as transforming time-domain data into a frequency-domain repre-
sentation. Well, we can reverse this process and obtain the original time-
domain signal by performing the IDFT on the X(m) frequency-domain values.
The standard expressions for the IDFT are
x(n)= 1
N∑–1
X(m)ej2πmn/N (3–23)
N
m=0
and equally,
1
N∑–1
x(n)= X(m)[cos(2πmn/N)+ jsin(2πmn/N)]. (3–23’)
N
m=0
Remember the statement we made in Section 3.1 that a discrete time-
domain signal can be considered the sum of various sinusoidal analytical fre-
quencies and that the X(m) outputs of the DFT are a set of N complex values
indicating the magnitude and phase of each analysis frequency comprising
that sum. Equations (3–23) and (3–23’) are the mathematical expressions of
that statement. It’s very important for the reader to understand this concept.
If we perform the IDFT by plugging our results from DFT Example 1 into Eq.
(3–23), we’ll go from the frequency domain back to the time domain and get
our original real Eq. (3–11’) x(n) sample values of
x(0)=0.3535 + j0.0 x(1)=0.3535 + j0.0
x(2)=0.6464 + j0.0 x(3)=1.0607 + j0.0
x(4)=0.3535 + j0.0 x(5)=–1.0607 + j0.0
x(6)=–1.3535 + j0.0 x(7)=–0.3535 + j0.0.

3.8 DFT Leakage 81
Notice that Eq. (3–23)’s IDFT expression differs from the DFT’s Eq. (3–2)
only by a 1/N scale factor and a change in the sign of the exponent. Other
than the magnitude of the results, every characteristic that we’ve covered
thus far regarding the DFT also applies to the IDFT.
3.8 DFT LEAKAGE
Hold on to your seat now. Here’s where the DFT starts to get really interest-
ing. The two previous DFT examples gave us correct results because the input
x(n) sequences were very carefully chosen sinusoids. As it turns out, the DFT
of sampled real-world signals provides frequency-domain results that can be
misleading. A characteristic known as leakage causes our DFT results to be
only an approximation of the true spectra of the original input signals prior to
digital sampling. Although there are ways to minimize leakage, we can’t
eliminate it entirely. Thus, we need to understand exactly what effect it has on
our DFT results.
Let’s start from the beginning. DFTs are constrained to operate on a fi-
nite set of N input values, sampled at a sample rate of f , to produce an
s
N-point transform whose discrete outputs are associated with the individual
analytical frequencies f (m), with
analysis
mf
f analysis (m)= N s ,wherem=0,1,2,...,N–1. (3–24)
Equation (3–24), illustrated in DFT Example 1, may not seem like a problem,
but it is. The DFT produces correct results only when the input data sequence
contains energy precisely at the analysis frequencies given in Eq. (3–24), at in-
tegral multiples of our fundamental frequency f/N. If the input has a signal
s
component at some intermediate frequency between our analytical frequen-
cies of mf/N, say 1.5f/N, this input signal will show up to some degree in all
s s
of the N output analysis frequencies of our DFT! (We typically say that input
signal energy shows up in all of the DFT’s output bins, and we’ll see, in a mo-
ment, why the phrase “output bins” is appropriate. Engineers often refer to
DFT samples as “bins.” So when you see, or hear, the word bin it merely
means a frequency-domain sample.) Let’s understand the significance of this
problem with another DFT example.
Assume we’re taking a 64-point DFT of the sequence indicated by the
dots in Figure 3–7(a). The sequence is a sinewave with exactly three cycles con-
tained in our N = 64 samples. Figure 3–7(b) shows the first half of the DFT of
the input sequence and indicates that the sequence has an average value of
zero (X(0) = 0) and no signal components at any frequency other than the
m=3 frequency. No surprises so far. Figure 3–7(a) also shows, for example, the
m=4 sinewave analysis frequency, superimposed over the input sequence, to

82 The Discrete Fourier Transform
Input frequency = 3.0 cycles m = 4 analysis frequency
1
0.8
0.6
0.4
0.2
(a) 0
–0.2 Time
–0.4
–0.6
–0.8
–1
DFT output magnitude
35
30
25
20
15
(b)
10
5
0
m
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
(Freq)
Figure 3–7 Sixty-four-point DFT: (a) input sequence of three cycles and the m=4
analysis frequency sinusoid; (b) DFT output magnitude.
remind us that the analytical frequencies always have an integral number of
cycles over our total sample interval of 64 points. The sum of the products of
the input sequence and the m = 4 analysis frequency is zero. (Or we can say,
the correlation of the input sequence and the m=4 analysis frequency is zero.)
The sum of the products of this particular three-cycle input sequence and any
analysis frequency other than m = 3 is zero. Continuing with our leakage ex-
ample, the dots in Figure 3–8(a) show an input sequence having 3.4 cycles over
our N = 64 samples. Because the input sequence does not have an integral
number of cycles over our 64-sample interval, input energy has leaked into all
the other DFT output bins as shown in Figure 3–8(b). The m=4 bin, for exam-
ple, is not zero because the sum of the products of the input sequence and the
m=4 analysis frequency is no longer zero. This is leakage—it causes any input
signal whose frequency is not exactly at a DFT bin center to leak into all of the
other DFT output bins. Moreover, leakage is an unavoidable fact of life when
we perform the DFT on real-world finite-length time sequences.
Now, as the English philosopher Douglas Adams would say, “Don’t
panic.” Let’s take a quick look at the cause of leakage to learn how to predict

3.8 DFT Leakage 83
Input frequency = 3.4 cycles m = 4 analysis frequency
1
0.8
0.6
0.4
0.2
(a) 0
–0.2 Time
–0.4
–0.6
–0.8
–1
30 DFT output magnitude
25
20
15
(b) 10
5
0
m
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
(Freq)
Figure 3–8 Sixty-four-point DFT: (a) 3.4 cycles input sequence and the m = 4
analysis frequency sinusoid; (b) DFT output magnitude.
and minimize its unpleasant effects. To understand the effects of leakage, we
need to know the amplitude response of a DFT when the DFT’s input is an ar-
bitrary, real sinusoid. Although Sections 3.14 and 3.15 discuss this issue in de-
tail, for our purposes, here, we’ll just say that for a real cosine input having
kcycles (kneed not be an integer) in the N-point input time sequence, the am-
plitude response of an N-point DFT bin in terms of the bin index mis approx-
imated by the sinc function
X(m)=
A
o
N⋅sin[π(k–m)]
2
π(k–m)
(3–25)
where A is the peak value of the DFT’s input sinusiod. For our examples
o
here, A is unity. We’ll use Eq. (3–25), illustrated in Figure 3–9(a), to help us
o
determine how much leakage occurs in DFTs. We can think of the curve in
Figure 3–9(a), comprising a main lobe and periodic peaks and valleys known
as sidelobes, as the continuous positive spectrum of an N-point, real cosine

84 The Discrete Fourier Transform
N
2
Discrete DFT sequence defined by
fr C e o q n u t e in n u c o y u s s p , e p c o t s ru it m ive o - f X(m) = N 2 . sin π [ ( π k ( – k– m m ) )]
a discrete cosine
sequence
(a)
m = k Freq
(m)
k–3 k–1 k+1 k+3 k+5
N
2
(b)
Freq
kf s /N (Hz)
(k–3)fs /N (k–1)fs /N (k+1)fs /N (k+3)f s /N (k+5)f s /N
Figure 3–9 DFT positive-frequency response due to an N-point input sequence
containing k cycles of a real cosine: (a) amplitude response as a
function of bin index m; (b) magnitude response as a function of fre-
quency in Hz.
time sequence having k cycles in the N-point input time interval. The DFT’s
outputs are discrete samples that reside on the curves in Figure 3–9; that is,
our DFT output will be a sampled version of the continuous spectrum. (We
show the DFT’s magnitude response to a real input in terms of frequency
(Hz) in Figure 3–9(b).) When the DFT’s input sequence has exactly an integral
knumber of cycles (centered exactly in the m=kbin), no leakage occurs, as in
Figure 3–9, because when the angle in the numerator of Eq. (3–25) is a
nonzero integral multiple of π, the sine of that angle is zero.
By way of example, we can illustrate again what happens when the
input frequency kis not located at a bin center. Assume that a real 8 kHz sinu-
soid, having unity amplitude, has been sampled at a rate of f = 32000 sam-
s
ples/second. If we take a 32-point DFT of the samples, the DFT’s frequency
resolution, or bin spacing, is f/N = 32000/32 Hz = 1.0 kHz. We can predict
s
the DFT’s magnitude response by centering the input sinusoid’s spectral

3.8 DFT Leakage 85
16
Input frequency = 8.0 kHz
(a)
3 4 5 6 7 8 9 10 11 12 13 Freq
(kHz)
10.17
Input frequency = 8.5 kHz
(b)
3 4 5 6 7 8 9 10 11 12 13 Freq
(kHz)
14.05
Input frequency = 8.75 kHz
5.15
(c)
3 4 5 6 7 8 9 10 11 12 13 Freq
(kHz)
Figure 3–10 DFT bin positive-frequency responses: (a) DFT input frequency =
8.0 kHz; (b) DFT input frequency = 8.5 kHz; (c) DFT input fre-
quency=8.75 kHz.
curve at the positive frequency of 8 kHz, as shown in Figure 3–10(a). The dots
show the DFT’s output bin magnitudes.
Again, here’s the important point to remember: the DFT output is a sam-
pled version of the continuous spectral curve in Figure 3–10(a). Those sam-
pled values in the frequency domain, located at mf/N, are the dots in Figure
s
3–10(a). Because the input signal frequency is exactly at a DFT bin center, the
DFT results have only one nonzero value. Stated in another way, when an
input sinusoid has an integral number of cycles over N time-domain input
sample values, the DFT outputs reside on the continuous spectrum at its peak
and exactly at the curve’s zero crossing points. From Eq. (3–25) we know the
peak output magnitude is 32/2=16. (If the real input sinusoid had an ampli-
⋅
tude of 2, the peak of the response curve would be 2 32/2, or 32.) Figure
3–10(b) illustrates DFT leakage where the input frequency is 8.5 kHz, and we
see that the frequency-domain sampling results in nonzero magnitudes for all
DFT output bins. An 8.75 kHz input sinusoid would result in the leaky DFT
output shown in Figure 3–10(c). If we’re sitting at a computer studying leak-
age by plotting the magnitude of DFT output values, of course, we’ll get the
dots in Figure 3–10 and won’t see the continuous spectral curves.

86 The Discrete Fourier Transform
At this point, the attentive reader should be thinking: “If the continuous
spectra that we’re sampling are symmetrical, why does the DFT output in Figure
3–8(b) look so asymmetrical?” In Figure 3–8(b), the bins to the right of the third bin
are decreasing in amplitude faster than the bins to the left of the third bin. “And
another thing, with k= 3.4 and m= 3, from Eq. (3–25) the X(3) bin’s magnitude
should be approximately equal to 24.2—but Figure 3–8(b) shows the X(3) bin
magnitude to be slightly greater than 25. What’s going on here?” We answer
this by remembering what Figure 3–8(b) really represents. When examining a DFT
output, we’re normally interested only in the m=0 to m=(N/2–1) bins. Thus, for
our 3.4 cycles per sample interval example in Figure 3–8(b), only the first 32 bins
are shown. Well, the DFT is periodic in the frequency domain as illustrated in Fig-
ure 3–11. (We address this periodicity issue in Section 3.14.) Upon examining the
DFT’s output for higher and higher frequencies, we end up going in circles, and
the spectrum repeats itself forever.
The more conventional way to view a DFT output is to unwrapthe spec-
trum in Figure 3–11 to get the spectrum in Figure 3–12. Figure 3–12 shows
some of the additional replications in the spectrum for the 3.4 cycles per sam-
ple interval example. Concerning our DFT output asymmetry problem, as
some of the input 3.4-cycle signal amplitude leaks into the 2nd bin, the 1st bin,
and the 0th bin, leakage continues into the –1st bin, the –2nd bin, the –3rd bin,
etc. Remember, the 63rd bin is the –1st bin, the 62nd bin is the –2nd bin, and so
on. These bin equivalencies allow us to view the DFT output bins as if they ex-
tend into the negative-frequency range, as shown in Figure 3–13(a). The result
is that the leakage wraps aroundthem=0 frequency bin, as well as around the
m=Nfrequency bin. This is not surprising, because the m=0 frequency isthe
m = 33
m = 32
m = 31
58
(–6)
60
(–4) (– 6 2 2 ) 0 2 4 6 8 10 12 (F m req)
Figure 3–11 Cyclic representation of the DFT’s spectral replication when the DFT
input is 3.4 cycles per sample interval.

3.8 DFT Leakage 87
Input signal also shows up at Input signal also shows up at 64 – 3.4 = 60.6,
0 – 3.4 = –3.4 cycles/interval and 64 + 3.4 = 67.4 cycles/interval
Input signal is at 3.4
cycles/interval
Spectra repeat in Spectra repeat in
this direction this direction
–12 –10–8 –6 –4 –2 0 2 4 6 8 10 12 52 54 56 58 60 62 64 66 68 70 72 74 76 m
(Freq)
Figure 3–12 Spectral replication when the DFT input is 3.4 cycles per sample
interval.
m=Nfrequency. The leakage wraparound at the m=0 frequency accounts for
the asymmetry around the DFT’s m=3 bin in Figure 3–8(b).
Recall from the DFT symmetry discussion that when a DFT input se-
quencex(n) is real, the DFT outputs from m=0 to m=(N/2–1) are redundant
with frequency bin values for m > (N/2), where N is the DFT size. The mth
DFT output magnitude
(with input = 3.4 cycles/interval)
m = 0
(a)
m
50 52 54 56 58 60 62 0 2 4 6 8 10 12 14
(–14)(–12)(–10)(–8) (–6) (–4) (–2) (Freq)
DFT output magnitude
(with input = 28.6 cycles/interval)
m = N/2
= 32
(b)
18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 m
(Freq)
Figure 3–13 DFT output magnitude: (a) when the DFT input is 3.4 cycles per sam-
ple interval; (b) when the DFT input is 28.6 cycles per sample interval.

88 The Discrete Fourier Transform
DFT output will have the same magnitude as the (N–m)th DFT output. That
is, |X(m)|=|X(N–m)|. What this means is that leakage wraparound also oc-
curs around the m=N/2 bin. This can be illustrated using an input of 28.6 cy-
cles per sample interval (32–3.4) whose spectrum is shown in Figure 3–13(b).
Notice the similarity between Figures 3–13(a) and 3–13(b). So the DFT ex-
hibits leakage wraparound about the m=0 and m=N/2 bins. Minimum leak-
age asymmetry will occur near the N/4th bin as shown in Figure 3–14(a)
where the full spectrum of a 16.4 cycles per sample interval input is provided.
Figure 3–14(b) shows a close-up view of the first 32 bins of the 16.4 cycles per
sample interval spectrum.
You could read about leakage all day. However, the best way to appreci-
ate its effects is to sit down at a computer and use a software program to take
DFTs, in the form of fast Fourier transforms (FFTs), of your personally gener-
ated test signals like those in Figures 3–7 and 3–8. You can then experiment
with different combinations of input frequencies and various DFT sizes.
You’ll be able to demonstrate that the DFT leakage effect is troublesome
DFT output magnitude
(with input = 16.4 cycles/interval)
25
20
15
(a) 10
5
0
0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 m
(Freq)
DFT output magnitude
25 (with input = 16.4
cycles/interval)
20
15
(b)
10
5
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 m
(Freq)
m = N/4 = 16
Figure 3–14 DFT output magnitude when the DFT input is 16.4 cycles per sample
interval: (a) full output spectrum view; (b) close-up view showing
minimized leakage asymmetry at frequency m=N/4.

3.9 Windows 89
because the bins containing low-level signals are corrupted by the sidelobe
levels from neighboring bins containing high-amplitude signals.
Although there’s no way to eliminate leakage completely, an important
technique known as windowingis the most common remedy to reduce its un-
pleasant effects. Let’s look at a few DFT window examples.
3.9 WINDOWS
Windowing reduces DFT leakage by minimizing the magnitude of Eq.
(3–25)’s sinc function’s sin(x)/xsidelobes shown in Figure 3–9. We do this by
forcing the amplitude of the input time sequence at both the beginning and
the end of the sample interval to go smoothly toward a single common ampli-
tude value. Figure 3–15 shows how this process works. If we consider the
infinite-duration time signal shown in Figure 3–15(a), a DFT can only be per-
formed over a finite-time sample interval like that shown in Figure 3–15(c).
We can think of the DFT input signal in Figure 3–15(c) as the product of an
input signal existing for all time, Figure 3–15(a), and the rectangular window
whose magnitude is 1 over the sample interval shown in Figure 3–15(b). Any-
time we take the DFT of a finite-extent input sequence, we are, by default,
multiplying that sequence by a window of all ones and effectively multiply-
ing the input values outside that window by zeros. As it turns out,
Eq. (3–25)’s sinc function’s sin(x)/x shape, shown in Figure 3–9, is caused by
this rectangular window because the continuous Fourier transform of the rec-
tangular window in Figure 3–15(b) isthe sinc function.
As we’ll soon see, it’s the rectangular window’s abrupt changes be-
tween one and zero that are the cause of the sidelobes in the the sin(x)/x
sinc function. To minimize the spectral leakage caused by those sidelobes,
we have to reduce the sidelobe amplitudes by using window functions
other than the rectangular window. Imagine if we multiplied our DFT
input, Figure 3–15(c), by the triangular window function shown in Figure
3–15(d) to obtain the windowed input signal shown in Figure 3–15(e). No-
tice that the values of our final input signal appear to be the same at the be-
ginning and end of the sample interval in Figure 3–15(e). The reduced
discontinuity decreases the level of relatively high-frequency components
in our overall DFT output; that is, our DFT bin sidelobe levels are reduced
in magnitude using a triangular window. There are other window functions
that reduce leakage even more than the triangular window, such as the
Hanning window in Figure 3–15(f). The product of the window in Figure
3–15(f) and the input sequence provides the signal shown in Figure 3–15(g)
as the input to the DFT. Another common window function is the Hamming
window shown in Figure 3–15(h). It’s much like the Hanning window, but
it’s raised on a pedestal.

90 The Discrete Fourier Transform
Sample
interval
(a)
Time
(e)
Time
Sample
Sample interval
interval 1.0
1.0
Rectangular
(b) window (f)
Hanning
window
Time
Time
Sample
interval
(c)
Time (g)
Time
Sample
Sample interval
interval
1.0 1.0
(d) Tr w ia in n d g o u w lar (h) H w a i m nd m o i w ng
Time Time
Figure 3–15 Minimizing sample interval end-point discontinuities: (a) infinite-
duration input sinusoid; (b) rectangular window due to finite-time
sample interval; (c) product of rectangular window and infinite-
duration input sinusoid; (d) triangular window function; (e) product
of triangular window and infinite-duration input sinusoid; (f) Hanning
window function; (g) product of Hanning window and infinite-
duration input sinusoid; (h) Hamming window function.

3.9 Windows 91
Before we see exactly how well these windows minimize DFT leakage,
let’s define them mathematically. Assuming that our original N input signal
samples are indexed by n, where 0 ≤ n ≤ N–1, we’ll call the N time-domain
window coefficients w(n); that is, an input sequence x(n) is multiplied by the
corresponding window w(n) coefficients before the DFT is performed. So the
DFT of the windowed x(n) input sequence, X (m), takes the form of
w
N−1
X (m)= ∑ w(n)⋅x(n)e −j2πnm/N .
w (3–26)
n=0
To use window functions, we need mathematical expressions of them in
terms of n. The following expressions define our window function coeffi-
cients:
Rectangular window: w(n) = 1, for n=0, 1, 2, . . ., N–1. (3–27)
(also called the uniform,
or boxcar, window)
⎧ n
Triangular window: ⎪⎪ , for n=0, 1, 2,...,N/2
(very similar to the w = ⎨ N/2
Bartlett[3], and ⎪ 2− n , for n=N/2+1, N/2+2,...,N−1.
Parzen[4,5] windows) ⎩⎪ N/2
(3–28)
⎛2πn⎞
Hanning window: w(n)=0.5−0.5cos
⎝ ⎠
(also called the raised N
,
cosine, Hann, or
forn= 0, 1, 2, . . ., N–1. (3–29)
von Hann window)
⎛2πn⎞
Hamming window:
w(n)=0.54−0.46cos
⎝ ⎠
N
,
forn= 0, 1, 2, . . ., N–1. (3–30)
If we plot the w(n) values from Eqs. (3–27) through (3–30), we’d get the corre-
sponding window functions like those in Figures 3–15(b), 3–15(d), 3–15(f),
and 3–15(h).†
The rectangular window’s amplitude response is the yardstick we nor-
mally use to evaluate another window function’s amplitude response; that is,
we typically get an appreciation for a window’s response by comparing it to
†In the literature, the equations for window functions depend on the range of the sample index
n. We define n to be in the range 0 < n < N–1. Some authors define n to be in the range
–N/2≤n≤N/2–1, in which case, for example, the expression for the Hanning window would
have a sign change and be w(n)=0.5+0.5cos(2πn/N).

92 The Discrete Fourier Transform
the rectangular window that exhibits the magnitude response shown in Figure
3–9(b). The rectangular window’s sin(x)/xmagnitude response, |W(m)|, is re-
peated in Figure 3–16(a). Also included in Figure 3–16(a) are the Hamming,
Hanning, and triangular window magnitude responses. (The frequency axis in
Figure 3–16 is such that the curves show the response of a single N-point DFT
bin when the various window functions are used.) We can see that the last
three windows give reduced sidelobe levels relative to the rectangular win-
dow. Because the Hamming, Hanning, and triangular windows reduce the
time-domain signal levels applied to the DFT, their main lobe peak values are
reduced relative to the rectangular window. (Because of the near-zerow(n) co-
efficients at the beginning and end of the sample interval, this signal level loss
is called the processing gain, or loss, of a window.) Be that as it may, we’re pri-
marily interested in the windows’ sidelobe levels, which are difficult to see in
Figure 3–16(a)’s linear scale. We will avoid this difficulty by plotting the win-
dows’ magnitude responses on a logarithmic decibel scale, and normalize each
plot so its main lobe peak values are zero dB. (Appendix E provides a discus-
sion of the origin and utility of measuring frequency-domain responses on a
logarithmic scale using decibels.) Defining the log magnitude response to be
|W (m)|, we get |W (m)| by using the expression
dB dB
|W (m)|=20 ⋅ log ⎛ ⎜ |W(m)|⎞ ⎟. (3–31)
dB 10⎝ |W(0)| ⎠
(The |W(0)| term in the denominator of Eq. (3–31) is the value of W(m) at the
peak of the main lobe when m=0.) The |W (m)| curves for the various win-
dB
dow functions are shown in Figure 3–16(b). Now we can really see how the
various window sidelobe responses compare to each other.
Looking at the rectangular window’s magnitude response, we see that its
main lobe is the most narrow, f/N. However, unfortunately, its first sidelobe
s
level is only –13 dB below the main lobe peak, which is not so good. (Notice
that we’re only showing the positive-frequency portion of the window re-
sponses in Figure 3–16.) The triangular window has reduced sidelobe levels,
but the price we’ve paid is that the triangular window’s main lobe width is
twice as wide as that of the rectangular window’s. The various nonrectangular
windows’ wide main lobes degrade the windowed DFT’s frequency resolution
by almost a factor of two. However, as we’ll see, the important benefits of leak-
age reduction usually outweigh the loss in DFT frequency resolution.
Notice the further reduction of the first sidelobe level, and the rapid
sidelobe roll-off of the Hanning window. The Hamming window has even
lower first sidelobe levels, but this window’s sidelobes roll off slowly relative
to the Hanning window. This means that leakage three or four bins away
from the center bin is lower for the Hamming window than for the Hanning
window, and leakage a half-dozen or so bins away from the center bin is
lower for the Hanning window than for the Hamming window.

3.9 Windows 93
Linear-scale window magnitude responses |W(m)|
1
Rectangular (dotted)
0.8
Hamming (dashed-dot)
0.6
(a)
Triangular (dashed)
0.4
Hanning (solid)
0.2
0
0 f /N 2f /N 4f /N Freq
s s s
Logarithmic-scale window magnitude responses |W (m)| in dB
dB
0
Hanning (solid)
-10
Rectangular (dotted)
Triangular (dashed)
-20
(b)
-30
Hamming (dashed-dot)
-40
-50
-60
0 f /N 2f /N 4f /N Freq
s s s
Figure 3–16 Window magnitude responses: (a) |W(m)| on a linear scale; (b)
|W (m)| on a normalized logarithmic scale.
dB

94 The Discrete Fourier Transform
When we apply the Hanning window to Figure 3–8(a)’s 3.4 cycles per
sample interval example, we end up with the DFT input shown in Figure
3–17(a) under the Hanning window envelope. The DFT outputs for the win-
dowed waveform are shown in Figure 3–17(b) along with the DFT results
with no windowing, i.e., the rectangular window. As we expected, the shape
of the Hanning window’s response looks broader and has a lower peak am-
plitude, but its sidelobe leakage is noticeably reduced from that of the rectan-
gular window.
Windowed input signal
1
0.8 Hanning
window function
0.6
0.4
0.2
(a) 0
n
–0.2
(Time)
–0.4
–0.6
–0.8 x(n) = [0.5 – 0.5cos(2πn/64)] . [sin(2π3.4n/64)]
–1 window function 3.4-cycle sinewave
DFT output magnitudes
25
20
15
(b)
Rectangular window response
Hanning window response
10
5
0
0 1 2 3 4 5 6 7 8 9 101112131415161718 2728293031 m
(Freq)
Figure 3–17 Hanning window: (a) 64-sample product of a Hanning window and
a 3.4 cycles per sample interval input sinewave; (b) Hanning DFT out-
put response versus rectangular window DFT output response.

3.9 Windows 95
We can demonstrate the benefit of using a window function to help us
detect a low-level signal in the presence of a nearby high-level signal. Let’s
add 64 samples of a 7 cycles per sample interval sinewave, with a peak ampli-
tude of only 0.1, to Figure 3–8(a)’s unity-amplitude 3.4 cycles per sample
sinewave. When we apply a Hanning window to the sum of these sinewaves,
we get the time-domain input shown in Figure 3–18(a). Had we not win-
dowed the input data, our DFT output would be the squares in Figure 3–18(b)
where DFT leakage causes the input signal component at m = 7 to be barely
discernible. However, the DFT of the windowed data shown as the triangles
Windowed input signal Window function 3.4- and 7-cycle sinewaves
1
x(n) = [0.5 – 0.5cos(2πn/64)] . [sin(2π3.4n/64) +0.1sin(2π7n/64)]
0.8
0.6
0.4
0.2
(a) 0
n
–0.2 (Time)
–0.4
–0.6
–0.8
–1
–1.2
DFT output magnitudes
25
20
Rectangular window response
Hanning window response
15
(b)
10
Signal component
atm = 7
5
0
m
0 1 2 3 4 5 6 7 8 9 10111213141516171819 28293031
(Freq)
Figure 3–18 Increased signal detection sensitivity afforded using windowing: (a) 64-
sample product of a Hanning window and the sum of a 3.4 cycles and
a 7 cycles per sample interval sinewaves; (b) reduced leakage Han-
ning DFT output response versus rectangular window DFT output
response.

96 The Discrete Fourier Transform
in Figure 3–18(b) makes it easier for us to detect the presence of the m=7 sig-
nal component. From a practical standpoint, people who use the DFT to per-
form real-world signal detection have learned that their overall frequency
resolution and signal sensitivity are affected much more by the size and
shape of their window function than the mere size of their DFTs.
As we become more experienced using window functions on our DFT
input data, we’ll see how different window functions have their own individ-
ual advantages and disadvantages. Furthermore, regardless of the window
function used, we’ve decreased the leakage in our DFT output from that of
the rectangular window. There are many different window functions de-
scribed in the literature of digital signal processing—so many, in fact, that
they’ve been named after just about everyone in the digital signal processing
business. It’s not that clear that there’s a great deal of difference among many
of these window functions. What we find is that window selection is a trade-
off between main lobe widening, first sidelobe levels, and how fast the side-
lobes decrease with increased frequency. The use of any particular window
depends on the application[5], and there are many applications.
Windows are used to improve DFT spectrum analysis accuracy[6], to de-
sign digital filters[7,8], to simulate antenna radiation patterns, and even in the
hardware world to improve the performance of certain mechanical force to
voltage conversion devices[9]. So there’s plenty of window information avail-
able for those readers seeking further knowledge. (The mother of all technical
papers on windows is that by Harris[10]. Auseful paper by Nuttall corrected
and extended some portions of Harris’s paper[11].) Again, the best way to ap-
preciate windowing effects is to have access to a computer software package
that contains DFT, or FFT, routines and start analyzing windowed signals. (By
the way, while we delayed their discussion until Section 5.3, there are two
other commonly used window functions that can be used to reduce DFT leak-
age. They’re the Chebyshev and Kaiser window functions, which have ad-
justable parameters, enabling us to strike a compromise between widening
main lobe width and reducing sidelobe levels.)
3.10 DFT SCALLOPING LOSS
Scalloping is the name used to describe fluctuations in the overall magnitude
response of an N-point DFT. Although we derive this fact in Section 3.16, for
now we’ll just say that when no input windowing function is used, the
sin(x)/xshape of the sinc function’s magnitude response applies to each DFT
output bin. Figure 3–19(a) shows a DFT’s aggregate magnitude response by
superimposing several sin(x)/x bin magnitude responses.† (Because the sinc
†Perhaps Figure 3-19(a) is why individual DFT outputs are called “bins.” Any signal energy under
a sin(x)/xcurve will show up in the enclosed storage compartmentof that DFT’s output sample.

3.10 DFT Scalloping Loss 97
sin(x)
x
(a)
Freq
mfs (m+2)fs (m+4)fs
N N N
(m+1)fs (m+3)fs (m+5)fs
N N N
1.0
0.637
(b)
Freq
mfs (m+2)fs (m+4)fs
N N N
Figure 3–19 DFT bin magnitude response curves: (a) individual sin(x)/xresponses
for each DFT bin; (b) equivalent overall DFT magnitude response.
function’s sidelobes are not key to this discussion, we don’t show them in
Figure 3–19(a).) Notice from Figure 3–19(b) that the overall DFT frequency-
domain response is indicated by the bold envelope curve. This rippled curve,
also called the picket fence effect, illustrates the processing loss for input fre-
quencies between the bin centers.
From Figure 3–19(b), we can determine that the magnitude of the DFT
response fluctuates from 1.0, at bin center, to 0.637 halfway between bin cen-
ters. If we’re interested in DFT output power levels, this envelope ripple ex-
hibits a scalloping loss of almost –4 dB halfway between bin centers. Figure
3–19 illustrates a DFT output when no window (i.e., a rectangular window) is
used. Because nonrectangular window functions broaden the DFT’s main
lobe, their use results in a scalloping loss that will not be as severe as with the
rectangular window[10,12]. That is, their wider main lobes overlap more and
fill in the valleys of the envelope curve in Figure 3–19(b). For example, the
scalloping loss of a Hanning window is approximately 0.82, or –1.45 dB,
halfway between bin centers. Scalloping loss is not, however, a severe prob-
lem in practice. Real-world signals normally have bandwidths that span
many frequency bins so that DFT magnitude response ripples can go almost
unnoticed. Let’s look at a scheme called zero padding that’s used to both allevi-
ate scalloping loss effects and to improve the DFT’s frequency granularity.

98 The Discrete Fourier Transform
3.11 DFT RESOLUTION, ZERO PADDING,
AND FREQUENCY-DOMAIN SAMPLING
One popular method used to improve DFT spectral estimation is known as
zero padding. This process involves the addition of zero-valued data samples
to an original DFT input sequence to increase the total number of input data
samples. Investigating this zero-padding technique illustrates the DFT’s im-
portant property of frequency-domain sampling alluded to in the discussion
on leakage. When we sample a continuous time-domain function, having a
continuous Fourier transform (CFT), and take the DFT of those samples, the
DFT results in a frequency-domain sampled approximation of the CFT. The
more points in our DFT, the better our DFT output approximates the CFT.
To illustrate this idea, suppose we want to approximate the CFT of the
continuousf(t) function in Figure 3–20(a). This f(t) waveform extends to infin-
ity in both directions but is nonzero only over the time interval of T seconds.
If the nonzero portion of the time function is a sinewave of three cycles in T
seconds, the magnitude of its CFT is shown in Figure 3–20(b). (Because the
CFT is taken over an infinitely wide time interval, the CFT has infinitesimally
small frequency resolution, resolution so fine-grained that it’s continuous.)
It’s this CFT that we’ll approximate with a DFT.
Suppose we want to use a 16-point DFT to approximate the CFT of f(t) in
Figure 3–20(a). The 16 discrete samples of f(t), spanning the three periods of
f(t)’s sinusoid, are those shown on the left side of Figure 3–21(a). Applying
those time samples to a 16-point DFT results in discrete frequency-domain
T
− ∞ ∞
(a)
f(t) Continuous
time
Continuous Fourier
transform of f(t)
(b)
1/T 2/T 3/T 4/T 5/T Continuous
frequency
Figure 3–20 Continuous Fourier transform: (a) continuous time-domain f(t) of a
truncated sinusoid of frequency 3/T; (b) continuous Fourier transform
off(t).

3.11 DFT Resolution, Zero Padding, and Frequency-Domain Sampling 99
Input amplitude DFT magnitude
8
1
6
0.5
4
(a) 0
Time
0 1 2 3 4 5 6 7 8 9 101112131415 2
–0.5
0
Freq
–1 0 1 2 3 4 5 6
Input amplitude DFT magnitude
8
1
6
0.5
4
(b) 0
0 3 6 9 12 15 18 21 24 27 30 Time 2
–0.5
0
–1 0 1 2 3 4 5 6 7 8 9 10 11 12 Freq
Input amplitude DFT magnitude
8
1
6
0.5
4
(c) 0
0 5 10 1520 2530 35 4045 5055 60 Time 2
–0.5
0
–1 0 2 4 6 8 10 12 14 16 18 20 22 24 Freq
Input amplitude DFT magnitude
8
1
6
0.5
4
(d) 0
121 Time 2
–0.5
0
–1 0 4 8 12 16 20 24 28 3236 40 44 48 Freq
Figure 3–21 DFT frequency-domain sampling: (a) 16 input data samples and
N=16; (b) 16 input data samples, 16 padded zeros, and N=32; (c)
16 input data samples, 48 padded zeros, and N= 64; (d) 16 input
data samples, 112 padded zeros, and N=128.
samples, the positive frequencies of which are represented by the dots on the
right side of Figure 3–21(a). We can see that the DFT output comprises sam-
ples of Figure 3–20(b)’s CFT. If we append (or zero-pad) 16 zeros to the input
sequence and take a 32-point DFT, we get the output shown on the right side
of Figure 3–21(b), where we’ve increased our DFT frequency sampling by a
factor of two. Our DFT is sampling the input function’s CFT more often now.
Adding 32 more zeros and taking a 64-point DFT, we get the output shown

100 The Discrete Fourier Transform
on the right side of Figure 3–21(c). The 64-point DFT output now begins to
show the true shape of the CFT. Adding 64 more zeros and taking a 128-point
DFT, we get the output shown on the right side of Figure 3–21(d). The DFT
frequency-domain sampling characteristic is obvious now, but notice that the
bin index for the center of the main lobe is different for each of the DFT out-
puts in Figure 3–21.
Does this mean we have to redefine the DFT’s frequency axis when
using the zero-padding technique? Not really. If we perform zero padding on
Lnonzero input samples to get a total of Ntime samples for an N-point DFT,
the zero-padded DFT output bin center frequencies are related to the original
f by our old friend Eq. (3–5), or
s
mf
center frequency of the mth bin= s . (3–32)
N
So in our Figure 3–21(a) example, we use Eq. (3–32) to show that although the
zero-padded DFT output bin index of the main lobe changes as N increases,
the zero-padded DFT output frequency associated with the main lobe re-
mains the same. The following list shows how this works:
Main lobe peak Frequency of main lobe
Figure no. located at m= L= N= peak relative to f =
s
Figure 3–21(a) 3 16 16 3f/16
s
⋅
Figure 3–21(b) 6 16 32 6 f/32 = 3f/16
s s
⋅
Figure 3–21(c) 12 16 64 12 f/64 = 3f/16
s s
⋅
Figure 3–21(d) 24 16 128 24 f/128 = 3f/16
s s
Do we gain anything by appending more zeros to the input sequence
and taking larger DFTs? Not really, because our 128-point DFT is sampling
the input’s CFT sufficiently now in Figure 3–21(d). Sampling it more often
with a larger DFT won’t improve our understanding of the input’s frequency
content. The issue here is that adding zeros to an input sequence will improve
our DFT’s output resolution, but there’s a practical limit on how much we
gain by adding more zeros. For our example here, a 128-point DFT shows us
the detailed content of the input spectrum. We’ve hit a law of diminishing re-
turnshere. Performing a 256-point or 512-point DFT, in our case, would serve
little purpose.† There’s no reason to oversample this particular input se-
†Notice that the DFT sizes (N) we’ve discussed are powers of 2 (64, 128, 256, 512). That’s be-
cause we actually perform DFTs using a special algorithm known as the fast Fourier transform
(FFT). As we’ll see in Chapter 4, the typical implementation of the FFT requires that N be a
power of two.

3.11 DFT Resolution, Zero Padding, and Frequency-Domain Sampling 101
quence’s CFT. Of course, there’s nothing sacred about stopping at a 128-point
DFT. Depending on the number of samples in some arbitrary input sequence
and the sample rate, we might, in practice, need to append any number of
zeros to get some desired DFT frequency resolution.
There are two final points to be made concerning zero padding. First,
the DFT magnitude expressions in Eqs. (3–17) and (3–17’) don’t apply if zero
padding is being used. If we perform zero padding on Lnonzero samples of a
sinusoid whose frequency is located at a bin center to get a total of N input
samples for an N-point DFT, we must replace the NwithLin Eqs. (3–17) and
(3–17’) to predict the DFT’s output magnitude for that particular sinewave.
Second, in practical situations, if we want to perform both zero padding and
windowing on a sequence of input data samples, we must be careful not to
apply the window to the entire input including the appended zero-valued
samples. The window function must be applied only to the original nonzero
time samples; otherwise the padded zeros will zero outand distort part of the
window function, leading to erroneous results. (Section 4.2 gives additional
practical pointers on performing the DFT using the FFT algorithm to analyze
real-world signals.)
To digress slightly, now’s a good time to define the term discrete-time
Fourier transform (DTFT) which the reader may encounter in the literature.
The DTFT is the continuous Fourier transform of an L-point discrete time-
domain sequence, and some authors use the DTFT to describe many of the
digital signal processing concepts we’ve covered in this chapter. On a com-
puter we can’t perform the DTFT because it has an infinitely fine frequency
resolution—but we can approximate the DTFT by performing an N-point
DFT on an L-point discrete time sequence where N > L. That is, in fact, what
we did in Figure 3–21 when we zero-padded the original 16-point time
sequence. (When N=L,the DTFT approximation is identical to the DFT.)
To make the connection between the DTFT and the DFT, know that the
infinite-resolution DTFT magnitude (i.e., continuous Fourier transform mag-
nitude) of the 16 nonzero time samples in Figure 3–21(a) is the shaded
sin(x)/x-like spectral function in Figure 3–21. Our DFTs approximate (sam-
ple) that function. Increased zero padding of the 16 nonzero time samples
merely interpolates our DFT’s sampled version of the DTFT function with
smaller and smaller frequency-domain sample spacing.
Please keep in mind, however, that zero padding does not improve our
ability to resolve, to distinguish between, two closely spaced signals in the
frequency domain. (For example, the main lobes of the various spectra in Fig-
ure 3–21 do not change in width, if measured in Hz, with increased zero
padding.) To improve our true spectral resolution of two signals, we need
more nonzero time samples. The rule by which we must live is: To realize F
res
Hz spectral resolution, we must collect 1/F seconds, worth of nonzero time
res
samples for our DFT processing.

102 The Discrete Fourier Transform
We’ll discuss applications of time-domain zero padding in Section 13.15,
revisit the DTFT in Section 3.14, and frequency-domain zero padding in Sec-
tion 13.28.
3.12 DFT PROCESSING GAIN
There are two types of processing gain associated with DFTs. People who use
the DFT to detect signal energy embedded in noise often speak of the DFT’s
processing gainbecause the DFT can pullsignals out of background noise. This
is due to the inherent correlation gain that takes place in any N-point DFT. Be-
yond this natural processing gain, additional integration gainis possible when
multiple DFT outputs are averaged. Let’s look at the DFT’s inherent process-
ing gain first.
3.12.1 Processing Gain of a Single DFT
The concept of the DFT having processing gain is straightforward if we think
of a particular DFT bin output as the output of a narrowband filter. Because a
DFT output bin has the amplitude response of the sin(x)/xfunction, that bin’s
output is primarily due to input energy residing under, or very near, the bin’s
main lobe. It’s valid to think of a DFT bin as a kind of bandpass filter whose
band center is located at mf/N. We know from Eq. (3–17) that the maximum
s
possible DFT output magnitude increases as the number of points (N) in a
DFT increases. Also, as N increases, the DFT output bin main lobes become
narrower. So a DFT output bin can be treated as a bandpass filter whose gain
can be increased and whose bandwidth can be reduced by increasing the
value of N. Decreasing a bandpass filter’s bandwidth is useful in energy de-
tection because the frequency resolution improves in addition to the filter’s
ability to minimize the amount of background noise that resides within its
passband. We can demonstrate this by looking at the DFT of a spectral tone (a
constant-frequency sinewave) added to random noise. Figure 3–22(a) is a log-
arithmic plot showing the first 32 outputs of a 64-point DFT when the input
tone is at the center of the DFT’s m = 20th bin. The output power levels
(DFT magnitude squared) in Figure 3–22(a) are normalized so that the
highest bin output power is set to 0 dB. Because the tone’s original signal
power is below the average noise power level, the tone is a bit difficult to
detect when N = 64. (The time-domain noise, used to generate Figure
3–22(a), has an average value of zero, i.e., no DC bias or amplitude off-
set.) If we quadruple the number of input samples and increase the DFT
size to N = 256, we can now see the tone power raised above the average
background noise power as shown for m = 80 in Figure 3–22(b). Increas-
ing the DFT’s size to N = 1024 provides additional processing gain to pull
the tone further up out of the noise as shown in Figure 3–22(c).

3.12 DFT Processing Gain 103
Bin power in dB
0
–5
(a) –10
–15
–20
–25
0 5 10 15 20 25 30 m
DFT bin number
Bin power in dB
0
–5
SNR
–10
(b) –15
–20
–25
–30
–35
0 20 40 60 80 100 120 m
DFT bin number
Bin power in dB
0
–10 SNR
(c)
–20
–30
–40
0 100 200 300 400 500 m
DFT bin number
Figure 3–22 Single DFT processing gain: (a) N=64; (b) N=256; (c)N=1024.
To quantify the idea of DFT processing gain, we can define a signal-to-
noise ratio (SNR) as the DFT’s output signal-power levelover the average output
noise-power level. (In practice, of course, we like to have this ratio as large as
possible.) For several reasons, it’s hard to say what any given single DFT out-
put SNR will be. That’s because we can’t exactly predict the energy in any
givenNsamples of random noise. Also, if the input signal frequency is not at
bin center, leakage will raise the effective background noise and reduce the

104 The Discrete Fourier Transform
35
Input SNR
30 = +6 dB
0 dB
25
(a) 20
15
10
5
0 200 400 600 800 1000 N
35
Input SNR
30 = +6 dB
0 dB
25
(b) 20
15
10
5
101 102 103 N
Figure 3–23 DFT processing gain versus number of DFT points Nfor various input
signal-to-noise ratios: (a) linear Naxis; (b) logarithmic Naxis.
DFT’s output SNR. In addition, any window being used will have some effect
on the leakage and, thus, on the output SNR. What we’ll see is that the DFT’s
output SNR increases as Ngets larger because a DFT bin’s output noise stan-
dard deviation (rms) value is proportional to N, and the DFT’s output mag-
nitude for the bin containing the signal tone is proportional to N.† More
generally for real inputs, if N > N’, an N-point DFT’s output SNR increases
N
over the N’-point DFT SNR by the following relationship:
N
⎛ N ⎞
SNR = SNR +10log . (3–33)
N N' 10⎝ ⎠
N'
If we increase a DFT’s size from N’ to N=2N’, from Eq. (3–33), the DFT’s out-
put SNR increases by 3 dB. So we say that a DFT’s processing gain increases
by 3 dB whenever N is doubled. Be aware that we may double a DFT’s size
and get a resultant processing gain of less than 3 dB in the presence of ran-
dom noise; then again, we may gain slightly more than 3 dB. That’s the na-
ture of random noise. If we perform many DFTs, we’ll see an average
processing gain, shown in Figure 3–23(a), for various input signal SNRs. Be-
cause we’re interested in the slope of the curves in Figure 3–23(a), we plot
those curves on a logarithmic scale for N in Figure 3–23(b) where the curves
straighten out and become linear. Looking at the slope of the curves in Figure
3–23(b), we can now see the 3 dB increase in processing gain as Ndoubles so
†rms= root mean square.

3.13 The DFT of Rectangular Functions 105
long as Nis greater than 20 or 30 and the signal is not overwhelmed by noise.
There’s nothing sacred about the absolute values of the curves in Figures
3–23(a) and 3–23(b). They were generated through a simulation of noise and a
tone whose frequency was at a DFT bin center. Had the tone’s frequency been
between bin centers, the processing gain curves would have been shifted
downward, but their shapes would still be the same;†that is, Eq. (3–33) is still
valid regardless of the input tone’s frequency.
3.12.2 Integration Gain Due to Averaging Multiple DFTs
Theoretically, we could get very large DFT processing gains by increasing the
DFT size arbitrarily. The problem is that the number of necessary DFT multi-
plications increases proportionally to N2, and larger DFTs become very com-
putationally intensive. Because addition is easier and faster to perform than
multiplication, we can average the outputs of multiple DFTs to obtain further
processing gain and signal detection sensitivity. The subject of averaging mul-
tiple DFT outputs is covered in Section 11.3.
3.13 THE DFT OF RECTANGULAR FUNCTIONS
We continue this chapter by providing the mathematical details of two impor-
tant aspects of the DFT. First, we obtain the expressions for the DFT of a rec-
tangular function (rectangular window), and then we’ll use these results to
illustrate the magnitude response of the DFT. We’re interested in the DFT’s
magnitude response because it provides an alternate viewpoint to under-
stand the leakage that occurs when we use the DFT as a signal analysis tool.
One of the most prevalent and important computations encountered in
digital signal processing is the DFT of a rectangular function. We see it in
sampling theory, window functions, discussions of convolution, spectral
analysis, and in the design of digital filters. As common as it is, however, the
literature covering the DFT of rectangular functions can be confusing to the
digital signal processing beginner for several reasons. The standard mathe-
matical notation is a bit hard to follow at first, and sometimes the equations
are presented with too little explanation. Compounding the problem, for the
beginner, are the various expressions of this particular DFT. In the literature,
we’re likely to find any one of the following forms for the DFT of a rectangu-
lar function:
sin(x) sin(x) sin(Nx/2)
DFT = , or , or . (3–34)
rect.function sin(x/N) x sin(x/2)
†The curves would be shifted downward, indicating a lower SNR, because leakage would raise
the average noise-power level, and scalloping loss would reduce the DFT bin’s output power
level.

106 The Discrete Fourier Transform
In this section we’ll show how the forms in Eq. (3–34) were obtained, see
how they’re related, and create a kind of Rosetta Stone table allowing us to
move back and forth between the various DFT expressions. Take a deep breath
and let’s begin our discussion with the definition of a rectangular function.
3.13.1 DFT of a General Rectangular Function
Ageneral rectangular function x(n) can be defined as Nsamples containing K
unity-valued samples as shown in Figure 3–24. The full N-point sequence,
x(n), is the rectangular function that we want to transform. We call this the
general form of a rectangular function because the K unity samples begin at
an arbitrary index value of –n . Let’s take the DFT of x(n) in Figure 3–24 to get
o
our desired X(m). Using m as our frequency-domain sample index, the ex-
pression for an N-point DFT is
N∑/2
X(m)= x(n)e–j2πnm/N. (3–35)
n=–(N/2)+1
With x(n) being nonzero only over the range of –n ≤ n ≤ –n + (K–1), we can
o o
modify the summation limits of Eq. (3–35) to express X(m) as
–n +(K–1)
X(m)= o∑ 1 ⋅ e–j2πnm/N (3–36)
,
n=–n
o
because only the Ksamples contribute to X(m). That last step is important be-
cause it allows us to eliminate the x(n) terms and make Eq. (3–36) easier to
handle. To keep the following equations from being too messy, let’s use the
dummy variable q=2πm/N.
OK, here’s where the algebra comes in. Over our new limits of summa-
tion, we eliminate the factor of one and Eq. (3–36) becomes
x(n)
K
1
0 n
–N/2+1 n=–no n = –n o + (K–1) N/2
Figure 3–24 Rectangular function of width K samples defined over N samples
whereK<N.

3.13 The DFT of Rectangular Functions 107
–n +(K–1)
o∑
X(q)= e–jqn
n=–n
o
=e–jq(–n
o
)+e–jq(–n
o
+1)+e–jq(–n
o
+2)+...+e–jq(–n
o
+(K–1))
=e–jq(–n
o
)e–j0q +e–jq(–n
o
)e–j1q +e–jq(–n
o
)e–j2q +...+e–jq(–n
o
)e––jq(K–1)
[ ]
=ejq(n o )⋅ e–j0q +e–j1q +e–j2q +...+ejq(K–1) .
(3–37)
The series inside the brackets of Eq. (3–37) allows the use of a summation,
such as
∑K–1
X(q)=ejq(n o) e–jpq. (3–38)
p=0
Equation (3–38) certainly doesn’t look any simpler than Eq. (3–36), but it is.
Equation (3–38) is a geometric seriesand, from the discussion in Appendix B, it
can be evaluated to the closed form of
∑K–1 1–e–jqK
e–jpq = . (3–39)
1–e–jq
p=0
We can now simplify Eq. (3–39)—here’s the clever step. If we multiply and di-
vide the numerator and denominator of Eq. (3–39)’s right-hand side by the
appropriate half-angled exponentials, we break the exponentials into two
parts and get
∑K–1
e–jpq =
e–jqK/2(ejqK/2 –e–jqK/2)
p=0
e–jq/2(ejq/2 –e–jq/2)
=e–jq(K–1)/2
⋅(ejqK/2 –e–jqK/2)
. (3–40)
(ejq/2 –e–jq/2)
Let’s pause for a moment here to remind ourselves where we’re going.
We’re trying to get Eq. (3–40) into a usable form because it’s part of Eq.
(3–38) that we’re using to evaluate X(m) in Eq. (3–36) in our quest for an
understandable expression for the DFT of a rectangular function.

108 The Discrete Fourier Transform
Equation (3–40) looks even more complicated than Eq. (3–39), but things
can be simplified inside the parentheses. From Euler’s equation,
sin(ø)=(ejø–e–jø)/2j, Eq. (3–40) becomes
∑K–1
e–jpq =e–jq(K–1)/2
⋅2jsin(qK/2)
2jsin(q/2)
p=0
⋅sin(qK/2)
=e–jq(K–1)/2 . (3–41)
sin(q/2)
Substituting Eq. (3–41) for the summation in Eq. (3–38), our expression for
X(q) becomes
X(q)=ejq(n
o
) ⋅ e–jq(K–1)/2 ⋅sin(qK/2)
sin(q/2)
=ejq(n o –(K–1)/2)
⋅sin(qK/2)
. (3–42)
sin(q/2)
Returning our dummy variable qto its original value of 2πm/N,
X(m)=ej(2πm/N)(n o –(K–1)/2)
⋅sin(2πmK/2N)
, or
sin(2πm/2N)
General form of the
Dirichlet kernel:→ X(m)=ej(2πm/N)(n o –(K–1)/2)
⋅sin(πmK/N)
.
sin(πm/N)
(3–43)
So there it is (whew!). Equation (3–43) is the general expression for the DFT of
the rectangular function as shown in Figure 3–24. Our X(m) is a complex ex-
pression (pun intended) where a ratio of sine terms is the amplitude of X(m)
and the exponential term is the phase angle of X(m).†The ratio of sines factor
in Eq. (3–43) lies on the periodic curve shown in Figure 3–25(a), and like all
N-point DFT representations, the periodicity of X(m) is N. This curve is
known as the Dirichlet kernel (or the aliased sinc function) and has been thor-
oughly described in the literature[10,13,14]. (It’s named after the nineteenth-
century German mathematician Peter Dirichlet [pronounced dee-ree-'klay],
†Nwas an even number in Figure 3–24 depicting the x(n). Had Nbeen an odd number, the lim-
its on the summation in Eq. (3–35) would have been –(N–1)/2≤n≤(N–1)/2. Using these alter-
nate limits would have led us to exactly the same X(m) as in Eq. (3–43).

3.13 The DFT of Rectangular Functions 109
X(m)
K
(a)
0
m
–N 0 N 2N
X(m)
1.0 K
0.8
0.6
0.4
(b)
0.2
0
–0.2 0 m
m = –N/K m = N/K
|X(m)|
1.0 K
0.8 Main
0.6 lobe
(c) 0.4
0.2
0
m
2N/K
Side
lobes
Figure 3–25 The Dirichlet kernel of X(m): (a) periodic continuous curve on which
theX(m) samples lie; (b) X(m) amplitudes about the m= 0 sample;
(c) |X(m)| magnitudes about the m=0 sample.
who studied the convergence of trigonometric series used to represent arbi-
trary functions.)
We can zoom in on the curve at the m = 0 point and see more detail in
Figure 3–25(b). The dots are shown in Figure 3–25(b) to remind us that the
DFT of our rectangular function results in discrete amplitude values that lie
on the curve. So when we perform DFTs, our discrete results are sampled val-
ues of the continuous sinc function’s curve in Figure 3–25(a). As we’ll show
later, we’re primarily interested in the absolute value, or magnitude, of the
Dirichlet kernel in Eq. (3–43). That magnitude,|X(m)|, is shown in Figure
3–25(c). Although we first saw the sinc function’s curve in Figure 3–9 in Sec-
tion 3.8, where we introduced the topic of DFT leakage, we’ll encounter this
curve often in our study of digital signal processing.
For now, there are just a few things we need to keep in mind concerning
the Dirichlet kernel. First, the DFT of a rectangular function has a main lobe,

110 The Discrete Fourier Transform
centered about the m = 0 point. The peak amplitude of the main lobe is K.
This peak value makes sense, right? The m = 0 sample of a DFT X(0) is the
sum of the original samples, and the sum of Kunity-valued samples is K. We
can show this in a more substantial way by evaluating Eq. (3–43) for m=0. A
difficulty arises when we plug m = 0 into Eq. (3–43) because we end up with
sin(0)/sin(0), which is the indeterminate ratio 0/0. Well, hardcore mathemat-
ics to the rescue here. We can use L’Hopital’s Rule to take the derivative of the
numerator and the denominator of Eq. (3–43), and thensetm=0 to determine
the peak value of the magnitude of the Dirichlet kernel.†We proceed as
d d[sin(πmK/N)]/dm
|X(m)| = X(m)=
m→0 dm d[sin(πm/N)]/dm
=
cos(πmK/N)⋅d(πmK/N)/dm
cos(πm/N) d(πm/N)/dm
cos(0)⋅ πK/N ⋅
= =1 K =K,
cos(0) π/N (3–44)
which is what we set out to show. (We could have been clever and evaluated
Eq. (3–35) with m = 0 to get the result of Eq. (3–44). Try it, and keep in mind
that ej0 = 1.) Had the amplitudes of the nonzero samples of x(n) been other
than unity, say some amplitude A , then, of course, the peak value of the
o
Dirichlet kernel would be A K instead of just K. The next important thing to
o
notice about the Dirichlet kernel is the main lobe’s width. The first zero
crossing of Eq. (3–43) occurs when the numerator’s argument is equal to
π, that is, when πmK/N = π. So the value of m at the first zero crossing is
given by
πN N
m = = (3–45)
first zero crossing πK K
as shown in Figure 3–25(b). Thus the main lobe width 2N/K,as shown in Fig-
ure 3–25(c), is inversely proportional to K.††
Notice that the main lobe in Figure 3–25(a) is surrounded by a series of
oscillations, called sidelobes, as in Figure 3–25(c). These sidelobe magnitudes
decrease the farther they’re separated from the main lobe. However, no mat-
ter how far we look away from the main lobe, these sidelobes never reach
zero magnitude—and they cause a great deal of heartache for practitioners in
† L’Hopital is pronounced ‘lo¯-pe¯-to˙l, like baby doll.
†† This is a fundamental characteristic of Fourier transforms. The narrower the function in one
domain, the wider its transform will be in the other domain.

3.13 The DFT of Rectangular Functions 111
digital signal processing. These sidelobes cause high-amplitude signals to
overwhelm and hide neighboring low-amplitude signals in spectral analysis,
and they complicate the design of digital filters. As we’ll see in Chapter 5, the
unwanted ripple in the passband and the poor stopband attenuation in sim-
ple digital filters are caused by the rectangular function’s DFT sidelobes. (The
development, analysis, and application of window functions came about to
minimize the ill effects of those sidelobes in Figure 3–25.)
Let’s demonstrate the relationship in Eq. (3–45) by way of a simple but
concrete example. Assume that we’re taking a 64-point DFT of the 64-sample
rectangular function, with 11 unity values, shown in Figure 3–26(a). In this ex-
ample, N = 64 and K = 11. Taking the 64-point DFT of the sequence in Figure
3–26(a) results in an X(m) whose real and imaginary parts, X (m) and
real
X (m), are shown in Figures 3–26(b) and 3–26(c) respectively. Figure
imag
3–26(b) is a good illustration of how the real part of the DFT of a real input se-
quence has even symmetry, and Figure 3–26(c) confirms that the imaginary
part of the DFT of a real input sequence has odd symmetry.
AlthoughX (m) and X (m) tell us everything there is to know about
real imag
the DFT of x(n), it’s a bit easier to comprehend the true spectral nature of
X(m) by viewing its absolute magnitude. This magnitude, from Eq. (3–7), is
provided in Figure 3–27(a) where the main and sidelobes are clearly evident
K
1 x(n)
0.5
(a)
0
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28 32 n
12 X r e a l (m) K = 11
10
8
6
4
(b)
2 –8 8
0
–2 –20 –16 –12 –4 0 4 12 16 20 m
2 X (m)
imag
1.5
1
0.5 –20 –8 4 12 16 24 28
(c) 0
–0.5 –28 –24 –16 –4 0 8 20 m
–1
–1.5
–2
Figure 3–26 DFT of a rectangular function: (a) original function x(n) ; (b) real part
of the DFT of x(n), X (m); (c) imaginary part of the DFT of x(n),
real
X (m).
imag

112 The Discrete Fourier Transform
12 |X(m)| 11
10
8
6
(a) 4
2
0
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28 m
3 X (m)
Ø
2
1 4
(b) 0
–1 –28 –20 –16 –8 –4 0 8 16 20 28 m
–2
–3
Figure 3–27 DFT of a generalized rectangular function: (a) magnitude |X(m)|;
(b) phase angle in radians.
now. As we expected, the peak value of the main lobe is 11 because we had
K=11 samples in x(n). The width of the main lobe from Eq. (3–45) is 64/11, or
5.82. Thus, the first positive-frequency zero-crossing location lies just below
them=6 sample of our discrete |X(m)| represented by the squares in Figure
3–27(a). The phase angles associated with |X(m)|, first introduced in Eqs.
(3–6) and (3–8), are shown in Figure 3–27(b).
To understand the nature of the DFT of rectangular functions more fully,
let’s discuss a few more examples using less general rectangular functions
that are more common in digital signal processing than the x(n) in Fig-
ure3–24.
3.13.2 DFT of a Symmetrical Rectangular Function
Equation (3–43) is a bit complicated because our original function x(n) was so
general. In practice, special cases of rectangular functions lead to simpler ver-
sions of Eq. (3–43). Consider the symmetrical x(n) rectangular function in Fig-
ure 3–28. As shown in Figure 3–28, we often need to determine the DFT of a
rectangular function that’s centered about the n = 0 index point. In this case,
the K unity-valued samples begin at n = –n = –(K–1)/2. So substituting
o
(K–1)/2 for n in Eq. (3–43) yields
o
X(m)=ej(2πm/N)((K–1)/2–(K–1)/2)
⋅sin(πmK/N)
sin(πm/N)
=ej(2πm/N)(0)
⋅sin(πmK/N)
. (3–46)
sin(πm/N)

3.13 The DFT of Rectangular Functions 113
K
x(n)
1
0 n
–N/2+1 –nο=–(K–1)/2 n = (K–1)/2 N/2
Figure 3–28 Rectangularx(n) with Ksamples centered about n=0.
Becauseej0=1, Eq. (3–46) becomes
Symmetrical form of the sin(πmK/N)
Dirichlet kernel: → X(m)= .
sin(πm/N)
(3–47)
Equation (3–47) indicates that the DFT of the symmetrical rectangular
function in Figure 3–28 is itself a real function; that is, there’s no complex ex-
ponential in Eq. (3–47), so this particular DFT contains no imaginary part or
phase term. As we stated in Section 3.2, if x(n) is real and even, x(n) =x(–n),
then X (m) is nonzero and X (m) is always zero. We demonstrate this by
real imag
taking the 64-point DFT of the sequence in Figure 3–29(a). Our x(n) is 11
unity-valued samples centered about the n=0 index. Here the DFT results in
an X(m) whose real and imaginary parts are shown in Figures 3–29(b) and
3–29(c), respectively. As Eq. (3–47) predicted, X (m) is nonzero and X (m)
real imag
is zero. The magnitude and phase of X(m) are depicted in Figures 3–29(d) and
3–29(e).
Notice that the magnitudes in Figures 3–27(a) and 3–29(d) are identical.
This verifies the very important shifting theorem of the DFT; that is, the mag-
nitude |X(m)| depends only on the number of nonzero samples in x(n), K,
andnoton their position relative to the n=0 index value. Shifting the Kunity-
valued samples to center them about the n=0 index merely affects the phase
angle of X(m), not its magnitude.
Speaking of phase angles, it’s interesting to realize here that even
though X (m) is zero in Figure 3–29(c), the phase angle of X(m) is not al-
imag
ways zero. In this case, X(m)’s individual phase angles in Figure 3–29(e) are
either +π, zero, or –π radians. With ejπ and ej(–π) both being equal to –1, we
could easily reconstruct X (m) from |X(m)| and the phase angle X (m) if
real ø
we must. X (m) is equal to |X(m)| with the signs of |X(m)|’s alternate side-
real
lobes reversed.† To gain some further appreciation of how the DFT of a rec-
tangular function is a sampled version of the Dirichlet kernel, let’s increase
the number of our nonzero x(n) samples. Figure 3–30(a) shows a 64-point x(n)
†The particular pattern of +πand –πvalues in Figure 3–29(e) is an artifact of the software used
to generate that figure. Adifferent software package may show a different pattern, but as long
as the nonzero phase samples are either +πor –π, the phase results will be correct.

114 The Discrete Fourier Transform
1 x(n)
(a) 0.5
0
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28 32n
1 1 0 2 X r e a l (m) 11
8
6
(b) 4
2 –20 –8 8
0
–2 –28 –24 –16 –12 –4 0 4 12 16 2628 30m
–4
0.5 X im a g (m)
0.4
0.3
(c) 0.2
0.1
0
m
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28
12
|X(m)| 1111
10
8
6
(d) 4
2
0
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28 m
3 4 X Ø (m) π
2
1 4 8 12 16 20 24 28
(e) 0
–1 –28 –24 –20 –16 –12 –8 –4 0 m
–2
–3 –π
–4
Figure 3–29 DFT of a rectangular function centered about n=0: (a) original x(n) ;
(b)X (m); (c) X (m); (d) magnitude of X(m); (e) phase angle of
real imag
X(m) in radians.
where 31 unity-valued samples are centered about the n = 0 index location.
The magnitude of X(m) is provided in Figure 3–30(b). By broadening the x(n)
function, i.e., increasing K, we’ve narrowed the Dirichlet kernel of X(m). This
follows from Eq. (3–45), right? The kernel’s first zero crossing is inversely
proportional to K, so, as we extend the width of K, we squeeze |X(m)| in to-
ward m=0. In this example, N=64 and K=31. From Eq. (3–45) the first posi-
tive zero crossing of X(m) occurs at 64/31, or just slightly to the right of the
m = 2 sample in Figure 3–30(b). Also notice that the peak value of
|X(m)|=K=31, as mandated by Eq. (3–44).

3.13 The DFT of Rectangular Functions 115
1 x(n)
(a) 0.5
0
n
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28 32
35 |X(m)|
30 31
25
(b) 20
15
10
5
0
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28 m
Figure 3–30 DFT of a symmetrical rectangular function with 31 unity values: (a)
originalx(n); (b) magnitude of X(m).
3.13.3 DFT of an All-Ones Rectangular Function
The DFT of a special form of x(n) is routinely called for, leading to yet another
simplified form of Eq. (3–43). In the literature, we often encounter a rectangu-
lar function where K=N; that is, all Nsamples of x(n) are nonzero, as shown
in Figure 3–31. In this case, the N unity-valued samples begin at
n = –n = –(N–1)/2. We obtain the expression for the DFT of the function in
o
Figure 3–31 by substituting K=Nandn =(N–1)/2 in Eq. (3–43) to get
o
X(m)=ej(2πm/N)[(N–1)/2–(N–1)/2]
⋅sin(πmN/N)
sin(πm/N)
=ej(2πm/N)(0) ⋅
sin(πm)
, or
sin(πm/N)
All-ones form of
the Dirichlet sin(πm)
kernel (Type 1): → X(m)= .
sin(πm/N)
(3–48)
Equation (3–48) takes the first form of Eq. (3–34) that we alluded to at the
beginning of Section 3.13.† Figure 3–32 demonstrates the meaning of Eq.
(3–48). The DFT magnitude of the all-ones function, x(n) in Figure
3–32(a), is shown in Figures 3–32(b) and 3–32(c). Take note that if m is
continuous, Eq. (3–48) describes the shaded curves in Figure 3–32(b) and
† By the way, there’s nothing official about calling Eq. (3-48) a Type 1 Dirichlet kernel. We’re
using the phrase Type 1merely to distinguish Eq. (3-48) from other mathematical expressions for
the Dirichlet kernel that we’re about to encounter.

116 The Discrete Fourier Transform
K = N
1 x(n)
0 n
−no = −(N−1)/2 (N−1)/2
Figure 3–31 Rectangular function with Nunity-valued samples.
Figure 3–32(c). If m is restricted to being integers, then Eq. (3–48) repre-
sents the dots in those figures.
The Dirichlet kernel of X(m) in Figure 3–32(b) is now as narrow as it can
get. The main lobe’s first positive zero crossing occurs at the m = 64/64 = 1
sample in Figure 3–32(b) and the peak value of |X(m)| = N = 64. With x(n)
being all ones, |X(m)| is zero for all m≠0. The sinc function in Eq. (3–48) is of
utmost importance—as we’ll see at the end of this chapter, it defines the over-
all DFT frequency response to an input sinusoidal sequence, and it’s also the
amplitude response of a single DFT bin.
The form of Eq. (3–48) allows us to go one step further to identify the
most common expression for the DFT of an all-ones rectangular function
found in the literature. To do this, we have to use an approximation principle
found in the mathematics of trigonometry that you may have heard before. It
x(n)
1
(a) 0.5
0
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28 32 n
70 |X(m)|
60 64
50
40
(b) 30
20
10
0
–28 –24 –20 –16 –12 –8 –4 0 4 8 12 16 20 24 28 m
70 |X(m)|
60 64
50
40
(c) 30
20
10
0
–5 –4 –3 –2 –1 0 1 2 3 4 5 m
Figure 3–32 All-ones function: (a) rectangular function with N= 64 unity-valued
samples; (b) DFT magnitude of the all-ones time function; (c) close-
up view of the DFT magnitude of an all-ones time function.

3.13 The DFT of Rectangular Functions 117
radius = 1
α
b
(a) a
α
α
(b)
a b
Figure 3–33 Relationships between an angle α, line a= sin(α), and α’s chord b:
(a) large angle α; (b) small angle α.
states that when α is small, then sin(α) is approximately equal to α, i.e.,
sin(α)≈α. This idea comes about when we consider a pie-shaped section of a
circle whose radius is 1 as shown in Figure 3–33(a). That section is defined by
the length of the arc αmeasured in radians and α’s chord b. If we draw a right
triangle inside the section, we can say that a = sin(α). As α gets smaller, the
long sides of our triangle become almost parallel, the length of chord b ap-
proaches the length of arc α, and the length of line aapproaches the length of
b. So, as depicted in Figure 3–33(b), when αis small, α≈b ≈a=sin(α). We use
this sin(α)≈αapproximation when we look at the denominator of Eq. (3–48).
Whenπm/Nis small, then sin(πm/N) is approximately equal to πm/N. So we
can, when Nis large, state
All-ones form of the Dirichlet sin(πm) ⋅sin(πm)⋅
kernel (Type 2): → X(m)≈ =N (3–49)
πm/N πm
It has been shown that when Nis larger than, say, 10 in Eq. (3–48), Eq. (3–49)
accurately describes the DFT’s output.† Equation (3–49) is often normalized
by dividing it by N, so we can express the normalized DFT of an all-ones rec-
tangular function as
All-ones form of the sin(πm)
Dirichlet kernel (Type 3): → X(m)≈ . (3–50)
πm
† We can be comfortable with this result because, if we let K=N,we’ll see that the peak value of
X(m) in Eq. (3–49), for m=0, is equal to N,which agrees with Eq. (3-44).

118 The Discrete Fourier Transform
Equation (3–50), taking the second form of Eq. (3–34) that is so often seen in
the literature, also has the DFT magnitude shown in Figures 3–32(b) and
3–32(c).
3.13.4 Time and Frequency Axes Associated
with the DFT
Let’s establish the physical dimensions associated with the nandmindex val-
ues. So far in our discussion, the nindex was merely an integer enabling us to
keep track of individual x(n) sample values. If the n index represents instants
in time, we can identify the time period separating adjacent x(n) samples to es-
tablish the time scale for the x(n) axis and the frequency scale for the X(m) axis.
Consider the time-domain rectangular function given in Figure 3–34(a). That
function comprises N time samples obtained t seconds apart, and the full
s
sample interval is Nt seconds. Each x(n) sample occurs at nt seconds for some
s s
value of n. For example, the n=9 sample value, x(9)=0, occurs at 9t seconds.
s
The frequency axis of X(m) can be represented in a number of different
ways. Three popular types of DFT frequency axis labeling are shown in Figure
3–34(b) and listed in Table 3–1. Let’s consider each representation individually.
3.13.4.1 DFT Frequency Axis in Hz. If we decide to relate the frequen-
cies of X(m) to the time sample period t, or the sample rate f =1/t, then the
s s s
frequency axis variable is f=m/Nt =mf/NHz. So each X(m) DFT sample is
s s
associated with a frequency of mf/N Hz. In this case, the sample spacing of
s
X(m) is f/N Hz. The DFT repetition period, or periodicity, is f Hz as shown
s s
in Figure 3–34(b). The first row of Table 3–1 illustrates the characteristics of la-
beling the frequency axis in Hz.
3.13.4.2 DFT Frequency Axis Normalized by f . If we think of some fre-
s
quencyf, in Hz, we can divide that frequency by the sampling frequency f to
s
create a normalized frequency variable f/f. The dimensions of such a normal-
s
ized frequency are cycles/sample. Using this notation, each X(m) DFT sample
is associated with a normalized frequency of m/N cycles/sample, and our
highest frequency are 1/2 cycles/sample as shown in Figure 3–34(b). In this
scenario the sample spacing of X(m) is 1/Ncycles/sample, and the DFT repe-
tition period is one cycle/sample as shown by the expressions in parentheses
in Figure 3–34(b). This normalized f/f frequency variable only has meaning
s
in sampled-data systems. That is, this type of frequency notation has no
meaning in the world of continuous (analog) systems.
It may seem strange to use such a normalized f/f frequency variable,
s
but sometimes it’s convenient for us to do so. Furthermore, the built-in plot-
ting functions of MATLAB (a popular signal processing software package)
often label the frequency axis in terms of the normalized f/f variable.
s
3.13.4.3 DFT Frequency Axis Using a Normalized Angle. We can mul-
tiply the above normalized f/f frequency variable by 2π to create a normal-
s
ized angular notation representing frequency. Doing so would result in a

3.13 The DFT of Rectangular Functions 119
Nt
s
t
s
x(n) t = 1/f
(a) s s
–3 0 4 t seconds
s
|X(m)|
1/Nt =f/N Hz
s s
m
0
m = 3
f Hz
(b) (1 cyc s le/sample) f = f/2 Hz
[2π radians/sample] (f/f = 1/2 cy s cles/sample)
s
[ω = π radians/sample]
freq. = f = 3f/N Hz
s
(freq. = f/f = 3/N cycles/sample)
s
[freq. = ω = 2π(f/f) = 2π(3/N) radians/sample]
s
Figure 3–34 DFT time and frequency axis dimensions: (a) time-domain axis uses
time index n; (b) various representations of the DFT’s frequency axis.
frequency variable expressed as ω=2π(f/f) radians/sample. Using this nota-
s
tion, each X(m) DFT sample is associated with a normalized frequency of
2πm/N radians/sample, and our highest frequency is π radians/sample as
shown in Figure 3–34(b). In this scenario the sample spacing of X(m) is 2π/N
radians/sample, and the DFT repetition period is one radian/sample as
Table 3–1 Characteristics of Various DFT Frequency Axis Representations
DFT frequency Repetition Frequency
axis Frequency Resolution interval of axis
representation variable of X(m) X(m) range
Frequency in Hz f f/N f –f/2 to f/2
s s s s
Frequency in f/f 1/N 1 –1/2 to 1/2
s
cycles/sample
Frequency in ω 2π/N 2π –πtoπ
radians/sample

120 The Discrete Fourier Transform
shown by the expressions in brackets in Figure 3–34(b). Using the normalized
angular ω frequency variable is very popular in the literature of DSP, and its
characteristics are described in the last row of Table 3–1.
Unfortunately having three different representations of the DFT’s fre-
quency axis may initially seem a bit puzzling to a DSP beginner, but don’t
worry. You’ll soon become fluent in all three frequency notations. When re-
viewing the literature, the reader can learn to convert between these fre-
quency axis notation schemes by reviewing Figure 3–34 and Table 3–1.
3.13.5 Alternate Form of the DFT of an
All-Ones Rectangular Function
Using the radians/sample frequency notation for the DFT axis from the bot-
tom row of Table 3–1 leads to another prevalent form of the DFT of the all-
ones rectangular function in Figure 3–31. Letting our normalized discrete
frequency axis variable be ω=2πm/N, then πm=Nω/2. Substituting the term
Nω/2 for πmin Eq. (3–48), we obtain
∞
A
D
l
i
l
r
-
i
o
ch
n
l
e
e
s
t
f
k
o
e
r
r
m
n e
o
l
f
(
t
T
h
y
e
p e 4): →
∑
x(n)e
−jωn
(3–51)
n=−∞
Equation (3–51), taking the third form of Eq. (3–34) sometimes seen in the liter-
ature, also has the DFT magnitude shown in Figures 3–32(b) and 3–32(c).
3.14 INTERPRETING THE DFT USING THE DISCRETE-TIME
FOURIER TRANSFORM
Now that we’ve learned about the DFT, it’s appropriate to ensure we under-
stand what the DFT actually represents and avoid a common misconception
regarding its behavior. In the literature of DSP you’ll encounter the topics of
continuous Fourier transform, Fourier series, discrete-time Fourier transform, dis-
crete Fourier transform, and periodic spectra. It takes effort to keep all those no-
tions clear in your mind, especially when you read or hear someone say
something like “the DFT assumes its input sequence is periodic in time.” (You
wonder how this can be true because it’s easy to take the DFT of an aperiodic
time sequence.) That remark is misleading at best because DFTs don’t make
assumptions. What follows is how I keep the time and frequency periodicity
nature of discrete sequences straight in my mind.
Consider an infinite-length continuous-time signal containing a single
finite-width pulse shown in Figure 3–35(a). The magnitude of its continuous
Fourier transform (CFT) is the continuous frequency-domain function X (ω).
1
If the single pulse can be described algebraically (with an equation), then the
CFT function X (ω), also an equation, can be found using Fourier transform
1

3.14 Interpreting the DFT Using the DTFT 121
Continuous & aperiodic
Continuous
x 1 (t) CFT X 1 ( ) & aperiodic
(a)
Time 0 Freq
Continuous & periodic Continuous impulses
x(t) X( ) & aperiodic
2... ... CFT 2 (Fourier series)
(b)
Time 0 Freq
Infinite-length aperiodic
discrete sequence
Continuous
x(n) DTFT X 3 ( ) & periodic
... ...
(c)
Time - s 0 ω s Freq
x 1 (n) (-f s ) (f s )
DFT
Discrete & periodic inverse
CFT X(m) Discrete
x(n) 1 & periodic
2 ... ... CFT ... ...
(d)
Time -f 0 f Freq
s s
Figure 3–35 Time-domain signals and sequences, and the magnitudes of their
transforms in the frequency domain.
calculus. (Chances are very good that you actually did this as a homework, or
test, problem sometime in the past.) The continuous frequency variable ω is
radians per second. If the CFT was performed on the infinite-length signal of
periodic pulses in Figure 3–35(b), the result would be the line spectra known
as the Fourier seriesX (ω). Those spectral lines (impulses) are infinitely narrow
2
andX (ω) is well defined in between those lines, because X (ω) is continuous.
2 2
(A well-known example of this concept is the CFT of a continuous square-
wave, which yields a Fourier series whose frequencies are all the odd multi-
ples of the squarewave’s fundamental frequency.)
Figure 3–35(b) is an example of a continuous periodic function (in time)
having a spectrum that’s a series of individual spectral components. You’re
welcome to think of the X (ω) Fourier series as a sampled version of the con-
2
tinuous spectrum in Figure 3–35(a). The time-frequency relationship between
x (t) and X (ω) shows how a periodic function in one domain (time) leads to a
2 2
function in the other domain (frequency) that is a series of discrete samples.
Next, consider the infinite-length discrete time sequence x(n), containing
several nonzero samples, in Figure 3–35(c). We can perform a CFT of x(n)

122 The Discrete Fourier Transform
describing its spectrum as a continuous frequency-domain function X (ω).
3
This continuous spectrum is called a discrete-time Fourier transform (DTFT)
defined by (see page 48 of reference [15])
∞
∑
X(ω)= x(n)e
−jωn
(3–52)
n=−∞
where the ωfrequency variable is measured in radians/sample.
To illustrate the notion of the DTFT, let’s say we had a time sequence de-
fined as x (n)=(0.75)nforn≥0. Its DTFT would be
o
∞ ∞
X o (ω)= ∑ 0.75ne −jωn = ∑ (0.75e −jω )n. (3–53)
n=0 n=0
Equation (3–53) is a geometric series (see Appendix B) and can be evalu-
ated as
X o (ω)= 1−0.7 1 5e −jω = ejω e − jω 0.75 . (3–53’)
X (ω) is continuous and periodic with a period of 2π, whose magnitude
o
is shown in Figure 3–36. This is an example of a sampled (or discrete) time-
domain sequence having a periodic spectrum. For the curious reader, we can
verify the 2πperiodicity of the DTFT using an integer kin the following
∞ ∞
X(ω+ 2πk)= ∑ x(n)e −j(ω+2πk)n = ∑ x(n)e −jωne −j2πkn =
n=−∞ n=−∞
∞
∑ x(n)e −jωn =X(ω) (3–54)
n=−∞
becausee–j2πkn=1
for integer values of k.
X (ω) in Figure 3–35(c) also has a 2πperiodicity represented by ω =2πf,
3 s s
where the frequency f is the reciprocal of the time period between the x(n)
s
samples. The continuous periodic spectral function X (ω) is what we’d like to
3
be able to compute in our world of DSP, but we can’t. We’re using computers
and, sadly, we can’t perform continuous signal analysis with the discrete (bi-
nary number) nature of computers. All of our processing comprises discrete
numbers stored in our computer’s memory and, as such, all of our time-
domain signals and all of our frequency-domain spectra are discrete sampled
sequences. Consequently the CFT, or inverse CFT, of the sequences with
which we work will all be periodic.
The transforms indicated in Figures 3–35(a) through 3-35(c) are pencil-and-
paper mathematics of calculus. In a computer, using only finite-length discrete

3.14 Interpreting the DFT Using the DTFT 123
4
(cid:2))| 3
( X0 2
|
1
0 (cid:2)
- 0 2 4 6
Figure 3–36 DTFT magnitude |X (ω)|.
o
sequences, we can only approximate the CFT (the DTFT) of the infinite-length
x(n) time sequence in Figure 3–35(c). That approximation is called the discrete
Fourier transform (DFT), and it’s the only DSP Fourier transform tool we have
available to us. Taking the DFT of x (n), where x (n) is a finite-length portion of
1 1
x(n), we obtain the discrete periodic X (m) spectral samples in Figure 3–35(d).
1
Notice how X (m) is a sampled version of the continuous periodic X (ω).
1 3
That sampling is represented by
X
1
(m)= X
3
(ω)|ω=2πm/N =
N∑ −1
x
1
(n)e
−j2πnm/N.
(3–55)
n=0
We interpret Eq. (3–55) as follows: X (ω) is the continuous DTFT of the N–sample
3
time sequence x (n). We can evaluate X (ω) at the Nfrequencies of ω = 2πm/N,
1 3
where integer mis 0≤m≤N–1, covering a full period of X (ω). The result of those
3
Nevaluated values is a sequence equal to the X (m) DFT of x (n).
1 1
However, and here’s the crucial point, X (m) is also exactly equal to the
1
CFT of the periodic time sequence x (n) in Figure 3–35(d). So when people say
2
“the DFT assumes its input sequence is periodic in time,” what they really
mean is the DFT is equal to the continuous Fourier transform (the DTFT) of a peri-
odic time-domain discrete sequence. After all this rigmarole, the end of the story
is this: if a function is periodic, its forward/inverse DTFT will be discrete; if a
function is discrete, its forward/inverse DTFT will be periodic.
In concluding this discussion of the DTFT, we mention that in the litera-
ture of DSPthe reader may encounter the following expression
+∞
∑ −j2πnF
X(F)= x(n)e (3–56)
n=∞
as an alternate definition of the DTFT. Eq. (3–56) can be used to evaluate a full
period of the DTFT of an x(n) sequence by letting the frequency variable F,
whose dimensions are cycles/sample, be in either of the ranges of 0 ≤ F ≤ 1 or
–0.5≤F≤0.5.

124 The Discrete Fourier Transform
REFERENCES
[1] Bracewell, R. “The Fourier Transform,” Scientific American, June 1989.
[2] Struik, D. AConcise History of Mathematics, Dover Publications, New York, 1967, p.142.
[3] Williams, C. S. Designing Digital Filters, Prentice Hall, Englewood Cliffs, New Jersey, 1986,
Section 8.6, p. 122.
[4] Press, W., et al. Numerical Recipes: The Art of Scientific Computing, Cambridge University
Press, New York, 1989, p. 426.
[5] Geckinli, N. C., and Yavuz, D. “Some Novel Windows and a Concise Tutorial Compari-
son of Window Families,” IEEE Trans. on Acoust. Speech, and Signal Proc., Vol. ASSP-26,
No. 6, December 1978. (By the way, on page 505 of this paper, the phrase “such that
W(f)≥0∀f” indicates that W(f) is never negative. The symbol∀means “for all.”)
[6] O’Donnell, J. “Looking Through the Right Window Improves Spectral Analysis,” EDN,
November 1984.
[7] Kaiser, J. F. “Digital Filters,” in System Analysis by Digital Computer, ed. by F. F. Kuo and
J.F. Kaiser, John Wiley and Sons, New York, 1966, pp. 218–277.
[8] Rabiner, L. R., and Gold, B. The Theory and Application of Digital Signal Processing, Prentice
Hall, Englewood Cliffs, New Jersey, 1975, p. 88.
[9] Schoenwald, J. “The Surface Acoustic Wave Filter: Window Functions,” RF Design, March
1986.
[10] Harris, F. “On the Use of Windows for Harmonic Analysis with the Discrete Fourier
Transform,” Proceedings of the IEEE,Vol. 66, No. 1, January 1978.
[11] Nuttall, A. H. “Some Windows with Very Good Sidelobe Behavior,” IEEE Trans. on
Acoust. Speech, and Signal Proc.,Vol. ASSP-29, No. 1, February 1981.
[12] Yanagimoto, Y. “Receiver Design for a Combined RF Network and Spectrum Analyzer,”
Hewlett-Packard Journal, October 1993.
[13] Gullemin, E. A. The Mathematics of Circuit Analysis, John Wiley and Sons, New York, 1949,
p. 511.
[14] Lanczos, C. Discourse on Fourier Series, Hafner Publishing Co., New York, 1966, Chapter 1,
pp. 7–47.
[15] Oppenheim, A., et al. Discrete-Time Signal Processing, 2nd ed., Prentice Hall, Upper Saddle
River, New Jersey, 1999, pp. 48–51.

Chapter 3 Problems 125
CHAPTER 3 PROBLEMS
3.1 Let’s assume that we have performed a 20-point DFT on a sequence of real-
valued time-domain samples, and we want to send our X(m) DFT results to a
colleague using e-mail. What is the absolute minimum number of (complex)
frequency-domain sample values we will need to type in our e-mail so that
our colleague has complete information regarding our DFT results?
3.2 Assume a systems engineer directs you to start designing a system that per-
forms spectrum analysis using DFTs. The systems engineer states that the
spectrum analysis system’s input data sample rate, f, is 1000 Hz and specifies
s
that the DFT’s frequency-domain sample spacing must be exactly 45 Hz.
(a) What is the number of necessary input time samples, N, for a single DFT
operation?
(b) What do you tell the systems engineer regarding the spectrum analysis
system’s specifications?
3.3 We want to compute an N-point DFT of a one-second-duration compact disc
(CD) audio signal x(n), whose sample rate is f =44.1 kHz, with a DFT sample
s
spacing of 1 Hz.
(a) What is the number of necessary x(n) time samples, N?
(b) What is the time duration of the x(n) sequence measured in seconds?
Hint:This Part (b) of the problem is trickier than it first appears. Think carefully.
3.4 Assume we have a discrete x(n) time-domain sequence of samples obtained
from lowpass sampling of an analog signal, x(t). If x(n) contains N = 500 sam-
ples, and it was obtained at a sample rate of f =3000 Hz:
s
(a) What is the frequency spacing of x(n)’s DFT samples, X(m), measured in Hz?
(b) What is the highest-frequency spectral component that can be present in
the analog x(t) signal where no aliasing errors occur in x(n)?
(c) If you drew the full X(m) spectrum and several of its spectral replications,
what is the spacing between the spectral replications measured in Hz?
3.5 What are the magnitudes of the 8-point DFT samples of
(a) the x (n) = 9, 9, 9, 9, 9, 9, 9, 9 sequence (explain how you arrived at your
1
solution)?
(b) thex (n) = 1, 0, 0, 0, 0, 0, 0, 0 sequence?
2
(c) thex (n) = 0, 1, 0, 0, 0, 0, 0, 0 sequence?
3

126 The Discrete Fourier Transform
Because the x (n) sequence in Part (c) is merely a time-shifted version of the
3
x (n) sequence in Part (b), comment on the relationship of the |X (m)| and
2 2
|X (m)| DFT samples.
3
3.6 Consider sampling exactly three cycles of a continuous x(t) sinusoid resulting
in an 8-point x(n) time sequence whose 8-point DFT is the X(m) shown in Fig-
ure P3–6. If the sample rate used to obtain x(n) was 4000 Hz, write the time-
domain equation for the discrete x(n) sinusoid in trigonometric form. Show
how you arrived at your answer.
Real part of X(m) Imaginary part of X(m)
5.657 5.657
5
0 0
0 1 2 3 4 5 6 7 m 1 2 3 4 6 7 m
Figure P3–6
3.7 In the text’s Section 3.1 we discussed the computations necessary to compute
the X(0) sample of an N-point DFT. That X(0) output sample represents the
zero Hz (DC) spectral component of an x(n) input sequence. Because it is the
DC component, X(0) is real-only and we’re free to say that an X(0) sample al-
wayshas zero phase. With that said, here are two interesting DFT problems:
(a) Given that an N-point DFT’s input sequence x(n) is real-only, and N is an
even number, is there any value for m (other than m = 0) for which an
X(m) DFT output sample is always real-only?
(b) Given that Nis an odd number, is there any value for m(other than m=0)
where an X(m) DFT output sample is always real-only?
3.8 Using the following rectangular form for the DFT equation:
∑N−1
X(m)= x(n)⋅[cos(2πnm/N)− jsin(2πnm/N)]
n=0
(a) Prove that the f/2 spectral sample is X(N/2) = N · sin(θ) when the x(n)
s
input is a sinusoidal sequence defined by
x(n) = sin[2π(f/2)nt +θ].
s s

Chapter 3 Problems 127
Nis an even number, frequency f is the x(n) sequence’s sample rate in Hz,
s
time index n = 0, 1, 2, ..., N–1, and θ is an initial phase angle measured in
radians.
Hint:Recall the trigonometric identity sin(α+β)=sin(α)cos(β)+
cos(α)sin(β).
(b) What is X(N/2) when x(n) = sin[2π(f/2)nt]?
s s
(c) What is X(N/2) when x(n) = cos[2π(f/2)nt]?
s s
3.9 To gain some practice in using the algebra of discrete signals and the geomet-
ric series identities in Appendix B, and to reinforce our understanding of the
output magnitude properties of a DFT when its input is an exact integer num-
ber of sinusoidal cycles:
(a) Prove that when a DFT’s input is a complex sinusoid of magnitude A
o
(i.e., x(n)=A
ej2πfnts)
with exactly three cycles over N samples, the output
o
magnitude of the DFT’s m=3 bin will be |X(3)|=A N.
o
Hint:The first step is to redefine x(n)’sfandt variables in terms of a sam-
s
ple rate f andNso that x(n) has exactly three cycles over Nsamples. The
s
redefined x(n) is then applied to the standard DFT equation.
(b) Prove that when a DFT’s input is a real-only sinewave of peak amplitude
A (i.e.,x(n)=A sin(2πfnt)) with exactly three cycles over Nsamples, the
o o s
output magnitude of the DFT’s m=3 bin will be |X(3)|=A N/2.
o
Hint:Once you redefine x(n)’sfandt variables in terms of a sample rate
s
f and N so that x(n) has exactly three cycles over N samples, you must
s
convert that real sinewave to complex exponential form so that you can
evaluate its DFT for m=3.
The purpose of this problem is to remind us that DFT output magnitudes are
proportional to the size, N, of the DFT. That fact is important in a great many
DSPanalysis activities and applications.
3.10 Consider performing the 5-point DFT on the following x (n) time-domain
1
samples
x (n) = [1, 2.2, –4, 17, 21],
1
and the DFT’s first sample is X (0) = 37.2. Next, consider performing the
1
5-point DFT on the following x (n) time samples
2
x (n) = [1, 2.2, –4, 17, Q],
2
and that DFT’s first sample is X (0) = 57.2. What is the value of Qin the x (n)
2 2
time sequence? Justify your answer.

128 The Discrete Fourier Transform
3.11 Derive the equation describing X(m), the N-point DFT of the following x(n)
sequence:
x(n) = an, for 0 ≤n≤N–1.
Hint:Recall one of the laws of exponents, pbqbc= (pqc)b, and the geometric se-
ries identities in Appendix B.
3.12 Consider an N-samplex(n) time sequence whose DFT is represented by X(m),
where 0 (cid:3) m (cid:3) N–1. Given this situation, an Internet website once stated,
“The sum of the X(m) samples is equal to Ntimes the first x(n) sample.” Being
suspicious of anything we read on the Internet, show whether or not that
statement is true.
Hint: Use the inverse DFT process to determine the appropriate x(n) time
sample of interest in terms of X(m).
3.13 Here is a problem whose solution may be useful to you in the future. On the
Internet you will find information suggesting that an inverse DFT can be
computed using a forward DFT software routine in the process shown in Fig-
ure P3–13.
Inverse DFT
X real (m) ÷ N x real (n)
Forward
DFT
X (m) ÷ N x (n)
imag imag
Figure P3–13
(a) Using the forward and inverse DFT equations, and the material in Ap-
pendix A, show why the process in Figure P3–13 computes correct in-
verse DFTs.
Hint:Begin your solution by writing the inverse DFT equation and conju-
gating both sides of that equation.
(b) Comment on how the process in Figure P3–13 changes if the original fre-
quency-domainX(m) sequence is conjugate symmetric.

Chapter 3 Problems 129
3.14 One useful way to test the performance of commercial analog-to-digital
(A/D) converters is to digitize an f Hz analog sinewave, apply the N-sample
o
x(n) sequence to a DFT, and examine the DFT’s X(m) results. The process is
depicted in Figure P3–14. An ideal (A/D) converter will produce X(m) results
showing spectral energy at f Hz and no spectral energy at any other fre-
o
quency. As such, nonzero spectral energy in X(m) at frequencies other than f
o
Hz indicates real-world A/D converter performance. However, the DFT’s in-
herent property of leakage “smears” spectral energy over multiple X(m) sam-
ples, as was shown in the text’s Figure 3–8(b), which degrades the
effectiveness of this A/D converter test method. What can we do to minimize
the DFT’s inherent spectral leakage as much as possible for this type of con-
verter testing?
x(t) = sin(2 ft) A/D x(n) N-point X(m)
o
converter DFT
f (A/D clock)
s
Figure P3–14
3.15 Here is a real-world spectrum analysis problem. Figure P3–15(a) shows 902
samples of an x(n) time sequence. (For clarity, we do not show the x(n) sam-
ples as individual dots.) That sequence is the sound of the “A3” note (“A”
below middle “C”) from an acoustic guitar, sampled at f =22.255 kHz. Figure
s
P3–15(b) shows the X(m) spectral magnitude samples, the DFT of x(n), on a
linear scale for the frequency index range of 0(cid:3)m(cid:3)59.
(a) Based on the X(m) samples, what is the fundamental frequency, in Hz, of
the guitar’s “A3” note?
(b) When we plot the DFT magnitude samples on a logarithmic scale, as in
Figure P3–15(c), we see spectral harmonicsand learn that the guitar note is
rich in spectral content. (The harmonics are integer multiples of the fun-
damental frequency.) That’s why guitars have their pleasing sound, de-
pending on the guitarist’s skill, of course. What is the frequency of the
highest nonzero spectral component of the guitar’s “A3” note?

130 The Discrete Fourier Transform
100
x(n)
(a) 0
-100
0 0.01 0.02 0.03 0.04
Time (seconds)
1
|X(m)| Fundamental frequency
spectral component
(b) 0.5
0
0 10 20 30 40 50
m
0
dB |X(m)|
(c)
0 10 20 30 40 50
m
Figure P3–15
3.16 Figure P3–16(a) shows a 16-point Hanning window sequence, h (n), defined by
1
⎛2πn⎞
h (n)= 0.5−0.5cos , for n=0,1,2,...,15.
1 ⎝ ⎠
16
The magnitude of its DFT samples, |H (m)|, is shown in Figure P3–16(b).
1
(For simplicity, we show only the positive-frequency range of the |H (m)|
1
samples.) Notice that only the |H (0)| and the |H (1)| frequency-domain
1 1
samples are nonzero.
(a) Sequenceh (n) comprises two signals. Looking carefully at h (n), describe
1 1
what those two signals are and justify why |H (m)| looks the way it
1
does.
(b) Given your understanding of the relationship between h (n) and |H (m)|,
1 1
look at h (n), in Figure P3–16(c), which is two repetitions of the original
2
h (n) sequence. Draw a rough sketch of the spectral magnitude sequence
1
|H (m)| over its positive-frequency range.
2
(c) Given that the h (n) in Figure P3–16(d) is three repetitions of the original
3
h (n) sequence, draw the spectral magnitude sequence |H (m)| over its
1 3
positive-frequency range.

Chapter 3 Problems 131
1 8
0.8 h 1 (n) |H 1 (m)|
4
0.4
0 0
0 5 10 n 15 0 1 2 3 4 5 6 7 8
m
(a) (b)
1 1
0.8 h 2 (n) 0.8 h 3 (n)
0.4 0.4
0 0
0 5 15 25 31 0 5 15 25 35 47
n n
(c) (d)
Figure P3–16
(d) Considering the h (n), h (n), and h (n) sequences, and their |H (m)|,
1 2 3 1
|H (m)|, and |H (m)| spectral magnitude samples, complete the follow-
2 3
ing important statement: “K repetitions of an h (n) sequence result in an
1
extended-length time sequence whose spectral magnitudes have K–1 ...”
3.17 In the literature of DSP, you may see an alternate expression for an N-point
Hanning window defined by
⎛πn⎞
w (n)= sin2 , for n=0,1,2,...,N−1.
han,alt ⎝ N ⎠
Prove that the above alternate expression for a Hanning window is equiva-
lent to the Section 3.9 text’s definition of a Hanning window.
3.18 Considering the DFT of an N-point x(n) sequence, what is the spectral effect
of zero-padding the x(n) sequence to a length of Q samples (with Q being an
integer power of two, and Q>N) and performing a Q-point DFT on the zero-
padded sequence?
3.19 Assume that an N-point DFT, performed on an N-sample x(n) time-domain
sequence, results in a DFT frequency-domain sample spacing of 100 Hz. What
would be the DFT frequency-domain sample spacing in Hz if the N-sample
x(n) time sequence was padded with 4N zero-valued samples and we per-
formed a DFT on that extended-time sequence?
3.20 There is a program, in the U.S. and other countries, called “Search for Ex-
traterrestrial Intelligence” (SETI). These folk point radio antennas around in

132 The Discrete Fourier Transform
the night sky searching for “nonrandom radio” signals, hoping to find evi-
dence of “little green men.” They search for radio-frequency (RF) signal en-
ergy that significantly exceeds the background RF noise energy in the sky.
Their primary method for detecting low-level RF energy is to tune a narrow-
band receiver to some RF frequency and collect millions of time-domain sam-
ples, and then perform million-point DFTs in the hope of finding spectral
magnitude components that significantly exceed the background spectral
noise. High-level spectral components would indicate the existence of intelli-
gent life that’s broadcasting radio signals of some sort.
Here’s the question: If a SETI researcher collects one million time sam-
ples and performs a one-million-point DFT, roughly what DFT processing
gain (in dB) improvement can that person expect to achieve in pullinga weak
spectral component up above the background galactic spectral noise in com-
parison to using a 100-point DFT?
3.21 This problem tests your understanding of the DFT’s frequency-domain axis.
Consider sampling exactly two cycles of an analog x(t) cosine wave resulting
in the 8-point x (n) time sequence in Figure P3–21(a). The real part of the DFT
1
ofx (n) is the sequence shown in Figure P3–21(b). Because x (n) is exactly two
1 1
cycles of a cosine sequence, the imaginary parts of X (m) are all zero-valued
1
1 x(n)
1
(a) 0
x(t)
0 2 n 4 6
Real part of X(m)
4 1
(b) = |X(m)|
2 1
0 2 m 4 6
Real part of X(m)
4 2
(c) = |X(m)|
2 2
0 2 4 6 m 8 10 12 14 15
Imag. part
4
(d) ofX 2 (m)
2
0 2 4 6 m 8 10 12 14 15
Figure P3–21

Chapter 3 Problems 133
samples, making |X (m)| equal to the real part of X (m). (Note that no leak-
1 1
age is apparent in |X (m)|.) Think, now, of a new frequency-domain se-
1
quenceX (m) that is equal to X (m) with eight zero-valued samples, the white
2 1
squares in Figures P3–21(c) and P3–21(d), inserted in the center of the real
and imaginary parts of X (m).
1
(a) Draw the x (n) time sequence that is the inverse DFT of X (m).
2 2
(b) Comment on how the x (n) time sequence is related to the original analog
2
x(t) signal and the x (n) sequence.
1
3.22 There is a useful spectrum analysis process, discussed in Chapter 13, that uses
the results of an N-point DFT, X(m), and requires us to compute
S=P·X(0) – Q·X(N–1) – Q·X(1)
where PandQare scalar constants. Value Sis the sum of three complex num-
bers. If we represent the three DFT samples in rectangular form, we can write
S=P· [a+jb] – Q· [c+jd] – Q· [e+jg].
In the general case, the above expression for Srequires six real multiply oper-
ations. If the DFT’s x(n) input sequence is real-only, what is the equation for S
that requires fewer than six real multiplies? Show your work.
3.23 For an N-length time-domain sequence x(n), why is the DFT useful in plotting
x(n)’s discrete-time Fourier transform (DTFT) which is a function of the con-
tinuous frequency variable ω?
3.24 In Chapter 1 we mentioned a special time-domain sequence called a unit im-
pulse.We’ll be using that sequence, the x (n) shown in Figure P3–24, in later
imp
chapters to test digital filters. As such, it’s useful to know the spectral content
of this unit impulse.
x (n),unit impulse
imp
1
unity-valued sample
n
0
Figure P3–24
(a) Draw the continuous X (ω) discrete-time Fourier transform (DTFT),
imp
over the frequency range of 0 ≤ ω ≤2π, of the x (n) unit impulse
imp
sequence.

134 The Discrete Fourier Transform
(b) With your X (ω) solution in mind, assume a person is listening to an
imp
AM (amplitude modulation) radio station centered at 640 kHz in the
North American AM Broadcast band and a neighbor is listening to an in-
ternational shortwave AM signal on a radio receiver tuned to 5.2 MHz.
Can you explain why, when lightning strikes, both people hear the static
noise from the lightning on their radios even though the radios are tuned
to very different center frequencies?
3.25 Draw a rough sketch of the magnitude of the discrete-time Fourier transform
(DTFT), over the frequency range of –π≤ω≤π, of the x(n) sequence in Figure
P3–25.
x(n)
1
0 n
Figure P3–25

CHAPTER FOUR
The Fast
0
W
N
Fourier
N/2
Transform W
N
Although the DFT is the most straightforward mathematical procedure for
determining the frequency content of a time-domain sequence, it’s terribly in-
efficient. As the number of points in the DFT is increased to hundreds, or
thousands, the amount of necessary number crunching becomes excessive. In
1965 a paper was published by Cooley and Tukey describing a very efficient
algorithm to implement the DFT[1]. That algorithm is now known as the fast
Fourier transform (FFT).† Before the advent of the FFT, thousand-point DFTs
took so long to perform that their use was restricted to the larger research and
university computer centers. Thanks to Cooley, Tukey, and the semiconductor
industry, 1024-point DFTs can now be performed in a few seconds on home
computers.
Volumes have been written about the FFT, and, as for no other innova-
tion, the development of this algorithm transformed the discipline of digital
signal processing by making the power of Fourier analysis affordable. In this
chapter, we’ll show why the most popular FFT algorithm (called the radix-2
FFT) is superior to the classical DFT algorithm, present a series of recom-
mendations to enhance our use of the FFT in practice, and provide a list of
sources for FFT routines in various software languages. We conclude this
chapter, for those readers wanting to know the internal details, with a de-
rivation of the radix-2 FFT and introduce several different ways in which this
FFT is implemented.
†Actually, the FFT has an interesting history. While analyzing X-ray scattering data, a couple of
physicists in the 1940s were taking advantage of the symmetries of sines and cosines using a
mathematical method based on a technique published in the early 1900s. Remarkably, over 20
years passed before the FFT was (re)discovered. Reference [2] tells the full story.
135

136 The Fast Fourier Transform
4.1 RELATIONSHIP OF THE FFT TO THE DFT
Although many different FFT algorithms have been developed, in this section
we’ll see why the radix-2 FFT algorithm is so popular and learn how it’s re-
lated to the classical DFT algorithm. The radix-2 FFT algorithm is a very effi-
cient process for performing DFTs under the constraint that the DFT size be an
integral power of two. (That is, the number of points in the transform is N=2k,
where k is some positive integer.) Let’s see just why the radix-2 FFT is the fa-
vorite spectral analysis technique used by signal processing practitioners.
Recall that our DFT Example 1 in Section 3.1 illustrated the number of
redundant arithmetic operations necessary for a simple 8-point DFT. (For ex-
⋅
ample, we ended up calculating the product of 1.0607 0.707 four separate
times.) On the other hand, the radix-2 FFT eliminates these redundancies and
greatly reduces the number of necessary arithmetic operations. To appreciate
the FFT’s efficiency, let’s consider the number of complex multiplications nec-
essary for our old friend, the expression for an N-point DFT,
N∑–1
X(m)= x(n)e–j2πnm/N.
(4–1)
n=0
For an 8-point DFT, Eq. (4–1) tells us that we’d have to perform N2or 64 com-
plex multiplications. (That’s because we assume, in the general case, that x(n)
are complex-valued samples and for each of the eight X(m)s we have to sum
eight complex products as ngoes from 0 to 7.) As we’ll verify in later sections
of this chapter, the number of complex multiplications, for an N-point FFT, is
approximately
N⋅
log N. (4–2)
2 2
(We say approximately because some multiplications turn out to be multipli-
cations by +1 or –1, which amount to mere sign changes.) Well, this
(N/2)log N value is a significant reduction from the N2 complex multiplica-
2
tions required by Eq. (4–1), particularly for large N. To show just how signifi-
cant, Figure 4–1 compares the number of complex multiplications required by
DFTs and radix-2 FFTs as a function of the number of input data points N.
When N = 512, for example, the DFT requires 114 times the number of complex
multiplications than needed by the FFT. When N= 8192, the DFT must calcu-
late 1260 complex multiplications for eachcomplex multiplication in the FFT!
Here’s my favorite example of the efficiency of the radix-2 FFT. Say you
perform a two-million-point FFT (N = 2,097,152) on your desktop computer
and it takes 10 seconds. A two-million-point DFT, on the other hand, using
your computer, will take more than three weeks! The publication and dissem-
ination of the radix-2 FFT algorithm was, arguably, the most important event
in digital signal processing.

4.2 Hints on Using FFTs in Practice 137
Number of complex multiplications
8
10
DFT
6
10
4
10
FFT
2
10
0
10
0 1 2 3 4 5
10 10 10 10 10 10
N
Figure 4–1 Number of complex multiplications in the DFT and the radix-2 FFT as a
function of N.
It’s appropriate now to make clear that the FFT is not an approximation
of the DFT. It’s exactly equal to the DFT; it isthe DFT. Moreover, all of the per-
formance characteristics of the DFT described in the previous chapter, output
symmetry, linearity, output magnitudes, leakage, scalloping loss, etc., also de-
scribe the behavior of the FFT.
4.2 HINTS ON USING FFTS IN PRACTICE
Based on how useful FFTs are, here’s a list of practical pointers, or tips, on ac-
quiring input data samples and using the radix-2 FFT to analyze real-world
signals or data.
4.2.1 Sample Fast Enough and Long Enough
When digitizing continuous signals with an A/D converter, for example, we
know, from Chapter 2, that our sampling rate must be greater than twice the
bandwidth of the continuous A/D input signal to prevent frequency-
domain aliasing. Depending on the application, practitioners typically sam-
ple at 2.5 to 4 times the signal bandwidth. If we know that the bandwidth of
the continuous signal is not too large relative to the maximum sample rate
of our A/D converter, it’s easy to avoid aliasing. If we don’t know the
continuous A/D input signal’s bandwidth, how do we tell if we’re having
aliasing problems? Well, we should mistrust any FFT results that have

138 The Fast Fourier Transform
significant spectral components at frequencies near half the sample rate.
Ideally, we’d like to work with signals whose spectral amplitudes decrease
with increasing frequency. Be very suspicious of aliasing if there are any
spectral components whose frequencies appear to depend on the sample
rate. If we suspect that aliasing is occurring or that the continuous signal
contains broadband noise, we’ll have to use an analog lowpass filter prior to
A/D conversion. The cutoff frequency of the lowpass filter must, of course,
be greater than the frequency band of interest but less than half the sample
rate.
Although we know that an N-point radix-2 FFT requires N=2kinput sam-
ples, just how many samples must we collect before we perform our FFT? The
answer is that the data collection time interval must be long enough to satisfy
our desired FFT frequency resolution for the given sample rate f. The data col-
s
lection time interval is the reciprocal of the desired FFT frequency resolution,
and the longer we sample at a fixed f sample rate, the finer our frequency reso-
s
lution will be; that is, the total data collection time interval is N/f seconds, and
s
ourN-point FFT bin-to-bin (sample-to-sample) frequency resolution is f/NHz.
s
So, for example, if we need a spectral resolution of 5 Hz, then f/N=5 Hz, and
s
f f
N = s = s =0.2f . (4–3)
desired resolution 5 s
In this case, if f is, say, 10 kHz, then Nmust be at least 2000, and we’d choose
s
Nequal to 2048 because this number is a power of two.
4.2.2 Manipulating the Time Data Prior to Transformation
When using the radix-2 FFT, if we don’t have control over the length of our
time-domain data sequence, and that sequence length is not an integral
power of two, we have two options. We could discard enough data samples
so that the remaining FFT input sequence length is some integral power of
two. This scheme is not recommended because ignoring data samples de-
grades our resultant frequency-domain resolution. (The larger N is, the better
our frequency resolution, right?) Abetter approach is to append enough zero-
valued samples to the end of the time data sequence to match the number of
points of the next largest radix-2 FFT. For example, if we have 1000 time sam-
ples to transform, rather than analyzing only 512 of them with a 512-point
FFT, we should add 24 trailing zero-valued samples to the original sequence
and use a 1024-point FFT. (This zero-padding technique is discussed in more
detail in Section 3.11.)
FFTs suffer the same ill effects of spectral leakage that we discussed for
the DFT in Section 3.8. We can multiply the time data by a window function
to alleviate this leakage problem. Be prepared, though, for the frequency reso-
lution degradation inherent when windows are used. By the way, if append-

4.2 Hints on Using FFTs in Practice 139
ing zeros is necessary to extend a time sequence, we have to make sure that
we append the zeros after multiplying the original time data sequence by a
window function. Applying a window function to the appended zeros will
distort the resultant window and worsen our FFT leakage problems.
Although windowing will reduce leakage problems, it will not eliminate
them altogether. Even when windowing is employed, high-level spectral
components can obscure nearby low-level spectral components. This is espe-
cially evident when the original time data has a nonzero average, i.e., it’s rid-
ing on a DC bias. When the FFT is performed in this case, a large-amplitude
DC spectral component at 0 Hz will overshadow its spectral neighbors. We
can eliminate this problem by calculating the average of the time sequence
and subtracting that average value from each sample in the original sequence.
(The averaging and subtraction process must be performed before window-
ing.) This technique makes the new time sequence’s average (mean) value
equal to zero and eliminates any high-level, zero Hz component in the FFT
results.
4.2.3 Enhancing FFT Results
If we’re using the FFT to detect signal energy in the presence of noise and
enough time-domain data is available, we can improve the sensitivity of our
processing by averaging multiple FFTs. This technique, discussed in Section
11.3, can be implemented to detect signal energy that’s actually below the av-
erage noise level; that is, given enough time-domain data, we can detect sig-
nal components that have negative signal-to-noise ratios.
If our original time-domain data is real-valued only, we can take advan-
tage of the 2N-Point Real FFT technique in Section 13.5 to speed up our pro-
cessing; that is, a 2N-point real sequence can be transformed with a single
N-point complex radix-2 FFT. Thus we can get the frequency resolution of a
2N-point FFT for just about the computational price of performing a standard
N-point FFT. Another FFT speed enhancement is the possible use of the
frequency-domain windowing technique discussed in Section 13.3. If we need
the FFT of unwindowed time-domain data and, at the same time, we also
want the FFT of that same time data with a window function applied, we
don’t have to perform two separate FFTs. We can perform the FFT of the un-
windowed data, and then we can perform frequency-domain windowing to
reduce spectral leakage on any, or all, of the FFT bin outputs.
4.2.4 Interpreting FFT Results
The first step in interpreting FFT results is to compute the absolute frequency
of the individual FFT bin centers. Like the DFT, the FFT bin spacing is the
ratio of the sampling rate(f ) over the number of points in the FFT, or f/N.
s s
With our FFT output designated by X(m), where m = 0, 1, 2, 3, . . ., N–1, the

140 The Fast Fourier Transform
absolute frequency of the mth bin center is mf/N. If the FFT’s input time sam-
s
ples are real, only the X(m) outputs from m = 0 to m = N/2 are independent.
So, in this case, we need determine only the absolute FFT bin frequencies for
m over the range of 0 ≤ m ≤ N/2. If the FFT input samples are complex, all N
of the FFT outputs are independent, and we should compute the absolute FFT
bin frequencies for mover the full range of 0≤m≤N–1.
If necessary, we can determine the true amplitude of time-domain sig-
nals from their FFT spectral results. To do so, we have to keep in mind that
radix-2 FFT outputs are complex and of the form
X(m)=X (m)+jX (m). (4–4)
real imag
Also, the FFT output magnitude samples,
X mag (m)=|X(m)|= X real (m)2 +X imag (m)2, (4–5)
are all inherently multiplied by the factor N/2, as described in Section 3.4,
when the input samples are real. If the FFT input samples are complex, the
scaling factor is N. So to determine the correct amplitudes of the time-domain
sinusoidal components, we’d have to divide the FFT magnitudes by the ap-
propriate scale factor, N/2 for real inputs and Nfor complex inputs.
If a window function was used on the original time-domain data, some
of the FFT input samples will be attenuated. This reduces the resultant FFT
output magnitudes from their true unwindowed values. To calculate the cor-
rect amplitudes of various time-domain sinusoidal components, then, we’d
have to further divide the FFT magnitudes by the appropriate processing loss
factor associated with the window function used. Processing loss factors for
the most popular window functions are listed in reference [3].
Should we want to determine the power spectrum X (m) of an FFT re-
PS
sult, we’d calculate the magnitude-squared values using
X (m)=|X(m)|2=X (m)2+X (m)2. (4–6)
PS real imag
Doing so would allow us to compute the power spectrum in dB with
⋅
X (m) = 10 log (|X(m)|2) dB. (4–7)
dB 10
The normalized power spectrum in decibels can be calculated using
normalizedX (m)=10 ⋅ log ⎛ ⎜ |X(m)|2 ⎞ ⎟ , (4–8)
dB 10⎝(|X(m)| )2⎠
max

4.3 Derivation of the Radix-2 FFT Algorithm 141
or
⎛ ⎞
⋅ |X(m)|
normalizedX dB (m)=20 log 10⎝ ⎜ |X(m)| ⎠ ⎟. (4–9)
max
In Eqs. (4–8) and (4–9), the term |X(m)| is the largest FFT output magni-
max
tude sample. In practice, we find that plotting X (m) is very informative be-
dB
cause of the enhanced low-magnitude resolution afforded by the logarithmic
decibel scale, as described in Appendix E. If either Eq. (4–8) or Eq. (4–9) is
used, no compensation need be performed for the above-mentioned NorN/2
FFT scale or window processing loss factors. Normalization through division
by (|X(m)| )2 or |X(m)| eliminates the effect of any absolute FFT or
max max
window scale factors.
Knowing that the phase angles X (m) of the individual FFT outputs are
ø
given by
⎛X (m)⎞
X (m)=tan–1⎜ imag ⎟ , (4–10)
ø ⎝ X (m)⎠
real
it’s important to watch out for X (m) values that are equal to zero. That would
real
invalidate our phase-angle calculations in Eq. (4–10) due to division by a zero con-
dition. In practice, we want to make sure that our calculations (or software com-
piler) detect occurrences of X (m)=0 and set the corresponding X (m) to 90°if
real ø
X (m) is positive, set X (m) to 0° if X (m) is zero, and set X (m) to –90° if
imag ø imag ø
X (m) is negative. While we’re on the subject of FFT output phase angles, be
imag
aware that FFT outputs containing significant noise components can cause large
fluctuations in the computed X (m) phase angles. This means that the X (m) sam-
ø ø
ples are only meaningful when the corresponding |X(m)| is well above the aver-
age FFT output noise level.
4.3 DERIVATION OF THE RADIX-2 FFT ALGORITHM
Thissectionandthosethatfollowprovideadetaileddescriptionoftheinternal
datastructuresandoperationsoftheradix-2FFTforthosereadersinterestedin
developingsoftwareFFTroutinesordesigningFFThardware.Toseejustexactly
howtheFFTevolvedfromtheDFT,wereturntotheequationforanN-pointDFT,
N∑–1
X(m)= x(n)e–j2πnm/N.
(4–11)
n=0

142 The Fast Fourier Transform
Astraightforward derivation of the FFT proceeds with the separation of the
input data sequence x(n) into two parts. When x(n) is segmented into its even
and odd indexed elements, we can, then, break Eq. (4–11) into two parts as
(N∑/2)–1 (N∑/2)–1
X(m)= x(2n)e–j2π(2n)m/N + x(2n+1)e–j2π(2n+1)m/N. (4–12)
n=0 n=0
Pulling the constant phase angle outside the second summation,
(N∑/2)–1 (N∑/2)–1
X(m)= x(2n)e–j2π(2n)m/N +e–j2πm/N x(2n+1)e–j2π(2n)m/N. (4–13)
n=0 n=0
Well, here the equations become so long and drawn out that we’ll use a popu-
lar notation to simplify things. We’ll define
W
=e–j2π/N
(4–13’)
N
to represent the complex phase-angle factor that is constant with N. So, Eq.
(4–13) becomes
(N∑/2)–1 (N∑/2)–1
X(m)= x(2n)W2nm +Wm x(2n+1)W2nm. (4–14)
N N N
n=0 n=0
Because W2 = e–j2π2/(N) = e–j2π/(N/2), we can substitute W for W2 in Eq.
N N/2 N
(4–14), as
(Nπ/2)–1 (Nπ/2)–1
X(m)= x(2n)Wnm +Wm x(2n+1)Wnm (4–15)
N/2 N N/2
n=0 n=0
where m is in the range 0 to N/2–1. Index m has that reduced range because
each of the two N/2-point DFTs on the right side of Eq. (4–15) are periodic in
mwith period N/2.
So we now have two N/2 summations whose results can be combined to
give us the first N/2 samples of an N-point DFT.We’vereducedsomeofthenec-
essarynumbercrunchinginEq.(4–15)relativetoEq.(4–11)becausetheWterms
in the two summations of Eq. (4–15) are identical. There’s a further benefit in
breakingtheN-pointDFTintotwopartsbecausetheupperhalfoftheDFTout-
putsiseasytocalculate.ConsidertheX(m+N/2)output.Ifweplugm+N/2infor
minEq.(4–15),then

4.3 Derivation of the Radix-2 FFT Algorithm 143
(N∑/2)–1
X(m+N/2)= x(2n)Wn(m+N/2)
N/2
n=0
+W
(m+N/2) (N∑/2)–
x
1
(2n+1)Wn(m+N/2).
N
n=0 N/2 (4–16)
It looks like we’re complicating things, right? Well, just hang in there for a mo-
ment.Wecannowsimplifythephase-angletermsinsidethesummationsbecause
Wn(m+N/2) =Wnm WnN/2 =Wnm (e–j2πn2N/2N)
N/2 N/2
N/2 N/2
=Wnm (1)=Wnm ,
N/2 N/2
(4–17)
for any integer n. Looking at the so-called twiddle factorin front of the second
summation in Eq. (4–16), we can simplify it as
W(m+N/2) =WmWN/2 =Wm(e–j2πN/2N)=Wm(–1)=–Wm.
(4–18)
N N N N N N
OK, using Eqs. (4–17) and (4–18), we represent Eq. (4–16)’s X(m+N/2) as
(N∑/2)–1 (N∑/2)–1
X(m+N/2)= x(2n)Wnm –Wm x(2n+1)Wnm . (4–19)
N/2 N N/2
n=0 n=0
Now, let’s repeat Eqs. (4–15) and (4–19) to see the similarity:
(N∑/2)–1 (N∑/2)–1
X(m)= x(2n)Wnm +Wm x(2n+1)Wnm , (4–20)
N/2 N N/2
n=0 n=0
and
(N∑/2)–1 (N∑/2)–1
X(m+N/2)= x(2n)Wnm −Wm x(2n+1)Wnm . (4–20’)
N/2 N N/2
n=0 n=0
So here we are. We need not perform any sine or cosine multiplications to get
X(m+N/2). We just change the sign of the twiddle factor Wm and use the re-
N
sults of the two summations from X(m) to get X(m+N/2). Of course, m goes
from 0 to (N/2)–1 in Eq. (4–20), which means to compute an N-point DFT, we
actually perform two N/2-point DFTs—one N/2-point DFT on the even-
indexed x(n) samples and one N/2-point DFT on the odd-indexed x(n) sam-
ples. For N=8, Eqs. (4–20) and (4–20’) are implemented as shown in Figure 4–2.

144 The Fast Fourier Transform
Because
–e–j2πm/N
=
e–j2π(m+N/2)/N,
the negative W twiddle factors before
the second summation in Eq. (4–20’) are implemented with positive W twid-
dle factors that follow the lower DFT in Figure 4-2.
If we simplify Eqs. (4–20) and (4–20’) to the form
X(m)= A(m) + Wm B(m) (4–21)
N
and
X(m+N/2)= A(m) – Wm B(m) , (4–21’)
N
we can go further and think about breaking the two 4-point DFTs into four
2-point DFTs. Let’s see how we can subdivide the upper 4-point DFT in Fig-
m = 0
x(0) X(m)
4-point = X(0)
DFT W 0
8
m = 1
x(2) X(m)
3 = X(1)
nm
x(2n)W 4 W 1
8
0 m = 2
x(4) X(m)
= X(2)
W2
8
m = 3
x(6) X(m)
= X(3)
W3
8
4 m = 0
x(1) W X(m+4)
8
= X(4)
4-point
DFT
m = 1
x(3) W 5 X(m+4)
8
3 nm = X(5)
x(2n+1)W
4
0 m = 2
6
x(5) W X(m+4)
8
= X(6)
m = 3
7
x(7) W X(m+4)
8
= X(7)
Figure 4–2 FFT implementation of an 8-point DFT using two 4-point DFTs.

4.3 Derivation of the Radix-2 FFT Algorithm 145
ure 4–2 whose four outputs are A(m) in Eqs. (4–21) and (4–21’). We segment
the inputs to the upper 4-point DFT into their odd and even components:
(N/2)−1
∑
nm
A(m) = x(2n)W =
N/2
n=0
(N/4)−1 (N/4)−1
∑ x(4n)W 2nm + ∑ x(4n+2)W (2n+1)m . (4–22)
N/2 N/2
n=0 n=0
Because W 2nm = W nm , we can express A(m) in the form of two N/4-point
N/2 N/4
DFTs, as
(N/4)−1 (N/4)−1
∑ ∑
A(m) = x(4n)W nm + W m x(4n+2)W nm . (4–23)
N/4 N/2 N/4
n=0 n=0
Notice the similarity between Eqs. (4–23) and (4–20). This capability to
subdivide an N/2-point DFT into two N/4-point DFTs gives the FFT its ca-
pacity to greatly reduce the number of necessary multiplications to imple-
ment DFTs. (We’re going to demonstrate this shortly.) Following the same
steps we used to obtained A(m), we can show that Eq.(4–21)’s B(m) is
(N/4)−1 (N/4)−1
∑ ∑
B(m) = x(4n+1)W nm + W m x(4n+3)W nm . (4–24)
N/4 N/2 N/4
n=0 n=0
For our N = 8 example, Eqs. (4–23) and (4–24) are implemented as shown in
Figure 4–3. The FFT’s well-known butterflypattern of signal flows is certainly
evident, and we see the further shuffling of the input data in Figure 4–3. The
twiddle factor Wm in Eqs. (4–23) and (4–24), for our N = 8 example, ranges
N/2
from W0 to W3 because the m index, for A(m) and B(m), goes from 0 to 3. For
4 4
any N-point DFT, we can break each of the N/2-point DFTs into two
N/4-point DFTs to further reduce the number of sine and cosine multiplica-
tions. Eventually, we would arrive at an array of 2-point DFTs where no fur-
ther computational savings could be realized. This is why the number of
points in our FFTs is constrained to be some power of two and why this FFT
algorithm is referred to as the radix-2 FFT.
Moving right along, let’s go one step further, and then we’ll be finished
with our N=8-point FFT derivation. The 2-point DFT functions in Figure 4–3
cannot be partitioned into smaller parts—we’ve reached the end of our DFT
reduction process, arriving at the butterfly of a single 2-point DFT as shown
in Figure 4–4. From the definition of W , W0 = e–j2π0/N = 1 and WN/2 =
N N N
e–j2πN/2N=e–jπ
=–1. So the 2-point DFT blocks in Figure 4–3 can be replaced by
the butterfly in Figure 4–4 to give us a full 8-point FFT implementation of the
DFT as shown in Figure 4–5.

146 The Fast Fourier Transform
A(0)
x(0) X(0)
2- D p F o T int W 4 0 W 8 0
A(1)
x(4) X(1)
W 4 1 W 8 1
2 A(2)
x(2) W X(2)
4
2-point W 2
8
DFT
3 A(3)
x(6) W 4 X(3)
3
W
8
x(1) W 4 X(4)
B(0) 8
2-point W 0
2-poDinFtT 4
DFT
5
x(5) W X(5)
B(1) 8
1
W
4
x(3) W 2 W 6 X(6)
4 B(2) 8
2-point
DFT
3 7
x(7) W 4 B(3) W 8 X(7)
Figure 4–3 FFT implementation of an 8-point DFT as two 4-point DFTs and four
2-point DFTs.
OK, we’ve gone through a fair amount of algebraic foot shuffling
here. To verify that the derivation of the FFT is valid, we can apply the
8-point data sequence of Chapter 3’s DFT Example 1 to the 8-point
FFT represented by Figure 4–5. The data sequence representing
x(n) = sin(2π1000nt ) + 0.5sin(2π2000nt +3π/4) is
s s
x(0)=0.3535, x(1)=0.3535,
x(2)=0.6464, x(3)=1.0607,
(4–25)
x(4)=0.3535, x(5)=–1.0607,
x(6)=–1.3535, x(7)=–0.3535.

4.3 Derivation of the Radix-2 FFT Algorithm 147
x(k)
0
W
N
N/2
x(k+N/2) W
N
Figure 4–4 Single 2-point DFT butterfly.
A(0)
x(0) X(0)
1
W 4 0 W 8 0
A(1)
x(4) –1 X(1)
W 4 1 W 8 1
A(2)
x(2) W 2 X(2)
4
1 W2
8
A(3)
x(6) –1 W 3 X(3)
4
3
W
8
x(1) W 4 X(4)
B(0) 8
1
W0
4
x(5) –1 W 5 X(5)
B(1) 8
W1
4
x(3) W 2 W 6 X(6)
4 B(2) 8
1
x(7) –1 W 4 3 B(3) W 8 7 X(7)
Figure 4–5 Full decimation-in-time FFT implementation of an 8-point DFT.

148 The Fast Fourier Transform
We begin grinding through this example by applying the input values from
Eq. (4–25) to Figure 4–5, giving the data values shown on left side of Figure
4–6. The outputs of the second stage of the FFT are
A(0) =0.707 + W0(–0.707) = 0.707 + (1 + j0)(–0.707) = 0 + j0,
4
A(1) =0.0 + W1(1.999) = 0.0 + (0 – j1)(1.999) = 0 – j1.999,
4
A(2) =0.707 + W2(–0.707) = 0.707 + (–1 + j0)(–0.707) = 1.414 + j0,
4
A(3) =0.0 + W3(1.999) = 0.0 + (0 + j1)(1.999) = 0 + j1.999,
4
B(0) =0.707 + W0(0.707) = –0.707 + (1 + j0)(0.707) = 0 + j0,
4
B(1) =1.414 + W1(1.414) = 1.414 + (0 – j1)(1.414) = 1.414 – j1.414,
4
B(2) =–0.707 + W2(0.707) = –0.707 + (–1 + j0)(0.707) = –1.414 + j0, and
4
B(3) =1.414 + W3(1.414) = 1.414 + (0 + j1)(1.414) = 1.414 + j1.414 .
4
0.3535 0.707 A(0)
x(0) X(0)
1
W0 W 0
4 8
A(1)
x(4) –1 X(1)
0.3535 0
W1
4 W 8
1
x(2) 0.6464 W 2 A(2) X(2)
–0.707 4
1
2
W
8
A(3)
x(6) –1.3535 –1 1.999 W 4 3 X(3)
3
W
8
x(1) 0.3535 –0.707 W 4 X(4)
B(0) 8
1
0
W
4
x(5) –1.0607 –1 1.414 B(1) W 5 8 X(5)
1
W4
0.707
x(3) 1.0607 W 2 W 6 X(6)
4 B(2) 8
1
3 7
x(7) –0.3535 –1 1.414 W 4 B(3) W 8 X(7)
1st Stage 2nd Stage 3rd Stage
Figure 4–6 Eight-point FFT of Example 1 from Section 3.1.

4.4 FFT Input/Output Data Index Bit Reversal 149
Calculating the outputs of the third stage of the FFT to arrive at our final
answer:
X(0) = A(0) + W0B(0) = 0 + j0 + (1 + j0)( 0 + j0) = 0 + j0 + 0 + j0 = 0 (cid:2)0°,
8
X(1) = A(1) +W1B(1) = 0 – j1.999 + (0.707 – j0.707)(1.414 – j1.414)
8
= 0 – j1.999 + 0 – j1.999 = 0 – j4 =4 (cid:2)–90°,
X(2) = A(2) +W2B(2) = 1.414 + j0 + (0 – j1)( –1.414 + j0)
8
= 1.414 + j0 + 0 + j1.4242 = 1.414 + j1.414 =2 (cid:2)45°,
X(3) = A(3) +W3B(3) = 0 + j1.999 + (–0.707 – j0.707)(1.414 + j1.414 )
8
= 0 + j1.999 + 0 – j1.999 = 0 (cid:2)0°,
X(4) = A(0) –W0B(0) = A(0) +W4B(0) = 0 + j0 + (–1 + j0)(0 + j0)
8 8
= 0 + j0 + 0 + j0 = 0 (cid:2)0°,
X(5) = A(1) –W1B(1)
8
=A(1) +W5B(1) = 0 – j1.999 + (–0.707 + j0.707)(1.414 – j1.414)
8
= 0 – j1.999 + 0 + j1.999 = 0 (cid:2)0°,
X(6) = A(2) –W2B(2)
8
=A(2) +W6B(2) = 1.414 + j0 + (0 + j1)(–1.414 + j0)
8
= 1.414 + j0 + 0 – j1.414 = 1.414 – j1.414 =2 (cid:2)–45°, and
X(7) = A(3) –W3B(3)
8
=A(3) +W7B(3) = 0 + j1.999 + (0.707 + j0.707)(1.414 + j1.414)
8
= 0 + j1.999 + 0 + j1.999 = 0 + j4 =4 (cid:2)90°.
So, happily, the FFT gives us the correct results, and again we remind
the reader that the FFT is not an approximation to a DFT; it is the DFT with a
reduced number of necessary arithmetic operations. You’ve seen from the
above example that the 8-point FFT example required less effort than the
8-point DFT Example 1 in Section 3.1. Some authors like to explain this arith-
metic reduction by the redundancies inherent in the twiddle factors Wm. They
N
illustrate this with the starburst pattern in Figure 4–7 showing the equivalen-
cies of some of the twiddle factors in an 8-point DFT.
4.4 FFT INPUT/OUTPUT DATA INDEX BIT REVERSAL
OK, let’s look into some of the special properties of the FFT that are important
to FFT software developers and FFT hardware designers. Notice that Figure
4–5 was titled “Full decimation-in-time FFT implementation of an 8-point
DFT.” The decimation-in-time phrase refers to how we broke the DFT input
samples into odd and even parts in the derivation of Eqs. (4–20), (4–23), and
(4–24). This time decimation leads to the scrambled order of the input data’s
index n in Figure 4–5. The pattern of this shuffled order can be understood

150 The Fast Fourier Transform
6 3
w =w
8 4
5 7
w w
8 8
w 4 = w 2 w 0 = w 0
8 4 8 4
3 1
w w
8 8
2 1
w = w
8 4
Figure 4–7 Cyclic redundancies in the twiddle factors of an 8-point FFT.
with the help of Table 4–1. The shuffling of the input data is known as bit re-
versalbecause the scrambled order of the input data index can be obtained by
reversing the bits of the binary representation of the normal input data index
order. Sounds confusing, but it’s really not—Table 4–1 illustrates the input
index bit reversal for our 8-point FFT example. Notice the normal index order
in the left column of Table 4–1 and the scrambled order in the right column
that corresponds to the final decimated input index order in Figure 4–5.
We’ve transposed the original binary bits representing the normal index
order by reversing their positions. The most significant bit becomes the least
Table 4–1 Input Index Bit Reversal for an 8-Point FFT
Normal order Binary bits Reversed bits Bit-reversed
of index n of index n of index n order of index n
0 000 000 0
1 001 100 4
2 010 010 2
3 011 110 6
4 100 001 1
5 101 101 5
6 110 011 3
7 111 111 7

4.5 Radix-2 FFT Butterfly Structures 151
significant bit and the least significant bit becomes the most significant bit,
the next to the most significant bit becomes the next to the least significant bit,
and the next to the least significant bit becomes the next to the most signifi-
cant bit, and so on.†
4.5 RADIX-2 FFT BUTTERFLY STRUCTURES
Let’s explore the butterfly signal flows of the decimation-in-time FFT a bit
further. To simplify the signal flows, let’s replace the twiddle factors in Figure
4–5 with their equivalent values referenced to Wm, where N=8. We can show
N
just the exponents m of Wm, to get the FFT structure shown in Figure 4–8.
N
That is, W1 from Figure 4–5 is equal to W2 and is shown as a 2 in Figure 4–8,
4 8
W2from Figure 4–5 is equal to W4and is shown as a 4 in Figure 4–8, etc. The
4 8
1s and –1s in the first stage of Figure 4–5 are replaced in Figure 4–8 by 0s and
4s, respectively. Other than the twiddle factor notation, Figure 4–8 is identical
to Figure 4–5. We can shift around the signal nodes in Figure 4–5 and arrive at
an 8-point decimation-in-time FFT as shown in Figure 4–9. Notice that the
input data in Figure 4–9 is in its normal order and the output data indices are
bit-reversed. In this case, a bit-reversal operation needs to be performed at the
output of the FFT to unscramble the frequency-domain results.
Figure 4–10 shows an FFT signal-flow structure that avoids the bit-
reversal problem altogether, and the graceful weave of the traditional FFT
butterflies is replaced with a tangled, but effective, configuration.
Not too long ago, hardware implementations of the FFT spent most of
their time (clock cycles) performing multiplications, and the bit-reversal
process necessary to access data in memory wasn’t a significant portion of the
overall FFT computational problem. Now that high-speed multiplier/accu-
mulator integrated circuits can multiply two numbers in a single clock cycle,
FFT data multiplexing and memory addressing have become much more im-
portant. This has led to the development of efficient algorithms to perform bit
reversal[7–10].
There’s another derivation for the FFT that leads to butterfly structures
looking like those we’ve already covered, but the twiddle factors in the butter-
flies are different. This alternate FFT technique is known as the decimation-in-
frequency algorithm. Where the decimation-in-time FFT algorithm is based on
subdividing the input data into its odd and even components, the decimation-
in-frequency FFT algorithm is founded upon calculating the odd and even
output frequency samples separately. The derivation of the decimation-in-
frequency algorithm is straightforward and included in many tutorial papers
and textbooks, so we won’t go through the derivation here[4,5,15,16]. We
†Many that are first shall be last; and the last first. [Mark 10:31]

152 The Fast Fourier Transform
x(0) X(0)
0 0
0
x(4) 4 X(1)
2
1
x(2) 4 X(2)
0
2
x(6) 4 6 X(3)
3
x(1) 4 X(4)
0 0
x(5) 4 5 X(5)
2
x(3) 4 6 X(6)
0
x(7) 4 6 7 X(7)
Figure 4–8 Eight-point decimation-in-time FFT with bit-reversed inputs.
will, however, illustrate decimation-in-frequency butterfly structures (analo-
gous to the structures in Figures 4–8 through 4–10) in Figures 4–11 though 4–13.
So an equivalent decimation-in-frequency FFT structure exists for each
decimation-in-time FFT structure. It’s important to note that the number of
necessary multiplications to implement the decimation-in-frequency FFT al-
gorithms is the same as the number necessary for the decimation-in-time FFT
algorithms. There are so many different FFT butterfly structures described in
the literature that it’s easy to become confused about which structures are
x(0) X(0)
0 0
0
x(1) 4 X(4)
0
0
x(2) 4 X(2)
2
0
x(3) 4 6 X(6)
0
x(4) 4 X(1)
2 1
x(5) 4 5 X(5)
2
x(6) 4 6 X(3)
3
x(7) 4 6 7 X(7)
Figure 4–9 Eight-point decimation-in-time FFT with bit-reversed outputs.

4.5 Radix-2 FFT Butterfly Structures 153
x(0) X(0)
0 0
0
x(1) X(1)
0 4
0 1
x(2) X(2)
4
0
x(3) X(3)
0 4 5
x(4) 4 X(4)
2
x(5) 4 X(5)
2 6
x(6) 4 6 X(6)
2 3
x(7) 4 6 7 X(7)
Figure 4–10 Eight-point decimation-in-time FFT with inputs and outputs in normal
order.
decimation-in-time and which are decimation-in-frequency. Depending on
how the material is presented, it’s easy for a beginner to fall into the trap of
believing that decimation-in-time FFTs always have their inputs bit-reversed
and decimation-in-frequency FFTs always have their outputs bit-reversed.
This is not true, as the above figures show. Decimation-in-time or -frequency
is determined by whether the DFT inputs or outputs are partitioned when de-
riving a particular FFT butterfly structure from the DFT equations.
x(0) X(0)
0
x(4) 4 X(1)
0
x(2) 4 X(2)
0
2
x(6) 6 4 X(3)
0
x(1) 4 X(4)
0
1
x(5) 5 4 X(5)
0
2
x(3) 6 4 X(6)
0
2
3
x(7) 7 6 4 X(7)
Figure 4–11 Eight-point decimation-in-frequency FFT with bit-reversed inputs.

154 The Fast Fourier Transform
x(0) X(0)
0
x(1) 4 X(4)
0
x(2) 4 X(2)
2 0
x(3) 6 4 X(6)
0
x(4) 4 X(1)
1
0
x(5) 5 4 X(5)
2
0
x(6) 6 4 X(3)
3 2 0
x(7) 7 6 4 X(7)
Figure 4–12 Eight-point decimation-in-frequency FFT with bit-reversed outputs.
4.6 ALTERNATE SINGLE-BUTTERFLY STRUCTURES
Let’s take one more look at a single butterfly. The FFT butterfly structures in
Figures 4–8, 4–9, 4–11, and 4–12 are the direct result of the derivations of the
decimation-in-time and decimation-in-frequency algorithms. Although it’s
not very obvious at first, the twiddle factor exponents shown in these struc-
tures do have a consistent pattern. Notice how they always take the general
x(0) X(0)
x(1) 0 X(1)
4
0
x(2) X(2)
4
1 0
x(3) X(3)
5 4
0
x(4) 4 X(4)
0
2
x(5) 6 4 X(5)
2 0
x(6) 6 4 X(6)
3 2 0
x(7) 7 6 4 X(7)
Figure 4–13 Eight-point decimation-in-frequency FFT with inputs and outputs in
normal order.

4.6 Alternate Single-Butterfly Structures 155
Decimation in time Decimation in frequency
x x' x x''
k
W
N
(a)
k
W
N
k+N/2 k+N/2
y W y' y W y''
N N
x x' x x''
k
W
N
(b)
k
W
N
k k
y –W y' y –W y''
N N
x x' x x''
(c)
k k
y W –1 y' y –1 W y''
N N
Figure 4–14 Decimation-in-time and decimation-in-frequency butterfly struc-
tures: (a) original form; (b) simplified form; (c) optimized form.
forms shown in Figure 4–14(a).†To implement the decimation-in-time butter-
fly of Figure 4–14(a), we’d have to perform two complex multiplications and
two complex additions. Well, there’s a better way. Consider the decimation-
in-time butterfly in Figure 4–14(a). If the top input is xand the bottom input is
y, the top butterfly output would be
x’ = x+Wk y, (4–26)
N
and the bottom butterfly output would be
y’ = x+Wk+N/2y. (4–27)
N
†Remember, for simplicity the butterfly structures in Figures 4–8 through 4–13 show only the
twiddle factor exponents, kandk+N/2, and not the entire complex twiddle factors.

156 The Fast Fourier Transform
x x' x x''
+ +
– – k
W
k N
W
y N y' y y''
(a) (b)
Figure 4–15 Alternate FFT butterfly notation: (a) decimation in time; (b) decima-
tion in frequency.
Fortunately, the operations in Eqs. (4–26) and (4–27) can be simplified because
the two twiddle factors are related by
Wk+N/2 =WkWN/2 =Wk(e–j2πN/2N)=Wk(–1)=–Wk . (4–28)
N N N N N N
So we can replace the Wk+N/2 twiddle factors in Figure 4–14(a) with –Wk to
N N
give us the simplified butterflies shown in Figure 4–14(b). Because the twid-
dle factors in Figure 4–14(b) differ only by their signs, the optimized butter-
flies in Figure 4–14(c) can be used. Notice that these optimized butterflies
require two complex additions but only one complex multiplication, thus re-
ducing our computational workload.†
We’ll often see the optimized butterfly structures of Figure 4–14(c) in the
literature instead of those in Figure 4–14(a). These optimized butterflies give
us an easy way to recognize decimation-in-time and decimation-in-frequency
algorithms. When we do come across the optimized butterflies from Figure
4–14(c), we’ll know that the algorithm is decimation-in-time if the twiddle
factor precedes the –1, or else the algorithm is decimation-in-frequency if the
twiddle factor follows the –1.
Sometimes we’ll encounter FFT structures in the literature that use the
notation shown in Figure 4–15[5, 12]. These wingless butterflies are equiva-
lent to those shown in Figure 4–14(c). The signal-flow convention in Figure
4–15 is such that the plus output of a circle is the sum of the two samples that
enter the circle from the left, and the minus output of a circle is the difference
of the samples that enter the circle. So the outputs of the decimation-in-time
butterflies in Figures 4–14(c) and 4–15(a) are given by
x’ = x+Wk y, and y’ = x–Wk y. (4–29)
N N
† It’s because there are (N/2)log N butterflies in an N-point FFT that we said the number of
2
complex multiplications performed by an FFT is (N/2)log Nin Eq. (4–2).
2

4.6 Alternate Single-Butterfly Structures 157
The outputs of the decimation-in-frequency butterflies in Figures 4–14(c) and
4–15(b) are
x"=x+y,andy"=Wk(x–y)=Wkx–Wky.
(4–30)
N N N
So which FFT structure is the best one to use? It depends on the applica-
tion, the hardware implementation, and convenience. If we’re using a soft-
ware routine to perform FFTs on a general-purpose computer, we usually
don’t have a lot of choices. Most folks just use whatever existing FFT routines
happen to be included in their commercial software package. Their code may
be optimized for speed, but you never know. Examination of the software
code may be necessary to see just how the FFT is implemented. If we feel the
need for speed, we should check to see if the software calculates the sines and
cosines each time it needs a twiddle factor. Trigonometric calculations nor-
mally take many machine cycles. It may be possible to speed up the algo-
rithm by calculating the twiddle factors ahead of time and storing them in a
table. That way, they can be looked up, instead of being calculated each time
they’re needed in a butterfly. If we’re writing our own software routine,
checking for butterfly output data overflow and careful magnitude scaling
may allow our FFT to be performed using integer arithmetic that can be faster
on some machines.† Care must be taken, however, when using integer arith-
metic; some Reduced Instruction Set Computer (RISC) processors actually
take longer to perform integer calculations because they’re specifically de-
signed to operate on floating-point numbers.
If we’re using commercial array processor hardware for our calcula-
tions, the code in these processors is alwaysoptimized because their purpose
in life is high speed. Array processor manufacturers typically publicize their
products by specifying the speed at which their machines perform a 1024-
point FFT. Let’s look at some of our options in selecting a particular FFT
structure in case we’re designing special-purpose hardware to implement an
FFT.
The FFT butterfly structures previously discussed typically fall into one
of two categories: in-place FFT algorithms and double-memory FFT algo-
rithms. An in-place algorithm is depicted in Figure 4–5. The output of a but-
terfly operation can be stored in the same hardware memory locations that
previously held the butterfly’s input data. No intermediate storage is neces-
sary. This way, for an N-point FFT, only 2N memory locations are needed.
(The 2 comes from the fact that each butterfly node represents a data value
that has both a real and an imaginary part.) The rub with the in-place
† Overflow is what happens when the result of an arithmetic operation has too many bits, or
digits, to be represented in the hardware registers designed to contain that result. FFT data over-
flow is described in Section 12.3.

158 The Fast Fourier Transform
algorithms is that data routing and memory addressing are rather compli-
cated. Adouble-memory FFT structure is that depicted in Figure 4–10. With
this structure, intermediate storage is necessary because we no longer have
the standard butterflies, and 4Nmemory locations are needed. However, data
routing and memory address control are much simpler in double-memory
FFT structures than the in-place technique. The use of high-speed, floating-
point integrated circuits to implement pipelined FFT architectures takes bet-
ter advantage of their pipelined structure when the double-memory
algorithm is used[13].
There’s another class of FFT structures, known as constant-geometry al-
gorithms, that make the addressing of memory both simple and constant for
each stage of the FFT. These structures are of interest to those folks who
build special-purpose FFT hardware devices[4,14]. From the standpoint of
general hardware the decimation-in-time algorithms are optimum for real
input data sequences, and decimation-in-frequency is appropriate when the
input is complex[6]. When the FFT input data is symmetrical in time, special
FFT structures exist to eliminate unnecessary calculations. These special but-
terfly structures based on input data symmetry are described in the litera-
ture[15].
For two-dimensional FFT applications, such as processing photographic
images, the decimation-in-frequency algorithms appear to be the optimum
choice[16]. Your application may be such that FFT input and output bit rever-
sal is not an important factor. Some FFT applications allow manipulating a
bit-reversed FFT output sequence in the frequency domain without having to
unscramble the FFT’s output data. Then an inverse transform that’s expecting
bit-reversed inputs will give a time-domain output whose data sequence is
correct. This situation avoids the need to perform any bit reversals at all. Mul-
tiplying two FFT outputs to implement convolution or correlation are exam-
ples of this possibility.† As we can see, finding the optimum FFT algorithm
and hardware architecture for an FFT is a fairly complex problem to solve,
but the literature provides guidance[4,17,18].
REFERENCES
[1] Cooley, J., and Tukey, J. “An Algorithm for the Machine Calculation of Complex Fourier
Series,”Math. Comput., Vol. 19, No. 90, April 1965, pp. 297–301.
[2] Cooley, J., Lewis, P., and Welch, P. “Historical Notes on the Fast Fourier Transform,” IEEE
Trans. on Audio and Electroacoustics,Vol. AU-15, No. 2, June 1967.
†See Section 13.10 for an example of using the FFT to perform convolution.

References 159
[3] Harris, F. J. “On the Use of Windows for Harmonic Analysis with the Discrete Fourier
Transform,” Proceedings of the IEEE,Vol. 66, No. 1, January 1978, p. 54.
[4] Oppenheim, A. V., and Schafer, R. W. Discrete-Time Signal Processing, Prentice Hall, Engle-
wood Cliffs, New Jersey, 1989, p. 608.
[5] Rabiner, L. R., and Gold, B. Theory and Application of Digital Signal Processing, Prentice
Hall, Englewood Cliffs, New Jersey, 1975, p. 367.
[6] Sorenson, H. V., Jones, D. L., Heideman, M. T., and Burrus, C. S. “Real-Valued Fast
Fourier Transform Algorithms,” IEEE Trans. on Acoust. Speech, and Signal Proc., Vol.
ASSP-35, No. 6, June 1987.
[7] Evans, D. “An Improved Digit-Reversal Permutation Algorithm for the Fast Fourier and
Hartley Transforms,” IEEE Trans. on Acoust. Speech, and Signal Proc., Vol. ASSP-35, No. 8,
August 1987.
[8] Burrus, C. S. “Unscrambling for Fast DFT Algorithms,” IEEETrans. on Acoust. Speech, and
Signal Proc.,Vol. 36, No. 7, July 1988.
[9] Rodriguez, J. J. “An Improved FFT Digit-Reversal Algorithm,” IEEE Trans. on Acoust.
Speech, and Signal Proc.,Vol. ASSP-37, No. 8, August 1989.
[10] Land, A. “Bit Reverser Scrambles Data for FFT,” EDN,March 2, 1995.
[11] JG-AE Subcommittee on Measurement Concepts, “What Is the Fast Fourier Transform?,”
IEEETrans. on Audio and Electroacoustics,Vol. AU-15, No. 2, June 1967.
[12] Cohen, R., and Perlman, R. “500 kHz Single-Board FFT System Incorporates DSP-
Optimized Chips,” EDN, October 31, 1984.
[13] Eldon, J., and Winter, G. E. “Floating-Point Chips Carve Out FFT Systems,” Electronic De-
sign, August 4, 1983.
[14] Lamb, K. “CMOS Building Blocks Shrink and Speed Up FFT Systems,” Electronic Design,
August 6, 1987.
[15] Markel, J. D. “FFT Pruning,” IEEETrans. on Audio and Electroacoustics,Vol. AU-19, No. 4,
December 1971.
[16] Wu, H. R., and Paoloni, F. J. “The Structure of Vector Radix Fast Fourier Transforms,”
IEEETrans. on Acoust. Speech, and Signal Proc.,Vol. ASSP-37, No. 8, August 1989.
[17] Ali, Z. M. “High Speed FFT Processor,” IEEETrans. on Communications,Vol. COM-26, No.
5, May 1978.
[18] Bergland, G. “Fast Fourier Transform Hardware Implementations—An Overview,” IEEE
Trans. on Audio and Electroacoustics,Vol. AU-17, June 1969.

160 The Fast Fourier Transform
CHAPTER 4 PROBLEMS
4.1 Thinking about the FFT:
(a) How do the results differ between performing an N-point FFT and per-
forming an N-point discrete Fourier transform (DFT) on the same set of
time samples?
(b) What is the restriction on the number of time samples, N, in performing
anN-point radix-2 FFT?
4.2 Assume we want to compute an N-point FFT of an x(n) audio signal from a
compact disc (CD), with the FFT’s output frequency-domain sample spacing
no greater than 1 Hz. If x(n)’s sample rate is f = 44.1 kHz, what is the number
s
of necessary time samples, N, applied to the FFT?
4.3 Assume we have an x(n) time-domain sequence, whose length is 3800 sam-
ples, on which we want to perform an FFT. The 3800 time samples represent a
total signal collection-interval duration of 2 seconds.
(a) How many zero-valued samples must be appended (zero padding) to x(n)
in order to implement an FFT?
(b) After the FFT is performed, what is the spacing, measured in Hz, between
the frequency-domain FFT samples?
(c) In the case of lowpass sampling, what is the highest-frequency spectral
component permitted in the original analog x(t) signal such that no alias-
ing errors occur in x(n)?
4.4 This problem illustrates the computational savings afforded by the FFT over
that of the discrete Fourier transform (DFT). Suppose we wanted to perform a
spectrum analysis on a time-domain sequence whose length is 32768 (215)
samples. Estimate the ratio of the number of complex multiplications needed
by a 32768-point DFT over the number of complex multiplies needed by a
32768-point FFT. (Assume that one of the text’s optimized Figure 4–14(c) but-
terflies, requiring one complex multiply per butterfly operation, is used to im-
plement the FFT.)
4.5 Think about the system in Figure P4–5 using an FFT to measure the ampli-
tude of the p(t) signal. The output of the mixer, the product p(t)q(t), contains
the sum of two sinusoids whose amplitudes are proportional to the peak
value of p(t). The frequencies of those sinusoids are 50 Hz and 2050 Hz. The
lowpass filter rejects the 2050 Hz signal. Due to imperfections in the mixer,
signal p(t)q(t) is riding on a constant DC (zero Hz) bias represented as value
D. This scenario results in an x(n) time sequence whose average value is 17.

Chapter 4 Problems 161
(a) What is the minimum value for the analog-to-digital converter’s f sample
s
rate to satisfy the Nyquist criterion?
(b) If we collect 2048 filter output samples and perform a 2048-point FFT,
what will be the magnitude of the FFT’s X(0) sample?
Mixer
p(t) =
sin(2 1050t) p(t)q(t) + D Analog Analog-to- x(n)
lowpass digital FFT
filter converter
q(t) = sin(2 1000t) f clock
s
Figure P4–5
4.6 Assume you’ve purchased a high-performance commercial real-timespectrum
analyzer that contains an analog-to-digital converter so that the analyzer can
accept analog (continuous) x(t) input signals. The analyzer can perform a
1024-point FFT in 50 microseconds and has two banks of memory in which
the analog-to-digital converter samples are stored as shown in Figure P4–6(a).
An FFT is performed on 1024 x(n) signal samples stored in Memory Bank 1
while 1024 new x(n) time samples are being loaded into Memory Bank 2.
At the completion of the first FFT, the analyzer waits until Memory Bank
2 is filled with 1024 samples and then begins performing an FFT on the data
in that second memory. During the second FFT computation still newer x(n)
time samples are loaded into Memory Bank 1. Thus the analyzer can compute
1024 FFT results as often as once every 50 microseconds, and that is the
Memory
1
x(t) x(n)
A-to-D 1024-point
converter FFT
(a) Memory
2 X(m)
f clock Spectral
s
display
X(f)
B
max
(b)
0 Hz
Figure P4–6

162 The Fast Fourier Transform
meaning of the phrase “real-time spectrum analyzer.” Here’s your problem:
In a lowpass sampling scenario what is the maximum one-sided bandwidth
B of the analog x(t) input signal for which the analyzer can perform real-
max
time FFTs without discarding (ignoring) any discrete x(n) samples? (The defi-
nition of bandwidth B is shown in Figure P4–6(b).)
max
4.7 Here’s an interesting problem. Assume we performed lowpass sampling of
an analog x(t) signal, at a sample rate of f = 20 kHz, obtaining a discrete se-
s
quence x (n). Next we perform an FFT on x (n) to obtain the |X (m)| FFT
1 1 1
magnitude results presented in Figure P4–7(a). There we see our signal of in-
terest in the range of 0 to 4 kHz, but we detect a high-magnitude narrowband
spectral noise signal centered at 5 kHz.
Experimenting, as every good engineer should, we change the sampling
rate to f’ = 19 kHz, obtaining a new discrete sequence x (n). Performing an
s 2
FFT on x (n), we obtain the |X (m)| FFT magnitude results presented in Fig-
2 2
ure P4–7(b). In our new spectral results we see our signal of interest remains
in the frequency range of 0 to 4 kHz, but the narrowband spectral noise signal
is now centered near 4 kHz! (If this ever happens to you in practice, to quote
Veronica in the 1986 movie The Fly, “Be afraid. Be very afraid.”) Describe the
characteristic of the analog x(t) that would account for the unexpected shift in
center frequency of the narrowband noise in the |X (m)| FFT results.
2
|X (m)|
1
Signal of interest High-level
narrowband noise
0
B
d
(a)
0 2000 4000 6000 8000 10000
Freq (Hz) (f/2)
s
|X (m)|
2
Signal of interest High-level
narrowband noise
0
B
d
(b)
0 2000 4000 6000 8000 9500
Freq (Hz) (f /2)
s
Figure P4–7

Chapter 4 Problems 163
4.8 In the text’s derivation of the radix-2 FFT, to simplify the algebraic notation
we represented unity-magnitude complex numbers (what we called “twiddle
factors”) in the following form:
α= Wk.
N
Ifk=3 and N=16:
(a) Express αas a complex number in polar (complex exponential) form.
(b) Express αas a complex number in rectangular form.
4.9 Reviewing the 8-point FFT signal-flow diagram in the text’s Figure 4–5:
(a) Whichx(n) input samples affect the value of the FFT’s X(2) output sample?
(b) Whichx(n) input samples affect the value of the FFT’s X(5) output sample?
4.10 Figure P4–10 shows a 4-point FFT using standard decimation-in-time butter-
flies. Redraw that FFT using optimized decimation-in-time butterflies as
shown in the text’s Figure 4–14(c). In your drawing provide the correct in-
dices for the X(m) output samples.
x(0) X(?)
0
0 W
W 4
4
2
x(1) W X(?)
4
2
x(2) W X(?)
4 1
W
W 0 4
4
2 3
x(3) W W X(?)
4 4
Figure P4–10
4.11 Being able to compute individual twiddle factors within an FFT can be im-
portant when implementing specialized FFTs, such as pruned FFTs. (Pruned
FFTs are FFTs where we need not compute all N FFT output samples[Pruned
FFT–1-Pruned FFT 4]). Figure P4–11shows the signal-flow diagram of a stan-
dard 8-point decimation-in-time (DIT) FFT with bit-reversed inputs. As in the
text’s Figure 4–8, the number on an arrow is the integer kof a butterfly’s
Wk = e −j2πk/8
8
twiddle factor. Notice that the number of unique twiddle factors is different
in each of the three stages. The values of the R unique twiddle factors in the
qth stage of a general N-point DIT FFT are given by

164 The Fast Fourier Transform
kthtwiddlefactorofqthstage=WkN/P, fork=0,1,2,...,R−1.
N
What are the expressions for the above RandPfactors in terms of the FFT’s q
stage number?
Hint:Use the 8-point FFT in Figure P4–11 as a guide to find RandP.
Stage Stage Stage
q = 1 q = 2 q = 3
x(0) X(0)
0 0
0
x(4) 4 X(1)
2
1
x(2) 4 X(2)
0
2
x(6) 4 6 X(3)
3
x(1) 4 X(4)
0 0
x(5) 4 5 X(5)
2
x(3) 4 6 X(6)
0
x(7) 4 6 7 X(7)
Figure P4–11
4.12 Let’s become more familiar with the interesting internal computations of a
radix-2 FFT. Figure P4–12 shows the signal-flow diagram of a standard
x(0) X(0)
0 0
0
x(1) 4 X(4)
0
0
x(2) 4 X(2)
2
0
x(3) 4 6 X(6)
0
x(4) 4 X(1)
2 1
x(5) 4 5 X(5)
2
x(6) 4 6 X(3)
3
x(7) 4 6 7 X(7)
Figure P4–12

Chapter 4 Problems 165
8-point decimation-in-time FFT with bit-reversed outputs. In that figure, as in
the text’s Figure 4–9, the number on an arrow is the integer k of a butterfly’s
e–j2πk/8twiddle
factor.
(a) Redraw Figure P4–12, replacing the k factors with the butterflies’ full
complex twiddle factors in rectangular notation.
(b) Regarding your solution to the above Part (a), comment on any interest-
ing properties of the twiddle factors in the FFT’s first and second stages.
4.13 To reiterate the meaning and correctness of the FFT butterfly structures in the
text, we examine the 8-point decimation-in-time FFT with bit-reversed inputs.
That FFT, the text’s Figure 4–8 repeated here as Figure P4–13, uses our nota-
tion where a number on an arrow is the integer kof a butterfly’s
e–j2πk/8twid-
dle factor. Compute the values at sample nodes A through F, in terms of the
x(n) input samples, and show that the FFT’s X(2) output is equal to a DFT’s
output for m=2 in
N∑ −1 ∑7
X(m) = x(n)e
−j2πmn/N
= x(n)e
−j2π2n/N.
m=2 m=2
n=0 n=0
Hint:To keep the notation simple, use the term Wqto represente–j2πq/8.
A
x(0) X(0)
0 0
0
x(4) 4 X(1)
2
1
B E
x(2) 4 X(2)
0
2
x(6) 4 6 X(3)
3
C
x(1) 4 X(4)
0 0
x(5) 4 5 X(5)
2
D F
x(3) 4 6 X(6)
0
x(7) 4 6 7 X(7)
Figure P4–13
4.14 Consider the 16-point decimation-in-time FFT in Figure P4–14 that is imple-
mented in a similar manner to that shown in the text’s Figure 4–9. This FFT
has in-order input data indexing. That is, the x(n) input indexing is in normal

166 The Fast Fourier Transform
numerical order from x(0) to x(15). What will be the order of the frequency-
domain indexing of the X(m) output samples for this 16-point radix-2 FFT?
x(0) X(?)
x(1) X(?)
x(2) X(?)
x(3) X(?)
x(4) X(?)
x(5) 16-point X(?)
x(6) FFT X(?)
x(7) X(?)
x(8) X(?)
x(9) X(?)
x(10) X(?)
x(11) X(?)
x(12) X(?)
x(13) X(?)
x(14) X(?)
x(15) X(?)
Figure P4–14
4.15 Is it possible to examine the signal-flow diagram of a single standard butterfly,
such as that in Figure P4–15, and determine if it is a decimation-in-time (DIT)
butterfly or a decimation-in-frequency (DIF) butterfly? Justify your answer.
A C
j2 k/N
e
j2 m/N
B e D
Figure P4–15
4.16 Let’s explore the internal structure of a single radix-2 FFT butterfly. Figure
P4–16(a) shows our standard notation for a decimation-in-time butterfly
where the input and output samples (A, B, C, and D) are complex-valued.
Figure P4–16(b) shows the same decimation-in-time butterfly where the input
and output values are represented by real-valued samples. We use the nota-
tion that
A=A +jA
R I
where A andA are real-valued. Draw the real-valued block diagram of what
R I
arithmetic is performed inside the rectangle in Figure P4–16(b). Be sure to

Chapter 4 Problems 167
include in your diagram the expressions (the equations) for the real-valued
C , C, D , and D output samples in terms of the real-valued A , A, B , and
R I R I R I R
B input samples and the twiddle factor angle θ. The solution to this problem
I
illustrates the computational complexity of performing a single FFT butterfly.
Decimation-in-time butterfly
A C = A + Bej
(a)
j
B e -1 D = A Bej
A Real-valued C = func(A ,A,B ,B, )
R R R I R I
A decimation-in-time C = func(A ,A,B ,B, )
I butterfly I R I R I
(b)
B Twiddle D = func(A ,A,B ,B, )
R R R I R I
B factor = ej D = func(A ,A,B ,B, )
I I R I R I
Figure P4–16
4.17 Here’s a problem that has much practical importance. It concerns the data
word growth that can occur inside an FFT.
For this problem, our assumptions are:
• We are implementing an FFT using the optimized decimation-in-time FFT
butterfly structure, shown in Figure P4–17, to compute intermediate results.
• The complex data samples AandBare contained in 8-bit storage locations
using the sign-magnitudenumber format system. (In that number format the
most positive and most negative decimal numbers we can store, as binary
words in an 8-bit-wide memory location, are +127 and –127 respectively.)
It’s difficult at first to imagine that multiplying complex samples A and B by
sines and cosines (the real and imaginary parts of
e–j2πk/N)
can lead to
A C
j2 k/N
B e D
Figure P4–17

168 The Fast Fourier Transform
excessive data word growth—particularly because sines and cosines are
never greater than unity. However, significant data word growth can happen
within an FFT butterfly.
(a) In our 8-bit number format scenario, what is the maximum possible deci-
mal value of the real part of the complex output sample C?
(b) How many binary bits are needed for a storage register (memory loca-
tion) to hold that maximum real part of the complex output sample C?
4.18 In 2006 the scientists at the Max Planck Institute for Radio Astronomy, in
Bonn, Germany, built a hardware spectrum analyzer that performs 16384-
point FFTs. This massively parallel analyzer performs 1.744 x 105 such FFTs
per second. Assuming that the FFTs use the optimized decimation-in-
frequency FFT butterfly structure, shown in Figure P4–18, and that the Aand
B samples are complex-valued, how many real-valued multiplies per second
are being performed by the spectrum analyzer? Show your work.
A C
B e j2 k/N D
Figure P4–18
REFERENCES
[Pruned FFT 1] Nagai, K. “Pruning the Decimation-in-Time FFT Algorithm with Frequency
Shift,”IEEE Trans. on ASSP, Vol. ASSP-34, August 1986, pp. 1008–1010.
[Pruned FFT 2] Skinner, D. “Pruning the Decimation-in-Time FFT Algorithm,” IEEE Trans. on
ASSP, Vol. ASSP-24, April 1976, pp. 193–194.
[Pruned FFT 3] Markel, J. D. “FFT Pruning,” IEEE Trans. on Audio Electroacoustics, Vol. AU-19,
December 1971, pp. 305–311.
[Pruned FFT 4] Sreenivas, T., and Rao, P. “FFT Algorithm for Both Input and Ouput Pruning,”
IEEE Trans. on ASSP, Vol. ASSP-27, June 1979, pp. 291–292.

CHAPTER FIVE
Finite
Impulse
Response
Filters
The filtering of digitized data, if not the most fundamental, is certainly the
oldest discipline in the field of digital signal processing. Digital filtering’s ori-
gins go back 50 years. The growing availability of digital computers in the
early 1950s led to efforts in the smoothing of discrete sampled data and the
analysis of discrete data control systems. However, it wasn’t until the early to
mid-1960s, around the time the Beatles came to America, that the analysis and
development of digital equivalents of analog filters began in earnest. That’s
when digital signal processing experts realized that computers could go be-
yond the mere analysis of digitized signals into the domain of actually chang-
ing signal characteristics through filtering. Today, digital filtering is so
widespread that the quantity of literature pertaining to it exceeds that of any
other topic in digital signal processing. In this chapter, we introduce the fun-
damental attributes of digital filters, learn how to quantify their performance,
and review the principles associated with the design of finite impulse re-
sponse digital filters.
So let’s get started by illustrating the concept of filtering a time-domain
signal as shown in Figure 5–1.
In general, filtering is the processing of a time-domain signal resulting in
some change in that signal’s original spectral content. The change is usually
the reduction, or filtering out, of some unwanted input spectral components;
that is, filters allow certain frequencies to pass while attenuating other fre-
quencies. Figure 5–1 shows both analog and digital versions of a filtering
process. Where an analog filter operates on a continuous signal, a digital filter
processes a sequence of discrete sample values. The digital filter in Figure
5–1(b), of course, can be a software program in a computer, a programmable
hardware processor, or a dedicated integrated circuit. Traditional linear digital
169

170 Finite Impulse Response Filters
Time Time
(a)
Input signal Analog Output signal
Filter
Time Time
(b)
Input sequence Digital Output sequence
Filter
Figure 5–1 Filters: (a) an analog filter with a noisy tone input and a reduced-noise
tone output; (b) the digital equivalent of the analog filter.
filters typically come in two flavors: finite impulse response(FIR) filters and in-
finite impulse response (IIR) filters. Because FIR filters are the simplest type of
digital filter to analyze, we’ll examine them in this chapter and cover IIR fil-
ters in Chapter 6.
5.1 AN INTRODUCTION TO FINITE IMPULSE RESPONSE (FIR) FILTERS
Given a finite duration of nonzero input values, an FIR filter will always have
a finite duration of nonzero output values, and that’s how FIR filters got their
name. So, if the FIR filter’s input suddenly becomes a sequence of all zeros,
the filter’s output will eventually be all zeros. While not sounding all that un-
usual, this characteristic is, however, very important, and we’ll soon find out
why, as we learn more about digital filters.
FIR filters use addition to calculate their outputs in a manner much the
same as the process of averaging uses addition. In fact, averaging is a kind of
FIR filter that we can illustrate with an example. Let’s say we’re counting the
number of cars that pass over a bridge every minute, and we need to know
the average number of cars per minute over five-minute intervals; that is,
every minute we’ll calculate the average number of cars/minute over the last
five minutes. If the results of our car counting for the first ten minutes are
those values shown in the center column of Table 5–1, then the average num-
ber of cars/minute over the previous five one-minute intervals is listed in the
right column of the table. We’ve added the number of cars for the first five

5.1 An Introduction to Finite Impulse Response (FIR)Filters 171
Table 5–1 Values for the Averaging Example
Number of Number of cars/minute
Minute cars/minute over averaged over the
index the last minute last five minutes
1 10 –
2 22 –
3 24 –
4 42 –
5 37 27
6 77 40.4
7 89 53.8
8 22 53.4
9 63 57.6
10 9 52
one-minute intervals and divided by 5 to get our first five-minute average
output value, (10+22+24+42+37)/5 = 27. Next we’ve averaged the number of
cars/minute for the second to the sixth one-minute intervals to get our sec-
ond five-minute average output of 40.4. Continuing, we average the number
of cars/minute for the third to the seventh one-minute intervals to get our
third average output of 53.8, and so on. With the number of cars/minute for
the one-minute intervals represented by the dashed line in Figure 5–2, we
show our five-minute average output as the solid line. (Figure 5–2 shows
cars/minute input values beyond the first ten minutes listed in Table 5–1 to il-
lustrate a couple of important ideas to be discussed shortly.)
There’s much to learn from this simple averaging example. In Figure
5–2, notice that the sudden changes in our input sequence of cars/minute are
flattened out by our averager. The averager output sequence is considerably
smoother than the input sequence. Knowing that sudden transitions in a time
sequence represent high-frequency components, we can say that our averager
is behaving like a lowpass filter and smoothing sudden changes in the input.
Is our averager an FIR filter? It sure is—no previous averager output value is
used to determine a current output value; only input values are used to calcu-
late output values. In addition, we see that, if the bridge were suddenly
closed at the end of the 19th minute, the dashed line immediately goes to zero
cars/minute at the end of the 20th minute, and the averager’s output in

172 Finite Impulse Response Filters
Number of cars
90
Number of cars/minute Bridge closes at
80 the end of the
Average number of 19th minute.
70 cars/minute over last five
minutes
60
50
40
30
20
10
0
1 2 3 4 5 6 7 8 9 10 11 1213 14 15 1617 18 1920 21 22 2324 25 Minutes
Figure 5–2 Averaging the number of cars/minute. The dashed line shows the indi-
vidual cars/minute, and the solid line is the number of cars/minute av-
eraged over the last five minutes.
Figure 5–2 approaches and settles to a value of zero by the end of the 24th
minute.
Figure 5–2 shows the first averager output sample occurring at the end
of the 5th minute because that’s when we first have five input samples to cal-
culate a valid average. The 5th output of our averager can be denoted as
y (5) where
ave
1
y (5)= [x(1)+x(2)+x(3)+x(4)+x(5)]. (5–1)
ave
5
In the general case, if the kth input sample is x(k), then the nth output is
1 1
∑n
y (n)= [x(n–4)+x(n–3)+x(n–2)+x(n–1)+x(n)]= x(k). (5–2)
ave 5 5
k=n–4
Look at Eq. (5–2) carefully now. It states that the nth output is the average of
the nth input sample and the four previous input samples.
We can formalize the digital filter nature of our averager by creating the
block diagram in Figure 5–3 showing how the averager calculates its output
samples.
This block diagram, referred to as the filter structure, is a physical depic-
tion of how we might calculate our averaging filter outputs with the input se-
quence of values shifted, in order, from left to right along the top of the filter
as new output calculations are performed. This structure, implementing
Eqs.(5–1) and (5–2), shows those values used when the first five input sample
values are available. The delay elements in Figure 5–3, called unit delays,

5.1 An Introduction to Finite Impulse Response (FIR)Filters 173
37 42 24 22 10
Delay Delay Delay Delay
+
fifth output = 27
1/5
Figure 5–3 Averaging filter block diagram when the fifth input sample value, 37,
is applied.
merely indicate a shift register arrangement where input sample values are
temporarily stored during an output calculation.
In averaging, we add five numbers and divide the sum by 5 to get our
answer. In a conventional FIR filter implementation, we can just as well mul-
tiply each of the five input samples by the coefficient 1/5 and then perform
the summation as shown in Figure 5–4(a). Of course, the two methods in Fig-
ures 5–3 and 5–4(a) are equivalent because Eq. (5–2) describing the structure
shown in Figure 5–3 is equivalent to
1 1 1 1 1
y (n)= x(n–4)+ x(n–3)+ x(n–2)+ x(n–1)+ x(n)
ave
5 5 5 5 5
∑n
1
= x(k), (5–3)
5
k=n–4
which describes the structure in Figure 5–4(a).†
Let’s make sure we understand what’s happening in Figure 5–4(a). Each
of the first five input values is multiplied by 1/5, and the five products are
summed to give the fifth filter output value. The left-to-right sample shifting
is illustrated in Figures 5–4(b) and 5–4(c). To calculate the filter’s sixth output
value, the input sequence is right-shifted, discarding the first input value of
10, and the sixth input value, 77, is accepted on the left. Likewise, to calculate
the filter’s seventh output value, the input sequence is right-shifted, discard-
ing the second value of 22, and the seventh input value, 89, arrives on the left.
So, when a new input sample value is applied, the filter discards the oldest
sample value, multiplies the samples by the coefficients of 1/5, and sums the
products to get a single new output value. The filter’s structure using this
bucket brigade shifting process is often called a transversal filter due to the
† We’ve used the venerable distributive law for multiplication and addition of scalars,
a(b+c+d)=ab+ ac+ad, in moving Eq. (5–2)’s factor of 1/5 inside the summation in Eq. (5–3).

174 Finite Impulse Response Filters
37 42 24 22 10
Delay Delay Delay Delay
(a)
1/5 1/5 1/5 1/5 1/5
+
5th output = 27
77 37 42 24 22
Delay Delay Delay Delay
(b)
1/5 1/5 1/5 1/5 1/5
+
6th output = 40.4
89 77 37 42 24
Delay Delay Delay Delay
(c)
1/5 1/5 1/5 1/5 1/5
+
7th output = 53.8
Figure 5–4 Alternate averaging filter structure: (a) input values used for the fifth
output value; (b) input values used for the sixth output value; (c) input
values used for the seventh output value.
cross-directional flow of the input samples. Because we tap off five separate
input sample values to calculate an output value, the structure in Figure 5–4
is called a 5-tap tapped-delay line FIR filter, in digital filter vernacular.
One important and, perhaps, most interesting aspect of understanding
FIR filters is learning how to predict their behavior when sinusoidal samples
of various frequencies are applied to the input, i.e., how to estimate their
frequency-domain response. Two factors affect an FIR filter’s frequency re-
sponse: the number of taps and the specific values used for the multiplication
coefficients. We’ll explore these two factors using our averaging example and,
then, see how we can use them to design FIR filters. This brings us to the
point where we have to introduce the C word: convolution. (Actually, we al-
ready slipped a convolution equation in on the reader without saying so. It
was Eq. (5–3), and we’ll examine it in more detail later.)

5.2 Convolution in FIR Filters 175
5.2 CONVOLUTION IN FIR FILTERS
OK, here’s where we get serious about understanding the mathematics behind
FIR filters. We can graphically depict Eq. (5–3)’s and Figure 5–4’s calculations
as shown in Figure 5–5. Also, let’s be formal and use the standard notation
of digital filters for indexing the input samples and the filter coefficients by
First input sample, x(0)
Sum of the first
five products
(a) Computey(4)
Stationary 4 n
coefficients Output,y(n)
0 2 4
Sum of the
second
five products
(b) Computey(5)
4 5 n
Output,y(n)
0 2 4
(c) Computey(6)
4 5 6 n
Output,y(n)
0 2 4
(d) Computey(7)
4 5 6 7 n
Output,y(n)
0 2 4
Fifth input sample, x(4)
(e) Computey(8)
4 5 6 7 8 n
Output,y(n)
0 2 4
Figure 5–5 Averaging filter convolution: (a) first five input samples aligned with
the stationary filter coefficients, index n= 4; (b) input samples shift to
the right and index n= 5; (c) index n= 6; (d) index n= 7; (e) index
n=8.

176 Finite Impulse Response Filters
starting with an initial index value of zero; that is, we’ll call the initial input
value the 0th sample x(0). The next input sample is represented by the term
x(1), the following input sample is called x(2), and so on. Likewise, our five
coefficient values will be indexed from zero to four, h(0) through h(4). (This
indexing scheme makes the equations describing our example consistent with
conventional filter notation found in the literature.)
In Eq. (5–3) we used the factor of 1/5 as the filter coefficients multiplied
by our averaging filter’s input samples. The left side of Figure 5–5 shows the
alignment of those coefficients, black squares, with the filter input sample
values represented by the white squares. Notice in Figures 5–5(a) through
5–5(e) that we’re marching the input samples to the right, and, at each step,
we calculate the filter output sample value using Eq. (5–3). The output sam-
ples on the right side of Figure 5–5 match the first five values represented by
the black squares in Figure 5–2. The input samples in Figure 5–5 are those val-
ues represented by the white squares in Figure 5–2. Notice that the time order
of the inputs in Figure 5–5 has been reversed from the input sequence order
in Figure 5–2! That is, the input sequence has been flipped in the time domain
in Figure 5–5. This time order reversal is what happens to the input data
using the filter structure in Figure 5–4.
Repeating the first part of Eq. (5–3) and omitting the subscript on the
output term, our original FIR filter’s y(n)th output is given by
1 1 1 1 1
y(n)= x(n–4)+ x(n–3)+ x(n–2)+ x(n–1)+ x(n). (5–4)
5 5 5 5 5
Because we’ll explore filters whose coefficients are not all the same value, we
need to represent the individual filter coefficients by a variable, such as the
term h(k), for example. Thus we can rewrite the averaging filter’s output from
Eq. (5–4) in a more general way as
y(n)=h(4)x(n–4)+h(3)x(n–3)+h(2)x(n–2)+h(1)x(n–1)+h(0)x(n)
∑4
= h(k)x(n–k), (5–5)
k=0
where h(0) through h(4) all equal 1/5. Equation (5–5) is a concise way of de-
scribing the filter structure in Figure 5–4 and the process illustrated in Fig-
ure5–5.
Let’s take Eq. (5–5) one step further and say, for a general M-tap FIR fil-
ter, the nth output is
M∑–1
y(n)= h(k)x(n–k). (5–6)
k=0

5.2 Convolution in FIR Filters 177
Well, there it is. Eq. (5–6) is the infamous convolution equation as it applies to
digital FIR filters. Beginners in the field of digital signal processing often have
trouble understanding the concept of convolution. It need not be that way.
Eq. (5–6) is merely a series of multiplications followed by the addition of the
products. The process is actually rather simple. We just flip the time order of
an input sample sequence and start stepping the flipped sequence across the
filter’s coefficients as shown in Figure 5–5. For each new filter input sample,
we sum a series of products to compute a single filter output value.
Let’s pause for a moment and introduce a new term that’s important to
keep in mind, the impulse response. The impulse response of a filter is exactly
what its name implies—it’s the filter’s output time-domain sequence when the
input is a single unity-valued sample (impulse) preceded and followed by zero-
valued samples. Figure 5–6 illustrates this idea in the same way we determined
the filter’s output sequence in Figure 5–5. The left side of Figure 5–6 shows the
Impulse input Sum of the first
1 five products
Computey(4)
(a)
4 n
Stationary
coefficients 0 2 4 Impulse response, y(n)
Sum of the second
1 Computey(5) five products
(b)
4 5 n
Impulse response, y(n)
0 2 4
1
Computey(6)
(c)
4 5 6 n
Impulse response, y(n)
0 2 4
1 Computey(7)
(d)
4 5 6 7 n
Impulse response, y(n)
0 2 4
1
Computey(8)
(e)
4 5 6 7 8 n
Impulse response, y(n)
0 2 4
Figure 5–6 Convolution of filter coefficients and an input impulse to obtain the fil-
ter’s output impulse response: (a) impulse sample aligned with the first
filter coefficient, index n=4; (b) impulse sample shifts to the right and
indexn=5; (c)index n=6; (d)index n=7; (e)index n=8.

178 Finite Impulse Response Filters
alignment of the filter coefficients, black squares, with the filter input impulse
samplevaluesrepresentedbythewhitesquares.Again,inFigures5–6(a)through
5–6(e)we’reshiftingtheinputsamplestotheright,and,ateachstep,wecalculate
thefilteroutputsamplevalueusingEq.(5–4).Theoutputsamplesontheright
sideofFigure5–6arethefilter’simpulseresponse.Noticethekeypointhere:the
FIR filter’s impulse response is identical to the five filter coefficient values. For
thisreason,thetermsFIRfiltercoefficientsandimpulseresponsearesynonymous.
Thus,whensomeonereferstotheimpulseresponseofanFIRfilter,they’realso
talkingaboutthecoefficients.Becausethereareafinitenumberofcoefficients,the
impulseresponsewillbefiniteintimeduration(finiteimpulseresponse,FIR).
Returning to our averaging filter, recall that coefficients (or impulse re-
sponse) h(0) through h(4) were all equal to 1/5. As it turns out, our filter’s
performance can be improved by using coefficients whose values are not all
the same. By “performance” we mean how well the filter passes desired sig-
nals and attenuates unwanted signals. We judge that performance by deter-
mining the shape of the filter’s frequency-domain response that we obtain by
the convolution property of linear systems. To describe this concept, let’s re-
peat Eq.(5–6) using the abbreviated notation of
y(n) = h(k) x(n) (5–7)
*
where the * symbol means convolution. (Equation 5–7 is read as “yof nequals
the convolution of hof kand xof n.”) The process of convolution, as it applies
to FIR filters, is as follows: the discrete Fourier transform (DFT) of the convo-
lution of a filter’s impulse response (coefficients) and an input sequence is
equal to the product of the spectrum of the input sequence and the DFT of the
impulse response. The idea we’re trying to convey here is that if two time-
domain sequences h(k) and x(n) have DFTs of H(m) and X(m), respectively,
⋅
then the DFT of y(n) =h(k) * x(n) is H(m) X(m). Making this point in a more
compact way, we state this relationship with the expression
y(n)=h(k)*x(n) ← ⎯ ⎯ D⎯⎯ ⎯ FT→ ⎯ H(m) ⋅ X(m). (5–8)
IDFT
With IDFT indicating the inverse DFT, Eq. (5–8) indicates that two sequences
⋅
resulting from h(k) * x(n) and H(m) X(m) are Fourier transform pairs. So taking
⋅
the DFT of h(k) * x(n) gives us the product H(m) X(m) that is the spectrum of
our filter output Y(m). Likewise, we can determine h(k) x(n) by taking the in-
*
⋅
verse DFT of H(m) X(m). The very important conclusion to learn from Eq.
(5–8) is that convolution in the time domain is equivalent to multiplication in
the frequency domain. To help us appreciate this principle, Figure 5–7
sketches the relationship between convolution in the time domain and multi-
plication in the frequency domain. The process of convolution with regard to
linear systems is discussed in more detail in Section 5.9. The beginner is en-

5.2 Convolution in FIR Filters 179
Inputx(n)
Coefficientsh(k)
*
Filter output sequence in the Time
time domain, y(n) = h(k)*x(n) domain
Inverse DFT DFT DFT
DFT
Fourier
transform pair H(m) X(m)
Frequency
domain
Filter output sequence in the
frequency domain, Y(m) = H(m)•X(m)
Figure 5–7 Relationships of convolution as applied to FIR digital filters.
couraged to review that material to get a general idea of why and when the
convolution process can be used to analyze digital filters.
Equation (5–8) and the relationships in Figure 5–7 tell us what we need
to do to determine the frequency response of an FIR filter. The product
⋅
X(m)H(m) is the DFT of the filter output. Because X(m) is the DFT of the fil-
ter’s input sequence, the frequency response of the filter is then defined as
H(m), the DFT of the filter’s impulse response h(k).†Getting back to our origi-
nal problem, we can determine our averaging filter’s frequency-domain re-
sponse by taking the DFT of the individual filter coefficients (impulse
response) in Eq. (5–4). If we take the five h(k) coefficient values of 1/5 and ap-
pend 59 zeros, we have the sequence depicted in Figure 5–8(a). Performing a
64-point DFT on that sequence, and normalizing the DFT magnitudes, gives
us the filter’s frequency magnitude response |H(m)| in Figure 5–8(b) and
phase response shown in Figure 5–8(c).†† H(m) is our old friend, the sin(x)/x
function from Section 3.13.
†We use the termimpulse response here, instead of coefficients, because this concept also applies
to IIR filters. IIR filter frequency responses are also equal to the DFT of their impulse responses.
††There’s nothing sacred about using a 64-point DFT here. We could just as well have appended
only enough zeros to take a 16- or 32-point FFT. We chose 64 points to get a frequency resolution
that would make the shape of the response in Figure 5–8(b) reasonably smooth. Remember, the
more points in the FFT, the finer the frequency granularity—right?

180 Finite Impulse Response Filters
h(k)
0.2
0.1
(a)
0
0 1 3 5 7 9 11 13 15 17 59 61 63 k
|H(m)|
1
0.8
0.6
(b)
0.4
0.2
0
0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 m
H (m)
Ø
180
135
90
45
3
(c) 0
m
–45 6 9 12 15 24 27 30 39 42 45 54 57 60 63
–90
–135
–180
Figure 5–8 Averaging FIR filter: (a) filter coefficient sequence h(k) with appended
zeros; (b) normalized discrete frequency magnitude response |H(m)| of
theh(k) filter coefficients; (c) phase-angle response of H(m) in degrees.
Let’s relate the discrete frequency response samples in Figures 5–8(b)
and 5–8(c) to the physical dimension of the sample frequency f. We know,
s
from Section 3.5 and our experience with the DFT, that the m = N/2 discrete
frequency sample, m=32 in this case, is equal to the folding frequency, or half
the sample rate, f/2. Keeping this in mind, we can convert the discrete fre-
s
quency axis in Figure 5–8 to that shown in Figure 5–9. In Figure 5–9(a), notice
that the filter’s magnitude response is, of course, periodic in the frequency
domain with a period of the equivalent sample rate f. Because we’re primar-
s
ily interested in the filter’s response between 0 and half the sample rate, Fig-
ure 5–9(c) shows that frequency band in greater detail, affirming the notion
that averaging behaves like a lowpass filter. It’s a relatively poor lowpass fil-
ter compared to an arbitrary, ideallowpass filter indicated by the dashed lines
in Figure 5–9(c), but our averaging filter will attenuate higher-frequency in-
puts relative to its response to low-frequency input signals.
We can demonstrate this by way of example. Suppose we applied a low-
frequency sinewave to a 5-point averaging FIR filter as shown by the white

5.2 Convolution in FIR Filters 181
|H(m)|
1
0.8
0.6
(a)
0.4
0.2
0
Freq
f /8 f /4 f /2 f
s s s s
H (m)
Ø
135
90
45 f s /8 f s /4
(b) 0
–45 fs /2 f s Freq
–90
–135
|H(m)|
1
Main lobe
0.8
(c) 0.6
Sidelobes
0.4
0.2
0
Freq
fs /8 fs /4 fs /2
Figure 5–9 Averaging FIR filter frequency response shown as continuous curves:
(a) normalized frequency magnitude response, |H(m)|; (b) phase-
angle response of H(m) in degrees; (c) the filter’s magnitude response
between zero Hz and half the sample rate, f/2 Hz.
s
squares in Figure 5–10(a). The input sinewave’s frequency is f /32 Hz and its
s
peak amplitude is unity. The filter’s output sequence is shown by the black
squares.
Figure 5–10(a) is rich in information! First, the filter’s output is a
sinewave of the same frequency as the input. This is a characteristic of a linear
system. We apply a single sinewave input, and the output will be a single
sinewave (shifted in phase and perhaps reduced in amplitude) of the same
frequency as the input. Second, notice that the initial four output samples are
not exactly sinusoidal. Those output samples are the transient response of the
filter. With tapped-delay line FIR filters, the sample length of that transient re-
sponse is equal to the number of filter unit-delay elements D, after which the
filter’s output begins its steady-state time response.
The above transient response property is important. It means that
tapped-delay line FIR filter outputs are not valid until D+1 input samples
have been applied to the filter. That is, the output samples are not valid until

182 Finite Impulse Response Filters
1 pPeeaak ko ouutptpuut t= = 0 0.9.966
0.5
(a) 0
0 2 4 6 8 10 12 14 16 20 22 24 26 28 Time
-0.5
Input
Output
-1
Phase
delay = 22.5o
d P e h l a a s y e = 67.5o
1
Peak output
= 0.69
0.5
(b) 0
0 2 4 6 14 20 22 24 26 28 30 Time
-0.5
-1
Figure 5–10 Averaging FIR filter input and output responses: (a) with an input
sinewave of frequency f/32; (b) with an input sinewave of frequency
s
3f/32.
s
the filter’s delay line is filled with input data. So, for an FIR filter having D =
70 unit-delay elements the first 70 output samples are not valid and would be
ignored in practice. WARNING: There are tapped-delay line FIR filters, used
in practice, that have more unit-delay elements than nonzero-valued tap coef-
ficients. The transient response length for those filters, measured in samples,
is equal to the number of unit-delay elements, D(and is unrelated to the num-
ber of nonzero-valued tap coefficients).
The filter’s output sinewave peak amplitude is reduced to a value of
0.96 and the output sinewave is delayed from the input by a phase angle of
22.5 degrees. Notice that the time delay between the input and output
sinewaves, in Figure 5–10(a), is two samples in duration. (Although we dis-
cuss this time delay topic in more detail later, for now we’ll just say that, be-
cause the filter’s coefficients are symmetrical, the input/output delay
measured in samples is equal to half the number of unit-delay elements in the
filter’s tapped-delay line.)
Next, if we applied a higher-frequency sinewave of 3f /32 Hz to our 5-
s
tap FIR filter as shown in Figure 5–10(b), the filter output is a sinewave of fre-
quency 3f /32 Hz and its peak amplitude is even further reduced to a value of
s
0.69. That’s the nature of lowpass filters—they attenuate higher-frequency in-
puts more than they attenuate low-frequency inputs. As in Figure 5–10(a), the
time delay between the input and output sinewaves, in Figure 5–10(b), is two
samples in duration (corresponding to a phase-angle delay of 67.5 degrees).
That property, where the input/output delay does not depend on frequency,

5.2 Convolution in FIR Filters 183
is a very beneficial property of FIR filters having symmetrical coefficients.
We’ll discuss this important issue again later in this chapter. In Figure 5–10(b)
we see that the nonsinusoidal filter output transient response is even more
obvious than it was in Figure 5–10(a).
Although the output amplitudes and phase delays in Figure 5–10 were
measured values from actually performing a 5-tap FIR filter process on the
input sinewaves’ samples, we could have obtained those amplitude and
phase delay values directly from Figures 5–8(b) and 5–8(c). The point is, we
don’t have to implement an FIR filter and apply various sinewave inputs to
discover what its frequency response will be. We need merely take the DFT of
the FIR filter’s coefficients (impulse response) to determine the filter’s fre-
quency response as we did for Figure 5–8.
Figure 5–11 is another depiction of how well our 5-tap averaging FIR fil-
ter performs, where the dashed line is the filter’s magnitude response |H(m)|,
and the shaded line is the|X(m)| magnitude spectrum of the filter’s input val-
ues (the white squares in Figure 5–2). The solid line is the magnitude spectrum
of the filter’s output sequence, which is shown by the black squares in Figure
5–2. So in Figure 5–11, the solid output spectrum is the product of the dashed
⋅
filter response curve and the shaded input spectrum, or |X(m)H(m)|. Again,
we see that our averager does indeed attenuate the higher-frequency portion
of the input spectrum.
Let’s pause for a moment to let all of this soak in a little. So far we’ve
gone through the averaging filter example to establish that
• FIR filters perform time-domain convolution by summing the products
of the shifted input samples and a sequence of filter coefficients,
1.0
Filter magnitude response, |H(m)|
.8
.6
Filter input spectral magnitude, |X(m)|
.4
Spectral magnitude of filter output
is |Y(m)| = |X(m).H(m)|
.2
0
Freq
f/8 f/4 f/2
s s s
Figure 5–11 Averaging FIR filter input magnitude spectrum, frequency magni-
tude response, and output magnitude spectrum.

184 Finite Impulse Response Filters
• an FIR filter’s output sequence is equal to the convolution of the input
sequence and a filter’s impulse response (coefficients),
• an FIR filter’s frequency response is the DFT of the filter’s impulse
response,
• an FIR filter’s output spectrum is the product of the input spectrum and
the filter’s frequency response, and
• convolution in the time domain and multiplication in the frequency do-
main are Fourier transform pairs.
OK, here’s where FIR filters start to get really interesting. Let’s change
the values of the five filter coefficients to modify the frequency response of
our 5-tap lowpass filter. In fact, Figure 5–12(a) shows our original five filter
coefficients and two other arbitrary sets of 5-tap coefficients. Figure 5–12(b)
compares the frequency magnitude responses of those three sets of coeffi-
cients. Again, the frequency responses are obtained by taking the DFT of the
three individual sets of coefficients and plotting the magnitude of the trans-
forms, as we did for Figure 5–9(c). So we see three important characteristics in
Figure 5–12. First, as we expected, different sets of coefficients give us differ-
ent frequency magnitude responses. Second, a sudden change in the values of
the coefficient sequence, such as the 0.2 to 0 transition in the first coefficient
set, causes ripples, or sidelobes, in the frequency response. Third, if we mini-
mize the suddenness of the changes in the coefficient values, such as the third
set of coefficients in Figure 5–12(a), we reduce the sidelobe ripples in the fre-
quency response. However, reducing the sidelobes results in increasing the
main lobe width of our lowpass filter. (As we’ll see, this is exactly the same ef-
fect encountered in the discussion of window functions used with the DFT in
Section 3.9.)
To reiterate the function of the filter coefficients, Figure 5–13 shows the
5-tap FIR filter structure using the third set of coefficients from Figure 5–12.
The implementation of constant-coefficient transversal FIR filters does not get
any more complicated than that shown in Figure 5–13. It’s that simple. We
can have a filter with more than 5 taps, but the input signal sample shifting,
the multiplications by the constant coefficients, and the summation are all
there is to it. (By constant coefficients, we don’t mean coefficients whose val-
ues are all the same; we mean coefficients whose values remain unchanged,
or time invariant. There is a class of digital filters, called adaptive filters,
whose coefficient values are periodically changed to adapt to changing input
signal parameters. While we won’t discuss these adaptive filters in this intro-
ductory text, their descriptions are available in the literature[1–5].)
So far, our description of an FIR filter implementation has been pre-
sented from a hardware perspective. In Figure 5–13, to calculate a single fil-
ter output sample, five multiplications and five additions must take place
before the arrival of the next input sample value. In a software implemen-
tation of a 5-tap FIR filter, however, all of the input data samples would be

5.2 Convolution in FIR Filters 185
0.2 0.2 0.2
(a) 0.1 0.1 0.1
0 0 0
0 2 4 0 2 4 0 2 4
Filter magnitude response, |H(m)|
1
0.2, 0.2, 0.2, 0.2, 0.2
0.8
0.1, 0.2, 0.2, 0.2, 0.1
0.6
(b) 0.04, 0.12, 0.2, 0.12, 0.04
0.4
0.2
0
Freq
fs /8 fs /4 fs /2
Figure 5–12 Three sets of 5-tap lowpass filter coefficients: (a) sets of coefficients:
0.2, 0.2, 0.2, 0.2, 0.2; 0.1, 0.2, 0.2, 0.2, 0.1; and 0.04, 0.12, 0.2, 0.12, 0.04;
(b) frequency magnitude response of three lowpass FIR filters using
those sets of coefficients.
previously stored in memory. The software filter routine’s job, then, is to
access different five-sample segments of the x(n) input data space, perform
the calculations shown in Figure 5–13, and store the resulting filter y(n)
output sequence in an array of memory locations.†
x(n) x(n–1) x(n–2) x(n–3) x(n–4)
Delay Delay Delay Delay
h(0) = h(1) = h(2) = h(3) = h(4) =
0.04 0.12 0.2 0.12 0.04
+
y(n)
Figure 5–13 Five-tap lowpass FIR filter implementation using the coefficients 0.04,
0.12, 0.2, 0.12, and 0.04.
†In reviewing the literature of FIR filters, the reader will often find the term z–1replacing the
delay function in Figure 5–13. This equivalence is explained in the next chapter when we study
IIR filters.

186 Finite Impulse Response Filters
Now that we have a basic understanding of what a digital FIR filter is,
let’s see what effect is had by using more than 5 filter taps by learning to de-
sign FIR filters.
5.3 LOWPASS FIR FILTER DESIGN
OK, instead of just accepting a given set of FIR filter coefficients and analyz-
ing their frequency response, let’s reverse the process and design our own
lowpass FIR filter. The design procedure starts with the determination of a de-
sired frequency response followed by calculating the filter coefficients that
will give us that response. There are two predominant techniques used to de-
sign FIR filters: the window method and the so-called optimum method. Let’s
discuss them in that order.
5.3.1 Window Design Method
The window method of FIR filter design (also called the Fourier series
method) begins with our deciding what frequency response we want for our
lowpass filter. We can start by considering a continuous lowpass filter, and
simulating that filter with a digital filter. We’ll define the continuous fre-
quency response H(f) to be ideal, i.e., a lowpass filter with unity gain at low
frequencies and zero gain (infinite attenuation) beyond some cutoff frequency,
as shown in Figure 5–14(a). Representing this H(f) response by a discrete
frequency response is straightforward enough because the idea of a discrete
frequency response is essentially the same as a continuous frequency re-
sponse—with one important difference. As described in Sections 2.2 and 3.13,
discrete frequency-domain representations are always periodic with the
H(f)
Filter frequency
response 1 Cutoff
frequency
(a)
–f
s
–f
s
/2 0 fs /2 f
s
F
(
r
f
e
)
q
H(m)
1
(b)
Freq
m=–N m = –N/2 m = 0 m = N/2 m = N
(m)
(–f s) (–fs /2) (0) (fs /2) (fs)
–fs /2 to fs /2 interval
0 to fs interval
Figure 5–14 Lowpass filter frequency responses: (a) continuous frequency re-
sponseH(f); (b) periodic, discrete frequency response H(m).

5.3 Lowpass FIR Filter Design 187
period being the sample rate f. The discrete representation of our ideal, con-
s
tinuous lowpass filter H(f) is the periodic response H(m) depicted by the
frequency-domain samples in Figure 5–14(b).
We have two ways to determine our lowpass filter’s time-domain coeffi-
cients. The first way is algebraic:
1. Develop an expression for the discrete frequency response H(m).
2. Apply that expression to the inverse DFT equation to get the time do-
main h(k).
3. Evaluate that h(k) expression as a function of time index k.
The second method is to define the individual frequency-domain samples
representing H(m) and then have a software routine perform the inverse DFT
of those samples, giving us the FIR filter coefficients. In either method, we
need only define the periodic H(m) over a single period of f Hz. As it turns
s
out, defining H(m) in Figure 5–14(b) over the frequency span –f/2 to f/2 is
s s
the easiest form to analyze algebraically, and defining H(m) over the fre-
quency span 0 to f is the best representation if we use the inverse DFT to ob-
s
tain our filter’s coefficients. Let’s try both methods to determine the filter’s
time-domain coefficients.
In the algebraic method, we can define an arbitrary discrete frequency
response H(m) using Nsamples to cover the –f/2 to f/2 frequency range and
s s
establish K unity-valued samples for the passband of our lowpass filter as
shown in Figure 5–15. To determine h(k) algebraically we need to take the in-
verse DFT of H(m) in the form of
h(k)= 1
N∑/2
H(m)ej2πmk/N, (5–9)
N
m=–(N/2)+1
where our time-domain index is k. The solution to Eq. (5–9), derived in Sec-
tion 3.13 as Eq. (3–59), is repeated here as
h(k)=
1 ⋅sin(πkK/N)
. (5–10)
N sin(πk/N)
K
1 H(m)
0 m
–N/2 + 1 m=–(K–1)/2 m=(K–1)/2 N/2
Figure 5–15 Arbitrary, discrete lowpass FIR filter frequency response defined over
Nfrequency-domain samples covering the frequency range of f Hz.
s

188 Finite Impulse Response Filters
K/N h(k)
. . . . . .
k
Figure 5–16 Time-domain h(k) coefficients obtained by evaluating Eq. (5–10).
If we evaluate Eq. (5–10) as a function of k, we get the sequence shown in Fig-
ure 5–16, taking the form of the classic sin(x)/x function. By reviewing the
material in Section 3.13, it’s easy to see the great deal of algebraic manipula-
tion required to arrive at Eq. (5–10) from Eq. (5–9). So much algebra, in fact,
with its many opportunities for making errors, that digital filter designers
like to avoid evaluating Eq. (5–9) algebraically. They prefer to use software
routines to perform inverse DFTs (in the form of an inverse FFT) to determine
h(k), and so will we.
We can demonstrate the software inverse DFT method of FIR filter de-
sign with an example. Let’s say we need to design a lowpass FIR filter sim-
ulating the continuous frequency response shown in Figure 5–17(a). The
discrete representation of the filter’s frequency response H(m) is shown in
Figure 5–17(b), where we’ve used N = 32 points to represent the frequency-
domain variable H(f). Because it’s equivalent to Figure 5–17(b) but avoids
the negative values of the frequency index m, we represent the discrete fre-
Filter frequency H(f)
response Cutoff frequency
(a)
–f s /2 –f s /4 –f s /8 0 fs /8 fs /4 f s /2 Freq
1 H(m)
(b)
–15 –12 –8 –4 0 4 8 16 m
(–fs /4) (–fs /8) (fs /8) (fs /4) (fs /2)
H(m) 1
(c)
m
0 4 8 12 16 20 24 28 31
(fs /8) (fs /4) (fs /2) (3fs /4) (f
s
)
Figure 5–17 An ideal lowpass filter: (a) continuous frequency response H(f); (b)
discrete response H(m) over the range –f/2 to f/2 Hz; (c) discrete re-
s s
sponseH(m) over the range 0 to f Hz.
s

5.3 Lowpass FIR Filter Design 189
0.219
Inverse DFT of H(m)
(a) –8 8 16
–12 –4 0 4 12
k
h(k) = Shifted inverse
DFT of H(m)
(b) 0 8 22 30
4 12 15 18 26
k
Figure 5–18 Inverse DFT of the discrete response in Figure 5–17(c): (a) normal
inverse DFT indexing for k; (b) symmetrical coefficients used for a
31-tap lowpass FIR filter.
quency samples over the range 0 to f in Figure 5–17(c), as opposed to the
s
–f /2 to +f /2 range in Figure 5–17(b). OK, we’re almost there. Using a 32-
s s
point inverse FFT to implement a 32-point inverse DFT of the H(m) se-
quence in Figure 5–17(c), we get the 32 h(k) values depicted by the dots
from k=–15 to k=16 in Figure 5–18(a).†We have one more step to perform.
Because we want our final 31-tap h(k) filter coefficients to be symmetrical
with their peak value in the center of the coefficient sample set, we drop
the k = 16 sample and shift the k index to the left from Figure 5–18(a), giv-
ing us the desired sin(x)/x form of h(k) as shown in Figure 5–18(b). This
shift of the index k will not change the frequency magnitude response of
our FIR filter. (Remember from our discussion of the DFT shifting theorem
in Section 3.6 that a shift in the time domain manifests itself only as a linear
phase shift in the frequency domain with no change in the frequency-
domain magnitude.) The sequence in Figure 5–18(b), then, is now the coef-
ficients we use in the convolution process of Figure 5–5 to implement a
lowpass FIR filter.
It’s important to demonstrate that the more h(k) terms we use as filter co-
efficients, the closer we’ll approximate our ideal lowpass filter response. Let’s
be conservative, just use the center nine h(k) coefficients, and see what our fil-
ter response looks like. Again, our filter’s magnitude response in this case will
be the DFT of those nine coefficients as shown on the right side of Figure
5–19(a). The ideal filter’s frequency response is also shown for reference as the
dashed curve. (To show the details of its shape, we’ve used a continuous curve
for |H(m)| in Figure 5–19(a), but we have to remember that |H(m)| is really a
†If you want to use this FIR design method but only have a forward FFT software routine avail-
able, Section 13.6 shows a slick way to perform an inverse FFT with the forward FFT algorithm.

190 Finite Impulse Response Filters
h(k) |H(m)| for a 9-tap filter
Ideal lowpass
(a) response
DFT
Cutoff
frequency
–fs /8 0 fs /8 Freq
h(k) |H(m)| for a 19-tap filter
(b) DFT
–f /8 0 f /8 Freq
s s
|H(m)| for a 31-tap filter
(c)
–fs /8 0 fs /8 Freq
Figure 5–19 Coefficients and frequency responses of three lowpass filters: (a)
9-tap FIR filter; (b) 19-tap FIR filter; (c) frequency response of the full
31-tap FIR filter.
sequence of discrete values.) Notice that using nine coefficients gives us a low-
pass filter, but it’s certainly far from ideal. Using more coefficients to improve
our situation, Figure 5–19(b) shows 19 coefficients and their corresponding fre-
quency magnitude response that is beginning to look more like our desired
rectangular response. Notice that magnitude fluctuations, or ripples, are evi-
dent in the passband of our H(m) filter response. Continuing, using all 31 of
the h(k) values for our filter coefficients results in the frequency response in
Figure 5–19(c). Our filter’s response is getting better (approaching the ideal),
but those conspicuous passband magnitude ripples are still present.
It’s important that we understand why those passband ripples are in the
lowpass FIR filter response in Figure 5–19. Recall the above discussion of con-
volving the 5-tap averaging filter coefficients, or impulse response, with an
input data sequence to obtain the averager’s output. We established that con-
volution in the time domain is equivalent to multiplication in the frequency
domain, which we symbolized with Eq. (5–8) and repeat here as
⎯D⎯⎯FT→ ⋅
h(k)*x(n) ←⎯⎯⎯ H(m) X(m). (5–11)
IDFT

5.3 Lowpass FIR Filter Design 191
This association between convolution in the time domain and multiplication in
the frequency domain, sketched in Figure 5–7, indicates that if two time-domain
sequences h(k) and x(n) have DFTs of H(m) and X(m), respectively, then the DFT
⋅
of h(k) * x(n) is H(m) X(m). No restrictions whatsoever need be placed on what
the time-domain sequences h(k) and x(n) in Eq. (5–11) actually represent. As
detailed later in Section 5.9, convolution in one domain is equivalent to multi-
plication in the other domain, allowing us to state that multiplication in the
time domain is equivalent to convolution in the frequency domain, or
⋅ ⎯D⎯⎯FT→
h(k) x(n) ←⎯⎯⎯ H(m)*X(m). (5–12)
IDFT
Now we’re ready to understand why the magnitude ripples are present in
Figure 5–19.
Rewriting Eq. (5–12) and replacing the h(k) and x(n) expressions with
∞
h (k) and w(k), respectively,
∞ ⋅ ⎯D⎯⎯FT→ ∞
h (k) w(k) ←⎯⎯⎯ H (m)*W(m). (5–13)
IDFT
∞
Let’s say that h (k) represents an infinitely long sin(x)/x sequence of ideal
lowpass FIR filter coefficients and that w(k) represents a window sequence
that we use to truncate the sin(x)/x terms as shown in Figure 5–20. Thus, the
w(k) sequence is a finite-length set of unity values and its DFT is W(m). The
length of w(k) is merely the number of coefficients, or taps, we intend to use
∞
to implement our lowpass FIR filter. With h (k) defined as such, the product
∞
h (k)
. . . . . .
k
x
w(k)
1
n
=
h(k)
k
Figure 5–20 Infinite h∞(k) sequence windowed by w(k) to define the final filter
coefficientsh(k).

192 Finite Impulse Response Filters
⋅
∞
h (k) w(k) represents the truncated set of filter coefficients h(k) in Figures
5–19(a) and 5–19(b). So, from Eq. (5–13), the FIR filter’s true frequency re-
sponse H(m) is the convolution
∞
H(m) = H (m) W(m). (5–14)
*
We depict this convolution in Figure 5–21 where, to keep the figure from
∞ ∞
being so busy, we show H (m) (the DFT of the h (k) coefficients) as the gray
rectangle. Keep in mind that it’s really a sequence of constant-amplitude sam-
ple values.
Let’s look at Figure 5–21(a) very carefully to see why all three |H(m)|s
exhibit passband ripple in Figure 5–19. We can view a particular sample value
∞ ∞
H (m) = DFT of h (k) W(m) = DFT of
rectangularw(n)
1
(a)
0 Freq
(b)
0
Freq
(c)
0
Freq
(d)
0
Freq
Figure 5–21 ConvolutionW(m)*H∞(m): (a) unshifted W(m) and H∞(m); (b) shift of
W(m) leading to ripples within H(m)’s positive-frequency passband;
(c) shift of W(m) causing response roll-off near H(m)’s positive cutoff
frequency; (d) shift of W(m) causing ripples beyond H(m)’s positive
cutoff frequency.

5.3 Lowpass FIR Filter Design 193
∞
of the H(m) =H (m) W(m) convolution as being the sum of the products of
*
∞ ∞
H (m) and W(m) for a particular frequency shift of W(m). H (m) and the un-
shifted W(m) are shown in Figure 5–21(a.) With an assumed value of unity for
∞
all of H (m), a particular H(m) value is now merely the sum of the W(m) sam-
∞
ples that overlap the H (m) rectangle. So, with a W(m) frequency shift of 0 Hz,
∞
the sum of the W(m) samples that overlap the H (m) rectangle in Figure
5–21(a) is the value of H(m) at 0 Hz. As W(m) is shifted to the right to give us
additional positive-frequency H(m) values, we can see that the sum of the
positive and negative values of W(m) under the rectangle oscillates during the
shifting of W(m). As the convolution shift proceeds, Figure 5–21(b) shows
why there are ripples in the passband of H(m)—again, the sum of the positive
∞
and negative W(m) samples under the H (m) rectangle continues to vary as
the W(m) function is shifted. The W(m) frequency shift, indicated in Figure
∞
5–21(c), where the peak of W(m)’s main lobe is now outside the H (m) rectan-
gle, corresponds to the frequency where H(m)’s passband begins to roll off.
Figure 5–21(d) shows that, as the W(m) shift continues, there will be ripples in
H(m) beyond the positive cutoff frequency.† The point of all of this is that the
ripples in H(m) are caused by the sidelobes of W(m).
Figure 5–22 helps us answer the question “How many sin(x)/x coeffi-
cients do we have to use (or how wide must w(k) be) to get nice sharp falling
edges and no ripples in our H(m) passband?” The answer is that we can’t get
there from here. It doesn’t matter how many sin(x)/x coefficients (filter taps)
we use; there will always be filter passband ripple. As long as w(k) is a finite
number of unity values (i.e., a rectangular window of finite width), there will
be sidelobe ripples in W(m), and this will induce passband ripples in the final
H(m) frequency response. To illustrate that increasing the number of sin(x)/x
coefficients doesn’t reduce passband ripple, we repeat the 31-tap lowpass fil-
ter response in Figure 5–22(a). The frequency response, using 63 coefficients,
is shown in Figure 5–22(b), and the passband ripple remains. We can make
the filter’s transition region narrower using additional h(k) filter coefficients,
but we cannot eliminate the passband ripple. That ripple, known as Gibbs’s
phenomenon, manifests itself anytime a function (w(k) in this case) with an
instantaneous discontinuity is represented by a Fourier series[6–8]. No finite
set of sinusoids will be able to change fast enough to be exactly equal to an in-
stantaneous discontinuity. Another way to state this Gibbs’s dilemma is that,
no matter how wide our w(k) window is, W(m) will alwayshave sidelobe rip-
ples. As shown in Figure 5–22(b), we can use more coefficients by extending
the width of the rectangular w(k) to narrow the filter transition region, but a
wider w(k) does not eliminate the filter passband ripple, nor does it even
†In Figure 5–21(b), had we started to shift W(m)to the left in order to determine the negative-
frequency portion of H(m), we would have obtained the mirror image of the positive-frequency
portion of H(m).

194 Finite Impulse Response Filters
Transition region
|H(m)|
Ripple
(a)
–f/8 0 f/8 Freq
s s
Transition region
|H(m)|
Ripple
(b)
–f/8 0 f/8 Freq
s s
Figure 5–22 Passband ripple and transition regions: (a) for a 31-tap lowpass filter;
(b) for a 63-tap lowpass filter.
reduce their peak-to-peak ripple magnitudes, as long as w(k) has sudden dis-
continuities.
5.3.2 Windows Used in FIR Filter Design
OK. The good news is that we can minimize FIR passband ripple with win-
dow functions the same way we minimized DFT leakage in Section 3.9.
Here’s how. Looking back at Figure 5–20, by truncating the infinitely long
∞
h (k) sequence through multiplication by the rectangular w(k), our final h(k)
exhibited ripples in the frequency-domain passband. Figure 5–21 shows us
that the passband ripples were caused by W(m)’s sidelobes that, in turn, were
caused by the sudden discontinuities from zero to one and one to zero in w(k).
If we think of w(k) in Figure 5–20 as a rectangular window, then it is w(k)’s
abrupt amplitude changes that are the source of our filter passband ripple.
The window FIR design method is the technique of reducing w(k)’s disconti-
nuities by using window functions other than the rectangular window.
Consider Figure 5–23 to see how a nonrectangular window function can
be used to design low-ripple FIR digital filters. Imagine if we replaced Figure

5.3 Lowpass FIR Filter Design 195
∞
h (k)
. . . . . .
k
x
Blackman window
function 1
w(k)
(a)
k
=
h(k)
k
Transition region
|H(m)|
(b)
–f /8 0 f /8 Freq
s s
Transition region
|H(m)|
(c)
–fs /8 0 f
s
/8 Freq
Figure 5–23 Coefficients and frequency response of a 31-tap Blackman-
windowed FIR filter: (a) defining the windowed filter coefficients h(k);
(b) low-ripple 31-tap frequency response; (c) low-ripple 63-tap fre-
quency response.
5–20’s rectangular w(k) with the Blackman window function whose discrete
values are defined as
⎛ 2πk ⎞ ⎛ 4πk ⎞
w(k)=0.42−0.5cos⎜ ⎟+0.08cos⎜ ⎟,fork=0,1,2,...,N−1. (5–15)
⎝ N−1 ⎠ ⎝⎝ N−1 ⎠

196 Finite Impulse Response Filters
This situation is depicted for N=31 in Figure 5–23(a), where Eq. (5–15)’s w(k)
looks very much like the Hanning window function in Figure 3–17(a). This
Blackman window function results in the 31 smoothly tapered h(k) coeffi-
cients at the bottom of Figure 5–23(a). Notice two things about the resulting
H(m) in Figure 5–23(b). First, the good news. The passband ripples are greatly
reduced from those evident in Figure 5–22(a)—so our Blackman window
function did its job. Second, the price we paid for reduced passband ripple is
a wider H(m) transition region. We can get a steeper filter response roll-off by
increasing the number of taps in our FIR filter. Figure 5–23(c) shows the im-
proved frequency response had we used a 63-coefficient Blackman window
function for a 63-tap FIR filter. So using a nonrectangular window function re-
duces passband ripple at the expense of slower passband to stopband roll-off.
A graphical comparison of the frequency responses for the rectangular
and Blackman windows is provided in Figure 5–24. (The curves in Figure
5–24 were obtained for the window functions defined by 16 discrete samples,
1
Rectangular (dotted)
0.75
Blackman (dashed)
0.5
(a)
0.25
0
0 f/8 f/4 f/2 Freq
s s s
0
Blackman (dashed)
-10
Rectangular (dotted)
-20
-30
(b)
-40
-50
-60
-70
0 f/8 f/4 f/2 Freq
s s s
Figure 5–24 Rectangular versus Blackman window frequency magnitude responses:
(a)|W(m)| on a linear scale; (b) normalized logarithmic scale of W (m).
dB

5.3 Lowpass FIR Filter Design 197
to which 496 zeros were appended, applied to a 512-point DFT.) The sidelobe
magnitudes of the Blackman window’s |W(m)| are too small to see on a lin-
ear scale. We can see those sidelobe details by plotting the two windows’ fre-
quency responses on a logarithmic scale and normalizing each plot so that
their main lobe peak values are both zero dB. For a given window function,
we can get the log magnitude response of W (m) by using the expression
dB
⋅ ⎛|W(m)|⎞
W (m)=20 log ⎜ ⎟. (5–16)
dB 10⎝ |W(0)| ⎠
(The |W(0)| term in Eq. (5–16) is the magnitude of W(m) at the peak of the
main lobe when m=0.) Figure 5–24(b) shows us the greatly reduced sidelobe
levels of the Blackman window and how that window’s main lobe is almost
three times as wide as the rectangular window’s main lobe.
Of course, we could have used any of the other window functions, dis-
cussed in Section 3.9, for our lowpass FIR filter. That’s why this FIR filter design
technique is called the window design method. We pick a window function and
∞
multiply it by the sin(x)/xvalues from H (m) in Figure 5–23(a) to get our final
h(k) filter coefficients. It’s that simple. Before we leave the window method of
FIR filter design, let’s introduce two other interesting window functions.
Although the Blackman window and those windows discussed in Sec-
tion 3.9 are useful in FIR filter design, we have little control over their fre-
quency responses; that is, our only option is to select some window function
and accept its corresponding frequency response. Wouldn’t it be nice to have
more flexibility in trading off, or striking a compromise between, a window’s
main lobe width and sidelobe levels? Fortunately, there are two popular win-
dow functions that give us this opportunity. Called the Chebyshev (or Dolph-
Chebyshev) and the Kaiser window functions, they’re defined by the
following formidable expressions:
w(k) = the N-point inverse DFT of
⎡ ⋅ ⎡ ⋅ ⎛ m⎞⎤⎤
Chebyshev window:→ cos⎢N cos –1 ⎢ α cos ⎝ π ⎠ ⎥⎥
⎣ ⎣ N ⎦⎦
(also called the Dolph–
,
Chebyshev and the ⋅ –1
Tchebyschev window) cosh[N cosh (α)] (5–17)
where α=cosh ⎛ ⎜ 1 cosh –1 (10 γ ) ⎞ ⎟ and m=0,1,2,. . .,N
⎝ ⎠
N
⎡ ⎤
⎛ ⎞2
⎢ k–p ⎥
Kaiser window:→ I o⎢ β 1– ⎝ ⎜ p ⎠ ⎟ ⎥
(also called the ⎣⎢ ⎦⎥
Kaiser–Bessel window) w(k)= I (β) ,
o
for k=0, 1, 2, . . ., N–1, and p=(N–1)/2. (5–18)

198 Finite Impulse Response Filters
Two typical Chebyshev and Kaiser window functions and their fre-
quency magnitude responses are shown in Figure 5–25. For comparison, the
rectangular and Blackman window functions are also shown in that figure.
(Again, the curves in Figure 5–25(b) were obtained for window functions de-
fined by 32 discrete time samples, with 480 zeros appended, applied to a 512-
point DFT.)
Equation (5–17) was originally based on the analysis of antenna arrays
using the mathematics of Chebyshev polynomials[9–11]. Equation (5–18)
evolved from Kaiser’s approximation of prolate spheroid functions using
zeroth-order Bessel functions[12–13]. For each sample of the N-length se-
quence inside the brackets of the numerator of Eq. (5–18), as well as for the
Window functions
1
Rect.
0.8
0.6
(a)
0.4
Blackman
Chebyshev ( =2.5)
0.2
Kaiser ( =4)
Rectangular
0
0 5 10 15 20 25 30
k
Window freq. magnitude responses (dB)
0
Blackman
–10 Chebyshev ( =2.5)
Kaiser ( =4)
Rectangular
–20
–30
(b)
–40
–50
–60
–70
0 0.05f 0.1f 0.15f 0.2f 0.25f
s s s s s
Freq
Figure 5–25 Typical window functions used with digital filters: (a) window coeffi-
cients in the time domain; (b) frequency-domain magnitude re-
sponses in dB.

5.3 Lowpass FIR Filter Design 199
β term in the denominator, the I (x) zeroth-order Bessel function values can
o
be approximated using
∑24 x2q
I (x)= . (5–18’)
o 4q⋅(q!)2
q=0
In theory the upper limit of the summation in Eq. (5–18’) should be infinity
but, fortunately, 25 summations give us sufficient accuracy when evaluating
I (x).
o
Don’t be intimidated by the complexity of Eqs. (5–17) and (5–18)—at this
point, we need not be concerned with the mathematical details of their devel-
opment. We just need to realize that the γ and β control parameters give us
control over the Chebyshev and Kaiser windows’ main lobe widths and the
sidelobe levels.
Let’s see how this works for Chebyshev window functions, having four
separate values of γ, and their frequency responses shown in Figure 5–26. FIR
filter designers applying the window method typically use predefined software
routines to obtain their Chebyshev window coefficients. Commercial digital
signal processing software packages allow the user to specify three things: the
window function (Chebyshev in this case), the desired number of coefficients
(the number of taps in the FIR filter), and the value of γ. Selecting different val-
ues for γenables us to adjust the sidelobe levels and see what effect those val-
ues have on main lobe width, a capability that we didn’t have with the
Blackman window or the window functions discussed in Section 3.9. The
Chebyshev window function’s stopband attenuation, in dB, is equal to
Atten = –20γ. (5–19)
Cheb
So, for example, if we needed our sidelobe levels to be no greater than –60 dB
below the main lobe, we use Eq. (5–19) to establish a γvalue of 3.0 and let the
software generate the Chebyshev window coefficients.†
The same process applies to the Kaiser window, as shown in Figure
5–27. Commercial software packages allow us to specify β in Eq. (5–18) and
provide us with the associated window coefficients. The curves in Figure
5–27(b), obtained for Kaiser window functions defined by 32 discrete sam-
ples, show that we can select the desired sidelobe levels and see what effect
this has on the main lobe width.
Chebyshev or Kaiser, which is the best window to use? It depends on the
application. Returning to Figure 5–25(b), notice that, unlike the constant
†By the way, some digital signal processing software packages require that we specify Atten
Cheb
in decibels instead of γ. That way, we don’t have to bother using Eq. (5–19) at all.

200 Finite Impulse Response Filters
Chebyshev window functions
1
0.8
0.6
(a)
0.4
= 1.5
= 2.0
0.2
= 2.5
= 3.0
0
0 5 10 15 20 25 30
k
Window freq. magnitude responses (dB)
0
= 1.5
–10 = 2.0
= 2.5
–20 = 3.0
–30
(b)
–40
–50
–60
–70
0 0.05f 0.1f 0.15f 0.2f 0.25f
s s s s s
Freq
Figure 5–26 Chebyshev window functions for various γ values: (a) window coeffi-
cients in the time domain; (b) frequency-domain magnitude responses
in dB.
sidelobe peak levels of the Chebyshev window, the Kaiser window’s sidelobes
decrease with increased frequency. However, the Kaiser sidelobes are higher
than the Chebyshev window’s sidelobes near the main lobe. Our primary
trade-off here is trying to reduce the sidelobe levels without broadening the
main lobe too much. Digital filter designers typically experiment with various
values of γ and β for the Chebyshev and Kaiser windows to get the optimum
W (m) for a particular application. (For that matter, the Blackman window’s
dB
very low sidelobe levels outweigh its wide main lobe in many applications.)
For some reason, algorithms for computing Chebyshev window functions are
not readily available in the literature of DSP. To remedy that situation, Appen-
dix I presents a straightforward procedure for computing N-sample Cheby-
shev window sequences.

5.4 Bandpass FIR Filter Design 201
Kaiser window functions
1
0.8
0.6
(a)
0.4
= 6
= 7
0.2
= 8
= 9
0
0 5 10 15 20 25 30
k
Window freq. magnitude responses (dB)
0
= 6
–10 = 7
= 8
–20 = 9
–30
(b) –40
–50
–60
–70
–80
0 0.05f 0.1f 0.15f 0.2f 0.25f
s s s s s
Freq
Figure 5–27 Kaiser window functions for various βvalues: (a) window coefficients in
the time domain; (b) frequency-domain magnitude responses indB.
To conclude this section, remember that different window functions
have their own individual advantages and disadvantages for FIR filter de-
sign. Regardless of the non-rectangular window function used, they always
decrease an FIR filter’s passband ripple over that of the rectangular window.
For the enthusiastic reader, a thorough discussion of many window functions
can be found in reference [14].
5.4 BANDPASS FIR FILTER DESIGN
The window method of lowpass FIR filter design can be used as the first step
in designing a bandpass FIR filter. Let’s say we want a 31-tap FIR filter with
the frequency response shown in Figure 5–22(a), but instead of being centered

202 Finite Impulse Response Filters
about zero Hz, we want the filter’s passband to be centered about f/4 Hz. If
s
we define a lowpass FIR filter’s coefficients as h (k), our problem is to find
lp
the h (k) coefficients of a bandpass FIR filter. As shown in Figure 5–28, we
bp
can shift H (m)’s frequency response by multiplying the filter’s h (k) lowpass
lp lp
coefficients by a sinusoid of f/4 Hz. That sinusoid is represented by the
s
s (k) sequence in Figure 5–28(a), whose values are a sinewave sampled at a
shift
rate of four samples per cycle. Our final 31-tap h (k) FIR bandpass filter coef-
bp
ficients are
⋅
h (k) = h (k) s (k) , (5–20)
bp lp shift
h (k)
lp
k
x
s (k)
shift
(a)
k
=
h (k)
bp
k
Original lowpass
response|H lp (m)| |H b p (m)|
(b)
–f
s
/2 –fs/4 0 f
s
/4 f
s
/2 Freq
Figure 5–28 Bandpass filter with frequency response centered at f/4: (a) gener-
s
ating 31-tap filter coefficients h (k); (b) frequency magnitude re-
bp
sponse |H (m)|.
bp

5.5 Highpass FIR Filter Design 203
whose frequency magnitude response |H (m)| is shown as the solid curves
bp
in Figure 5–28(b). The actual magnitude of |H (m)| is half that of the origi-
bp
nal |H (m)| because half the values in h (k) are zero when s (k) corre-
lp bp shift
sponds exactly to f/4. This effect has an important practical implication. It
s
means that, when we design an N-tap bandpass FIR filter centered at a fre-
quency of f/4 Hz, we only need to perform approximately N/2 multiplica-
s
tions for each filter output sample. (There’s no reason to multiply an input
sample value, x(n–k), by zero before we sum all the products from Eq. (5–6)
and Figure 5–13, right? We just don’t bother to perform the unnecessary mul-
tiplications at all.) Of course, when the bandpass FIR filter’s center frequency
is other than f/4, we’re forced to perform the full number of N multiplica-
s
tions for each FIR filter output sample.
Notice, here, that the h (k) lowpass coefficients in Figure 5–28(a) have
lp
not been multiplied by any window function. In practice, we’d use an h (k)
lp
that has been windowed prior to implementing Eq. (5–20) to reduce the pass-
band ripple. If we wanted to center the bandpass filter’s response at some
frequency other than f/4, we merely need to modify s (k) to represent sam-
s shift
pled values of a sinusoid whose frequency is equal to the desired bandpass
center frequency. That new s (k) sequence would then be used in Eq. (5–20)
shift
to get the new h (k).
bp
5.5 HIGHPASS FIR FILTER DESIGN
Going one step further, we can use the bandpass FIR filter design technique
to design a highpass FIR filter. To obtain the coefficients for a highpass filter,
we need only modify the shifting sequence s (k) to make it represent a sam-
shift
pled sinusoid whose frequency is f/2. This process is shown in Figure 5–29.
s
Our final 31-tap highpass FIR filter’s h (k) coefficients are
hp
⋅
h (k) = h (k) s (k)
hp lp shift
⋅
= h (k) (1, –1, 1, –1, 1, –1, etc.), (5–21)
lp
whose |H (m)| frequency response is the solid curve in Figure 5–29(b). Be-
hp
cause s (k) in Figure 5–29(a) has alternating plus and minus ones, we can see
shift
that h (k) is merely h (k) with the sign changed for every other coefficient.
hp lp
Unlike |H (m)| in Figure 5–28(b), the |H (m)| response in Figure 5–29(b)
bp hp
has the same amplitude as the original |H (m)|.
lp
Again, notice that the h (k) lowpass coefficients in Figure 5–29(a) have
lp
not been modified by any window function. In practice, we’d use a win-
dowed h (k) to reduce the passband ripple before implementing Eq. (5–21).
lp

204 Finite Impulse Response Filters
h (k)
lp
k
x
s (k)
shift
(a)
k
=
h (k)
hp
k
Original lowpass
response |H lp (m)| |H h p (m)|
(b)
–f
s
/2 –fs/4 0 f
s
/4 f
s
/2 Freq
Figure 5–29 Highpass filter with frequency response centered at f/2: (a) gener-
s
ating 31-tap filter coefficients h (k); (b) frequency magnitude re-
hp
sponse |H (m)|.
hp
5.6 PARKS-MCCLELLAN EXCHANGE FIR FILTER DESIGN METHOD
Let’s introduce one last FIR filter design technique that has found wide
acceptance in practice. The Parks-McClellan FIR filter design method (also
called the Remez Exchange, or Optimal method†) is a popular technique
used to design high-performance FIR filters. To use this design method, we
have to visualize a desired frequency response H (m) like that shown in Fig-
d
ure 5–30.
†Remez is pronounced re-’ma¯.

5.6 Parks-McClellan Exchange FIR Filter Design Method 205
H (m)
d 1 + δ
p
1
1–δ
p
δ
s
0
0 f pass f s /2 Freq
f
stop
–δ
s
Passband Stopband
Figure 5–30 Desired frequency response definition of a lowpass FIR filter using the
Parks-McClellan Exchange design method.
We have to establish a desired passband cutoff frequency f and the
pass
frequency where the attenuated stopband begins, f . In addition, we must
stop
establish the variables δ and δ that define our desired passband and stop-
p s
band ripple. Passband and stopband ripples, in decibels, are related to δ and
p
δ by[15]
s
⋅
Passband ripple = 20 log (1 + δ ) (5–22)
10 p
and
⋅
Stopband ripple = –20 log (δ). (5–22’)
10 s
(Some of the early journal papers describing the Parks-McClellan design
⋅
method used the equally valid expression –20 log (δ ) to define the pass-
10 p
band ripple in dB. However, Eq. (5–22) is the most common form used today.)
Next, we apply these parameters to a computer software routine that gener-
ates the filter’s N time-domain h(k) coefficients where N is the minimum
number of filter taps to achieve the desired filter response.
On the other hand, some software Parks-McClellan routines assume that
we want δ and δ to be as small as possible and require us only to define the
p s
desired values of the H (m) response as shown by the solid black dots in Fig-
d
ure 5–31. The software then adjusts the values of the undefined (shaded dots)
values of H (m) to minimize the error between our desired and actual fre-
d
quency response while minimizing δ and δ. The filter designer has the op-
p s
tion to define some of the H (m) values in the transition band, and the
d
software calculates the remaining undefined H (m) transition band values.
d
With this version of the Parks-McClellan algorithm, the issue of most

206 Finite Impulse Response Filters
H (m)
d
1
0
0 f pass f stop f s /2 Freq
Passband Transition Stopband
band
Figure 5–31 Alternate method for defining the desired frequency response of a
lowpass FIR filter using the Parks-McClellan Exchange technique.
importance becomes how we define the transition region. We want to mini-
mize its width while, at the same time, minimizing passband and stopband
ripple. So exactly how we design an FIR filter using the Parks-McClellan Ex-
change technique is specific to the available filter design software. Although
the mathematics involved in the development of the Parks-McClellan Ex-
change method is rather complicated, we don’t have to worry about that
here[16–20]. Just remember that the Parks-McClellan Exchange design method
gives us a Chebyshev-type filter whose actual frequency response is as close as
possible to the desired H (m) response for a given number of filter taps.
d
To illustrate the advantage of the Parks-McClellan method, the solid curve
in Figure 5–32 shows the frequency response of a 31-tap FIR designed using
this technique. For comparison, Figure 5–32 also shows the frequency re-
sponses of two 31-tap FIR filters for the same passband width using the Cheby-
shev and Kaiser windowing techniques. Notice how the three filters have
roughly the same stopband sidelobe levels, near the main lobe, but that the
Parks-McClellan filter has the more desirable (steeper) transition band roll-off.
The Parks-McClellan Exchange filter design method revolutionized the
art of, and has become the predominant technique for, designing linear-phase
FIR filters. As a historical note, when Profs. Parks and McClellan (James Mc-
Clellan was a graduate student at the time) developed their triumphant filter
design method in 1971, they submitted a paper to Electronics Letters to publi-
cize their achievement. Surprisingly, the editors of Electronics Letters rejected
the paper because the reviewers didn’t believe that such a flexible, and opti-
mized, FIR design procedure was possible. A description of Parks and
McClellan’s revolutionary design method was eventually published in refer-
ence[17]. That story is reminiscent of when Decca Records auditioned a group
of four young musicians in 1961. Decca executives decided not to sign the

5.7 Half-Band FIR Filters 207
Filter frequency magnitude responses (dB)
0
Chebyshev (dashed)
–20
Kaiser (dotted)
Parks-McClellan (solid)
–40
–60
0 0.05f 0.1f 0.15f 0.2f 0.25f 0.3f 0.35f
s s s s s s s
Figure 5–32 Frequency response comparison of three 31-tap FIR filters: Parks-
McClellan, Chebyshev windowed, and Kaiser windowed.
group to a contract. You may have heard of that musical group—they were
called the Beatles.
5.7 HALF-BAND FIR FILTERS
There’s a specialized FIR filter that’s proved very useful in signal decimation
and interpolation applications[21–25]. Called a half-band FIR filter, its fre-
quency magnitude response is symmetrical about the f/4 point as shown in
s
Figure 5–33(a). As such, the sum of f and f is f/2. When the filter has an
pass stop s
odd number of taps, this symmetry has the beautiful property that the filter’s
time-domain impulse response has every other filter coefficient being zero,
except the center coefficient. This enables us to avoid approximately half the
number of multiplications when implementing this kind of filter. By way of
example, Figure 5–33(b) shows the coefficients for a 31-tap half-band filter
where Δf was defined to be approximately f/32 using the Parks-McClellan
s
FIR filter design method.
Notice how the alternating h(k) coefficients are zero, so we perform 17
multiplications per output sample instead of the expected 31 multiplications.
Stated in different words, we achieve the performance of a 31-tap filter at
the computational expense of only 17 multiplies per output sample. In the
general case, for an N-tap half-band FIR filter, we’ll only need to perform
(N + 1)/2 + 1 multiplications per output sample. (Section 13.7 shows a tech-
nique to further reduce the number of necessary multiplies for linear-phase
tapped-delay line FIR filters, including half-band filters.) The structure of a
simple seven-coefficient half-band filter is shown in Figure 5–33(c), with the
h(1) and h(5) multipliers absent.

208 Finite Impulse Response Filters
Desired 1 + δ Δf Δf
response p
1.0
(a)
0.5 δ
s
0
f f Freq
pass f s /4 stop f s /2
h(k)
1.0
(b)
0.6
0.2
0
k
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
-0.2
x(n) x(n-2) x(n-3) x(n-4) x(n-6)
Delay Delay Delay Delay Delay Delay
h(0) h(2) h(3) h(4) h(6)
(c)
+
y(n)
Figure 5-33 Half-band FIR filter: (a) frequency magnitude response [transition re-
gion centered at f/4]; (b) 31-tap filter coefficients; (c) 7-tap half-
s
band filter structure.
Be aware, there’s a restriction on the number of half-band filter coefficients.
To build linear-phase N-tap half-band FIR filters, having alternating zero-valued
coefficients, N + 1 must be an integer multiple of four. If this restriction is not
met, for example when N = 9, the first and last coefficients of the filter will both
be equal to zero and can be discarded, yielding a 7-tap half-band filter.
On a practical note, there are two issues to keep in mind when we use an
FIR filter design software package to design a half-band filter. First, assuming
that the modeled filter has a passband gain of unity, ensure that your filter
has a gain of 0.5 (–6 dB) at a frequency of f/4. Second, unavoidable numerical
s
computation errors will yield alternate filter coefficients that are indeed very
small but not exactly zero-valued as we desire. So in our filter modeling ef-
forts, we must force those very small coefficient values to zero before we pro-
ceed to analyze half-band filter frequency responses.
You might sit back and think, “OK, these half-band filters are mildly in-
teresting, but they’re certainly not worth writing home about.” As it turns
out, half-band filters are very important because they’re widely used in appli-
cations with which you’re familiar—like pagers, cell phones, digital re-

5.8 Phase Response of FIR Filters 209
ceivers/televisions, CD/DVD players, etc. We’ll learn more about half-band
filter applications in Chapter 10.
5.8 PHASE RESPONSE OF FIR FILTERS
Although we illustrated a couple of output phase shift examples for our origi-
nal averaging FIR filter in Figure 5–10, the subject of FIR phase response de-
serves additional attention. One of the dominant features of FIR filters is their
linear phase response which we can demonstrate by way of example. Given
the 25 h(k) FIR filter coefficients in Figure 5–34(a), we can perform a DFT to
determine the filter’s H(m) frequency response. The normalized real part,
imaginary part, and magnitude of H(m) are shown in Figures 5–34(b) and
5–34(c), respectively.† Being complex values, each H(m) sample value can be
described by its real and imaginary parts, or equivalently, by its magnitude
|H(m)| and its phase H (m) shown in Figure 5–35(a).
ø
The phase of a complex quantity is, of course, the arctangent of the
imaginary part divided by the real part, or ø = tan–1(imag/real). Thus the
phase of H (m) is determined from the samples in Figure 5–34(b).
ø
The phase response in Figure 5–35(a) certainly looks linear over selected
frequency ranges, but what do we make of those sudden jumps, or disconti-
nuities, in this phase response? If we were to plot the angles of H (m) starting
ø
with the m = 0 sample on a polar graph, using the nonzero real part of H(0),
and the zero-valued imaginary part of H(0), we’d get the zero-angled H (0)
ø
phasor shown on the right side of Figure 5–35(b). Continuing to use the real
and imaginary parts of H(m) to plot additional phase angles results in the
phasors going clockwise around the circle in increments of –33.75°. It’s at the
H (6) that we discover the cause of the first discontinuity in Figure 5–35(a).
ø
Taking the real and imaginary parts of H(6), we’d plot our phasor oriented at
an angle of –202.5°. But Figure 5–35(a) shows that H (6) is equal to 157.5°. The
ø
problem lies in the software routine used to generate the arctangent values
plotted in Figure 5–35(a). The software adds 360° to any negative angles in
the range of –180° > ø≥ –360°, i.e., angles in the upper half of the circle. This
makes ø a positive angle in the range of 0° < ø≤ 180° and that’s what gets
plotted. (This apparent discontinuity between H (5) and H (6) is called phase
ø ø
wrapping.) So the true H (6) of –202.5° is converted to a +157.5° as shown in
ø
parentheses in Figure 5–35(b). If we continue our polar plot for additional
H (m) values, we’ll see that their phase angles continue to decrease with an
ø
angle increment of –33.75°. If we compensate for the software’s behavior and
†Any DFT size greater than the h(k) width of 25 is sufficient to obtain H(m). The h(k) sequence was
padded with 103 zeros to take a 128-point DFT, resulting in the H(m) sample values in Figure 5–34.

210 Finite Impulse Response Filters
h(k)
1 1
0.5
(a) 0
k
2 4 6 8 10 12 14 16 18 20 22 24
–0.5
1.0
0.75 Real part of H(m)
0.5
Imaginary part of H(m)
0.25
(b) 0
m
–0.25 2 4 6 10 12 16 22 24 26 28 30
m = 17
–0.5
–0.75
–1.0
|H(m)|
1.0
16f f
0.75 Frequency = s = s
128 8
0.5
(c)
0.25
0
m
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Passband
Figure 5–34 FIR filter frequency response H(m): (a) h(k) filter coefficients; (b) real
and imaginary parts of H(m); (c) magnitude of H(m).
plot phase angles more negative than –180°, by unwrappingthe phase, we get
the true H (m) shown in Figure 5–35(c).
ø
Notice that H (m) is, indeed, linear over the passband of H(m). It’s at
ø
H (17) that our particular H(m) experiences a polarity change of its real part
ø
while its imaginary part remains negative—this induces a true phase-angle
discontinuity that really is a constituent of H(m) at m = 17. (Additional phase
discontinuities occur each time the real part of H(m) reverses polarity, as
shown in Figure 5–35(c).) The reader may wonder why we care about the lin-
ear phase response of H(m). The answer, an important one, requires us to in-
troduce the notion of group delay.

5.8 Phase Response of FIR Filters 211
H (m) in degrees
200 Ø
100
2 4 12 14 16 18 20 22 24 26 28 30
(a) 0
6 8 10 m
–100
–200
Discontinuity induced by
arctangent calculation method
–270o
–236.25o (90 o ) –303.75o
(123.75 o ) (56.25 o )
H (6) =–202.5o
Ø (157.5o )
(b) –168.75o H Ø (0) = 0o
–33.75o
–135o
–101.25o –67.5o
H (m) in degrees
Ø
100
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
0
m
–100
(c) –200
–300
–400
Legitimate
–500
discontinuities
–600
Passband =
fs inH Ø (m)
8
Figure 5–35 FIR filter phase response H(m) in degrees: (a) calculated H(m); (b)
ø ø
polar plot of H(m)’s first ten phase angles in degrees; (c) actual
ø
H(m).
ø
Group delay is defined as the negative of the derivative of the phase with
respect to frequency, or G = –dø/df. For FIR filters, then, group delay is the
slope of the H (m) response curve. When the group delay is constant, as it is
ø
over the passband of all FIR filters having symmetrical coefficients, all fre-
quency components of the filter input signal are delayed by an equal amount of
time Gbefore they reach the filter’s output. This means that no phase distortion
is induced in the filter’s desired output signal, and this is crucial in communica-
tions signals. For amplitude modulation (AM) signals, constant group delay
preserves the time waveform shape of the signal’s modulation envelope. That’s
important because the modulation portion of an AM signal contains the

212 Finite Impulse Response Filters
signal’s information. Conversely, a nonlinear phase will distort the audio of
AM broadcast signals, blur the edges of television video images, blunt the
sharp edges of received radar pulses, and increase data errors in digital com-
munications signals. (Group delay is sometimes called envelope delay because
group delay was originally the subject of analysis due to its effect on the enve-
lope, or modulation signal, of amplitude modulation AM systems.) Of course
we’re not really concerned with the group delay outside the passband because
signal energy outside the passband is what we’re trying to eliminate through
filtering.
Over the passband frequency range for a linear-phase, S-tap FIR filter,
group delay has been shown to be given by
D⋅t
G= s seconds, (5–23)
2
where D = S–1 is the number of unit-delay elements in the filter’s delay line,
and t is the sample period (1/f).† This group delay is measured in seconds.
s s
Eliminating the t factor in Eq. (5–23) would change its dimensions to sam-
s
ples. The value G, measured in samples, is always an integer for odd-tap FIR
filters and a noninteger for even-tap filters.
Although we used a 128-point DFT to obtain the frequency responses in
Figures 5–34 and 5–35, we could just as well have used N = 32-point or
N = 64-point DFTs. These smaller DFTs give us the phase response curves
shown in Figures 5–36(a) and 5–36(b). Notice how different the phase re-
sponse curves are when N = 32 in Figure 5–36(a) compared to when N = 128
in Figure 5–36(c). The phase-angle resolution is much finer in Figure 5–36(c).
The passband phase-angle resolution, or increment Δø, is given by
⋅
–G 360°
Δø= , (5–24)
N
where N is the number of points in the DFT. So, for our S = 25-tap filter in
⋅
Figure 5–34(a), G = 12, and Δø is equal to –12 360°/32 = –135° in Figure
5–36(a), and Δø is –33.75°in Figure 5–36(c). If we look carefully at the sample
values in Figure 5–36(a), we’ll see that they’re all included within the samples
in Figures 5–36(b) and 5–36(c).
Let’s conclude this FIR phase discussion by reiterating the meaning of
phase response. The phase, or phase delay, at the output of an FIR filter is the
phase of the first output sample relative to the phase of the filter’s first input
sample. Over the passband, that phase shift, of course, is a linear function of
frequency. This will be true only as long as the filter has symmetrical coeffi-
cients. Figure 5–10 is a good illustration of an FIR filter’s output phase delay.
†As derived in Section 3.4 of reference [16], and page 597 of reference [19].

5.8 Phase Response of FIR Filters 213
H (m) in degrees
200 Ø
150
32-point DFT
100
50
(a) 0
m
–50
–100
–150
–200
200 H Ø (m) in degrees
150
64-point DFT
100
50
(b) 0
m
–50
–100
–150
–200
H (m) in degrees
200 Ø
150
128-point DFT
100
50
(c) 0
m
–50
–100
–150
–200
Figure 5–36 FIR filter phase response H(m) in degrees: (a) calculated using a
ø
32-point DFT; (b) using a 64-point DFT; (c) using a 128-point DFT.
For FIR filters, the output phase shift measured in degrees, for the pass-
band frequency f=mf/N, is expressed as
s
⋅ ⋅
phase delay=H (mf /N)=m ⋅Δø= –m G 360° . (5–25)
ø s N
We can illustrate Eq. (5–25) and show the relationship between the phase
responses in Figure 5–36 by considering the phase delay associated with
the frequency of f/32 in Table 5–2. The subject of group delay is described
s

214 Finite Impulse Response Filters
Table 5–2 Values Used in Eq. (5–25) for the Frequency f/32
s
DFT size, N Indexm H (mf /N)
ø s
32 1 –135o
64 2 –135o
128 4 –135o
further in Appendix F, where an example of envelope delay distortion, due to
a filter’s nonlinear phase, is illustrated.
5.9 A GENERIC DESCRIPTION OF DISCRETE CONVOLUTION
Although convolution was originally an analysis tool used to prove continu-
ous signal processing theorems, we now know that convolution affects every
aspect of digital signal processing. Convolution influences our results when-
ever we analyze or filter any finite set of data samples from a linear time-
invariant system. Convolution not only constrains DFTs to be just
approximations of the continuous Fourier transform; it is the reason that dis-
crete spectra are periodic in the frequency domain. It’s interesting to note
that, although we use the process of convolution to implement FIR digital fil-
ters, convolution effects induce frequency response ripple, preventing us
from ever building a perfect digital filter. Its influence is so pervasive that to
repeal the law of convolution, quoting a phrase from Dr. Who, would “un-
ravel the entire causal nexus” of digital signal processing.
Convolution has always been a somewhat difficult concept for the be-
ginner to grasp. That’s not too surprising for several reasons. Convolution’s
effect on discrete signal processing is not intuitively obvious for those with-
out experience working with discrete signals, and the mathematics of convo-
lution does seem a little puzzling at first. Moreover, in their sometimes
justified haste, many authors present the convolution equation and abruptly
start using it as an analysis tool without explaining its origin and meaning.
For example, this author once encountered what was called a tutorial article
on the FFT in a professional journal that proceeded to define convolution
merely by presenting something like that shown in Figure 5–37 with no fur-
ther explanation!
Unfortunately, few beginners can gain an understanding of the convolu-
tion process from Figure 5–37 alone. Here, we avoid this dilemma by defining
the process of convolution and gently proceed through a couple of simple
convolution examples. We conclude this chapter with a discussion of the

5.9 A Generic Description of Discrete Convolution 215
N–1
Y j = P k .Q j–k, or
k = 0
Y
0
Q
0
QN–1 QN–2 . . . Q
1
P
0
Y Q Q Q . . . Q P
1 1 0 N–1 2 1
Y 2 Q 2 Q 1 Q 0 . . . Q 3 . P 2
. = . . . . . .
. . . . . . .
. . . . . . .
Y Q Q Q . . . Q P
N–1 N–1 N–2 N–3 0 N–1
Theorem: if
Pj ← DFT→ A n ,
Qj ← DFT→ Bn , and
Yj ← DFT→ Cn ,
then
C n = N . A n . B n .
Figure 5–37 One very efficient, but perplexing, way of defining convolution.
powerful convolution theorem and show why it’s so useful as a qualitative
tool in discrete system analysis.
5.9.1 Discrete Convolution in the Time Domain
Discrete convolution is a process whose input is two sequences and that pro-
vides a single output sequence. Convolution inputs can be two time-domain
sequences giving a time-domain output, or two frequency-domain input se-
quences providing a frequency-domain result. (Although the two input se-
quences must both be in the same domain for the process of convolution to
have any practical meaning, their sequence lengths need not be the same.)
Let’s say we have two input sequences h(k) of length Pand x(k) of length Qin
the time domain. The output sequence y(n) of the convolution of the two in-
puts is defined mathematically as
P+ ∑Q−2
y(n)= h(k)x(n−k). (5–26)
k=0
Let’s examine Eq. (5–26) by way of example, using the h(k) and x(k) sequences
shown in Figure 5–38. In this example, we can write the terms for each y(n) in
Eq. (5–26) as

216 Finite Impulse Response Filters
h(k) x(k)
3
2 2
1 1
0 0
–3–2–10 1 2 3 4 5 6 k –3–2–1 0 1 2 3 4 5 6 k
(a) (b)
Figure 5–38 Convolution example input sequences: (a) first sequence h(k) of
lengthP=4; (b) second sequence x(k) of length Q=3.
y(0) = h(0)x(0–0) + h(1)x(0–1) + h(2)x(0–2) +h(3)x(0–3) + h(4)x(0–4)+ h(5)x(0–5),
y(1) = h(0)x(1–0) + h(1)x(1–1) + h(2)x(1–2) + h(3)x(1–3) + h(4)x(1–4) + h(5)x(1–5),
y(2) = h(0)x(2–0) + h(1)x(2–1) + h(2)x(2–2) + h(3)x(2–3) + h(4)x(2–4) + h(5)x(2–5),
y(3) = h(0)x(3–0) + h(1)x(3–1) + h(2)x(3–2) + h(3)x(3–3) + h(4)x(3–4) + h(5)x(3–5),
y(4) = h(0)x(4–0) + h(1)x(4–1) + h(2)x(4–2) + h(3)x(4–3) + h(4)x(4–4) + h(5)x(4–5),
and
y(5) = h(0)x(5–0) + h(1)x(5–1) + h(2)x(5–2) + h(3)x(5–3) + h(4)x(5–4) + h(5)x(5–5).
(5–27)
With P = 4 and Q = 3, we need evaluate only 4 + 3 – 1 = 6 individual y(n)
terms. Because h(4) and h(5) are zero, we can eliminate some of the terms in
Eq. (5–27) and evaluate the remaining x(n–k) indices, giving the following ex-
pressions for y(n) as
y(0) = h(0)x(0) + h(1)x(–1) + h(2)x(–2) + h(3)x(–3),
y(1) = h(0)x(1) + h(1)x(0) + h(2)x(–1) + h(3)x(–2),
y(2) = h(0)x(2) + h(1)x(1) + h(2)x(0) + h(3)x(–1),
y(3) = h(0)x(3) + h(1)x(2) + h(2)x(1) + h(3)x(0),
y(4) = h(0)x(4) + h(1)x(3) + h(2)x(2) + h(3)x(1),
and
y(5) = h(0)x(5) + h(1)x(4) + h(2)x(3) + h(3)x(2). (5–28)
Looking at the indices of the h(k) and x(k) terms in Eq. (5–28), we see two very
important things occurring. First, convolution is merely the summation of a
series of products—so the process itself is not very complicated. Second, no-
tice that, for a given y(n), h(k)’s index is increasing as x(k)’s index is decreas-
ing. This fact has led many authors to introduce a new sequence x(–k) and use
that new sequence to graphically illustrate the convolution process. The x(–k)

5.9 A Generic Description of Discrete Convolution 217
sequence is simply our original x(k) reflected about the 0 index of the kaxis as
shown in Figure 5–39. Defining x(–k) as such enables us to depict the products
and summations of Eq. (5–28)’s convolution as in Figure 5–40; that is, we can
now align the x(–k) samples with the samples of h(k) for a given n index to
calculate y(n). As shown in Figure 5–40(a), the alignment of h(k) and x(n–k),
for n = 0, yields y(0) = 1. This is the result of the first line in Eq. (5–28) re-
peated on the right side of Figure 5–40(a). The calculation of y(1), for n=1, is
depicted in Figure 5–40(b), where x(n–k) is shifted one element to the right,
resulting in y(1)=3. We continue this x(n–k) shifting and incrementing nuntil
we arrive at the last nonzero convolution result of y(5) shown in Figure
5–40(f). So, performing the convolution of h(k) and x(k) comprises
1. plotting both the h(k) and x(k) sequences,
2. flipping the x(k) sequence around the k= 0 sample to obtain x(–k),
3. summing the products of h(k) and x(0–k) for all kto yield y(0),
4. shifting the x(–k) sequence one sample to the right,
5. summing the products of h(k) and x(1–k) for all kto obtain y(1), and
6. continuing to shift x(–k) and sum products until there’s no overlap of
h(k) and the shifted x(n–k), in which case all further y(n) output samples
are zero and we’re done.
The full convolution of our h(k) and x(k) is the y(n) sequence on the right side
of Figure 5–40(f). We’ve scanned the x(–k) sequence across the h(k) sequence
and summed the products where the sequences overlap. By the way, notice
that the y(n) sequence in Figure 5–40(f) has six elements where h(k) had a
length of four and x(k) was of length three. In the general case, if h(k) is of
length Pand x(k) is of length Q, the length of y(n) will have a sequence length
of L, where
L= P+ Q– 1. (5–29)
x(k) x(–k)
3 3
2 2
1 1
0 0
–3–2–1 0 1 2 3 4 5 6 k –3–2–10 1 2 3 4 5 6 k
(a) (b)
Figure 5–39 Convolution example input sequence: (a) second sequence x(k) of
length 3; (b) reflection of the second sequence about the k = 0
index.

218 Finite Impulse Response Filters
1 h(k) y(0) = h(0)x(0) + h(1)x(–1) + h(2)x(–2) + h(3)x(–3)
convolution 6 = 1 + 0 + 0 + 0 = 1
(a) 0 –5–4–3–2–1 0 1 2 3 4 5 6 k withn = 0 4
3 x(0–k) 2
2
1 0
0
–4–3–2–1 0 1 2 3 4 5 6 k 0 1 2 3 4 5 6 7 8 n
1 h(k) y(1) = h(0)x(1) + h(1)x(0) + h(2)x(–1) + h(3)x(–2)
0 convolution 6 = 2 + 1 + 0 + 0 = 3
(b) –5–4–3–2–1 0 1 2 3 4 5 6 k withn = 1 4
3 x(1–k) 2
2
1 0
0 –4–3–2–1 0 1 2 3 4 5 6 k 0 1 2 3 4 5 6 7 8 n
1 h(k) convolution y(2) = h(0)x(2) = + 3 h ( + 1 ) 2 x ( + 1 ) 1 + + h 0 (2 = )x 6 (0) + h(3)x(–1)
0 withn = 2 6
–5–4–3–2–1 0 1 2 3 4 5 6 k 4
(c)
3 x(2–k) 2
2
1 0
0
–4–3–2–1 0 1 2 3 4 5 6 k 0 1 2 3 4 5 6 7 8 n
1 h(k) y(3) = h(0)x(3) + h(1)x(2) + h(2)x(1) + h(3)x(0)
convolution 6 = 0 + 3 + 2 + 1 = 6
0
–5–4–3–2–1 0 1 2 3 4 5 6 k withn = 3 4
(d)
3 x(3–k) 2
2
1 0
0
–4–3–2–1 0 1 2 3 4 5 6 k 0 1 2 3 4 5 6 7 8 n
1 h(k) y(4) = h(0)x(4) + h(1)x(3) + h(2)x(2) + h(3)x(1)
convolution 6 = 0 + 0 + 3 + 2 = 5
0 withn = 4
–5–4–3–2–1 0 1 2 3 4 5 6 k 4
(e)
3 x(4–k) 2
2
1 0
0
–4–3–2–1 0 1 2 3 4 5 6 k 0 1 2 3 4 5 6 7 8 n
1 h(k) y(5) = h(0)x(5) + h(1)x(4) + h(2)x(3) + h(3)x(2)
convolution 6 = 0 + 0 + 0 + 3 = 3
0
–5–4–3–2–1 0 1 2 3 4 5 6 k withn = 5 4
(f)
3 x(5–k) 2
2
1 0
0
–4–3–2–1 0 1 2 3 4 5 6 k 0 1 2 3 4 5 6 7 8 n
Figure 5–40 Graphical depiction of the convolution of h(k) and x(k) in Figure
5–38.

5.9 A Generic Description of Discrete Convolution 219
At this point, it’s fair for the beginner to ask, “OK, so what? What does
this strange convolution process have to do with digital signal processing?”
The answer to that question lies in understanding the effects of the convolu-
tion theorem.
5.9.2 The Convolution Theorem
The convolution theorem is a fundamental constituent of digital signal pro-
cessing. It impacts our results anytime we filter or Fourier transform discrete
data. To see why this is true, let’s simplify the notation of Eq. (5–26) and use
the abbreviated form
y(n) = h(k) x(k), (5–30)
*
where, again, the “ ” symbol means convolution. The convolution theorem
*
may be stated as follows: If two time-domain sequences h(k) and x(k) have
DFTs of H(m) and X(m), respectively, then the DFT of h(k) x(k) is the product
*
⋅ ⋅
H(m) X(m). Likewise, the inverse DFT of H(m) X(m) is h(k)
*
x(k). We can rep-
resent this relationship with the expression
⎯D⎯⎯FT→ ⋅
h(k)*x(k) ←⎯⎯⎯ H(m) X(m). (5–31)
IDFT
Equation (5–31) tells us that two sequences resulting from h(k) x(k) and
*
⋅
H(m) X(m) are Fourier transform pairs. So, taking the DFT of h(k) x(k) always
*
⋅
gives us H(m) X(m). Likewise, we can determine h(k) x(k) by taking the inverse
*
⋅
DFT of H(m) X(m). The important point to learn from Eq. (5–31) is that convolu-
tion in the time domain is equivalent to multiplication in the frequency domain.
(We won’t derive the convolution theorem here because its derivation is readily
available to the interested reader[26–29].) To help us appreciate this principle,
Figure 5–41 sketches the relationship between convolution in the time domain
and multiplication in the frequency domain.
We can easily illustrate the convolution theorem by taking 8-point DFTs of
h(k) and x(k) to get H(m) and X(m), respectively, and listing these values as in
Table 5–3. (Of course, we have to pad h(k) and x(k) with zeros, so they both have
lengths of 8 to take 8-point DFTs.) Tabulating the inverse DFT of the product
⋅
H(m) X(m) allows us to verify Eq. (5–31), as listed in the last two columns of
Table 5–3, where the acronym IDFT again means inverse DFT. The values from
Table 5–3 are shown in Figure 5–42. (For simplicity, only the magnitudes of
⋅
H(m), X(m), and H(m) X(m) are shown in the figure.) We need to become
comfortable with convolution in the time domain because, as we’ve learned,
it’s the process used in FIR filters. As detailed in Section 5.2, we perform dis-
crete time-domain FIR filtering by convolving an input sequence, x(n) say,

220 Finite Impulse Response Filters
Sequencex(k)
Sequenceh(k)
*
Convolution in the time Time
domain = h(k)*x(k) domain
Inverse DFT DFT DFT
DFT
Fourier
transform pair H(m) X(m)
Frequency
domain
Multiplication in the frequency
domain = H(m)•X(m)
Figure 5–41 Relationships of the convolution theorem.
with the impulse response x(k) of a filter, and for FIR filters that impulse
response happens to also be the filter’s coefficients.†The result of that convo-
lution is a filtered time-domain sequence whose spectrum is modified (multi-
plied) by the filter’s frequency response X(m). Section 13.10 describes a clever
scheme to perform FIR filtering efficiently using the FFT algorithm to imple-
ment convolution.
Because of the duality of the convolution theorem, we could have
swapped the time and frequency domains in our discussion of convolution
and multiplication being a Fourier transform pair. This means that, similar to
Eq. (5–31), we can also write
⋅ ⎯D⎯⎯FT→
h(k) x(k) ←⎯⎯⎯ H(m)*X(m). (5–32)
IDFT
So the convolution theorem can be stated more generally as Convolution
in one domain is equivalent to multiplication in the other domain. Figure 5–43
†As we’ll see in Chapter 6, the coefficients used for an infinite impulse response (IIR) filter are
not equal to that filter’s impulse response.

Table 5–3 Convolution Values of h(k) and x(k) from Figure 5–38
Index DFT of DFT of IDFT of
k or m h(k) x(k) h(k) = H(m) x(k) = X(m) H(m)(cid:2)X(m) H(m)(cid:2)X(m) h(k)*x(k)
0 1 1 4.0 + j0.0 6.0 + j0.0 24.0 + j0.0 1.0 + j0.0 1
1 1 2 1.00 – j2.41 2.41 – j4.41 –8.24 – j10.24 3.0 + j0.0 3
2 1 3 0 –2.0 – j2.0 0 6.0 + j0.0 6
3 1 0 1.00 – j0.41 –0.41 + j1.58 0.24 + j1.75 6.0 + j0.0 6
4 0 0 0 2.0 + j0.0 0 5.0 + j0.0 5
5 0 0 1.00 + j0.41 –0.41 – j1.58 0.24 – j1.75 3.0 + j0.0 3
6 0 0 0 –2.00 + j2.00 0 0.0 + j0.0 0
7 0 0 1.00 + j2.41 2.41 + j4.41 –8.24 + j10.24 0.0 + j0.0 0
221

222 Finite Impulse Response Filters
Sequencex(k)
4
2 Sequenceh(k)
0 1
0 1 2 3 4 5 6 7 k 0.5
0
0 1 2 3 4 5 6 7 k
*
y(n) = h(k)*x(k)
6
Convolution in the 4
time domain 2
0
0 1 2 3 4 5 6 7 n
Time
domain
Inverse DFT DFT DFT
DFT Frequency
domain
4 |H(m)| 6 |X(m)|
3 4
2
2
1
0 0
0 1 2 3 4 5 6 7 m 0 1 2 3 4 5 6 7 m
30 |H(m)•X(m)|
20
Multiplication in the
frequency domain 10
0
0 1 2 3 4 5 6 7m
Figure 5–42 Convolution relationships of h(k), x(k), H(m), and X(m) from Fig-
ure5–38.
shows the relationship between multiplication in the time domain and convo-
lution in the frequency domain. Equation (5–32) is the fundamental relation-
ship used in the process of windowing time-domain data to reduce DFT
leakage, as discussed in Section 3.9.
5.9.3 Applying the Convolution Theorem
The convolution theorem is useful as a qualitative tool in predicting the ef-
fects of different operations in discrete linear time-invariant systems. For ex-
ample, many authors use the convolution theorem to show why periodic
sampling of continuous signals results in discrete samples whose spectra are
periodic in the frequency domain. Consider the real continuous time-domain
waveform in Figure 5–44(a), with the one-sided spectrum of bandwidth B.

5.9 A Generic Description of Discrete Convolution 223
Sequencex(k)
Sequenceh(k)
Multiplication in the time Time
domain = h(k)•x(k) domain
Inverse DFT DFT DFT
DFT
Fourier
transform pair H(m) X(m)
Frequency
domain
*
Convolution in the frequency
domain = H(m)*X(m)
Figure 5–43 Relationships of the convolution theorem related to multiplication in
the time domain.
Being a real signal, of course, its spectrum is symmetrical about 0 Hz. (In Fig-
ure 5–44, the large right-pointing arrows represent Fourier transform opera-
tions.) Sampling this waveform is equivalent to multiplying it by a
sequence of periodically spaced impulses, Figure 5–44(b), whose values are
unity. If we say that the sampling rate is f samples/second, then the sam-
s
ple period t =1/f seconds. The result of this multiplication is the sequence
s s
of discrete time-domain impulses shown in Figure 5–44(c). We can use the
convolution theorem to help us predict what the frequency-domain effect
is of this multiplication in the time domain. From our theorem, we now re-
alize that the spectrum of the time-domain product must be the convolu-
tion of the original spectra. Well, we know what the spectrum of the
original continuous waveform is. What about the spectrum of the time-
domain impulses? It has been shown that the spectrum of periodic im-
pulses, whose period is t seconds, is also periodic impulses in the
s
frequency domain with a spacing of f Hz as shown in Figure 5–44(b)[30].
s
Now, all we have to do is convolve the two spectra. In this case, convo-
lution is straightforward because both of the frequency-domain functions are
symmetrical about the zero-Hz point, and flipping one of them about zero Hz

224 Finite Impulse Response Filters
. . . . . .
(a) FT
0 Time –B 0 B Frequency
x *
1
. . . . . . . . . . . .
(b) 0 Time FT –2/ts –1/ts 0 1/ts 2/ts Frequency
t
s
= =
f = 1/t
s s
. . . . . . . . . . . .
(c) DFT
0 t Time –2fs –fs 0 fs 2fs Frequency
s
Figure 5–44 Using convolution to predict the spectral replication effects of peri-
odic sampling.
is superfluous. So we merely slide one of the functions across the other and
plot the product of the two. The convolution of the original waveform spec-
trum and the spectral impulses results in replications of the waveform spec-
trum every f Hz, as shown in Figure 5–44(c). This discussion reiterates the
s
fact that the DFT is always periodic with a period of f Hz.
s
Here’s another example of how the convolution theorem can come in
handy when we try to understand digital signal processing operations. This
author once used the theorem to resolve the puzzling result, at the time, of a
triangular window function having its first frequency response null at twice
the frequency of the first null of a rectangular window function. The question
was “If a rectangular time-domain function of width T has its first spectral
null at 1/T Hz, why does a triangular time-domain function of width T have
its first spectral null at 2/T Hz?” We can answer this question by considering
convolution in the time domain.
Look at the two rectangular time-domain functions shown in Figures
5–45(a) and 5–45(b). If their widths are each Tseconds, their spectra are shown
to have nulls at 1/T Hz as depicted in the frequency-domain functions in Fig-
ures 5–45(a) and 5–45(b). We know that the frequency magnitude responses will

5.9 A Generic Description of Discrete Convolution 225
|sin(x)/x|
First null
(a) FT
–T/2 0 T/2 Time –1/T 0 1/T Freq
x
*
|sin(x)/x|
(b) FT
–T/2 0 T/2 Time –1/T 0 1/T Freq
=
=
2
(sin(x)/x)
(c)
FT
–T 0 T Time –1/T 0 1/T Freq
2
(sin(x)/x)
(d) FT
–T/2 0 T/2 Time –2/T 0 2/T Freq
Figure 5–45 Using convolution to show that the Fourier transform of a triangular
function has its first null at twice the frequency of the Fourier trans-
form of a rectangular function.
be the absolute value of the classic sin(x)/xfunction.†If we convolve those two
rectangular time-domain functions of width T, we’ll get the triangular function
shown in Figure 5–45(c). Again, in this case, flipping one rectangular function
about the zero time axis is unnecessary. To convolve them, we need only scan
†The sin(x)/xfunction was introduced in our discussion of window functions in Section 3.9 and
is covered in greater detail in Section 3.13.

226 Finite Impulse Response Filters
one function across the other and determine the area of their overlap. The time
shift where they overlap the most happens to be a zero time shift. Thus, our re-
sultant convolution has a peak at a time shift of zero seconds because there’s 100
percent overlap. If we slide one of the rectangular functions in either direction,
the convolution decreases linearly toward zero. When the time shift is T/2 sec-
onds, the rectangular functions have a 50 percent overlap. The convolution is
zero when the time shift is T seconds—that’s when the two rectangular func-
tions cease to overlap.
Notice that the triangular convolution result has a width of 2T, and
that’s really the key to answering our question. Because convolution in
the time domain is equivalent to multiplication in the frequency domain, the
Fourier transform magnitude of our 2T-width triangular function is the
|sin(x)/x| in Figure 5–45(a) times the |sin(x)/x| in Figure 5–45(b), or
the (sin(x)/x)2 function in Figure 5–45(c). If a triangular function of width 2T
has its first frequency-domain null at 1/T Hz, then the same function of
width T must have its first frequency null at 2/T Hz as shown in Figure
5–45(d), and that’s what we set out to show. Comparison of Figures 5–45(c)
and 5–45(d) illustrates a fundamental Fourier transform property that com-
pressing a function in the time domain results in an expansion of its corre-
sponding frequency-domain representation.
We cannot overemphasize the importance of the convolution theorem as
an analysis tool. As an aside, for years I thought convolution was a process
developed in the second half of the twentieth century to help us analyze
discrete-time signal processing systems. Later I learned that statisticians had
been using convolution since the late 1800s. In statistics the probability den-
sity function (PDF) of the sum of two random variables is the convolution of
their individual PDFs.
5.10 ANALYZING FIR FILTERS
There are two popular ways to analyze tapped-delay line, nonrecursive FIR
filters. The first way uses continuous-time Fourier algebra, and the second
way uses the discrete Fourier transform. (By “analyze an FIR filter” we mean
determining the FIR filter’s frequency response based on known filter coeffi-
cients.) Let’s quickly review the two FIR filter analysis methods.
5.10.1 Algebraic Analysis of FIR Filters
The algebraic method used to analyze nonrecursive FIR filters uses the
discrete-time Fourier transform (DTFT) equation. Linear system theory tells
us that the frequency response of a linear system (our filter) is the Fourier

5.10 Analyzing FIR Filters 227
transform of that system’s impulse response. Because a tapped-delay line FIR
filter’s impulse response is equal to its coefficient values, we proceed by ex-
pressing the Fourier transform of the filter’s coefficients. In Section 3.14 we
learned that we can describe the continuous Fourier transform of a discrete
sequence using the DTFT expressed as
∞
∑
X(ω)= x(n)e −jnω . (5–33)
n=−∞
Modifying Eq. (5–33)’s notation to correspond to the DTFT of an FIR filter
having Ncoefficients (impulse response) represented by h(k), where index k=
0, 1, 2, ..., N–1, we can express the filter’s complex frequency response as
N∑ −1
H(ω)= h(k)e −jkω
k=0
=
h(0)e–j0ω
+
h(1)e–jω
+
h(2)e–j2ω
+ . . . +
h(N–1)e–j(N–1)ω
. (5–34)
H(ω) is an (N–1)th-order polynomial, and this is why, for example, a 6-tap FIR
filter is often called a 5th-order FIR filter. In Eq. (5–34) the digital frequency
variable ω is continuous and ranges from 0 to 2π radians/sample, corre-
sponding to a continuous-time frequency range of 0 to f Hz.
s
Let’s see how Eq. (5–34) is used to determine the frequency response of
an FIR filter. Assume we have a 4-tap FIR filter whose coefficients are
h(k)=[0.2, 0.4, 0.4, 0.2]. In this case our continuous H(ω) equation becomes
∑3
H(ω)= h(k)e −jkω =h(0)e −j0ω+h(1)e −jω+h(2)ee −j0ω+h(3)e −j3ω
k=0
= 0.2 +
0.4e–jω
+
0.4e–j2ω
+
0.2e–j3ω
. (5–35)
We can, if we wish, write the complex 3rd-order Eq. (5–35) in rectangular
form as
H(ω) = 0.2 + 0.4cos(ω) + 0.4cos(2ω) + 0.2cos(3ω)
–j[0.4sin(ω) + 0.4sin(2ω) + 0.2sin(3ω)]. (5–36)
Evaluating Eq. (5–35), or Eq. (5–36), and plotting the magnitude of the contin-
uous complex H(ω) function results in the curve in Figure 5–46(a). To com-
pute the continuous H (ω) phase function, we merely take the arctangent of
ø
the ratio of the imaginary part over the real part of H(ω), yielding the H (ω)
ø
phase response in Figure 5–46(b).

228 Finite Impulse Response Filters
|H( )|
1
(a) 0.5
0
0 /4 /2 2
(f/8) (f/4) (f/2) (f)
s s s s
200
H ( )
s ø
e
e
(b) gr 0
e
D
0 /4 /2 2
Figure 5–46 FIR filter frequency response: (a) magnitude; (b) phase.
In practice, evaluating Eq. (5–34) would be performed using some sort
of commercial math software, where code must be written to compute a sam-
pled version of the continuous H(ω). Rather than writing the code to imple-
ment Eq. (5–34), fortunately we can conveniently compute an FIR filter’s H(ω)
frequency response using software that performs the discrete Fourier trans-
form. That’s the subject we discuss next.
5.10.2 DFT Analysis of FIR Filters
The most convenient way to determine an FIR filter’s frequency response is
to perform the discrete Fourier transform (DFT) of the filter’s coefficients.
This analysis method is popular because the DFT is built into most commer-
cial signal processing software packages such as MathCAD, LabView, MAT-
LAB, etc. (In fact, in a pinch, we can even compute DFTs with Microsoft
Excel.) The DFT of an FIR filter’s coefficients is computed using
N∑ −1
H(m)= h(k)e
−j2πmk/N,
(5–37)
k=0
which we normally implement with the high-speed fast Fourier transform
(FFT) algorithm. Variables mand nboth range from 0 to N-1.
Ah, but there’s trouble in paradise because Eq. (5–37) poses a problem. If
we perform a 4-point DFT of the above 4-tap FIR filter coefficients, h(k) =
[0.2, 0.4, 0.4, 0.2] as shown in Figure 5–47(a), we obtain the |H(m)| samples in
Figure 5–47(b). That |H(m)| sequence reveals very little about the frequency
response of the 4-tap FIR filter. We need more |H(m)| frequency-domain in-
formation. That is, we need improved frequency resolution.

5.10 Analyzing FIR Filters 229
1.4
|H(m)|
1.2
h(k) 1.0
0.4 0.8
0.6
0.2
0.4
0 0.2
0 1 3 k 0
0 1 2 3
m
(a) (b)
Figure 5–47 Four-tap FIR filter: (a) impulse response; (b) 4-point DFT frequency
magnitude response.
Fortunately we can obtain a finer-granularity version of H(m) by zero
padding the h(k) coefficients with zero-valued samples and performing a
larger-sized DFT. Figure 5–48(a) shows the 4-tap FIR filter’s coefficients
padded with 60 zero-valued samples. Performing a 64-point DFT on that
padded h(k) sequence yields the higher-resolution discrete |H(m)| magni-
tude response sequence shown in Figure 5–48(b). Sequence |H(m)| is, of
course, computed using
|H(m)|= H (m)2 +H (m)2 (5–38)
real imag
where H (m) and H (m) are the real and imaginary parts computed using
real imag
Eq. (5–37). The circular white dots in Figure 5–48(b) correspond to the square
dots in Figure 5–47(b).
Remember, now, a filter’s complex H(m) frequency response sequence is
H(m) = |H(m)|ejHø(m), (5–38’)
comprising a real-valued |H(m)| magnitude response times a complex ejHø(m)
phase response. The real-valued phase-angle samples, shown in Figure
5–48(c), are computed using
⎛H (m)⎞
H φ (m)=tan −1 ⎝ ⎜ H imag (m)⎠ ⎟. (5–39)
real
So, our FIR filter analysis rule of thumb is to append a sequence of zero-
valued samples (whose length is, say, 10N) to an N-tap filter’s h(k) impulse re-
sponse. Appending those zero-valued samples is called zero padding the h(k)
sequence. Next we compute the DFT of that padded sequence. Of course the

230 Finite Impulse Response Filters
h(k), zero-padded
0.4
(a) 0.2
0
0 1 3 5 7 9 11 13 15 17 59 61 63 k
|H(m)| for zero-padded h(k)
1
(b)
0.5
0
0 10 20 30 40 50 60
m
(f/4) (f/2) (3f/4)
s s s
200
s H (m) for zero-padded h(k)
e
e
gr
e
D
0
(c)
–200
0 10 20 30 40 50 60
m
Figure 5–48 High-resolution FIR filter frequency response: (a) zero-padded h(k);
(b) discrete magnitude response; (c) phase response.
final zero-padded sequence should have a length that is an integer power of
two so that we can use the FFT to compute the high-resolution H(m).
By the way, it doesn’t matter if the zero-valued samples are placed be-
fore or after the original h(k) coefficients prior to performing the DFT. The
computed high-resolution |H(m)| magnitude sequence will be the same in
either case, and the resulting H (m) phase samples in the two cases will differ
ø
only by a constant phase angle. (The DFT shifting theorem discussed in Sec-
tion 3.6 explains why this is true.)
5.10.3 FIR Filter Group Delay Revisited
We mentioned in Section 5.8 how a constant time delay, what we formally
refer to as group delay, through a filter was crucial in many applications. A
constant group delay means a filter has a linear phase response over its pass-
band and will induce no phase distortion in its output signals. Here we ex-
plore the concept of group delay a bit further.

5.10 Analyzing FIR Filters 231
The group delay, as a function of frequency, of a filter having a fre-
quency response of H(ω) = |H(ω)|ejHø(ω) is the negative of the derivative of
the filter’s H (ω) phase response with respect to frequency ωand is expressed
ø
as
−dH (ω)
G(ω)= φ samples (5–40)
d(ω)
where digital frequency ω is continuous and ranges from –π to π
radians/sample, corresponding to a continuous-time frequency range of –f/2
s
to f/2 Hz. Because the dimensions of H (ω) are radians, and the dimensions
s ø
of ω are radians/sample, the dimensions of group delay G(ω) are time mea-
sured in samples. We graphically depict the notion of the group delay, for a
lowpass filter, in Figure 5–49.
For example, the complex-valued frequency response of a K-tap moving
average filter is
H (ω)=e −jω(K−1)/2
sin(ωK/2)
, (5–41)
ma sin(ω/2)
where the subscript “ma” means moving average. As such, from Eq. (5–41)
the phase response of a K = 5-tap moving average filter is
H (ω) = –ω(5–1)/2 = –2ω. (5–42)
φ,K=5
Using Eq. (5–40), the group delay of a K=5-tap moving average filter is
G (ω)=
−dH
φ,K=5
(ω)
=
−d(−2ω)
=2samples. (5–43)
ma,K=5 d(ω) d(ω)
H( ) in radians
0
H( )
Passband
Figure 5–49 FIR filter group delay derived from a filter’s phase response.

232 Finite Impulse Response Filters
Luckily for us, Eq. (5–40) becomes very simple to evaluate if an N-tap
FIR filter’s h(k) coefficients (impulse response samples) are symmetrical. By
“symmetrical” we mean h(k) coefficients that abide by
h(k) = h(N–k–1) (5–44)
where 0≤k≤(N–1)/2 when N is odd, and 0≤k≤(N/2)–1 when N is even. Equa-
tion (5–44) merely means that the first coefficient equals the last coefficient,
the second coefficient equals the next to the last coefficient, and so on. All of
the FIR filters we’ve discussed, so far, fall into this category.
OK, here’s the point we’re making. For symmetrical-coefficient FIR filters
that comply with Eq. (5–44), their group delay is simple to compute. The
group delay of such filters, measured in samples, is a constant equal to half the
number of delay elements in the filter’s tapped-delay line structure. That is,
D
G samples = 2 samples (5–45)
where D is the number of unit-delay elements in the filter’s delay line. Mea-
sured in seconds, a symmetrical FIR filter’s group delay is
Dt D
G = s = seconds (5–46)
seconds 2 2f
s
where t is the reciprocal of the filter’s f input signal sample rate.
s s
We can now make the following all-encompassing statement: The group
delay of a tapped-delay line FIR digital filter, whose impulse response is sym-
metric, is equal to
impulseresponselength−1
G = samples. (5–47)
samples 2
For our purposes, we view a linear-phase FIR filter’s group delay as
simply the time delay through the filter. That is, if the group delay of a filter is
G samples, then the filter’s output sequence is delayed by G samples relative
the filter’s input sequence.
It’s worth mentioning at this point that although we have not yet dis-
cussed such filter networks, if a tapped-delay line (FIR) network has an anti-
symmetrical impulse response defined by
h(k) = –h(N–k–1) (5–48)
where 0≤k≤(N–1)/2 when N is odd and 0≤k≤(N/2)–1 when N is even, such a
network also has a linear phase response and its group delay is also described
by Eq. (5–47). Digital differentiators and Hilbert transformers, discussed in
later chapters, fall into this category.

5.10 Analyzing FIR Filters 233
1 x(n)
0.5
0
–0.5
–1
0 5 10 n 15 20
5/2 = 2.5
sample delay
0.4 y(n)
0.2
0
–0.2
0 5 10 15 20
n
Filter transient response
Figure 5–50 Group delay of a 6-tap (5 delay elements) FIR filter.
At this point, looking at Eq. (5–45), the DSPnovice may wonder, “If Dis
an odd number, how is it possible to have a discrete signal sequence delayed
by a noninteger number of samples?” The answer to this sensible question is
illustrated in Figure 5–50, where x(n) is a sinusoidal sequence applied to a
symmetrical FIR filter having 6 taps (D = 5 delay elements in the tapped-
delay line). There we see that the sinusoidal sequence is preserved at the fil-
ter’s y(n) output and delayed relative to input x(n) by a group delay value of
exactly 5/2 samples. In the lingo of digital filters, the behavior in Figure 5–50
is called fractional delay.
Again, constant group delay—linear phase—is a desirable filter property
because the spectral components in the filter’s output signal will suffer no phase
distortion. Stated in different words: all spectral components within a linear-
phase filter’s passband will be delayed by the same amount of time as they pass
through the filter. If a linear-phase filter’s input is a complicated digital commu-
nications signal, rich in spectral-phase complexity representing digital data, the
spectral-phase relationships and the digital data are preserved undistorted at
the filter’s output. Their linear-phase property is the reason we use FIR filters!
5.10.4 FIR Filter Passband Gain
One FIR filter property that is of interest is the filter’s passband gain. The stan-
dard definition of passband gain is that it is the filter’s passband magnitude

234 Finite Impulse Response Filters
1 Passband gain
0.8 Passband
ripple
)| 0.6
H(f
| 0.4
0.2
0
0 0.1f 0.2f 0.3f 0.4f 0.5f
s s s s s
Frequency
Figure 5–51 FIR filter passband gain definition.
response level around which the passband ripple fluctuates, as shown by the
lowpass filter in Figure 5–51 where the passband gain equals unity. In prac-
tice we design filters to have very small passband ripple, so a lowpass filter’s
passband gain is roughly equal to its DC gain (gain at zero Hz), which is the
sum of the filter’s impulse response sequence, i.e., the sum of the FIR filter’s
coefficients. (We leave the proof of this as a homework problem.) Most com-
mercial FIR filter design software packages compute filter coefficients such
that their passband gain is unity.
5.10.5 Estimating the Number of FIR Filter Taps
Our final topic regarding the analysis of FIR filters is: How do we estimate
the number of filter taps (coefficients), N, that can satisfy a given frequency
magnitude response of an FIR filter? Several authors have proposed empirical
relationships for estimating N for traditional tapped-delay line lowpass FIR
filters based on the desired passband ripple, stopband attenuation, and tran-
sition region width[24,31–33]. A particularly simple expression proposed by
Prof. Fred Harris for N, giving results consistent with other estimates for
passband ripple values near 0.1 dB, is
Atten
N ≈ (5–49)
FIR 22(f − f )
stop pass
where Atten is the filter’s desired stopband attenuation measured in dB, and
f and f are frequencies normalized to the f sample rate in Hz as illus-
pass stop s
trated in Figure 5-52. For example, f = 0.2 means that the continuous-time
pass
frequency of f is 0.2f Hz.
pass s
As an example, let’s obtain a rough estimate of the number of lowpass
FIR filter taps (coefficients) needed to achieve the magnitude response shown
in Figure 5-52. Assuming f = 1000 Hz, we want the end of a lowpass filter’s
s
passband to be at 250 Hz, the beginning of the stopband is 350 Hz, and we

References 235
0
–10
B
d
)|
i n –20 Atten = 48 dB
H(f –30
|
–40
–50
0 0.1f 0.2f 0.3f 0.4f 0.5f
s s s s s
f =0.25 f =0.35
pass stop
Frequency (Hz)
Figure 5–52 Example FIR filter frequency definitions.
need a stopband attenuation of 48 dB. Applying those values to Eq. (5–49),
we have
48 48
N ≈ = =21.8. (5–50)
FIR 22(350/1000−250/1000) 22(0.35−0.255)
Taking the integer closest to 21.8, i.e., 22, we then state that the lowpass filter
in Figure 5-52 can be built using a 22-tap FIR filter. We’ll use Eq. (5–49) many
times in later chapters of this book.
REFERENCES
[1] Shynk, J. J. “Adaptive IIR Filtering,” IEEEASSP Magazine,April 1989.
[2] Laundrie, A. “Adaptive Filters Enable Systems to Track Variations,” Microwaves & RF,
September 1989.
[3] Bullock, S. R. “High Frequency Adaptive Filter,” Microwave Journal,September 1990.
[4] Haykin, S. S. Adaptive Filter Theory, Prentice Hall, Englewood Cliffs, New Jersey, 1986.
[5] Goodwin, G. C., and Sin, K. S. Adaptive Filtering Prediction and Control, Prentice Hall, En-
glewood Cliffs, New Jersey, 1984.
[6] Gibbs, J. W. Nature,Vol. 59, 1899, p. 606.
[7] Stockham, T. G. “High-Speed Convolution and Correlation with Applications to Digital
Filtering,” Chapter 7 in Digital Processing of Signals, ed. by B. Gold et al., McGraw-Hill,
New York, 1969, pp. 203–232.
[8] Wait, J. V. “Digital Filters,” in Active Filters: Lumped, Distributed, Integrated, Digital, and
Parametric, ed. by L. P. Huelsman, McGraw-Hill, New York, 1970, pp. 200–277.

236 Finite Impulse Response Filters
[9] Dolph, C. L. “ACurrent Distribution for Broadside Arrays Which Optimizes the Relation-
ship Between Beam Width and Side-Lobe Level,” Proceedings of the IRE,Vol. 35, June 1946.
[10] Barbiere, D. “AMethod for Calculating the Current Distribution of Chebyshev Arrays,”
Proceedings of the IRE,Vol. 40, January 1952.
[11] Cook, C. E., and Bernfeld, M. Radar Signals, Academic Press, New York, 1967, pp. 178–180.
[12] Kaiser, J. F. “Digital Filters,” in System Analysis by Digital Computer, ed. by F. F. Kuo and
J.F. Kaiser, John Wiley and Sons, New York, 1966, pp. 218–277.
[13] Williams, C. S. Designing Digital Filters, Prentice Hall, Englewood Cliffs, New Jersey, 1986,
p. 117.
[14] Harris, F. J. “On the Use of Windows for Harmonic Analysis with the Discrete Fourier
Transform,” Proceedings of the IEEE,Vol. 66, No. 1, January 1978.
[15] McClellan, J. H., Parks, T. W., and Rabiner, L. R. “AComputer Program for Designing Op-
timum FIR Linear Phase Digital Filters,” IEEE Trans. on Audio and Electroacoustics, Vol.
AU-21, No. 6, December 1973, p. 515.
[16] Rabiner, L. R., and Gold, B. Theory and Application of Digital Signal Processing, Prentice
Hall, Englewood Cliffs, New Jersey, 1975, p. 136.
[17] Parks, T. W., and McClellan, J. H. “Chebyshev Approximation for Nonrecursive Digital Filters
with Linear Phase,” IEEE Trans. on Circuit Theory, Vol. CT-19, March 1972.
[18] McClellan, J. H., and Parks, T. W. “AUnified Approach to the Design of Optimum FIR
Linear Phase Digital Filters,” IEEE Trans. on Circuit Theory, Vol. CT-20, November 1973.
[19] Rabiner, L. R., McClellan, J. H., and Parks, T. W. “FIR Digital Filter Design Techniques
Using Weighted Chebyshev Approximation,” Proceedings of the IEEE, Vol. 63, No. 4, April
1975.
[20] Oppenheim, A. V., and Schafer, R. W. Discrete-Time Signal Processing, Prentice Hall, Engle-
wood Cliffs, New Jersey, 1989, p. 478.
[21] Funderburk, D. M., and Park, S. “Implementation of a C-QUAM AM-Stereo Receiver
Using a General Purpose DSPDevice,” RF Design, June 1993.
[22] Harris Semiconductor Inc. “ADigital, 16-Bit, 52 Msps Halfband Filter,” Microwave Journal,
September 1993.
[23] Ballanger, M. G. “Computation Rate and Storage Estimation in Multirate Digital Filtering
with Half-Band Filters,” IEEE Trans. on Acoust. Speech, and Signal Proc., Vol. ASSP-25,
No.4, August 1977.
[24] Crochiere, R. E., and Rabiner, L. R. “Decimation and Interpolation of Digital Signals—
ATutorial Review,” Proceedings of the IEEE,Vol. 69, No. 3, March 1981, p. 318.
[25] Ballanger, M. G., Daguet, J. L., and Lepagnol, G. P. “Interpolation, Extrapolation, and Re-
duction of Computational Speed in Digital Filters,” IEEE Trans. on Acoust. Speech, and Sig-
nal Proc.,Vol. ASSP-22, No. 4, August 1974.

References 237
[26] Oppenheim, A. V., Willsky, A. S., and Young, I. T. Signals and Systems, Prentice Hall,
Englewood Cliffs, New Jersey, 1983, p. 212.
[27] Stearns, S. Digital Signal Analysis, Hayden Book Co., Rochelle Park, New Jersey, 1975,
p.93.
[28] Oppenheim, A. V., and Schafer, R. W. Discrete-Time Signal Processing, Prentice Hall, Engle-
wood Cliffs, New Jersey, 1989, p. 58.
[29] Rabiner, L. R., and Gold, B. Theory and Application of Digital Signal Processing, Prentice
Hall, Englewood Cliffs, New Jersey, 1975, p. 59.
[30] Oppenheim, A. V., Willsky, A. S., and Young, I. T. Signals and Systems, Prentice Hall,
Englewood Cliffs, New Jersey, 1983, p. 201.
[31] Rorabaugh, C. DSPPrimer, McGraw-Hill, New York, 1999, pp. 278–279.
[32] Kaiser, J. “Nonrecursive Digital Filter Design Using Io-Sinh Window Function,” Proc. 1974
IEEE Int. Symp. Circuits Systems, April 1974, pp. 20–23.
[33] Harris, F. Multirate Signal Processing for Communication Systems, Prentice Hall, Upper Sad-
dle River, New Jersey, 2004, pp. 56–57.

238 Finite Impulse Response Filters
CHAPTER 5 PROBLEMS
5.1 We first introduced the notion of impulse response in Chapter 1, and here in
Chapter 5 we discussed the importance of knowing the impulse response of
FIR filter networks. With that said, if the y(n) output of a discrete system is
equal to the system’s x(n) input sequence:
(a) Draw the unit impulse response of such a system.
(b) Draw the block diagram (structure) of that system.
(c) What is the frequency magnitude response of such a system? Prove your
answer.
5.2 Consider a simple analog signal defined by x(t)=cos(2π800t) shown in Figure
P5–2. The FIR lowpass filter has a passband extending from –400 Hz to +400
Hz, a passband gain of unity, a transition region width of 20 Hz, and a stop-
band attenuation of 60 dB.
(a) Draw the spectral magnitude of x(n) showing all spectral components in
the range of –2f to +2f.
s s
(b) Draw the spectral magnitude of y(n) showing all spectral components in
the range of –2f to +2f.
s s
(c) What is the time-domain peak amplitude of the sinusoidal y(n) output?
x(t) Analog-to-digital x(n) FIR lowpass y(n)
converter filter
f = 1000 Hz
s
Figure P5–2
5.3 Assume we want to filter the audio signal from a digital video disc (DVD)
player as shown in Figure P5–3. The filtered audio signal drives, by way of
a digital-to-analog (D/A) converter, a speaker. For the audio signal to have
acceptable time synchronization with the video signal, video engineers have
determined that the time delay of the filter must be no greater than 6x10–3
seconds. If the f sample rate of the audio is 48 kHz, what is the maximum
s
number of taps in the FIR filter that will satisfy the time delay restriction?
(Assume a linear-phase FIR filter, and zero time delay through the D/A
converter.)

Chapter 5 Problems 239
Video Video
display
DVD Speaker
player
Audio samples FIR D/A
filter converter
f = 48 kHz
s
Figure P5–3
5.4 There are times when we want to build a lowpass filter and a highpass filter
that are complementary. By “complementary” we mean that a highpass filter’s
passband covers the frequency range defined by a lowpass filter’s stopband
range. This idea is illustrated in Figure P5–4(a). An example of such filters is
an audio system, shown in Figure P5–4(b), where the low-frequency spectral
components of an x(n) audio signal drive, by way of a digital-to-analog (D/A)
converter, a low-frequency speaker (woofer). Likewise, the high-frequency
spectral components of x(n) drive a high-frequency speaker (tweeter). Audio
Woofer
1 h (k) D/A
Low converter
x(n)
|H (f)| |H (f)|
Low High h (k) D/A
High converter
0 Tweeter
0 Freq f/2
s
(a) (b)
h (k) h (k)
Low High
–h (7) + 1
Low
14
0 0
7 k 0 7 14 k
0
(c) (d)
Figure P5–4

240 Finite Impulse Response Filters
enthusiasts call Figure P5–4(b) a “crossover” network. Assuming that the
lowpass filter is implemented with a 15-tap FIR filter whose h (k) coeffi-
Low
cients are those in Figure P5–4(c), the complementary highpass filter will
have the coefficients shown in Figure P5–4(d). Highpass coefficients h (k)
High
are defined by
⎧ −h (k), when k ≠7
h (k)=⎨ Low
High ⎩−h (k)+1, when k =7..
Low
Here is the problem: Draw a block diagram of a system that performs
the process in P5–4(b) where only the h (k) lowpass FIR filter need be
Low
implemented.
5.5 Think about a discrete System A, shown in Figure P5–5, that has an undesir-
able amplitude (gain) loss by a factor 0.5 (–6 dB), whose output requires low-
pass linear-phase filtering. What can we do in the design of the lowpass FIR
filter so the filter has an amplitude gain of 2 to compensate for System A’s
amplitude loss?
x(n) System A with Lowpass y(n)
amplitude loss
FIR filter
by a factor of 0.5
Figure P5–5
5.6 Let’s assume we have an x(n) time sequence, whose f sample rate is 20 kHz,
s
and its |X(f)| spectral magnitude is that shown in Figure P5–6(a). We are re-
quired to design a linear-phase lowpass FIR filter that will attenuate the un-
desired high-frequency noise indicated in Figure P5–6(a). So we design a
lowpass FIR filter whose frequency magnitude response is the |H(f)| shown
in Figure P5–6(b) and assume our filter design exercise is complete. Sometime
later, unfortunately, we learn that the original x(n) sequence’s sample rate was
not20 kHz, but is in fact 40 kHz.
Here is the problem: What must we do to our lowpass filter’s h(k) coefficients,
originally designed based on a 20 kHz sample rate, so that they will still at-
tenuate x(n)’s undesired high-frequency noise when the f sample rate is actu-
s
ally 40 kHz?

Chapter 5 Problems 241
|X(f)|
10 To be "filtered out"
0
dB
–10
(a) –20
–30
–40
–50
0 2 4 6 8 10
Freq (kHz) (f/2)
s
|H(f)|
0
dB
–20
(b)
–40
–60
0 2 4 6 8 10
Freq (kHz) (f/2)
s
Figure P5–6
5.7 Here is an interesting little problem. Think about applying the sinusoidal
input sequence shown in Figure P5–7(a) to an 8-point moving average FIR fil-
ter. The filter’s output sequence is that depicted in Figure P5–7(b).
1 Filter input
0.5
(a) 0
–0.5
–1
0 5 10 15 20 Time
0.3 Filter output
0.2
(b)
0.1
0
0 5 10 15 20 Time
Figure P5–7

242 Finite Impulse Response Filters
(a) What characteristic of the filter’s frequency response causes the filter’s
output sequence to go to all zeros as shown in Figure P5–7(b)?
(b) In Figure P5–7(b), what do we call those initial nonzero-valued filter out-
put samples?
5.8 Are abrupt (sudden) changes in the amplitude of a continuous, or discrete,
signal associated with low or high frequencies?
5.9 Consider an FIR filter whose impulse response is shown in Figure P5–9(a).
Given the x(n) filter input sequence shown in Figure P5–9(b):
(a) What is the length, measured in samples, of the nonzero-valued samples
of the filter’s output sequence?
(b) What is the maximum sample value of the filter’s output sequence?
h(k) x(n)
1.0 1.0
0.75 0.75
. . .
0.5 0.5
0.25 0.25
0 0
0 1 2 k 0 1 2 3 4 5 6 7 8 9 n
(a) (b)
Figure P5–9
5.10 Consider an FIR filter whose impulse response is that shown in Figure
P5–10(a). Given the x(n) filter input sequence shown in Figure P5–10(b), draw
the filter’s output sequence.
h(k) x(n)
1.0 1.0
0.75 0.75
0.5 0.5
0.25 0.25
0 0
0 1 2 k 0 1 2 3 4 n
(a) (b)
Figure P5–10

Chapter 5 Problems 243
5.11 Regarding the material in this chapter, it’s educational to revisit the idea of
periodic sampling that was presented in Chapter 2. Think about a continuous
x(t) signal in Figure P5–11(a) whose spectrum is depicted in Figure P5–11(b).
Also, consider the continuous periodic infinitely narrow impulses, s(t), shown
in Figure P5–11(c). Reference [28] provides the algebraic acrobatics to show
that the spectrum of s(t) is the continuous infinitely narrow impulses, S(f),
shown in Figure P5–11(d). If we multiply the x(t) signal by the s(t) impulses,
we obtain the continuous y(t) =s(t)x(t) impulse signal shown by the arrows
in Figure P5–11(e).
Now, if we use an analog-to-digital converter to represent those y(t) impulse
values as a sequence of discrete samples, we obtain the y(n) sequence shown
in Figure P5–11(f). Here is the problem: Briefly discuss what we learned in
this Chapter 5 that tells us the spectrum of the y(n) samples comprises peri-
odic replications of the X(f) in Figure P5–11(b). Your brief discussion should
confirm the material in Chapter 2 which stated that discrete-time sequences
have periodic (replicated) spectra.
x(t) X(f)
0 Time 0 Freq
(a) (b)
1 s(t) S(f)
–2t –t 0 t 2t Time –2f –f 0 f 2f Freq
s s s s s s s s
(c) (d)
y(t)= s(t)x(t) y(n) = x(nt)
s
–2t s –t s 0 t s 2t s Time –2 –1 0 1 2 n (Time)
(e) (f)
Figure P5–11

244 Finite Impulse Response Filters
5.12 Now that we’re familiar with the powerful convolution theorem, think about
the discrete system shown in Figure P5–12(a).
Spectral
X(m) replication
x(n) y(n)
–f 0 f Freq
s –0.4f 0.4f s
s s
(a) (b)
Figure P5–12
Given that x(n)’s spectrum is the X(m) shown in Figure P5–12(b):
(a) Draw the Y(m) spectrum of sequence y(n). (We’re not worried about the
vertical axis scale here, merely the frequency axis and spectral shape of
Y(m).)
(b) Will aliasing errors occur in the y(n) =x(n) 2output? (That is, will spectral
replications in Y(m) overlap each other?)
(c) What is x(n)’s maximum one-sided bandwidth that will avoid aliasing er-
rors in y(n)? (Stated in different words, what is the maximum one-sided
bandwidth of x(n) that will avoid overlapped spectral replications in
Y(m)?)
5.13 It’s likely that you have heard of the process called linear interpolation. It’s a
computationally simple (but not terribly accurate) scheme for estimating
sample values of a continuous function in between some given x(n) sample
values of that function. For the x(n) time samples in Figure P5–13(a), linear in-
terpolation is the process of computing the intermediate y(n) samples shown
as the black squares in Figure P5–13(b). That is, the interpolated sample y(1) is
the value lying on the center of the straight line connecting x(0) and x(1), the
interpolated sample y(2) is the value lying on the center of the straight line
connecting x(1) and x(2), and so on. Given this process of linear interpolation:
(a) What is the equation defining y(n) in terms of the x(n) samples?
(b) The implementation of linear interpolation is often called a filter because
we build interpolators using tapped-delay line structures, just like stan-
dard FIR filter structures. Draw the block diagram of a linear interpola-
tion filterthat computes y(n) from the input x(n) sequence.

Chapter 5 Problems 245
x(n) Some continuous function
(a) . . .
0
0 1 2 n
y(n) Sequencey(n) is the black squares
x(1) x(2)
x(0)
(b)
. . .
0
1 2 n
Figure P5–13
5.14 Consider a linear-phase lowpass FIR filter whose coefficients are
h (k) = [–0.8, 1.6, 25.5, 47, 25.5, 1.6, –0.8],
1
and whose DC gain, H (0), is equal to 99.6. If we change those coefficients to
1
h (k) = [–0.8, 1.6, Q, 47, Q, 1.6, –0.8],
2
we obtain a new DC gain equal to 103.6. What is the value of Q?
5.15 Figure P5–15 shows a linear-phase 5-tap FIR filter.
x(n) x(n–1) x(n–2) x(n–3) x(n–4)
z–1 z–1 z–1 z –1
h(0) h(1) h(2) h(3) h(4)
y(n)
Figure P5–15
DSP engineers always seek to reduce the number of multipliers in their sys-
tems. Redesign the filter in Figure P5–15 to a form that reduces the number of
necessary multiplications per output sample. Draw the block diagram of your
new design.
Hint: Write the difference equation for the y(n) output sequence, and recall
the relationships between the filter’s coefficients.

246 Finite Impulse Response Filters
5.16 The two linear-phase lowpass filters in Figure P5–16 have very similar fre-
quency responses, but those responses are not identical except at a single fre-
quency. If we replaced Filter h with Filter h to reduce our filtering
1 2
computational workload, determine the frequency, ω , where the two H (ω)
o 1
and H (ω) frequency responses are equal.
2
Hint: Begin by creating closed-form equations for H (ω) and H (ω) using the
1 2
discrete-time Fourier transform (DTFT).
Filter h Filter h
x(n) 1 y(n) 2
1
x(n)
Delay
Delay
Delay y(n)
2 2
(a) (b)
Figure P5–16
5.17 The following is a useful problem regarding the 3-tap nonrecursive FIR filter
shown in Figure P5–17(a). The problem’s solution shows us how to design
computationally efficient narrowband-noise reduction filters. If |h| ≤ 2, the
1
filter will have an |H(ω)| frequency magnitude response having two nulls at
±ω as shown in Figure P5–17(b). (Here, the frequency axis value of π radi-
n
ans/sample corresponds to a cyclic frequency of half the sample rate, f/2.)
s
(a) Assume we have a low-frequency signal of interest that’s contaminated
with high-level narrowband noise located at ±3.35 MHz when the sample
rate is f = 8.25 MHz as shown in Figure P5–17(c). To attenuate that noise,
s
for what value of h will the 3-tap FIR filter’s nulls be located at the noise
1
center frequency of ±3.35 MHz? Show your work.
Hint: Use the discrete-time Fourier transform (DTFT) of the filter’s im-
pulse response to create a closed-form equation for the filter’s H(ω) fre-
quency response in terms of the coefficient h and frequency ω. Next,
1
obtain the expression for h in terms of the filter’s null frequency ω .
1 n
(b) What is the DC gain (gain at zero Hz) of our 3-tap FIR filter?
(c) Explain why the filter has a linear, or nonlinear, phase response.

Chapter 5 Problems 247
x(n) |H( )|
Delay Delay B 0
d
–20
h
1
–40
y(n)
– – 0
(–f/2) n Freq n (f/2)
s s
(a) (b)
0
Noise Signal of Noise
–20 interest
B
d
–40
–5 –4 –3 –2 –1 0 1 2 3 4 5
Freq (MHz)
(c) 3.35 MHz
Figure P5–17
5.18 What characteristic must the coefficients of an FIR filter have to ensure that its
frequency-domain phase response is a linear function of frequency (i.e., linear
phase)?
5.19 Quickfilter Technologies Inc. produces a tapped-delay line FIR filter chip
(Part #QF1D512) that has an astounding N = 512 taps. When a new filter
input sample is applied to the chip, how many addition operations must this
chip perform to compute a single filter output sample?
5.20 Intersil Corp. produces an HSP5021 down-converterintegrated circuit contain-
ing a symmetrical-coefficient FIR filter having 255 taps. If the down-converter
chip’s input signal sample rate is f = 8 MHz, what is the group delay (delay
s
through the filter) of their 255-tap FIR filter measured in samples?
5.21 Assume we have digitized an analog signal at an f sample rate of 2x106sam-
s
ples/second. Next we pass the samples through a 70-tap linear-phase low-
pass FIR filter whose cutoff frequency (end of the passband) is 600 kHz. What
would be the time delay, measured in seconds, between the lowpass filter’s
input and output for a sinusoidal tone whose frequency is 200 kHz?

248 Finite Impulse Response Filters
5.22 Think about two linear-phase FIR filters whose frequency magnitude re-
sponses are shown in Figure P5–22.
|H(f)| |H(f)|
1 2
1.0 1.0
0.5
0 0
0 f/4 f/2 0 f/4 f/2
s s s s
Figure P5–22
(a) Let’s assume that filter H (f) is a 17-tap FIR filter. What is the group delay
1
of this linear-phase filter measured in samples?
(b) Next, let’s assume that filter H (f) is a 17-tap half-bandFIR filter. H (f), like
2 2
all half-band FIR filters, has a gain of 0.5 at the frequency f = f/4. What
s
is the group delay of this linear-phase H (f) filter, measured in samples?
2
5.23 Reverberation, a kind of echo, is a popular audio effectapplied to guitar music.
(Most commercial electric guitar amplifiers have a reverberation capability.)
In the world of continuous signals reverberation is implemented with an ana-
log delay line as shown in Figure P5–23(a). That analog delay line is typically
a kind of speakerat one end of a coiled metal spring, and a kind of microphone
at the other end of the spring. However, analog reverberation units have no
convenient way to control the amount of time delay, and unfortunately their
hardware is physically large.
Making use of digital signal processing on the other hand, the process of re-
verberation seems easy to implement using a delay line network like that
shown in Figure P5–23(b). For the digital reverberation process to be usable,
however, it must have a constant gain, where
y(n)
Gain = =constant,
x(n)
over the full operating frequency range of the system. That is, we want our
reverberator to have a flat frequency magnitude response. (By “Gain” we
mean the steady-state gain after the delay line is filled with input samples.)
(a) Assume we have the Figure P5–23(b) delay line with N = 8 delay ele-
ments. What is the N = 8 digital reverberator’s h(n) time-domain im-
pulse response?
(b) What is the equation for the digital reverberator’s |H(ω)| frequency
magnitude response?

Chapter 5 Problems 249
Hint:Use what you learned in Section 3.14, and don’t forget yourtrigono-
metric identities.
(c) Draw a rough sketch of the |H(ω)| frequency magnitude response from
Part (b). (This curve shows us how well simple digital delay-line rever-
berators work.)
x(t) y(t)
+
(a)
Analog
delay line time-delayedx(t)
x(n) y(n)
+
(b)
Delay Delay Delay . . . Delay
x(n–N)
x(n–1) x(n–3)
Figure P5–23
5.24 There are digital filtering schemes that use the process conceptually shown in
Figure P5–24(a). In that parallel-path filter the x(n) input is filtered to generate
sequence w(n). The network’s y(n) output is the x(n) input sequence minus
the w(n) sequence. The w(n) sequence is defined by
w(n) = x(n) + 2x(n–2) +x(n–4).
The actual implementation of such a parallel-path filter is shown in Figure
P5–24(b) where the multi-element delay line in the upper path of Figure
P5–24(b) is needed for time alignment to compensate for the time delay of the
bottom-path FIR filter. How many unit-delay elements must be used in the
upper path in Figure P5–24(b)?
x(n) y(n)
–
(a)
x(n)+2x(n–2)+x(n–4)
w(n)
Time-synchronization delay line
. . .
(b) Delay Delay Delay
x(n) y(n)
–
w(n)= x(n)+2x(n–2)+x(n–4)
w(n)
Figure P5–24

250 Finite Impulse Response Filters
5.25 As we stated in Section 5.10, a lowpass FIR filter’s frequency magnitude re-
sponse at zero Hz (DC) is equal to the sum of the filter’s impulse response
samples (sum of the filter’s coefficients). Prove this important lowpass FIR fil-
ter property.
5.26 Although we didn’t state it explicitly in the text, the continuous frequency
magnitude response of a symmetrical 7-tap FIR filter (for example, an FIR fil-
ter whose h(k) coefficients are indexed as shown in Figure P5–26) can be com-
puted using
∑3
H(ω) =|h(0)+2 h(k)cos(ωk)|.
k=1
(The normalized frequency range is –π ≤ ω ≤ πwhere ω is a continuous nor-
malized angle with ω = π corresponding to a cyclic frequency of f/2 Hz.)
s
There are two reasons we introduce the above |H(ω)| expression:
• Such |H(ω)| equations can be used to compute the magnitude responses
of linear-phase FIR filters, having an odd number of taps, when no FFT
software routine is available.
• You won’t be surprised when you see, in the literature of DSP, FIR filter
frequency magnitude response equations such as the above summation of
cosine functions.
Derive the general equation for the |H(ω)| for an N-tap symmetrical FIR fil-
ter’s magnitude response, when Nis odd. Show your work.
Hint: Use what you learned in Section 3.14, and pay careful attention to the
range of the kindex in Figure P5–26. Notice how h(0) is the center coefficient!
Also, don’t forget our friend Leonhard Euler.
h(k)
0.2
0.1
0
–0.1
–3 –2 –1 0 1 2 3 k
Figure P5–26

Chapter 5 Problems 251
5.27 Assume a commercial data acquisition device has the ability to implement a
191-tap digital FIR filter. What is the narrowest transition region width
(f – f ), stated in terms of f, we can expect to achieve for a lowpass FIR
stop pass s
filter using this device if we desire at least 55 dB of stopband attenuation?
5.28 TexasInstrumentsInc.producesavideoprocessingchip(Part #TMS320DM646x)
containing the FIR filter shown in Figure P5–28. Coefficient b, defined by the
user,controlsthefrequencymagnituderesponseofthefilter.
(a) What is the time-domain difference equation for the filter?
(b) Does the filter have a linear-phase frequency response? Justify your
answer.
(c) What is the group delay of the filter measured in samples?
x(n)
Delay Delay Delay Delay
256
y(n)
–2 b
Figure P5–28
5.29 Here is a fun problem proving that you have actually been performing convo-
lutions since you were a child. Show how the multiplication (computing the
product) of the two numbers 24 and 13 can be performed by convolving their
digits.

This page intentionally left blank

CHAPTER SIX
Infinite
Impulse
Response
Filters
Infinite impulse response (IIR) digital filters are fundamentally different from
FIR filters because practical IIR filters always require feedback. Where FIR fil-
ter output samples depend only on past input samples, each IIR filter output
sample depends on previous input samples and previous filter output sam-
ples. IIR filters’ memoryof past outputs is both a blessing and a curse. As in all
feedback systems, perturbations at the IIR filter input could, depending on
the design, cause the filter output to become unstable and oscillate indefi-
nitely. This characteristic of possibly having an infinite duration of nonzero
output samples, even if the input becomes all zeros, is the origin of the phrase
infinite impulse response. It’s interesting at this point to know that, relative to
FIR filters, IIR filters have more complicated structures (block diagrams), are
harder to design and analyze, and do not have linear phase responses. Why
in the world, then, would anyone use an IIR filter? Because they are very effi-
cient. IIR filters require far fewer multiplications per filter output sample to
achieve a given frequency magnitude response. From a hardware standpoint,
this means that IIR filters can be very fast, allowing us to build real-time IIR
filters that operate over much higher sample rates than FIR filters.†
To illustrate the utility of IIR filters, Figure 6–1 contrasts the frequency
magnitude responses of what’s called a 4th-order lowpass IIR filter and the
19-tap FIR filter of Figure 5–19(b) from Chapter 5. Where the 19-tap FIR filter
in Figure 6–1 requires 19 multiplications per filter output sample, the
4th-order IIR filter requires only 9 multiplications for each filter output sam-
ple. Not only does the IIR filter give us reduced passband ripple and a
†At the end of this chapter, we briefly compare the advantages and disadvantages of IIR filters
relative to FIR filters.
253

254 Infinite Impulse Response Filters
|H(m)| for a 19-tap FIR filter
|H(m)| for a 4th-order IIR filter
0 Freq (m)
–f /8 f /8
s s
Figure 6–1 Comparison of the frequency magnitude responses of a 19-tap low-
pass FIR filter and a 4th-order lowpass IIR filter.
sharper filter roll-off, it does so with less than half the multiplication
workload of the FIR filter.
Recall from Section 5.3 that to force an FIR filter’s frequency response to
have very steep transition regions, we had to design an FIR filter with a very
long impulse response. The longer the impulse response, the more ideal our
filter frequency response will become. From a hardware standpoint, the maxi-
mum number of FIR filter taps we can have (the length of the impulse re-
sponse) depends on how fast our hardware can perform the required number
of multiplications and additions to get a filter output value before the next fil-
ter input sample arrives. IIR filters, however, can be designed to have impulse
responses that are longer than their number of taps! Thus, IIR filters can give
us much better filtering for a given number of multiplications per output
sample than FIR filters. With this in mind, let’s take a deep breath, flex our
mathematical muscles, and learn about IIR filters.
6.1 AN INTRODUCTION TO INFINITE IMPULSE RESPONSE FILTERS
Given a finite duration of nonzero input values, an IIR filter will have an infi-
nite duration of nonzero output samples. So, if the IIR filter’s input suddenly
becomes a sequence of all zeros, the filter’s output could conceivably remain
nonzero forever. This peculiar attribute of IIR filters comes about because of
the way they’re realized, i.e., the feedback structure of their delay units,
multipliers, and adders. Understanding IIR filter structures is straightforward
if we start by recalling the building blocks of an FIR filter. Figure 6–2(a) shows
the now familiar structure of a 4-tap FIR digital filter that implements the
time-domain FIR equation
y(n)= h(0)x(n) + h(1)x(n–1) + h(2)x(n–2) + h(3)x(n–3). (6–1)

6.1 An Introduction to Infinite Impulse Response Filters 255
x(n) x(n–1) x(n–2) x(n–3)
Delay Delay Delay
(a) h(0) h(1) h(2) h(3)
+ y(n)
x(n) y(n)
+
Delay
h(0)
x(n–1)
(b) Delay h(1)
x(n–2)
Delay h(2)
x(n–3)
h(3)
Figure 6–2 FIR digital filter structures: (a) traditional FIR filter structure; (b) re-
arranged, but equivalent, FIR filter structure.
Although not specifically called out as such in Chapter 5, Eq. (6–1) is known
as a difference equation. To appreciate how past filter output samples are used
in the structure of IIR filters, let’s begin by reorienting our FIR structure in
Figure 6–2(a) to that of Figure 6–2(b). Notice how the structures in Figure 6–2
are computationally identical, and both are implementations, or realizations,
of Eq. (6–1).
We can now show how past filter output samples are combined with
past input samples by using the IIR filter structure in Figure 6–3. Because IIR
filters have two sets of coefficients, we’ll use the standard notation of the vari-
ables b(k) to denote the feedforward coefficients and the variables a(k) to indi-
cate the feedback coefficients in Figure 6–3. OK, the difference equation
describing the IIR filter in Figure 6–3 is

256 Infinite Impulse Response Filters
x(n) d(n) y(n)
+ +
Delay Delay
b(0)
x(n–1) y(n–1)
Delay b(1) a(1) Delay
x(n–2) y(n–2)
Delay b(2) a(2) Delay
x(n–3) y(n–3)
b(3) a(3)
Feedforward calculations Feedback calculations
Figure 6–3 IIR digital filter structure showing feedforward and feedback calculations.
y(n)=b(0)x(n)+b(1)x(n−1)+b(2)x(n−2)+b(3)x(n−3)
(6–2)
+a(1)y(n−1)+a(2)y(n−2)+a(3)y(n−3).
Look at Figure 6–3 and Eq. (6–2) carefully. It’s important to convince our-
selves that Figure 6–3 really is a valid implementation of Eq. (6–2) and that,
conversely, difference equation Eq. (6–2) fully describes the IIR filter structure
in Figure 6–3. Keep in mind, now, that the sequence y(n) in Figure 6–3 is not
the same y(n) sequence that’s shown in Figure 6–2. The d(n) sequence in Fig-
ure 6–3 is equal to the y(n) sequence in Figure 6–2.
By now you’re probably wondering, “Just how do we determine those
a(k) and b(k) IIR filter coefficients if we actually want to design an IIR filter?”
Well, fasten your seat belt because this is where we get serious about under-
standing IIR filters. Recall from the last chapter concerning the window
method of lowpass FIR filter design that we defined the frequency response
of our desired FIR filter, took the inverse Fourier transform of that frequency
response, and then shifted that transform result to get the filter’s time-
domain impulse response. Happily, due to the nature of transversal FIR fil-
ters, the desired h(k) filter coefficients turned out to be exactly equal to the
impulse response sequence. Following that same procedure with IIR filters,
we could define the desired frequency response of our IIR filter and then take

6.2 The Laplace Transform 257
the inverse Fourier transform of that response to yield the filter’s time-
domain impulse response. The bad news is that there’s no direct method for
computing the IIR filter’s a(k) and b(k) coefficients from the impulse response!
Unfortunately, the FIR filter design techniques that we’ve learned so far sim-
ply cannot be used to design IIR filters. Fortunately for us, this wrinkle can be
ironed out by using one of several available methods of designing IIR filters.
Standard IIR filter design techniques fall into three basic classes: the im-
pulse invariance, bilinear transform, and optimization methods. These design
methods use a discrete sequence, mathematical transformation process
known as the z-transform whose origin is the Laplace transform traditionally
used in the analyzing of continuous systems. With that in mind, let’s start this
IIR filter analysis and design discussion by briefly reacquainting ourselves
with the fundamentals of the Laplace transform.
6.2 THE LAPLACE TRANSFORM
The Laplace transform is a mathematical method of solving linear differential
equations that has proved very useful in the fields of engineering and
physics. This transform technique, as it’s used today, originated from the
work of the brilliant English physicist Oliver Heaviside.† The fundamental
process of using the Laplace transform goes something like the following:
Step 1: A time-domain differential equation is written that describes the
input/output relationship of a physical system (and we want to find
the output function that satisfies that equation with a given input).
Step 2: The differential equation is Laplace transformed, converting it to an
algebraic equation.
Step 3: Standard algebraic techniques are used to determine the desired out-
put function’s equation in the Laplace domain.
Step 4: The desired Laplace output equation is, then, inverse Laplace trans-
formed to yield the desired time-domain output function’s equation.
This procedure, at first, seems cumbersome because it forces us to go the long
way around, instead of just solving a differential equation directly. The justifi-
cation for using the Laplace transform is that although solving differential
†Heaviside (1850–1925), who was interested in electrical phenomena, developed an efficient al-
gebraic process of solving differential equations. He initially took a lot of heat from his contem-
poraries because they thought his work was not sufficiently justified from a mathematical
standpoint. However, the discovered correlation of Heaviside’s methods with the rigorous
mathematical treatment of the French mathematician Marquis Pierre Simon de Laplace’s
(1749–1827) operational calculus verified the validity of Heaviside’s techniques.

258 Infinite Impulse Response Filters
equations by classical methods is a very powerful analysis technique for all
but the most simple systems, it can be tedious and (for some of us) error
prone. The reduced complexity of using algebra outweighs the extra effort
needed to perform the required forward and inverse Laplace transformations.
This is especially true now that tables of forward and inverse Laplace trans-
forms exist for most of the commonly encountered time functions. Well-
known properties of the Laplace transform also allow practitioners to
decompose complicated time functions into combinations of simpler functions
and, then, use the tables. (Tables of Laplace transforms allow us to translate
quickly back and forth between a time function and its Laplace transform—
analogous to, say, a German-English dictionary if we were studying the
German language.†) Let’s briefly look at a few of the more important charac-
teristics of the Laplace transform that will prove useful as we make our way
toward the discretez-transform used to design and analyze IIR digital filters.
The Laplace transform of a continuous time-domain function f(t), where
f(t) is defined only for positive time (t>0), is expressed mathematically as
∞
F(s)= ∫ f(t)e −stdt. (6–3)
0
F(s) is called “the Laplace transform of f(t),” and the variable s is the com-
plex number
s= σ+ jω. (6–4)
A more general expression for the Laplace transform, called the bilateral or
two-sided transform, uses negative infinity (–∞) as the lower limit of integra-
tion. However, for the systems that we’ll be interested in, where system con-
ditions for negative time (t < 0) are not needed in our analysis, the one-sided
Eq. (6–3) applies. Those systems, often referred to as causal systems,may have
initial conditions at t = 0 that must be taken into account (velocity of a mass,
charge on a capacitor, temperature of a body, etc.), but we don’t need to know
what the system was doing prior tot=0.
In Eq. (6–4), σ is a real number and ω is frequency in radians/second.
Because e–stis dimensionless, the exponent termsmust have the dimension of
1/time, or frequency. That’s why the Laplace variablesis often called a com-
plex frequency.
To put Eq. (6–3) into words, we can say that it requires us to multiply,
point for point, the function f(t) by the complex function e–stfor a given value
of s. (We’ll soon see that using the function e–st here is not accidental; e–st is
†Although tables of commonly encountered Laplace transforms are included in almost every
system analysis textbook, very comprehensive tables are also available[1–3].

6.2 The Laplace Transform 259
used because it’s the general form for the solution of linear differential equa-
tions.) After the point-for-point multiplications, we find the area under the
curve of the function f(t)e–st by summing all the products. That area, a com-
plex number, represents the value of the Laplace transform for the particular
value of s = σ + jω chosen for the original multiplications. If we were to go
through this process for all values of s,we’d have a full description of F(s) for
every value of s.
I like to think of the Laplace transform as a continuous function, where
the complex value of that function for a particular value ofsis a correlation of
f(t) and a damped complex e–st sinusoid whose frequency is ω and whose
damping factor is σ. What do these complex sinusoids look like? Well, they
are rotating phasors described by
e −st =e −(σ+jω)t =e −σte −jω t = e −jω t . (6–5)
e σ t
From our knowledge of complex numbers, we know that
e–jωt
is a unity-
magnitude phasor rotating clockwise around the origin of a complex plane at
a frequency of ωradians/second. The denominator of Eq. (6–5) is a real num-
ber whose value is one at time t = 0. As t increases, the denominator e
σt
gets
larger (when σ is positive), and the complex e–st phasor’s magnitude gets
smaller as the phasor rotates on the complex plane. The tip of that phasor
traces out a curve spiraling in toward the origin of the complex plane. One
way to visualize a complex sinusoid is to consider its real and imaginary
parts individually. We do this by expressing the complex e–st sinusoid from
Eq. (6–5) in rectangular form as
e −st = e −jω t = cos(ωt) − j sin(ωt) . (6–5’)
e σ t e σ t e σ t
Figure 6–4 shows the real parts (cosine) of several complex sinusoids with dif-
ferent frequencies and different damping factors. In Figure 6–4(a), the complex
sinusoid’s frequency is the arbitrary ω’, and the damping factor is the arbitrary
σ’. So the real part of F(s), ats=σ’+jω’, is equal to the correlation of f(t) and
the wave in Figure 6–4(a). For different values ofs,we’ll correlate f(t) with dif-
ferent complex sinusoids as shown in Figure 6–4. (As we’ll see, this correlation
is very much like the correlation of f(t) with various sine and cosine waves
when we were calculating the discrete Fourier transform.) Again, the real part
of F(s), for a particular value of s,is the correlation of f(t) with a cosine wave of
frequency ω and a damping factor of σ, and the imaginary part of F(s) is the
correlation of f(t) with a sinewave of frequency ωand a damping factor of σ.
Now, if we associate each of the different values of the complex s vari-
able with a point on a complex plane, rightfully called the s-plane, we could
plot the real part of the F(s) correlation as a surface above (or below) that

260 Infinite Impulse Response Filters
ω = ω'
σ = σ'
(a)
Time
ω = ω'
σ = 2σ'
(b)
Time
ω = ω'
σ = 0
(c)
Time
ω = 1.5ω'
σ = σ'
(d)
Time
Figure 6–4 Real part (cosine) of various e–stfunctions, wheres=σ+jω, to be cor-
related with f(t).
s-plane and generate a second plot of the imaginary part of the F(s) correla-
tion as a surface above (or below) the s-plane. We can’t plot the full complex
F(s) surface on paper because that would require four dimensions. That’s be-
cause s is complex, requiring two dimensions, and F(s) is itself complex and
also requires two dimensions. What we can do, however, is graph the magni-
tude |F(s)| as a function of s because this graph requires only three dimen-
sions. Let’s do that as we demonstrate this notion of an |F(s)| surface by
illustrating the Laplace transform in a tangible way.
Say, for example, that we have the linear system shown in Figure 6–5.
Also, let’s assume that we can relate the x(t) input and the y(t) output of the
linear time-invariant physical system in Figure 6–5 with the following messy
homogeneous constant-coefficient differential equation:

6.2 The Laplace Transform 261
x(t) y(t)
System
Figure 6–5 System described by Eq. (6–6). The system’s input and output are the
continuous-time functions x(t) and y(t) respectively.
d2y(t) dy(t) dx(t)
a +a +a y(t)=b +b x(t). (6–6)
2 dt2 1 dt 0 1 dt 0
We’ll use the Laplace transform toward our goal of figuring out how the sys-
tem will behave when various types of input functions are applied, i.e., what
the y(t) output will be for any given x(t) input.
Let’s slow down here and see exactly what Figure 6–5 and Eq. (6–6) are
telling us. First, if the system is time invariant, then the a and b coefficients in
n n
Eq. (6–6) are constant. They may be positive or negative, zero, real or complex,
but they do not change with time. If the system is electrical, the coefficients
might be related to capacitance, inductance, and resistance. If the system is
mechanical with masses and springs, the coefficients could be related to mass,
coefficient of damping, and coefficient of resilience. Then, again, if the system
is thermal with masses and insulators, the coefficients would be related to
thermal capacity and thermal conductance. To keep this discussion general,
though, we don’t really care what the coefficients actually represent.
OK, Eq. (6–6) also indicates that, ignoring the coefficients for the mo-
ment, the sum of the y(t) output plus derivatives of that output is equal to the
sum of the x(t) input plus the derivative of that input. Our problem is to de-
termine exactly what input and output functions satisfy the elaborate rela-
tionship in Eq. (6–6). (For the stout-hearted, classical methods of solving
differential equations could be used here, but the Laplace transform makes
the problem much simpler for our purposes.) Thanks to Laplace, the complex
exponential time function of est is the one we’ll use. It has the beautiful prop-
erty that it can be differentiated any number of times without destroying its
original form. That is,
d(est) d2(est) d3(est) dn(est)
=sest, =s2est, =s3est,..., =snest.
(6–7)
dt dt2 dt3 dtn
If we let x(t) and y(t) be functions of est, x(est) and y(est), and use the properties
shown in Eq. (6–7), Eq. (6–6) becomes
a s2y(est)+a sy(est)+a y(est)=b sx(est)+b x(est),
2 1 0 1 0
or
(a s2 +a s+a )y(est)=(b s+b )x(est). (6–8)
2 1 0 1 0

262 Infinite Impulse Response Filters
Although it’s simpler than Eq. (6–6), we can further simplify the rela-
tionship in the last line in Eq. (6–8) by considering the ratio of y(est) over x(est)
as the Laplace transfer function of our system in Figure 6–5. If we call that
ratio of polynomials the transfer function H(s),
y(e st ) b s+b
H(s)= = 1 0 . (6–9)
x(e st ) a s 2 +a s+a
2 1 0
To indicate that the original x(t) and y(t) have the identical functional form of
est, we can follow the standard Laplace notation of capital letters and show
the transfer function as
Y(s) b s+b
H(s)= = 1 0 , (6–10)
X(s) a s2 +a s+a
2 1 0
where the output Y(s) is given by
b s+b
Y(s)=X(s) 1 0 =X(s)H(s). (6–11)
a s2 +a s+a
2 1 0
Equation (6–11) leads us to redraw the original system diagram in a form that
highlights the definition of the transfer function H(s) as shown in Figure 6–6.
The cautious reader may be wondering, “Is it really valid to use this
Laplace analysis technique when it’s strictly based on the system’s x(t) input
being some function of est, or x(est)?” The answer is that the Laplace analysis
technique, based on the complex exponential x(est), is valid because all practical
x(t) input functions can be represented with complex exponentials, for example,
• a constant, c= ce0t,
• sinusoids, sin(ωt) =(ejωt–e–jωt)/2jor cos(ωt) =(ejωt+e–jωt)/2 ,
• a monotonic exponential, eat, and
• an exponentially varying sinusoid, e–atcos(ωt).
X(s) Y(s)
System
H(s)
Figure 6–6 Linear system described by Eqs. (6–10) and (6–11). The system’s input
is the Laplace function X(s), its output is the Laplace function Y(s),
and the system transfer function is H(s).

6.2 The Laplace Transform 263
With that said, if we know a system’s transfer function H(s), we can take
the Laplace transform of any x(t) input to determine X(s), multiply that X(s)
by H(s) to get Y(s), and then inverse Laplace transform Y(s) to yield the time-
domain expression for the output y(t). In practical situations, however, we
usually don’t go through all those analytical steps because it’s the system’s
transfer function H(s) in which we’re most interested. Being able to express
H(s) mathematically or graph the surface |H(s)| as a function of swill tell us
the two most important properties we need to know about the system under
analysis: is the system stable, and if so, what is its frequency response?
“But wait a minute,” you say. “Equations (6–10) and (6–11) indicate
that we have to know the Y(s) output before we can determine H(s)!” Not
really. All we really need to know is the time-domain differential equation
like that in Eq. (6–6). Next we take the Laplace transform of that differential
equation and rearrange the terms to get the H(s) ratio in the form of
Eq. (6–10). With practice, systems designers can look at a diagram (block,
circuit, mechanical, whatever) of their system and promptly write the
Laplace expression for H(s). Let’s use the concept of the Laplace transfer
function H(s) to determine the stability and frequency response of simple
continuous systems.
6.2.1 Poles and Zeros on the s-Plane and Stability
One of the most important characteristics of any system involves the concept
of stability. We can think of a system as stable if, given any bounded input,
the output will always be bounded. This sounds like an easy condition to
achieve because most systems we encounter in our daily lives are indeed sta-
ble. Nevertheless, we have all experienced instability in a system containing
feedback. Recall the annoying howl when a public address system’s micro-
phone is placed too close to the loudspeaker. Asensational example of an un-
stable system occurred in western Washington when the first Tacoma
Narrows Bridge began oscillating on the afternoon of November 7, 1940.
Those oscillations, caused by 42 mph winds, grew in amplitude until the
bridge destroyed itself. For IIR digital filters with their built-in feedback, in-
stability would result in a filter output that’s not at all representative of the
filter input; that is, our filter output samples would not be a filtered version of
the input; they’d be some strange oscillating or pseudo-random values—a sit-
uation we’d like to avoid if we can, right? Let’s see how.
We can determine a continuous system’s stability by examining several
different examples of H(s) transfer functions associated with linear time-
invariant systems. Assume that we have a system whose Laplace transfer
function is of the form of Eq. (6–10), the coefficients are all real, and the

264 Infinite Impulse Response Filters
coefficients b and a are equal to zero. We’ll call that Laplace transfer function
1 2
H (s), where
1
b b /a
H (s)= 0 = 0 1 . (6–12)
1 a s+a s+a /a
1 0 0 1
Notice that if s = –a /a , the denominator in Eq. (6–12) equals zero and H (s)
0 1 1
would have an infinite magnitude. This s = –a /a point on the s-plane is
0 1
called a pole,and that pole’s location is shown by the “x” in Figure 6–7(a). No-
tice that the pole is located exactly on the negative portion of the realσ axis. If
the system described by H were at rest and we disturbed it with an impulse
1
like x(t) input at time t = 0, its continuous time-domain y(t) output would be
the damped exponential curve shown in Figure 6–7(b). We can see that H (s)
1
is stable because its y(t) output approaches zero as time passes. By the way,
the distance of the pole from theσ=0 axis, a /a for our H (s), gives the decay
0 1 1
rate of the y(t) impulse response. To illustrate why the term pole is appropri-
ate, Figure 6–8(b) depicts the three-dimensional surface of |H (s)| above the
1
s-plane. Look at Figure 6–8(b) carefully and see how we’ve reoriented the
s-plane axis. This new axis orientation allows us to see how the H (s) system’s
1
frequency magnitude response can be determined from its three-dimensional
s-plane surface. If we examine the |H (s)| surface at σ = 0, we get the bold
1
curve in Figure 6–8(b). That bold curve, the intersection of the vertical σ = 0
plane (the jωaxis plane) and the |H (s)| surface, gives us thefrequency mag-
1
nitude response |H (ω)| of the system—and that’s one of the things we’re
1
after here. The bold |H (ω)| curve in Figure 6–8(b) is shown in a more con-
1
ventional way in Figure 6–8(c). Figures 6–8(b) and 6–8(c) highlight the very
important property that the Laplace transform is a more general case of the
Fourier transform because ifσ=0, thens=jω. In this case, the |H (s)| curve
1
for σ = 0 above the s-plane becomes the |H (ω)| curve above the jω axis in
1
Figure 6–8(c).
Another common system transfer function leads to an impulse response
that oscillates. Let’s think about an alternate system whose Laplace transfer
jω y(t)
σ = –a 0/a 1 y(t) = (b 0 /a 1 )e–a 0 /a1t
s-plane
σ
0 Time
(a) (b)
Figure 6–7 Descriptions of H(s): (a) pole located ats=σ+jω=–a/a +j0 on the
1 0 1
s-plane; (b) time-domain y(t) impulse response of the system.

6.2 The Laplace Transform 265
jω
(a)
σ = –a0 /a1 σ
∞
|H (s)| surface
1
|H (s)|
1
(b)
0
σ –a 0 /a 1
jω
|H (ω)|
1
(c)
0 jω
Figure 6–8 Further depictions of H(s): (a) pole located atσ=–a /a on thes-plane;
1 0 1
(b) |H(s)| surface; (c) curve showing the intersection of the |H(s)| sur-
1 1
face and the verticalσ=0 plane. This is the conventional depiction of the
|H(ω)| frequency magnitude response.
1
function is of the form of Eq. (6–10), the coefficient b equals zero, and the co-
0
efficients lead to complex terms when the denominator polynomial is fac-
tored. We’ll call this particular 2nd-order transfer function H (s), where
2
b s (b /a )s
H (s)= 1 = 1 2 . (6–13)
2 a s2 +a s+a s2 +(a /a )s+a /a
2 1 0 1 2 0 2

266 Infinite Impulse Response Filters
(By the way, when a transfer function has the Laplace variable s in both
the numerator and denominator, the order of the overall function is defined
by the largest exponential order of sin either the numerator or the denomina-
tor polynomial. So our H (s) is a 2nd-order transfer function.) To keep the fol-
2
lowing equations from becoming too messy, let’s factor its denominator and
rewrite Eq. (6–13) as
As
H (s)= (6–14)
2 (s+p)(s+p*)
where A = b /a , p = p + jp , and p* = p – jp (complex conjugate
1 2 real imag real imag
of p). Notice that if s is equal to –p or –p*, one of the polynomial roots in the
denominator of Eq. (6–14) will equal zero, and H (s) will have an infinite
2
magnitude. Those two complex poles, shown in Figure 6–9(a), are located off
the negative portion of the real σ axis. If the H system were at rest and we
2
disturbed it with an impulselike x(t) input at time t = 0, its continuous time-
domain y(t) output would be the damped sinusoidal curve shown in Figure
6–9(b). We see that H (s) is stable because its oscillating y(t) output, like a
2
plucked guitar string, approaches zero as time increases. Again, the distance
of the poles from the σ = 0 axis (–p ) gives the decay rate of the sinu-
real
soidal y(t) impulse response. Likewise, the distance of the poles from the
jω = 0 axis (±p ) gives the frequency of the sinusoidal y(t) impulse re-
imag
sponse. Notice something new in Figure 6–9(a). Whens=0, the numerator of
Eq. (6–14) is zero, making the transfer function H (s) equal to zero. Any value
2
of s where H (s) = 0 is sometimes of interest and is usually plotted on the
2
s-plane as the little circle, called a zero, shown in Figure 6–9(a). At this point
we’re not very interested in knowing exactly what pand p*are in terms of the
coefficients in the denominator of Eq. (6–13). However, an energetic reader
could determine the values of p and p* in terms of a , a , and a by using the
0 1 2
following well-known quadratic factorization formula: Given the 2nd-order
polynomial f(s)=as2+bs+c, then f(s) can be factored as
y(t)
s = –p
r e a l
+jp
imag
jω y(t) = 2|A|e–p real t cos(pimagt + θ)
θ = tan–1(Ai m a g /Areal )
σ 0 Time
"zero" at s = 0
s = –p –jp
real imag
(a) (b)
Figure 6–9 Descriptions of H(s): (a) poles located at s = p ± jp on the
2 real imag
s-plane; (b) time-domain y(t) impulse response of the system.

6.2 The Laplace Transform 267
⎛ ⎞ ⎛ ⎞
f(s)=as2 +bs+c=⎜ ⎜s+ b + b2 −4ac ⎟ ⎟ ⋅ ⎜ ⎜s+ b − b2 −4ac ⎟ ⎟. (6–15)
⎝ 2a 4a2 ⎠ ⎝ 2a 4a2 ⎠
Figure 6–10(b) illustrates the |H (s)| surface above the s-plane. Again,
2
the bold |H (ω)| curve in Figure 6–10(b) is shown in the conventional way in
2
Figure 6–10(c) to indicate the frequency magnitude response of the system
described by Eq. (6–13). Although the three-dimensional surfaces in Figures
jω
(a)
σ
∞
∞
|H (s)| surface
2
|H (s)|
2
(b)
H (s) = 0, at s = 0
2
0
σ
jω
|H 2 (ω)|
(c)
0 jω
Figure 6–10 Further depictions of H(s): (a) poles and zero locations on the
2
s–plane; (b) |H(s)| surface; (c) |H(ω)| frequency magnitude re-
2 2
sponse curve.

268 Infinite Impulse Response Filters
6–8(b) and 6–10(b) are informative, they’re also unwieldy and unnecessary.
We can determine a system’s stability merely by looking at the locations of
the poles on the two-dimensionals-plane.
To further illustrate the concept of system stability, Figure 6–11 shows the
s-plane pole locations of several example Laplace transfer functions and their
corresponding time-domain impulse responses. We recognize Figures 6–11(a)
and 6–11(b), from our previous discussion, as indicative of stable systems.
When disturbed from their at-rest condition, they respond and, at some later
time, return to that initial condition. The single pole location ats=0 in Figure
6–11(c) is indicative of the 1/stransfer function of a single element of a linear
system. In an electrical system, this 1/s transfer function could be a capacitor
that was charged with an impulse of current, and there’s no discharge path in
the circuit. For a mechanical system, Figure 6–11(c) would describe a kind of
spring that’s compressed with an impulse of force and, for some reason, re-
mains under compression. Notice, in Figure 6–11(d), that if an H(s) transfer
function has conjugate poles located exactly on the jω axis (σ = 0), the system
will go into oscillation when disturbed from its initial condition. This situa-
tion, called conditional stability, happens to describe the intentional transfer
function of electronic oscillators. Instability is indicated in Figures 6–11(e) and
6–11(f). Here, the poles lie to the right of the jω axis. When disturbed from
their initial at-rest condition by an impulse input, their outputs grow without
bound.† See how the value of σ, the real part of s, for the pole locations is the
key here? Whenσ<0, the system is well behaved and stable; whenσ=0, the
system is conditionally stable; and when σ > 0, the system is unstable. So we
can say that whenσis located on the right half of thes-plane, the system is un-
stable. We show this characteristic of linear continuous systems in Figure 6–12.
Keep in mind that real-world systems often have more than two poles, and a
system is only as stable as its least stable pole. For a system to be stable, all of
its transfer-function poles must lie on the left half of thes-plane.
To consolidate what we’ve learned so far: H(s) is determined by writing
a linear system’s time-domain differential equation and taking the Laplace
transform of that equation to obtain a Laplace expression in terms of X(s),
Y(s), s, and the system’s coefficients. Next we rearrange the Laplace expres-
sion terms to get the H(s) ratio in the form of Eq. (6–10). (The really slick part
is that we do not have to know what the time-domain x(t) input is to analyze
a linear system!) We can get the expression for the continuous frequency re-
sponse of a system just by substituting jωforsin the H(s) equation. To deter-
†Impulse response testing in a laboratory can be an important part of the system design process.
The difficult part is generating a true impulselike input. If the system is electrical, for example, al-
though somewhat difficult to implement, the input x(t) impulse would be a very short-duration
voltage or current pulse. If, however, the system were mechanical, a whack with a hammer would
suffice as an x(t) impulse input. For digital systems, on the other hand, an impulse input is easy to
generate; it’s a single unity-valued sample preceded and followed by all zero-valued samples.

6.2 The Laplace Transform 269
jω y(t)
(a)
σ
t
y(t)
jω
(b)
σ t
y(t)
jω
(c)
σ t
y(t)
jω
(d)
σ t
jω y(t)
(e)
σ t
jω y(t)
(f)
σ t
Figure 6–11 Various H(s) pole locations and their time-domain impulse responses:
(a) single pole atσ<0; (b) conjugate poles atσ<0; (c) single pole
located at σ = 0; (d) conjugate poles located at σ = 0; (e) single
pole atσ>0; (f) conjugate poles atσ>0.
mine system stability, the denominator polynomial of H(s) is factored to find
each of its roots. Each root is set equal to zero and solved forsto find the loca-
tion of the system poles on the s-plane. Any pole located to the right of the
jωaxis on thes-plane will indicate an unstable system.
OK, returning to our original goal of understanding thez-transform, the
process of analyzing IIR filter systems requires us to replace the Laplace
transform with thez-transform and to replace thes-plane with az-plane. Let’s

270 Infinite Impulse Response Filters
jω
Stable Unstable
region region
0
σ
Conditionally stable
Figure 6–12 The Laplace s–plane showing the regions of stability and instability
for pole locations for linear continuous systems.
introduce the z-transform, determine what this new z-plane is, discuss the
stability of IIR filters, and design and analyze a few simple IIR filters.
6.3 THE z-TRANSFORM
The z-transform is the discrete-time cousin of the continuous Laplace trans-
form.†While the Laplace transform is used to simplify the analysis of contin-
uous differential equations, the z-transform facilitates the analysis of discrete
difference equations. Let’s define the z-transform and explore its important
characteristics to see how it’s used in analyzing IIR digital filters.
Thez-transform of a discrete sequence h(n), expressed as H(z), is defined as
∞
∑
H(z)= h(n)z −n (6–16)
n=−∞
where the variable zis complex. Where Eq. (6–3) allowed us to take the Laplace
transform of a continuous signal, the z-transform is performed on a discrete
h(n) sequence, converting that sequence into a continuous function H(z) of the
continuous complex variable z. Similarly, as the function e–stis the general form
for the solution of linear differential equations,z–nis the general form for the so-
lution of linear difference equations. Moreover, as a Laplace function F(s) is a
† In the early 1960s, James Kaiser, after whom the Kaiser window function is named, con-
solidated the theory of digital filters using a mathematical description known as the
z-transform[4,5]. Until that time, the use of the z-transform had generally been restricted to the
field of discrete control systems[6–9].

6.3 The Z-Transform 271
continuous surface above thes-plane, thez-transform function H(z) is a contin-
uous surface above az-plane. To whet your appetite, we’ll now state that if H(z)
represents an IIR filter’sz-domain transfer function, evaluating the H(z) surface
will give us the filter’s frequency magnitude response, and H(z)’s pole and zero
locations will determine the stability of the filter.
We can determine the frequency response of an IIR digital filter by ex-
pressing zin polar form as z=rejω , where ris a magnitude and ωis the angle.
In this form, thez-transform equation becomes
∞ ∞
∑ ∑
H(z)=H(rejω )= h(n)(rejω ) −n = h(n)r −n(e −jωn). (6–16’)
n=–∞ n=–∞
Equation(6–16’)canbeinterpretedastheFouriertransformoftheproductofthe
originalsequenceh(n)andtheexponentialsequencer–n.Whenrequalsone,Eq.
(6–16’) simplifies to the Fourier transform. Thus on the z-plane, the contour of
theH(z)surfaceforthosevalueswhere|z|=1istheFouriertransformofh(n).If
h(n) represents a filter impulse response sequence, evaluating the H(z) transfer
functionfor|z|=1yieldsthefrequencyresponseofthefilter.Sowhereonthe
z-planeis|z|=1?It’sacirclewitharadiusofone,centeredaboutthez=0point.
Thiscircle,soimportantthatit’sbeengiventhenameunitcircle,isshowninFig-
ure6–13.RecallthatthejωfrequencyaxisonthecontinuousLaplaces-planewas
linearandrangedfrom–∞to+∞radians/second.Theωfrequencyaxisonthe
complexz-plane,however,spansonlytherangefrom–πto+πradians.Withthis
relationshipbetweenthejωaxisontheLaplaces-planeandtheunitcircleonthe
z-plane, we can see that the z-plane frequency axis is equivalent to coiling the
s-plane’sjωaxisabouttheunitcircleonthez-planeasshowninFigure6–14.
Then, frequency ωon thez-plane is not a distance along a straight line, but
rather an angle around a circle. With ωin Figure 6–13 being a general normal-
ized angle in radians ranging from –π to +π, we can relate ω to an equivalent
z
imag
At this point,
z-plane
z = ejω
ω = 0
ω
z
real
At this point,
ω = π = –π
Unit circle
(where |z| = 1)
Figure 6–13 Unit circle on the complexz–plane.

272 Infinite Impulse Response Filters
s-plane
jω
z-plane
z
imag
Unstable
ω =ω s /2 = πfs ω =ω s /2 region
=πfs
Stable ω = 0
Stable Unstable region
region region
0
σ z
real
ω = –ω s /2 = –πfs ω = = – – ω πf s s /2 ω = = ω 2π s fs
Figure 6–14 Mapping of the Laplace s–plane to the z–plane. All frequency val-
ues are in radians/second.
f sampling rate by defining a new frequency variable ω =2πf in radians/sec-
s s s
ond. The periodicity of discrete frequency representations, with a period of
ω = 2πf radians/second or f Hz, is indicated for the z-plane in Figure 6–14.
s s s
Where a walk along the jωfrequency axis on thes-plane could take us to infin-
ity in either direction, a trip on the ωfrequency path on thez-plane leads us in
circles (on the unit circle). Figure 6–14 shows us that only the –πf to +πf radi-
s s
ans/second frequency range for ωcan be accounted for on thez-plane, and this
is another example of the universal periodicity of the discrete frequency do-
main. (Of course, the –πf to +πf radians/second range corresponds to a cyclic
s s
frequency range of –f/2 to +f/2.) With the perimeter of the unit circle being
s s
z =
ejω
, later, we’ll show exactly how to substitute
ejω
for z in a filter’s H(z)
transfer function, giving us the filter’s frequency response.
6.3.1 Poles,Zeros,and Digital Filter Stability
One of the most important characteristics of the z-plane is that the region of
filter stability is mapped to the inside of the unit circle on the z-plane. Given
the H(z) transfer function of a digital filter, we can examine that function’s
pole locations to determine filter stability. If all poles are located inside the
unit circle, the filter will be stable. On the other hand, if any pole is located
outside the unit circle, the filter will be unstable.
For example, if a causal filter’s H(z) transfer function has a single pole at
location pon the z-plane, its transfer function can be represented by
1
H(z)= (6–17)
1−pz −1
and the filter’s time-domain impulse response sequence is
h(n) = pn· u(n) (6–17’)

6.3 The Z-Transform 273
where u(n) represents a unit step (all ones) sequence beginning at time n = 0.
Equation (6–17’) tells us that as time advances, the impulse response will be p
raised to larger and larger powers. When the pole location phas a magnitude
less than one, as shown in Figure 6–15(a), the h(n) impulse response sequence
is unconditionally bounded in amplitude. And a value of |p| < 1 means that
the pole must lie inside the z-plane’s unit circle.
Figure 6–15 shows thez-plane pole locations of several examplez-domain
transfer functions and their corresponding discrete time-domain impulse
z
imag
h(n)
(a)
z
real
n
h(n)
z 1/fo
imag
= 2 / o
= /4
o
(b)
z real n
o
h(n)
z
imag
o
(c)
o z real n
h(n)
z
imag
(d)
z n
real
h(n)
z
imag
o
(e)
z n
o real
Figure 6–15 Various H(z) pole locations and their discrete time-domain impulse
responses: (a) single pole inside the unit circle; (b) conjugate poles
located inside the unit circle; (c) conjugate poles located on the
unit circle; (d) single pole outside the unit circle; (e) conjugate poles
located outside the unit circle.

274 Infinite Impulse Response Filters
responses. It’s a good idea for the reader to compare thez-plane and discrete-
time responses of Figure 6–15 with the s-plane and the continuous-time re-
sponses of Figure 6–11. The y(n) outputs in Figures 6–15(d) and 6–15(e) show
examples of how unstable filter outputs increase in amplitude, as time passes,
whenever their x(n) inputs are nonzero. To avoid this situation, any IIR digital
filter that we design should have an H(z) transfer function with all of its indi-
vidual poles inside the unit circle. Like a chain that’s only as strong as its
weakest link, an IIR filter is only as stable as its least stable pole.
The ω oscillation frequency of the impulse responses in Figures 6–15(c)
o
and 6–15(e) is, of course, proportional to the angle of the conjugate pole pairs
from the z axis, or ω radians/second corresponding to f = ω /2π Hz. Be-
real o o o
cause the intersection of the –z axis and the unit circle, point z = –1, corre-
real
sponds to π radians (or πf radians/second = f/2 Hz), the ω angle of π/4 in
s s o
Figure 6–15 means that f =f/8 and our y(n) will have eight samples per cycle
o s
of f .
o
6.4 USING THE z-TRANSFORM TO ANALYZE IIR FILTERS
We have one last concept to consider before we can add the z-transform to
our collection of digital signal processing tools. We need to determine how to
represent Figure 6–3’s delay operation as part of our z-transform filter analy-
sis equations. To do this, assume we have a sequence x(n) whose z-transform
is X(z) and a sequence y(n) = x(n–1) whose z-transform is Y(z) as shown in
Figure 6–16(a). The z-transform of y(n) is, by definition,
(cid:4) (cid:4)
∑ ∑
Y(z)= y(n)z −n= x(n−1)z −n. (6–18)
n=−(cid:4) n=−(cid:4)
Now if we let k= n–1, then Y(z) becomes
(cid:4) (cid:4)
∑ ∑
Y(z)= x(k)z
−(k+1)=
x(k)z
−kz −1,
(6–19)
k=−(cid:4) k=−(cid:4)
which we can write as
(cid:4)
∑
Y(z)=z
−1 x(k)z(−k)
=z
−1⎡⎣X(z)⎤⎦.
(6–20)
k=−(cid:4)
Thus, the effect of a single unit of time delay is to multiply the z-transform of
the undelayed sequence by z–1.

6.4 Using the z-Transform to Analyze IIR Filters 275
x(n) y(n) = x(n–1)
(a) Delay
X(z) Y(z)
Time domain
x(n) x(n–1) x(n–2) x(n–3) x(n–k)
Delay Delay Delay ... Delay
(b)
z domain
X(z) X(z)z–1 X(z)z–2 X(z)z–3 X(z)z–k
z–1 z–1 z–1 ... z–1
Figure 6–16 Time- and z-domain delay element relationships: (a) single delay;
(b) multiple delays.
6.4.1 z-Domain IIR Filter Analysis
Interpreting a unit time delay to be equivalent to the z–1 operator leads us to
the relationship shown in Figure 6–16(b), where we can say X(z)z0 = X(z) is
the z-transform of x(n), X(z)z–1is the z-transform of x(n) delayed by one sam-
ple, X(z)z–2 is the z-transform of x(n) delayed by two samples, and X(z)z–k is
the z-transform of x(n) delayed by k samples. So a transfer function of z–k is
equivalent to a delay of kt seconds from the instant when t= 0, where t is the
s s
period between data samples, or one over the sample rate. Specifically, t =
s
1/f . Because a delay of one sample is equivalent to the factor z–1, the unit
s
time delay symbol used in Figures 6–2 and 6–3 is usually indicated by the z–1
operator as in Figure 6–16(b).
Let’s pause for a moment and consider where we stand so far. Our ac-
quaintance with the Laplace transform with its s-plane, the concept of stabil-
ity based on H(s) pole locations, the introduction of the z-transform with its
z-plane poles, and the concept of the z–1 operator signifying a single unit of
time delay has led us to our goal: the ability to inspect an IIR filter difference
equation or filter structure (block diagram) and immediately write the filter’s
z-domain transfer function H(z). Accordingly, by evaluating an IIR filter’s
H(z) transfer function appropriately, we can determine the filter’s frequency
response and its stability. With those ambitious thoughts in mind, let’s de-
velop the z-domain equations we need to analyze IIR filters. Using the rela-
tionships of Figure 6–16(b), we redraw Figure 6–3 as a general Mth-order IIR
filter using the z–1 operator as shown in Figure 6–17. (In hardware, those z–1
operations are memory locations holding successive filter input and output
sample values. When implementing an IIR filter in a software routine, the z–1
operation merely indicates sequential memory locations where input and out-
put sequences are stored.) The IIR filter structure in Figure 6–17 is called the
Direct Form I structure. The time-domain difference equation describing the

276 Infinite Impulse Response Filters
x(n) y(n)
z–1 b(0) z–1
b(1) a(1)
z–1 z–1
b(2) a(2)
z–1
b(N) a(M)
general Mth-order IIR filter, having N feedforward stages and M feedback
stages, in Figure 6–17 is
y(n) = b(0)x(n) + b(1)x(n–1) + b(2)x(n–2) + . . . + b(N)x(n–N)
+ a(1)y(n–1) + a(2)y(n–2) + . . . + a(M)y(n–M). (6–21)
In the z-domain, that IIR filter’s output can be expressed by
Y(z) = b(0)X(z) + b(1)X(z)z–1+ b(2)X(z)z–2+ . . . + b(N)X(z)z–N
+ a(1)Y(z)z–1+ a(2)Y(z)z–2+ . . . + a(M)Y(z)z–M (6–22)
where Y(z) and X(z) represent the z-transform of y(n) and x(n). Look Eqs. (6–21)
and (6–22) over carefully and see how the unit time delays translate to negative
powers of zin the z-domain expression. Amore compact notation for Y(z) is
∑N ∑M
Y(z)=X(z) b(k)z −k +Y(z) a(k)z −k. (6–23)
k=0 k=1
OK, now we’ve arrived at the point where we can describe the trans-
fer function of a general IIR filter. Rearranging Eq. (6–23), to collect like
terms, we write
⎡ ∑M ⎤ ∑∑N
Y(z)⎢1− a(k)z −k⎥=X(z) b(k)z −k. (6–24)
⎣⎢
k=1
⎦⎥
k=0
...
z–1
...
x(n–1) y(n–1)
x(n–2) y(n–2)
x(n–N)
y(n–M)
Figure 6–17 General (Direct Form I) structure of an Mth-order IIR filter, having
Nfeedforward stages and Mfeedback stages, with the z–1operator
indicating a unit time delay.

6.4 Using the z-Transform to Analyze IIR Filters 277
Finally, we define the filter’s z-domain transfer function as H(z) = Y(z)/X(z),
where H(z) is given by
∑N
b(k)z
−k
Y(z)
H(z)= =
k=0
. (6–25)
X(z) ∑M
1− a(k)z −k
k=1
Just as with Laplace transfer functions, the order of our z-domain transfer
function and the order of our filter are defined by the largest exponential
order of zin either the numerator or the denominator in Eq. (6–25).
There are two things we need to know about an IIR filter: its frequency
response and whether or not the filter is stable. Equation (6–25) is the origin
of that information. We can evaluate the denominator of Eq. (6–25) to deter-
mine the positions of the filter’s poles on the z-plane indicating the filter’s
stability. Next, from Eq. (6–25) we develop an expression for the IIR filter’s
frequency response.
Remember, now, just as the Laplace transfer function H(s) in Eq. (6–9)
was a complex-valued surface on the s-plane, H(z) is a complex-valued surface
above, or below, the z-plane. The intersection of that H(z) surface and the
perimeter of a cylinder representing the z=
ejω
unit circle is the filter’s complex
frequency response. This means that substituting
ejω
for zin Eq. (6–25)’s trans-
fer function gives us the expression for the filter’s H(ω) frequency response as
∑N
b(k)e
−jkω
H(ω)=H(z) = k=0 . (6–26)
z=ejω ∑M
1− a(k)e −jkω
k==1
In rectangular form, using Euler’s identity, e–jω = cos(ω)–jsin(ω), the filter’s
H(ω) frequency response is
∑N ∑N
b(k)⋅cos(kω)− j b(k)⋅sin(kω)
H(ω)= k=0 k=0 . (6–27)
∑M ∑M
1−− a(k)⋅cos(kω)− j a(k)⋅sin(kω)
k=1 k=1
Shortly, we’ll use the above expressions to analyze an actual IIR filter.
Pausing a moment to gather our thoughts, we realize that H(ω) is the
ratio of complex functions and we can use Eq. (6–27) to compute the

278 Infinite Impulse Response Filters
magnitude and phase response of IIR filters as a function of the frequency ω.
And again, just what is ω? It’s the normalized frequency represented by the
angle around the unit circle in Figure 6–13, having a range of –π≤ω≤+ω radi-
ans/sample. In terms of our old friend f Hz, Eq. (6–27) applies over the
s
equivalent frequency range of –f /2 to +f /2 Hz. So, for example, if digital
s s
data is arriving at the filter’s input at a rate of f =1000 samples/second, we
s
could use Eq. (6–27) to plot the filter’s frequency magnitude response over
the frequency range of –500 Hz to +500 Hz.
6.4.2 IIR Filter Analysis Example
Although Eqs. (6–25) and (6–26) look somewhat complicated at first glance,
let’s illustrate their simplicity and utility by analyzing the simple 2nd-order
lowpass IIR filter in Figure 6–18(a) whose positive cutoff frequency is ω=π/5
(f /10 Hz).
s
By inspection, we can write the filter’s time-domain difference equation as
y(n) = 0.0605 .x(n) + 0.121 .x(n–1) + 0.0605 .x(n–2)
+ 1.194 .y(n–1) – 0.436 . (n–2). (6–28)
x(n) y(n)
z–1 0.0605 z–1
x(n–1) y(n–1)
(a)
0.121 1.194
z–1 z–1
x(n–2) y(n–2)
0.0605 –0.436
X(z) Y(z)
z–1 0.0605 z–1
X(z)z–1 Y(z)z–1
(b)
0.121 1.194
z–1 z–1
X(z)z–2 Y(z)z–2
0.0605 –0.436
Figure 6–18 Second-order lowpass IIR filter example.

6.4 Using the z-Transform to Analyze IIR Filters 279
There are two ways to obtain the z-domain expression of our filter. The first
way is to look at Eq. (6–28) and by inspection write
Y(z) = 0.0605 .X(z) + 0.121 .X(z)z–1+ 0.0605 .X(z)z–2
+ 1.194
.Y(z)z–1–
0.436
.Y(z)z–2.
(6–29)
The second way to obtain the desired z-domain expression is to redraw Fig-
ure 6–18(a) with the z-domain notation as in Figure 6–18(b). Then by inspec-
tion of Figure 6–18(b) we could have written Eq. (6–29).
A piece of advice for the reader to remember: although not obvious in
this IIR filter analysis example, it’s often easier to determine a digital net-
work’s transfer function using the z-domain notation of Figure 6–18(b) rather
than using the time-domain notation of Figure 6–18(a). (Writing the z-domain
expression for a network based on the Figure 6–18(b) notation, rather than
writing a time-domain expression based on the Figure 6–18(a) time notation,
generally yields fewer unknown variables in our network analysis equa-
tions.) Over the years of analyzing digital networks, I regularly remind my-
self, “z-domain produces less pain.” Keep this advice in mind if you attempt
to solve the homework problems at the end of this chapter.
Back to our example: We can obtain the desired H(z) filter transfer func-
tion by rearranging Eq. (6–29), or by using Eq. (6–25). Either method yields
Y(z) 0.0605⋅z0+0.121⋅z −1+0.0605⋅⋅z −2
H(z)= = . (6–30)
X(z) 1−1.194⋅z −1+0.436⋅z −2
Replacing z with
ejω
, we see that the frequency response of our example IIR
filter is
0.0605⋅e −j0ω +0.121⋅e −j1ω +0.0605⋅e −j2ω
H(ω)= . (6–31)
11−1.194⋅e −j1ω +0.436⋅e −j2ω
We’re almost there. Remembering Euler’s equations and that cos(0) = 1 and
sin(0)=0, we can write the rectangular form of H(ω) as
H(ω)=
0.0605+0.121⋅cos(1ω)+0.0605⋅cos(2ω)−j[[0.121⋅sin(1ω)+0.0605⋅sin(2ω)]
. (6–32)
1−1.194⋅cos(11ω)+0.436⋅cos(2ω)+j[1.194⋅cos(1ω)−0.436⋅coss(2ω)]
Equation (6–32) is what we’re after here, and if we compute that messy ex-
pression’s magnitude over the frequency range of –π≤ω≤π, we produce the
|H(ω)| shown as the solid curve in Figure 6–19(a). For comparison purposes

280 Infinite Impulse Response Filters
we also show a 5-tap lowpass FIR filter magnitude response in Figure 6–19(a).
Although both filters require the same computational workload, five multi-
plications per filter output sample, the lowpass IIR filter has the superior fre-
quency magnitude response. Notice the steeper magnitude response roll-off
and lower sidelobes of the IIR filter relative to the FIR filter. (To make this IIR
and FIR filter comparison valid, the coefficients used for both filters were cho-
sen so that each filter would approximate the ideal lowpass frequency re-
sponse shown in Figure 5–17(a).)
A word of warning here. It’s easy to inadvertently reverse some of the
signs for the terms in the denominator of Eq. (6–32), so be careful if you at-
tempt these calculations at home. Some authors avoid this problem by show-
ing the a(k) coefficients in Figure 6–17 as negative values, so that the
summation in the denominator of Eq. (6–25) is always positive. Moreover,
some commercial software IIR design routines provide a(k) coefficients whose
signs must be reversed before they can be applied to the IIR structure in Fig-
ure 6–17. (If, while using software routines to design or analyze IIR filters,
your results are very strange or unexpected, the first thing to do is reverse the
signs of the a(k) coefficients and see if that doesn’t solve the problem.)
The solid curve in Figure 6–19(b) is our IIR filter’s ø(ω) phase response.
Notice its nonlinearity relative to the FIR filter’s phase response. (Remember,
now, we’re only interested in the filter phase responses over the lowpass fil-
ter’s passband. So those phase discontinuities for the FIR filter are of no con-
sequence.) Phase nonlinearity is inherent in IIR filters and, based on the ill
effects of nonlinear phase introduced in the group delay discussion of Section
5.8, we must carefully consider its implications whenever we decide to use an
IIR filter instead of an FIR filter in any given application. The question any fil-
ter designer must ask and answer is “How much phase distortion can I toler-
ate to realize the benefits of the reduced computational workload and high
data rates afforded by IIR filters?”
Figure 6–19(c) shows our filter’s time-domain h(k) impulse response.
Knowing that the filter’s phase response is nonlinear, we should expect the
impulse response to be asymmetrical as it indeed is. That figure also illus-
trates why the term infinite impulse responseis used to describe IIR filters. If we
used infinite-precision arithmetic in our filter implementation, the h(k) im-
pulse response would be infinite in duration. In practice, of course, a filter’s
output samples are represented by a finite number of binary bits. This means
that a stable IIR filter’s h(k) samples will decrease in amplitude, as time index
kincreases, and eventually reach an amplitude level that’s less than the small-
est representable binary value. After that, all future h(k) samples will be zero-
valued.
To determine our IIR filter’s stability, we must find the roots of the 2nd-
orderpolynomialofH(z)’sdenominatorinEq.(6–30).Thoserootsarethez-plane
polesofH(z)andiftheirmagnitudesarelessthanone,theIIRfilterisstable.To

6.4 Using the z-Transform to Analyze IIR Filters 281
|H(ω)| for the IIR filter
1
|H (ω)| for a
FIR
5-tap FIR filter
(a) 0.5
ω
–π –π/2 0 π/4 π/2 π
(–f/2) (– f/4) Freq (f/8) (f/4) (f/2)
s s s s s
ø(ω) for the IIR filter Degrees ø (ω) for a
FIR
180 5-tap FIR filter
90 (f/4)
(b) πs
/2
0
–π –π/2 π ω
(–f s /2) (– f s /4) –90 (f s /2)
–180
0.3
h(k)
0.2
(c) 0.1
0
–0.05
0 5 10 k 15 20
z
z-plane imag
p = 0.597 + j0.282
Unit 1
circle
(d)
0
z
real
Two zeros at
z = –1 + j0
p = 0.597 – j0.282
0
Figure 6–19 Performances of the example IIR filter (solid curves) in Figure 6–18
and a 5-tap FIR filter (dashed curves): (a) magnitude responses;
(b) phase responses; (c) IIR filter impulse response; (d) IIR filter poles
and zeros.

282 Infinite Impulse Response Filters
determinethetwopolelocations,p andp ,firstwemultiplyH(z)byz2/z2toob-
0 1
tainpolynomialswithpositiveexponents.Afterdoingso,H(z)becomes
0.0605z2 +0.121z+0.0605
H(z)= . (6–33)
z2 −1.194z+0.436
Factoring Eq. (6–33) using the quadratic factorization formula from Eq.
(6–15), we obtain the ratio of factors
(z−z )(z−z ) (z+1)(z+1)
H(z)= 0 1 = . (6–34)
(z−p )(z−p ) (z−00.597+j0.282)(z−0.597 − j0.282)
0 1
So when z= p = 0.597 – j0.282, or when z= p = 0.597 + j0.282, the filter’s
0 1
H(z) transfer function’s denominator is zero and |H(z)| is infinite. We show
the p and p pole locations in Figure 6–19(d). Because those pole locations are
0 1
inside the unit circle (their magnitudes are less than one), our example IIR fil-
ter is unconditionally stable. The two factors in the numerator of Eq. (6–34)
correspond to two z-plane zeros at z = z = z = –1 (at a continuous-time fre-
0 1
quency of ±f /2), shown in Figure 6–19(d).
s
To help us understand the relationship between the poles/zeros of H(z)
and the magnitude of the H(z) transfer function, we show a crude depiction
of the |H(z)| surface as a function of zin Figure 6–20(a).
Continuing to review the |H(z)| surface, we can show its intersection
with the unit circle as the bold curve in Figure 6–20(b). Because z=
rejω
, with r
restricted to unity, then z=ejω and the bold curve is |H(z)| =|H(ω)|, rep-
|z|=1
resenting the lowpass filter’s frequency magnitude response on the z-plane. If
we were to unwrap the bold |H(ω)| curve in Figure 6–20(b) and lay it on a flat
surface, we would have the |H(ω)| curve in Figure 6–19(a). Neat, huh?
6.5 USING POLES AND ZEROS TO ANALYZE IIR FILTERS
In the last section we discussed methods for finding an IIR filter’s z-domain
H(z) transfer function in order to determine the filter’s frequency response
and stability. In this section we show how to use a digital filter’s pole/zero lo-
cations to analyze that filter’s frequency-domain performance. To understand
this process, first we must identify the two most common algebraic forms
used to express a filter’s z-domain transfer function.
6.5.1 IIR Filter Transfer Function Algebra
We have several ways to write the H(z) = Y(z)/X(z) z-domain transfer func-
tion of an IIR filter. For example, similar to Eq. (6–30), we can write H(z) in the

6.5 Using Poles and Zeros to Analyze IIR Filters 283
•
•
|H(z)| surface
(a)
Imagz p 1 = 0.597 + j0.282
p = 0.597 – j0.282 Realz
0
•
•
r = 1
|H(z)|
Bold
intersection
(b) curve is |H(ω)|.
|H(ω)| is zero
atz = –1.
Imagz
Realz
Figure 6–20 IIR filter’s |H(z)| surface: (a) pole locations; (b) frequency magnitude
response.
form of a ratio of polynomials in negative powers of z. For a 4th-order IIR fil-
ter such an H(z) expression would be
b(0)+b(1)z −1+b(2)z −2 +b(3)z −3+b(4)z −4
H(z)= . (6–35)
1+aa(1)z −1+a(2)z −2 +a(3)z −3+a(4)z −4

284 Infinite Impulse Response Filters
Expressions like Eq. (6–35) are super-useful because we can replace zwith
ejω
to obtain an expression for the frequency response of the filter. We used that
substitution in the last section.
On the other hand, multiplying Eq. (6–35) by z4/z4, we can express H(z)
in the polynomialform
b(0)z4+b(1)z3+b(2)z2 +b(3)z+b(4)
H(z)= . (6–36)
z4+a(1)zz3+a(2)z2 +a(3)z+a(4)
Expressions in the form of Eq. (6–36) are necessary so we can factor (find the
roots of) the polynomials to obtain values (locations) of the numerator zeros
and denominator poles, such as in the following factoredform:
(z−z )(z−z )(z−z )(z−z )
H(z)= 0 1 2 3 .
(z−p )(z−p )(z−−p )(z−p ) (6–37)
0 1 2 3
Such an H(z) transfer function has four zeros (z , z , z , and z ) and four poles
0 1 2 3
(p , p , p , and p ). We’re compelled to examine a filter’s H(z) transfer function
0 1 2 3
in the factored form of Eq. (6–37) because the p pole values tell us whether or
k
not the IIR filter is stable. If the magnitudes of all p poles are less than one,
k
the filter is stable. The filter zeros, z , do not affect filter stability.
k
As an aside, while we won’t encounter such filters until Chapter 7 and
Chapter 10, it is possible to have a digital filter whose transfer function, in the
factored form of Eq. (6–37), has common (identical) factors in its numerator
and denominator. Those common factors produce a zero and a pole that lie
exactly on top of each other. Like matter and anti-matter, such zero-pole com-
binations annihilate each other, leaving neither a zero nor a pole at that
z-plane location.
Multiplying the factors in Eq. (6–37), a process called “expanding the trans-
fer function” allows us to go from the factored form of Eq. (6–37) to the polyno-
mial form in Eq. (6–36). As such, in our digital filter analysis activities we can
translate back and forth between the polynomial and factored forms of H(z).
Next we review the process of analyzing a digital filter given the filter’s
poles and zeros.
6.5.2 Using Poles/Zeros to Obtain Transfer Functions
As it turns out, we can analyze an IIR filter’s frequency-domain performance
based solely on the filter’s poles and zeros. Given that we know the values of
a filter’s z zeros and p poles, we can write the factored form of the filter’s
k k
transfer function as

6.5 Using Poles and Zeros to Analyze IIR Filters 285
G (z−z )(z−z )(z−z )(z−z )(z−z )...
H(z)= 1 0 1 2 3 4
G (zz−p )(z−p )(z−p )(z−p )(z−p )...
2 0 1 2 3 4
G(z−z )(z−z )(z−z )(z−z )(z−z )...
= 0 1 2 3 4
(6–38)
(z−p )(z−−p )(z−p )(z−p )(z−p )...
0 1 2 3 4
where G = G /G is an arbitrary gain constant. Thus, knowing a filter’s z
1 2 k
zeros and p poles, we can determine the filter’s transfer function to within a
k
constant scale factor G.
Again, filter zeros are associated with decreased frequency magnitude
response, and filter poles are associated with increased frequency magnitude
response. For example, if we know that a filter has no z-plane zeros, and one
pole at p = 0.8, we can write its transfer function as
0
G
H 1 (z)= z−0.8 . (6–39)
The characteristics of such a filter are depicted in Figure 6–21(a). The |H (ω)|
1
frequency magnitude response in the figure is normalized so that the peak
magnitude is unity. Because the p pole is closest to the ω=0 radians/sample
0
frequency point (z = 1) on the unit circle, the filter is a lowpass filter. Addi-
tionally, because |p | is less than one, the filter is unconditionally stable.
0
If a filter has a zero at z =1, and a pole at p =–0.8, we write its transfer
0 0
function as
G(z−1) Gz−G
H 2 (z)= z−(−0.8) = z+0.8 . (6–40)
The characteristics of this filter are shown in Figure 6–21(b). Because the pole
is closest to the ω=πradians/sample frequency point (z=–1) on the unit cir-
cle, the filter is a highpass filter. Notice that the zero located at z=1 causes the
filter to have infinite attenuation at ω = 0 radians/sample (zero Hz). Because
this pole/zero filter analysis topic is so important, let us look at several more
pole/zero examples.
Consider a filter having two complex conjugate zeros at –0.707 ± j0.707,
as well as two complex conjugate poles at 0.283 ± j0.283. This filter’s transfer
function is
G[z−(−0.707+ j0.707)]⋅[z−(−0.707− j0.7077)]
H (z)=
3 [z−(0.283+ j0.283)]⋅[z−(0.283− j0.283)]

286 Infinite Impulse Response Filters
G(z+0.707− j0.707)⋅(z+0.707+ j0.707)
= (z−0.283−− j0.283)⋅(z−0.283+ j0.283) . (6–41)
The properties of this H (z) filter are presented in Figure 6–21(c). The two
3
poles on the right side of the z-plane make this a lowpass filter having a
wider passband than the above H (z) lowpass filter. Two zeros are on the unit
1
circle at angles of ω=±3π/4 radians, causing the filter to have infinite attenu-
ation at the frequencies ω = ±3π/4 radians/sample (±3f/8 Hz) as seen in the
s
|H (ω)| magnitude response.
3
1
0
–1
–1 0 1
Real part
Infinite
attenuation
trap
yranigamI
1
(a) 0.5
0 –π 0 π
1
0
–1
–1 0 1
Real part
trap
yranigamI
|H(ω)|
1
p = 0.8
0
ω (rad./sample) (f s /2 )
1
(b) 0.5
0
–π
0
π
ω (rad./sample) (f s /2 )
1
0
–1
–1 0 1
Real part
trap
yranigamI
|H(ω)|
2
p = –0.8
0
1
|H(ω)|
3
θ (c) 0.5
–θ
θ = π/4
0
–π
0
π
–3π/4 ω (rad./sample) 3π/4 (f s /2 )
(–3f/8) (3f/8)
s s
Figure 6–21 IIR filter poles/zeros and normalized frequency magnitude responses.

6.5 Using Poles and Zeros to Analyze IIR Filters 287
If we add a z-plane zero at z=1 to the above H (z), we create an H (z) fil-
3 4
ter whose transfer function is
G(z−1)⋅(z+0.707− j0.707)⋅(z+0.707+ j0.7007)
H 4 (z)= (z−0.283− j0.283)⋅(z−0.283+ j0.283) . (6–42)
The characteristics of this filter are shown in Figure 6–21(d). The zero at z=1
yields infinite attenuation at ω=0 radians/sample (zero Hz), creating a band-
pass filter. Because the p and p poles of H (z) are oriented at angles of
0 1 4
1
0
–1
–1 0 1
Real part
(f)
trap
yranigamI
1
0
–1
–1 0 1
Real part
1
θ
(e) 0.5
–θ
θ = π/4
0
–π 0 π
π/4 (f/2 )
s
(f/8)
s
trap
yranigamI
1
θ (d) 0.5
–θ
θ = π/4
0
–π
0
π
ω (rad./sample) π/4 3π/4 (f s /2 )
(f/8) (3f/8)
s s
ω (rad./sample)
1
0
–1
–1 0 1
Real part
trap
yranigamI
|H(ω)|
4
|H(ω)|
5
z = Mejα
0
z = ejα/M
1
α
–α
z = e–jα/M
3
z = Me–jα
2
2
Figure 6–21 (continued) IIR filter poles/zeros and normalized frequency magni-
tude responses.

288 Infinite Impulse Response Filters
θ=±π/4 radians, the filter’s passbands are centered in the vicinity of frequen-
cies ω=±π/4 radians/sample (±f/8 Hz).
s
Next, if we increase the magnitude of the H (z) filter’s poles, making
4
them equal to 0.636 ± j0.636, we position the conjugate poles much closer to
the unit circle as shown by the H (z) characteristics in Figure 6–21(e). The
5
H (z) filter transfer function is
5
G(z−1)⋅(z+0.707− j0.707)⋅(z+0.707+ j0.7007)
H 5 (z)= (z−0.636− j0.636)⋅(z−0.636+ j0.636) . (6–43)
There are two issues to notice in this scenario. First, poles near the unit circle
now have a much more profound effect on the filter’s magnitude response.
The poles’ infinite gains cause the H (z) passbands to be very narrow (sharp).
5
Second, when a pole is close to the unit circle, the center frequency of its asso-
ciated passband can be accurately estimated to be equal to the pole’s angle.
That is, Figure 6–21(e) shows us that with the poles’ angles being θ=±π/4 ra-
dians, the center frequencies of the narrow passbands are verynearly equal to
ω=±π/4 radians/sample (±f/8 Hz).
s
For completeness, one last pole/zero topic deserves mention. Consider a
finite impulse response (FIR) filter—a digital filter whose H(z) transfer func-
tion denominator is unity. For an FIR filter to have linear phase each z-plane
zero located at z=z =Mejα , where M≠1, must be accompanied by a zero hav-
0
ing an angle of –α and a magnitude of 1/M. (Proof of this restriction is avail-
able in reference [10].) We show this restriction in Figure 6–21(f) where the z
0
zero is accompanied by the z zero. If the FIR filter’s transfer function polyno-
3
mial has real-valued b coefficients, then a z zero not on the z-plane’s real axis
k 0
will be accompanied by a complex conjugate zero at z = z . Likewise, for the
2
FIR filter to have linear phase the z zero must be accompanied by the z zero.
2 1
Of course, the z and the z zeros are complex conjugates of each other.
1 3
To conclude this section, we provide the following brief list of z-plane
pole/zero properties that we should keep in mind as we work with digital
filters:
• Filter poles are associated with increased frequency magnitude re-
sponse (gain).
• Filter zeros are associated with decreased frequency magnitude re-
sponse (attenuation).
• Tobeunconditionallystableallfilterpolesmustresideinsidetheunitcircle.
• Filter zeros do not affect filter stability.
• The closer a pole (zero) is to the unit circle, the stronger will be its effect
on the filter’s gain (attenuation) at the frequency associated with the
pole’s (zero’s) angle.

6.6 Alternate IIR Filter Structures 289
• A pole (zero) located on the unit circle produces infinite filter gain (at-
tenuation).
• If a pole is at the same z-plane location as a zero, they cancel each other.
• Poles or zeros located at the origin of the z-plane do not affect the fre-
quency response of the filter.
• Filters whose transfer function denominator (numerator) polynomial
has real-valued coefficients have poles (zeros) located on the real z-plane
axis, or pairs of poles (zeros) that are complex conjugates of each other.
• For an FIR filter (transfer function denominator is unity) to have linear
phase, any zero on the z-plane located at z
=Mejα
, where z is not on the
0 0
unit circle and α is not zero, must be accompanied by a reciprocal zero
whose location is 1/z
=e–jα
/M.
0
• What the last two bullets mean is that if an FIR filter has real-valued co-
efficients, is linear phase, and has a z-plane zero not located on the real
z-plane axis or on the unit circle, that z-plane zero is a member of a
“gang of four” zeros. If we know the z-plane location of one of those
four zeros, then we know the location of the other three zeros.
6.6 ALTERNATE IIR FILTER STRUCTURES
In the literature of DSP, it’s likely that you will encounter IIR filters other than
the Direct Form I structure of the IIR filter in Figure 6–17. This point of our IIR
filter studies is a good time to introduce those alternate IIR filter structures
(block diagrams).
6.6.1 Direct Form I,Direct Form II,and Transposed Structures
The Direct Form I structure of the IIR filter in Figure 6–17 can be converted to
several alternate forms. It’s easy to explore this idea by assuming that there
are two equal-length delay lines, letting M = N = 2 as in Figure 6–22(a), and
thinking of the feedforward and feedback portions as two separate filter
stages. Because both stages of the filter are linear and time invariant, we can
swap them, as shown in Figure 6–22(b), with no change in the y(n) output.
The two identical delay lines in Figure 6–22(b) provide the motivation
for this reorientation. Because the sequence g(n) is being shifted down along
both delay lines in Figure 6–22(b), we can eliminate one of the delay paths
and arrive at the simplified Direct Form II filter structure shown in Figure
6–22(c), where only half the delay storage registers are required compared to
the Direct Form I structure.
Another popular IIR structure is the transposedform of the Direct Form II
filter. We obtain a transposed form by starting with the Direct Form II filter,

290 Infinite Impulse Response Filters
x(n) Direct Form I y(n) x(n) Modified Direct Form I y(n)
g(n)
z–1 b(0) z–1 b(0)
z–1 z–1
b(1) a(1) a(1) b(1)
z–1 z–1 z–1 z–1
b(2) a(2) a(2) b(2)
(a) (b)
Direct Form II Transposed Direct Form II
x(n) y(n) x(n) y(n)
z–1 b(0) b(0) z–1
a(1) b(1) b(1) a(1)
z–1 z–1
a(2) b(2) b(2) a(2)
(c) (d)
Figure 6–22 Rearranged 2nd-order IIR filter structures: (a) Direct Form I; (b) modi-
fied Direct Form I; (c) Direct Form II; (d) transposed Direct Form II.
convert its signal nodes to adders, convert its adders to signal nodes, reverse
the direction of its arrows, and swap x(n) and y(n). (The transposition steps
can also be applied to FIR filters.) Following these steps yields the transposed
Direct Form II structure given in Figure 6–22(d).
All the filters in Figure 6–22 have the same performance just so long as
infinite-precision arithmetic is used. However, using quantized binary arith-
metic to represent our filter coefficients, and with truncation or rounding
being used to combat binary overflow errors, the various filters in Figure 6–22
exhibit different quantization noise and stability characteristics. In fact, the
transposed Direct Form II structure was developed because it has improved
behavior over the Direct Form II structure when fixed-point binary arithmetic
is used. Common consensus among IIR filter designers is that the Direct
Form I filter has the most resistance to coefficient quantization and stability
problems. We’ll revisit these finite-precision arithmetic issues in Section 6.7.
By the way, because of the feedback nature of IIR filters, they’re often re-
ferred to as recursive filters. Similarly, FIR filters are often called nonrecursive

6.6 Alternate IIR Filter Structures 291
filters. A common misconception is that all recursive filters are IIR. This not
true because FIR filters can be implemented with recursive structures. (Chap-
ters 7 and 10 discuss filters having feedback but whose impulse responses are
finite in duration.) So, the terminology recursive and nonrecursive should be
applied to a filter’s structure, and the terms IIR and FIR should only be used
to describe the time duration of the filter’s impulse response[11,12].
6.6.2 The Transposition Theorem
There is a process in DSPthat allows us to change the structure (the block dia-
gram implementation) of a linear time-invariant digital network without
changing the network’s transfer function (its frequency response). That net-
work conversion process follows what is called the transposition theorem. That
theorem is important because a transposed version of some digital network
might be easier to implement, or may exhibit more accurate processing, than
the original network.
We primarily think of the transposition theorem as it relates to digital fil-
ters, so below are the steps to transpose a digital filter (or any linear time-
invariant network for that matter):
1. Reverse the direction of all signal-flow arrows.
2. Convert all adders to signal nodes.
3. Convert all signal nodes to adders.
4. Swap the x(n) input and y(n) output labels.
An example of this transposition process is shown in Figure 6–23. The
Direct Form II IIR filter in Figure 6–23(a) is transposed to the structure shown
in Figure 6–23(b). By convention, we flip the network in Figure 6–23(b) from
left to right so that the x(n) input is on the left as shown in Figure 6–23(c).
Notice that the transposed filter contains the same number of delay ele-
ments, multipliers, and addition operations as the original filter, and both fil-
ters have the same transfer function given by
Y(z) b(0)+b(1)z −1+b(2)z −2
H(z)= = . (6–44)
X(z) 1−a(1)z −1−−a(2)z −2
When implemented using infinite-precision arithmetic, the Direct Form
II and the transposed Direct Form II filters have identical frequency response
properties. As mentioned in Section 6.6.1, however, the transposed Direct
Form II structure is less susceptible to the errors that can occur when finite-
precision binary arithmetic, for example, in a 16-bit processor, is used to rep-
resent data values and filter coefficients within a filter implementation. That
property is because Direct Form II filters implement their (possibly high-gain)

292 Infinite Impulse Response Filters
x(n) Direct Form II y(n) y(n) x(n)
b(0) z–1 b(0)
z–1
a(1) b(1)
a(1) z–1 b(1)
z–1
a(2) b(2) a(2) b(2)
(a) (b)
Transposed Direct
x(n) Form II y(n)
b(0) z–1
b(1) z–1 a(1)
b(2) a(2)
(c)
Figure 6–23 Converting a Direct Form II filter to its transposed form.
feedback pole computations before their feedforward zeros computations,
and this can lead to problematic (large) intermediate data values which must
be truncated. The transposed Direct Form II filters, on the other hand, imple-
ment their zeros computations first followed by their pole computations.
6.7 PITFALLS IN BUILDING IIR FILTERS
There’s an old saying in engineering: “It’s one thing to design a system on
paper, and another thing to actually build one and make it work.” (Recall the
Tacoma Narrows Bridge episode!) Fabricating a working system based on
theoretical designs can be difficult in practice. Let’s see why this is often true
for IIR digital filters.
Again, the IIR filter structures in Figures 6–18 and 6–22 are called Direct
Form implementations of an IIR filter. That’s because they’re all equivalent to
directly implementing the general time-domain expression for an Mth-order

6.7 Pitfalls in Building IIR Filters 293
IIR filter given in Eq. (6–21). As it turns out, there can be stability problems and
frequency response distortion errors when Direct Form implementations are
used for high-order filters. Such problems arise because we’re forced to repre-
sent the IIR filter coefficients and results of intermediate filter calculations with
binary numbers having a finite number of bits. There are three major categories
of finite-word-length errors that plague IIR filter implementations: coefficient
quantization, overflow errors, and roundoff errors.
Coefficient quantization (limited-precision coefficients) will result in fil-
ter pole and zero shifting on the z-plane, and a frequency magnitude response
that may not meet our requirements, and the response distortion worsens for
higher-order IIR filters.
Overflow, the second finite-word-length effect that troubles IIR filters, is
what happens when the result of an arithmetic operation is too large to be
represented in the fixed-length hardware registers assigned to contain that re-
sult. Because we perform so many additions when we implement IIR filters,
overflow is always a potential problem. With no precautions being taken to
handle overflow, large nonlinearity errors can result in our filter output sam-
ples—often in the form of overflow oscillations.
The most common way of dealing with binary overflow errors is called
roundoff,or rounding, where a data value is represented by, or rounded off to,
the b-bit binary number that’s nearest the unrounded data value. It’s usually
valid to treat roundoff errors as a random process, but conditions occur in IIR
filters where rounding can cause the filter output to oscillate forever even
when the filter input sequence is all zeros. This situation, caused by the
roundoff noise being highly correlated with the signal, going by the names
limit cyclesand deadband effects, has been well analyzed in the literature[13,14].
We can demonstrate limit cycles by considering the 2nd-order IIR filter in Fig-
ure 6–24(a) whose time-domain expression is
y(n) = x(n) + 1.3y(n–1) – 0.76y(n–2). (6–45)
Let’s assume this filter rounds the adder’s output to the nearest integer
value. If the situation ever arises where y(–2) = 0, y(–1) = 8, and x(0) and all
successive x(n) inputs are zero, the filter output goes into endless oscillation
as shown in Figure 6–24(b). If this filter were to be used in an audio applica-
tion, when the input signal went silent the listener could end up hearing an
audio tone instead of silence. The dashed line in Figure 6–24(b) shows the fil-
ter’s stable response to this particular situation if no rounding is used. With
rounding, however, this IIR filter certainly lives up to its name. (Welcome to
the world of binary arithmetic!)
There are several ways to reduce the ill effects of coefficient quantization
errors and limit cycles. We can increase the word widths of the hardware reg-
isters that contain the results of intermediate calculations. Because roundoff
limit cycles affect the least significant bits of an arithmetic result, larger word

294 Infinite Impulse Response Filters
x(n) y(n)
Rounding
z–1
y(n–1) = 8
(a)
1.3 z–1
y(n–2) = 0
–0.76
y(n)
10
8 Reponse with
6 no rounding error
4
2
(b) 4 12 20
0
2 8 16 24 Time
–2
–4
–6
Figure 6–24 Limit cycle oscillations due to rounding: (a) 2nd-order IIR filter; (b)
one possible time-domain response of the IIR filter.
sizes diminish the impact of limit cycles should they occur. To avoid filter
input sequences of all zeros, some practitioners add a dither sequenceto the fil-
ter’s input signal sequence. Adither sequence is a sequence of low-amplitude
pseudo-random numbers that interferes with an IIR filter’s roundoff error
generation tendency, allowing the filter output to reach zero should the input
signal remain at zero. Dithering, however, decreases the effective signal-
to-noise ratio of the filter output[12]. Finally, to avoid limit cycle problems,
we can just use an FIR filter. Because FIR filters by definition have finite-
length impulse responses, and have no feedback paths, they cannot support
output oscillations of any kind.
As for overflow errors, we can eliminate them if we increase the word
width of hardware registers so overflow never takes place in the IIR filter. Fil-
ter input signals can be scaled (reduced in amplitude by multiplying signals
within the filter by a factor less than one) so overflow is avoided. We discuss
such filter scaling in Section 6.9. Overflow oscillations can be avoided by
using saturation arithmetic logic where signal values aren’t permitted to ex-
ceed a fixed limit when an overflow condition is detected[15,16]. It may be
useful for the reader to keep in mind that when the signal data is represented
in two’s complement arithmetic, multiple summations resulting in intermedi-
ate overflow errors cause no problems if we can guarantee that the final mag-

6.8 Improving IIR Filters with Cascaded Structures 295
nitude of the sum of the numbers is not too large for the final accumulator
register. Of course, standard floating-point number formats can greatly re-
duce the errors associated with overflow oscillations and limit cycles[17]. (We
discuss floating-point number formats in Chapter 12.)
These quantized coefficient and overflow errors, caused by finite-width
words, have different effects depending on the IIR filter structure used. Refer-
ring to Figure 6–22, practice has shown the Direct Form II structure to be the
most error-prone IIR filter implementation.
The most popular technique for minimizing the errors associated with
finite-word-length widths is to design IIR filters comprising a cascade string,
or parallel combination, of low-order filters. The next section tells us why.
6.8 IMPROVING IIR FILTERS WITH CASCADED STRUCTURES
Filter designers minimize IIR filter stability and quantization noise problems
in high-performance filters by implementing combinations of cascaded lower-
performance filters. Before we consider this design idea, let’s review several
important issues regarding the behavior of combinations of multiple filters.
6.8.1 Cascade and Parallel Filter Properties
Here we summarize the combined behavior of linear time-invariant filters (be
they IIR or FIR) connected in cascade and in parallel. As indicated in Figure
6–25(a), the resultant transfer function of two cascaded filter transfer func-
tions is the product of those functions, or
H (z) = H (z)H (z) (6–46)
cas 1 2
with an overall frequency response of
H (ω) = H (ω)H (ω). (6–47)
cas 1 2
It’s also important to know that the resultant impulse response of cascaded
filters is
h (k) = h (k)*h (k), (6–48)
cas 1 2
where “*” means convolution.
As shown in Figure 6–25(b), the combined transfer function of two fil-
ters connected in parallel is the sum of their transfer functions, or
H (z) = H (z) + H (z) (6–49)
par 1 2
with an overall frequency response of
H (ω) = H (ω) + H (ω). (6–50)
par 1 2

296 Infinite Impulse Response Filters
(a) X(z) H(z) H 1 (z)X(z) H(z) Y cas (z) = H 1 (z)H 2 (z)X(z)
1 2
H(z)X(z)
H(z) 1
1
Y (z) = [H(z) + H(z)]X(z)
X(z) par 1 2
(b) +
H(z)
2 H(z)X(z)
2
Figure 6–25 Combinations of two filters: (a) cascaded filters; (b) parallel filters.
The resultant impulse response of parallel filters is the sum of their individual
impulse responses, or
h (k) = h (k) + h (k). (6–51)
par 1 2
While we are on the subject of cascaded filters, let’s develop a rule of
thumb for estimating the combined passband ripple of the two cascaded fil-
ters in Figure 6–25(a). The cascaded passband ripple is a function of each in-
dividual filter’s passband ripple. If we represent an arbitrary filter’s peak
passband ripple on a linear (not dB) vertical axis as shown in Figure 6–26, we
can begin our cascaded ripple estimation.
From Eq. (6–47), the upper bound (the peak) of a cascaded filter’s pass-
band response, 1 + R , is the product of the two H (ω) and H (ω) filters’ peak
cas 1 2
passband responses, or
1 + R = (1 + R )(1 + R ) = 1 + R + R + R R . (6–52)
cas 1 2 1 2 1 2
|H(ω)|
1 + R
1
1 – R
0 f pass Freq
Figure 6–26 Definition of filter passband ripple R.

6.8 Improving IIR Filters with Cascaded Structures 297
For small values of R and R , the R R term becomes negligible, and we state
1 2 1 2
our rule of thumb as
R ≈R + R . (6–53)
cas 1 2
Thus, in designs using two cascaded filters it’s prudent to specify their indi-
vidual passband ripple values to be roughly half the desired R ripple speci-
cas
fication for the final combined filter, or
R = R ≈R /2. (6–54)
1 2 cascaded
6.8.2 Cascading IIR Filters
Experienced filter designers routinely partition high-order IIR filters into a
string of 2nd-order IIR filters arranged in cascade because these lower-order
filters are easier to design, are less susceptible to coefficient quantization errors
and stability problems, and their implementations allow easier data word scal-
ing to reduce the potential overflow effects of data word size growth.
Optimizing the partitioning of a high-order filter into multiple 2nd-
order filter sections is a challenging task, however. For example, say we have
the 6th-order Direct Form I filter in Figure 6–27(a) that we want to partition
into three 2nd-order sections. In factoring the 6th-order filter’s H(z) transfer
function, we could get up to three separate sets of feedforward coefficients in
the factored H(z) numerator: b’(k), b’’(k), and b’’’(k). Likewise, we could have
up to three separate sets of feedback coefficients in the factored denominator:
a’(k), a’’(k), and a’’’(k). Because there are three 2nd-order sections, there are
three factorial, or six, ways of pairing the sets of coefficients. Notice in Figure
6–27(b) how the first section uses the a’(k) and b’(k) coefficients, and the sec-
ond section uses the a’’(k) and b’’(k) coefficients. We could just as well have in-
terchanged the sets of coefficients so the first 2nd-order section uses the a’(k)
and b’’(k) coefficients, and the second section uses the a’’(k) and b’(k) coeffi-
cients. So, there are six different mathematically equivalent ways of combin-
ing the sets of coefficients. Add to this the fact that for each different
combination of low-order sections there are three factorial distinct ways those
three separate 2nd-order sections can be arranged in cascade.
This means if we want to partition a 2M-order IIR filter into M distinct
2nd-order sections, there are M factorial squared, (M!)2, ways to do so. As
such, there are then (3!)2= 24 different cascaded filters we could obtain when
going from Figure 6–27(a) to Figure 6–27(b). To further complicate this filter
partitioning problem, the errors due to coefficient quantization will, in gen-
eral, be different for each possible filter combination. Although full details of
this subject are outside the scope of this introductory text, ambitious readers
can find further material on optimizing cascaded filter sections in references
[14] and [18], and in Part 3 of reference [19].

298 Infinite Impulse Response Filters
x(n) y(n)
z–1 z–1
b(0)
z–1 b(1) a(1) z–1
(a)
z–1 b(2) a(2) z–1
. . . . . .
b(6) a(6)
x(n) Section 1 Section 2 Section 3 y(n)
z–1 b'(0) z–1 z–1 b' (0) z–1 z–1 b'''(0) z–1
(b)
z–1 b'(1) a'(1) z–1 z–1 b' '(1) a'('1) z–1 z–1 b'''(1) a'''(1) z–1
b'(2) a'(2) b' ' (2) a' ' (2) b'''(2) a'''(2)
Figure 6–27 IIR filter partitioning: (a) initial 6th-order IIR filter; (b) three 2nd-order
sections.
One simple (although perhaps not optimum) method for arranging cas-
caded 2nd-order sections has been proposed[18]. First, factor a high-order IIR
filter’s H(z) transfer function into a ratio of the form
(z−z )(z−z )(z−z )(z−z )(z−z )(z−z )....
H(z)= 0 1 2 3 4 5 (6–55)
(z−p )(z−p )(z−p )(z−p )(z−p )(z−p )...
0 1 2 3 4 5
with the z zeros in the numerator and p poles in the denominator. (Ideally
k k
you have a signal processing software package to perform the factorization.)
Next, the 2nd-order section assignments go like this:
1. Find the pole, or pole pair, in H(z) closest to the unit circle.
2. Find the zero, or zero pair, closest to the pole, or pole pair, found in Step 1.

6.8 Improving IIR Filters with Cascaded Structures 299
3. Combine those poles and zeros into a single 2nd-order filter section.
This means your first 2nd-order section may be something like
(z−z )(z−z )
H (z)= 4 5 . (6–56)
1 (z−p )(z−p )
0 1
4. Repeat Steps 1 to 3 until all poles and zeros have been combined into
2nd-order sections.
5. The final ordering (cascaded sequence) of the sections is based on how
far the sections’ poles are from the unit circle. Order the sections in ei-
ther increasing or decreasing pole distances from the unit circle.
6. Implement your filter as cascaded 2nd-order sections in the order
from Step 5.
In digital filter vernacular, a 2nd-order IIR filter is called a biquadfor two rea-
sons. First, the filter’s z-domain transfer function includes two quadratic
polynomials. Second, the word biquadsounds cool.
By the way, we started our 2nd-order sectioning discussion with a high-
order Direct Form I filter in Figure 6–27(a). We chose that filter form because
it’s the structure most resistant to coefficient quantization and overflow prob-
lems. As seen in Figure 6–27(a), we have redundant delay elements. These
can be combined, as shown in Figure 6–28, to reduce our temporary storage
requirements as we did with the Direct Form II structure in Figure 6–22.
There’s much material in the literature concerning finite word effects as
they relate to digital IIR filters. (References [18], [20], and [21] discuss quanti-
zation noise effects in some detail as well as providing extensive bibliogra-
phies on the subject.)
x(n) Section 1 Section 2 Section 3 y(n)
z–1 b'(0) z–1 b' '(0) z–1 b'''(0) z–1
b'(1) a' (1) b' '(1) a' '(1) b'''(1) a'''(1)
z–1 z–1 z–1 z–1
b'(2) a'(2) b' '(2) a' '(2) b'''(2) a'''(2)
Figure 6–28 Cascaded Direct Form I filters with reduced temporary data storage.

300 Infinite Impulse Response Filters
6.9 SCALING THE GAIN OF IIR FILTERS
In order to impose limits on the magnitudes of data values within an IIR fil-
ter, we may wish to change the passband gain of that filter[22,23].
For example, consider the 1st-order lowpass IIR filter in Figure 6–29(a)
that has a DC gain (gain at zero Hz) of 3.615. (This means that, just as with
FIR filters, the sum of the IIR filter’s impulse response samples is equal to the
DC gain of 3.615.)
The DC gain of an IIR filter is the sum of the filter’s feedforward coeffi-
cients divided by 1 minus the sum of the filter’s feedback coefficients. (We
leave the proof of that statement as a homework problem.) That is, the DC
gain of the Figure 6–29(a) 1st-order filter is
b(0)+b(1) 0.22+0.25
DCgain = 1−a(1) = 1−0.87 =3.6155. (6–57)
Now let’s say we want, for some reason, the filter’s DC gain to be one
(unity gain). This is easy to accomplish. We merely divide the filter’s feedfor-
ward coefficients by the original DC gain as
0.22 0.25
b (0)= =0.061,andb (1)= =0.069. (6–58)
new 3.615 new 3.6155
Doing so gives us a new filter whose feedforward coefficients are those
shown in Figure 6–29(b). That new lowpass filter has a DC gain of one.
Changing a filter’s coefficients in this way is called filter scaling. Happily, this
filter scaling does not change the shape of the original filter’s frequency mag-
nitude or phase response.
Likewise, to force the passband gain of a highpass filter to be unity, we
divide the filter’s feedforward coefficients by the original filter’s frequency
magnitude response at f/2 (half the sampling rate).
s
DC gain = 3.615 DC gain = 1.0
x(n) y(n) x(n) y(n)
z–1 0.22 z–1 0.061
0.87 0.25 0.87 0.069
(a) (b)
Figure 6–29 Lowpass IIR filters: (a) DC gain = 3.615; (b) DC gain = 1.

6.9 Scaling the Gain of IIR Filters 301
Unlike passive analog (continuous-time) filters that operate by attenuat-
ing spectral energy in their stopbands, digital IIR filters operate by amplify-
ing spectral energy in their passbands. Because of this positive passband gain
behavior, there is another type of IIR filter scaling that’s used in many situa-
tions. It is possible that an IIR filter may have a passband gain so high that the
filter generates internal sample values too large to be accommodated by the
hardware, with its internal binary number format, used to implement the fil-
ter. Stated in different words, it’s possible for a filter to generate internal data
values so large that they overflow the registers in which the data is to be
stored. This situation can also occur when multiple 2nd-order IIR filters are
cascaded as discussed in Section 6.8.
In such cases, should we wish to reduce the passband gain of an IIR filter
without changing the shape of its frequency magnitude or phase responses,
we can do so by implementing one of the filters shown in Figure 6–30.
If an IIR filter has an original passband gain of G , we can change that
IIR
passband gain by modifying the original filter’s coefficients using the scalar
Direct Form I Direct Form I
x(n) y(n) x(n) y(n)
z–1 Gb(0) G z–1 z–1 Gb(0) G z–1
1 2 1 2
Gb(1) a(1)/G Gb(1) a(1)
z–1 1 2 z–1 z–1 1 z–1
Gb(2) a(2)/G Gb(2) a(2)
1 2 (a) 1
Direct Form II Direct Form II
x(n) y(n) x(n) y(n)
G z–1 Gb(0) G z–1 Gb(0)
2 1 2 1
a(1)/G Gb(1) a(1) Gb(1)
2 z–1 1 z–1 1
a(2)/G Gb(2) a(2) Gb(2)
2 1 (b) 1
Figure 6–30 Scaled IIR filter structures: (a) Direct Form I; (b) Direct Form II.

302 Infinite Impulse Response Filters
G and G gain factors shown in Figure 6–30. Changing a filter’s coefficients
1 2
in this way is also called filter scaling. The passband gain of a scaled filter is
G = G G G . (6–59)
IIR-scaled 1 2 IIR
The general philosophy in these matters is to choose factors G and G so that
1 2
we preserve the filter’s output signal quality (called the signal-to-noise ratio,
SNR, as discussed in Chapter 12 and Appendix D) as much as possible. This
means keeping all internal sample values as large as can be accommodated
by the filter hardware registers. The problem is, there’s no simple way to de-
termine the values of G and G . The suggested procedure is to select one of
1 2
the Figure 6–30 implementations and apply the expected input signal to the
filter. Next we experiment with different values for gain factors G and G
1 2
from Eq. (6–59) until the final filter gain, G , is an acceptable value. Fol-
IIR-scaled
lowing that, we select an alternate Figure 6–30 filter structure and experiment
with different values for gains G and G to see if we can improve on the pre-
1 2
vious scaled-filter structure.
For computational efficiency reasons, if we’re able to set G to be the re-
2
ciprocal of an integer power of two, then we can eliminate one of the multi-
plies in Figure 6–30. That is, in this scenario the multiply by G operation can
2
then be implemented with binary right shifts. Then again, perhaps factors G
1
and G can be chosen so that one of the modified filter coefficients is unity in
2
order to eliminate a multiply operation.
Now that we have some understanding of the performance and imple-
mentation structures of IIR filters, let’s briefly introduce three filter design
techniques. These IIR design methods go by the impressive names of impulse
invariance, bilinear transform, and optimized methods. The first two methods
use analytical, pencil and paper algebra, filter design techniques to approxi-
mate continuous analog filters. (By “analog filters” we mean those hardware
filters made up of resistors, capacitors, and perhaps operational amplifiers.)
Because analog filter design methods are very well understood, design-
ers can take advantage of an abundant variety of analog filter design tech-
niques to design, say, a digital IIR Butterworth filter with its very flat
passband response, or perhaps go with a Chebyshev filter with its fluctuating
passband response and sharper passband-to-stopband cutoff characteristics.
The optimized methods (by far the most popular way of designing IIR filters)
comprise linear algebra algorithms available in commercial filter design soft-
ware packages.
The impulse invariance, bilinear transform filter design methods are
somewhat involved, so a true DSPbeginner is justified in skipping those sub-
jects upon first reading this book. However, those filter design topics may
well be valuable sometime in your future as your DSP knowledge, experi-
ence, and challenges grow.

6.10 Impulse Invariance IIR Filter Design Method 303
6.10 IMPULSE INVARIANCE IIR FILTER DESIGN METHOD
The impulse invariance method of IIR filter design is based upon the notion
that we can design a discrete filter whose time-domain impulse response is a
sampled version of the impulse response of a continuous analog filter. If that
analog filter (often called the prototype filter) has some desired frequency re-
sponse, then our IIR filter will yield a discrete approximation of that desired
response. The impulse response equivalence of this design method is de-
picted in Figure 6–31, where we use the conventional notation of δ to repre-
sent an impulse function and h(t) is the analog filter’s impulse response. We
c
use the subscript “c” in Figure 6–31(a) to emphasize the continuous nature of
the analog filter. Figure 6–31(b) illustrates the definition of the discrete filter’s
impulse response: the filter’s time-domain output sequence when the input is
a single unity-valued sample (impulse) preceded and followed by all zero-
valued samples. Our goal is to design a digital filter whose impulse response is a
sampled version of the analog filter’s continuous impulse response. Implied in the
correspondence of the continuous and discrete impulse responses is the property
that we can map each pole on the s-plane for the analog filter’s H(s) transfer
c
function to a pole on thez-plane for the discrete IIR filter’s H(z) transfer func-
tion. What designers have found is that the impulse invariance method does
yield useful IIR filters, as long as the sampling rate is high relative to the
bandwidth of the signal to be filtered. In other words, IIR filters designed
using the impulse invariance method are susceptible to aliasing problems be-
cause practical analog filters cannot be perfectly band-limited. Aliasing will
occur in an IIR filter’s frequency response as shown in Figure 6–32.
Analog Continuous
(a) Filter time
x(t) = δ(t)
H (s)
c Impulse response
output = h c (t)
IIR Digital Discrete
(b) Filter time
x(n) = δ(n) H(z)
Impulse response
output= h(n) =h c (nt s )
Figure 6–31 Impulse invariance design equivalence of (a) analog filter continu-
ous impulse response; (b) digital filter discrete impulse response.

304 Infinite Impulse Response Filters
|H(ω)|
c
(a)
0 ω
|H (ω)|
IIR
(b)
−4p −2p 0 2p 4p ω
(−2f) (−f) (f) (2f)
s s s s
Final |H (ω)|
IIR
(c)
−4p −2p 0 2p 4p ω
(−2f) (−f) (f) (2f)
s s s s
Figure 6–32 Aliasing in the impulse invariance design method: (a) prototype
analog filter magnitude response; (b) replicated magnitude re-
sponses where H (ω) is the discrete Fourier transform of
IIR
h(n) = h (nt); (c) potential resultant IIR filter magnitude response
c s
with aliasing effects.
From what we’ve learned in Chapter 2 about the spectral replicating ef-
fects of sampling, if Figure 6–32(a) is the spectrum of the continuous h(t) im-
c
pulse response, then the spectrum of the discrete h(nt) sample sequence is
c s
the replicated spectra in Figure 6–32(b).
In Figure 6–32(c) we show the possible effect of aliasing where the
dashed curve is a desired H (ω) frequency magnitude response. However,
IIR
the actual frequency magnitude response, indicated by the solid curve, can
occur when we use the impulse invariance design method. For this reason,
we prefer to make the sample frequency f as large as possible to minimize the
s
overlap between the primary frequency response curve and its replicated im-
ages spaced at multiples of ±f Hz.
s
Due to the aliasing behavior of the impulse invariance design method,
this filter design process should never be used to design highpass digital fil-
ters. To see how aliasing can affect IIR filters designed with this method, let’s

6.10 Impulse Invariance IIR Filter Design Method 305
list the necessary impulse invariance design steps and then go through a low-
pass filter design example.
There are two different methods for designing IIR filters using impulse
invariance. The first method, which we’ll call Method 1, requires that an in-
verse Laplace transform as well as a z-transform be performed[24,25]. The
second impulse invariance design technique, Method 2, uses a direct substitu-
tion process to avoid the inverse Laplace and z-transformations at the ex-
pense of needing partial fraction expansion algebra necessary to handle
polynomials[20,21,26,27]. Both of these methods seem complicated when de-
scribed in words, but they’re really not as difficult as they sound. Let’s com-
pare the two methods by listing the steps required for each of them. The
impulse invariance design Method 1 goes like this:
Method 1, Step 1: Design (or have someone design for you) a prototype ana-
log filter with the desired frequency response.† The result
of this step is a continuous Laplace transfer function H(s)
c
expressed as the ratio of two polynomials, such as
N
∑
k
b(k)s
H c (s)= a b ( ( M N ) ) s s M N + +b a ( ( N M − − 1 1 ) ) s s N M − − 1 1 + + .. . . .. + + b a ( ( 1 1 ) ) s s + + b a ( ( 0 0 ) ) = k ∑ M =0 , (6–60)
k
a(k)s
k=0
which is the general form of Eq. (6–10) with N < M, and
a(k) and b(k) are constants.
Method 1, Step 2: Determine the analog filter’s continuous time-domain im-
pulse response h(t) from the H(s) Laplace transfer func-
c c
tion. I hope this can be done using Laplace tables as
opposed to actually evaluating an inverse Laplace trans-
form equation.
Method 1, Step 3: Determine the digital filter’s sampling frequency f, and
s
calculate the sample period as t = 1/f. The f sampling
s s s
rate is chosen based on the absolute frequency, in Hz, of
the prototype analog filter. Because of the aliasing prob-
lems associated with this impulse invariance design
method, later, we’ll see why f should be made as large as
s
is practical.
†In a lowpass filter design, for example, the filter type (Chebyshev, Butterworth, elliptic), filter
order (number of poles), and the cutoff frequency are parameters to be defined in this step.

306 Infinite Impulse Response Filters
Method 1, Step 4: Find the z-transform of the continuous h(t) to obtain the
c
IIR filter’sz-domain transfer function H(z) in the form of a
ratio of polynomials in z.
Method 1, Step 5: Substitute the value (not the variable) t for the continuous
s
variable t in the H(z) transfer function obtained in Step 4.
In performing this step, we are ensuring, as in Figure 6–31,
that the IIR filter’s discrete h(n) impulse response is a sam-
pled version of the continuous filter’s h (t) impulse re-
c
sponse so that h(n)=h (nt), for 0≤n≤∞.
c s
Method 1, Step 6: Our H(z) from Step 5 will now be of the general form
N
∑ −k
b(k)z
H(z)= b(N)z −N +b(N−1)z −(N−1)+...+b(1)z −1+b(0) = k=0 .
a(M)z −M +a(M−1)z −(M−1)+...+a(1)z −1+a(0) 1− ∑ M a(k)z −k (6–61)
k=1
Because the process of sampling the continuous impulse
response results in a digital filter frequency response that’s
scaled by a factor of 1/t, many filter designers find it ap-
s
propriate to include the t factor in Eq. (6–61). So we can
s
rewrite Eq. (6–61) as
∑N
t b(k)z
−k
s
H(z)= Y(z) = k=0 . (6–62)
X(z) ∑M
1− a(k)z −k
k=1
Incorporating the value of t in Eq. (6–62), then, makes the
s
IIR filter time-response scaling independent of the sam-
pling rate, and the discrete filter will have the same gain as
the prototype analog filter.†
Method 1, Step 7: Because Eq. (6–61) is in the form of Eq. (6–25), by inspec-
tion, we can express the filter’s time-domain difference
equation in the general form of Eq. (6–21) as
y(n)=b(0)x(n)+b(1)x(n−1)+b(2)x(n−2)+...+b(N)x(n−N)
+a(1)y(n−1)+a(2)y(n−2)+...+a(M)y(n−M). (6–63)
†Some authors have chosen to include the t factor in the discrete h(n)impulse response in the
s
above Step 4, that is, make h(n)=t h(nt)[20, 28]. The final result of this, of course, is the same as
s c s
that obtained by including t as described in Step 6.
s

6.10 Impulse Invariance IIR Filter Design Method 307
Choosing to incorporate t , as in Eq. (6–62), to make the
s
digital filter’s gain equal to the prototype analog filter’s
gain by multiplying the b(k) coefficients by the sample pe-
riod t leads to an IIR filter time-domain expression of the
s
form
y(n)=t ⋅ [b(0)x(n)+b(1)x(n−1)+b(2)x(n−2)+...+b(N)x(n−N)]
s
+a(1)y(n−1)+a(2)y(n−2)+...+a(M)y(n−M). (6–64)
Notice how the signs changed for the a(k) coefficients
from Eqs. (6–61) and (6–62) to Eqs. (6–63) and (6–64).
These sign changes always seem to cause problems for
beginners, so watch out. Also, keep in mind that the
time-domain expressions in Eqs. (6–63) and (6–64)
apply only to the filter structure in Figure 6–18. The
⋅
a(k) and b(k), or t b(k), coefficients, however, can be
s
applied to the improved IIR structure shown in Figure
6–22 to complete our design.
Before we go through an actual example of this design process, let’s discuss
the other impulse invariance design method.
The impulse invariance Design Method 2, also called the standard
z-transform method, takes a different approach. It mathematically partitions
the prototype analog filter into multiple single-pole continuous filters and
then approximates each one of those by a single-pole digital filter. The set of
Msingle-pole digital filters is then algebraically combined to form an M-pole,
Mth-ordered IIR filter. This process of breaking the analog filter to discrete fil-
ter approximation into manageable pieces is shown in Figure 6–33. The steps
necessary to perform an impulse invariance Method 2 design are:
Method 2, Step 1: Obtain the Laplace transfer function H(s) for the proto-
c
type analog filter in the form of Eq. (6–60). (Same as
Method 1, Step 1.)
Method 2, Step 2: Select an appropriate sampling frequency f and calculate
s
the sample period as t =1/f. (Same as Method 1, Step 3.)
s s
Method 2, Step 3: Express the analog filter’s Laplace transfer function H(s)
c
as the sum of single-pole filters. This requires us to use
partial fraction expansion methods to express the ratio of
polynomials in Eq. (6–60) in the form of

308 Infinite Impulse Response Filters
M-pole analog prototype filter
x(t) N M y(t)
b(k)sk a(k)sk
0 0
Partial fraction expand
M single-pole discrete filters
K1
s A + 1 p Perform 1 – e–p1tsz–1
1 substitution
x(t) A2 + y(t) x(n) K2 + y(n)
s + . p 2 1 – e– . p2tsz–1
. .
. .
Ak Kk
s + p k 1 – e–pktsz–1
M single-pole analog filters
Algebraically combine
x(n) N M y(n)
b(k)z–k a(k)z–k
0 0
M-pole discrete IIR filter
Figure 6–33 Mathematical flow of the impulse invariance design Method 2.
b(N)sN +b(N−1)sN−1+...+b(1)s+b(0)
H (s)=
c a(M)sM +a(M−1)sM−1+...+a(1)s+a(0)
(6–65)
∑M
A A A A
= k = 1 + 2 +...+ M
s+p s+p s+p s+p
k=1 k 1 2 M
where M > N, the individual A factors are constants, and
k
the kth pole is located at –p on the s-plane. We’ll denote
k
the kth single-pole analog filter as H (s), or
k
A
H (s)= k . (6–66)
k s+p
k
Method 2, Step 4: Substitute 1 – e–pktsz–1for s + p in Eq. (6–65). This mapping
k
of each H (s) pole, located at s = –p on the s-plane, to the
k k
z=e–pktslocation on thez-plane is how we approximate the
impulse response of each single-pole analog filter by a

6.10 Impulse Invariance IIR Filter Design Method 309
single-pole digital filter. (The reader can find the deriva-
tion of this 1 – e–pktsz–1substitution, illustrated in our Figure
6–33, in references [20], [21], and [26].) So, the kth analog
single-pole filter H (s) is approximated by a single-pole
k
digital filter whosez-domain transfer function is
A
H (z)= k . (6–67)
k 1−e −p k t sz −1
The final combined discrete filter transfer function H(z) is
the sum of the single-poled discrete filters, or
∑M ∑M
A
H(z)= H k (z)= 1−e−p k ktsz −1 . (6–68)
k=1 k=1
Keep in mind that the above H(z) is not a function of time.
The t factor in Eq. (6–68) is a constant equal to the
s
discrete-time sample period.
Method 2, Step 5: Calculate the z-domain transfer function of the sum of the
M single-pole digital filters in the form of a ratio of two
polynomials in z. Because the H(z) in Eq. (6–68) will be a
series of fractions, we’ll have to combine those fractions
over a common denominator to get a single ratio of poly-
nomials in the familiar form of
∑N
b(k)z
−k
H(z)= Y(z) = k=0 . (6–69)
X(z) ∑M
1− a(k)z −k
k=1
Method 2, Step 6: Just as in Method 1, Step 6, by inspection, we can express
the filter’s time-domain equation in the general form of
y(n)=b(0)x(n)+b(1)x(n−1)+b(2)x(n−2)+...+b(N)x(n−N)
+a(1)y(n−1)+a(2)y(n−2)+...+a(M)y(n−M). (6–70)
Again, notice the a(k) coefficient sign changes from Eq.
(6–69) to Eq. (6–70). As described in Method 1, Steps 6 and
7, if we choose to make the digital filter’s gain equal to the
prototype analog filter’s gain by multiplying the b(k) coef-
ficients by the sample period t, then the IIR filter’s time-
s
domain expression will be in the form

310 Infinite Impulse Response Filters
y(n)=t ⋅ [b(0)x(n)+b(1)x(n−1)+b(2)x(n−2)+...+b(N)x(n−N)]
s
+a(1)y(n–1)+a(2)y(n−2)+...+a(M)y(n−M), (6–71)
yielding a final H(z)z-domain transfer function of
t
⋅∑N
b(k)z
−k
s
H(z)= Y(z) = k=0 . (6–71’)
X(z) ∑M
1− a(k)z −k
k=1
Finally, we can implement the improved IIR structure
shown in Figure 6–22 using the a(k) and b(k) coefficients
from Eq. (6–70) or the a(k) and t.b(k) coefficients from
s
Eq.(6–71).
To provide a more meaningful comparison between the two impulse in-
variance design methods, let’s dive in and go through an IIR filter design ex-
ample using both methods.
6.10.1 Impulse Invariance Design Method 1 Example
Assume that we need to design an IIR filter that approximates a 2nd-order
Chebyshev prototype analog lowpass filter whose passband ripple is 1 dB.
Our f sampling rate is 100 Hz (t =0.01), and the filter’s 1 dB cutoff frequency
s s
is 20 Hz. Our prototype analog filter will have a frequency magnitude re-
sponse like that shown in Figure 6–34.
Given the above filter requirements, assume that the analog prototype
filter design effort results in the H(s) Laplace transfer function of
c
17410.145
H (s)= . (6–72)
c s2 +137.94536s+17410.145
It’s the transfer function in Eq. (6–72) that we intend to approximate with our
discrete IIR filter. To find the analog filter’s impulse response, we’d like to get
H(s) into a form that allows us to use Laplace transform tables to find h(t).
c c
Searching through systems analysis textbooks, we find the following Laplace
transform pair:
X(s), Laplace transform of x(t): x(t):
(s+α
A
)
ω
2 +ω2
Ae −α t ⋅ sin(ω t). (6–73)

6.10 Impulse Invariance IIR Filter Design Method 311
|H (f)|
dB
0 dB
–1dB
0 20 Hz 50 Hz Freq
(f /2)
s
Figure 6–34 Frequency magnitude response of the example prototype analog
filter.
Our intent, then, is to modify Eq. (6–72) to get it into the form on the left side
of Eq. (6–73). We do this by realizing that the Laplace transform expression in
Eq. (6–73) can be rewritten as
Aω Aω
= . (6–74)
(s+α)2 +ω2 s2 +2α s+α2 +ω2
If we set Eq. (6–72) equal to the right side of Eq. (6–74), we can solve for A,
α,and ω. Doing that,
17410.145 Aω
H (s)= = . (6–75)
c s2 +137.94536s+17410.145 s2 +2α s+α2 +ω2
Solving Eq. (6–75) for A, α, and ω, we first find
137.94536
α= =68.972680;
(6–76)
2
α2 +ω2 =17410.145, (6–77)
so
ω= 17410.145−α2 =112.485173;
(6–78)
and
17410.145
A= =154.77724. (6–79)
ω
OK, we can now express H(s) in the desired form of the left side of Eq. (6–74) as
c
(154.77724)(112.485173)
H (s)= . (6–80)
c (s+68.972680)2 +(112.485173)2

312 Infinite Impulse Response Filters
Using the Laplace transform pair in Eq. (6–73), the time-domain impulse re-
sponse of the prototype analog filter becomes
h (t)=Ae −α t ⋅ sin(ωt)=154.77724e −68.972680t ⋅ sin(112.485173t). (6–81)
c
OK, we’re ready to perform Method 1, Step 4, to determine the discrete IIR fil-
ter’s z-domain transfer function H(z) by performing the z-transform of h(t).
c
Again, scanning through digital signal processing textbooks or a good math
reference book, we find the following z-transform pair where the time-
domain expression is in the same form as Eq. (6–81)’s h(t) impulse response:
c
x(t): X(z),z-transform of x(t):
Ce −αt ⋅ sin(ωt) 1−2[e −α C t e ⋅ − c α o t s ⋅ ( s ω in t ( ) ω ]z t − ) 1 z + −1 e −2αtz −2 . (6–82)
Remember, now, the α and ω in Eq. (6–82) are generic and are not related to
the α and ω values in Eqs. (6–76) and (6–78). Substituting the constants from
Eq. (6–81) into the right side of Eq. (6–82), we get thez-transform of the IIR fil-
ter as
154.77724e
−68.972680t ⋅
sin(112.485173t)z
−1
H(z)= 1−2[e −68.972680t ⋅ cos(112.485173t)]z −1+e −2 ⋅ 68 ⋅ 972680tz −2 . (6–83)
Performing Method 1, Step 5, we substitute the t value of 0.01 for the contin-
s
uous variabletin Eq. (6–83), yielding the final H(z) transfer function of
154.77724e −68.972680 ⋅ 0.01 ⋅ sin(112.485173 ⋅ 0.01)z −1
H(z)=
1−2[e −68.972680 ⋅ 0.01 ⋅ cos(112.485173 ⋅ 0.01)]z −1+e −2 ⋅ 68.972680 ⋅ 0.01z −2
154.77724e
−0.68972680 ⋅
sin(1.12485173)z
−1
=
1−2[e −0.68972680 ⋅ cos(1.12485173)]z −1+e −2 ⋅ 00.68972680z −2
Y(z) 70.059517z
−1
= = . (6–84)
X(z) 1−0.43278805z −1+0.25171605z −2
OK, hang in there; we’re almost finished. Here are the final steps of Method 1.
Because of the transfer function H(z) =Y(z)/X(z), we can cross-multiply the
denominators to rewrite the bottom line of Eq. (6–84) as
Y(z) ⋅ (1−0.43278805z −1+0.25171605z −2)=X(z) ⋅ (70.059517z −1),
or
Y(z)=70.059517 ⋅ X(z)z −1+0.43278805 ⋅ Y(z)z −1−0.25171605 ⋅ Y(z)z −2. (6–85)

6.10 Impulse Invariance IIR Filter Design Method 313
By inspection of Eq. (6–85), we can now get the time-domain expression for
our IIR filter. Performing Method 1, Steps 6 and 7, we multiply the x(n–1) co-
efficient by the sample period value of t =0.01 to allow for proper scaling as
s
y(n)=0.01 ⋅ 70.059517 ⋅ x(n−1)+0.43278805 ⋅ y(n−1)−0.25171605 ⋅ y(n−2)
=0.70059517 ⋅ x(n−1)+0.43278805 ⋅ y(n−1)−0.25171605 ⋅ y(n−2), (6–86)
and there we (finally) are. The coefficients from Eq. (6–86) are what we use in
implementing the improved IIR structure shown in Figure 6–22 to approxi-
mate the original 2nd-order Chebyshev analog lowpass filter.
Let’s see if we get the same result if we use the impulse invariance de-
sign Method 2 to approximate the example prototype analog filter.
6.10.2 Impulse Invariance Design Method 2 Example
Given the original prototype filter’s Laplace transfer function as
17410.145
H (s)= , (6–87)
c s2 +137.94536s+17410.145
and the value of t = 0.01 for the sample period, we’re ready to proceed with
s
Method 2’s Step 3. To express H(s) as the sum of single-pole filters, we’ll have
c
to factor the denominator of Eq. (6–87) and use partial fraction expansion
methods. For convenience, let’s start by replacing the constants in Eq. (6–87)
with variables in the form of
c
H (s)= (6–88)
c s2 +bs+c
where b = 137.94536, and c = 17410.145. Next, using Eq. (6–15) with a = 1, we
can factor the quadratic denominator of Eq. (6–88) into
c
H (s)= .
c ⎛ ⎞ ⎛ ⎞ (6–89)
⎜ ⎜s+ b + b2 −4c ⎟
⎟
⋅ ⎜ ⎜s+ b − b2 −4c ⎟
⎟
⎝ 2 4 ⎠ ⎝ 2 4 ⎠
If we substitute the values for band cin Eq. (6–89), we’ll find that the quantity
under the radical sign is negative. This means that the factors in the denomi-
nator of Eq. (6–89) are complex. Because we have lots of algebra ahead of us,
let’s replace the radicals in Eq. (6–89) with the imaginary term jR, where
j= –1 and R=|(b2–4c)/4|, such that
c
H (s)= . (6–90)
c (s+b/2+ jR)(s+b/2− jR)

314 Infinite Impulse Response Filters
OK, partial fraction expansion methods allow us to partition Eq. (6–90) into
two separate fractions of the form
c
H (s)=
c (s+b/2+ jR)(s+b/2− jR)
K K
= 1 + 2 , (6–91)
(s+b/2+ jR) (s+b/2− jR)
where the K constant can be found to be equal to jc/2Rand constant K is the
1 2
complex conjugate of K , or K = –jc/2R. (To learn the details of partial frac-
1 2
tion expansion methods, the interested reader should investigate standard
college algebra or engineering mathematics textbooks.) Thus, H(s) can be of
c
the form in Eq. (6–65) or
jc/2R −jc/2R
H (s)= + . (6–92)
c (s+b/2+ jR) (s+b/2− jR)
We can see from Eq. (6–92) that our 2nd-order prototype filter has two poles,
one located at p =–b/2– jRand the other at p =–b/2+ jR. We’re now ready
1 2
to map those two poles from the s-plane to the z-plane as called out in
Method 2, Step 4. Making our 1 – e–pktsz–1 substitution for the s+ p terms in
k
Eq.(6–92), we have the following expression for thez-domain single-pole dig-
ital filters:
jc/2R −jc/2R
H(z)= + . (6–93)
1−e −(b/2+jR)t sz −1 1−e −(b/2−jR)t sz −1
Our objective in Method 2, Step 5, is to massage Eq. (6–93) into the form of
Eq. (6–69), so that we can determine the IIR filter’s feedforward and feedback
coefficients. Putting both fractions in Eq. (6–93) over a common denominator
gives us
(jc/2R)(1−e −(b/2−jR)t sz−1)−(jc/2R)(1−e −(b/2+jR)t sz−1)
H(z)= . (6–94)
(1−e −(b/2+jR)t sz−1)(1−e −(b/2−jR)t sz−1)
Collecting like terms in the numerator and multiplying out the denominator
gives us
(jc/2R)(1−e −(b/2−jR)t sz−1−1+e −(b/2+jR)t sz−1)
H(z)= 1−e −(b/2−jR)t sz−1−e −(b/2+jR)t sz−1+e −bt sz−2 . (6–95)
Factoring the exponentials and collecting like terms of powers of z in Eq.
(6–95),

6.10 Impulse Invariance IIR Filter Design Method 315
H(z)= (jc/2R)(e −(b/2+jR)t s −e −(b/2−jR)t s)z −1 . (6–96)
1−(e −(b/2−jR)t s +e −(b/2+jR)t s)z −1+e −bt sz −2
Continuing to simplify our H(z) expression by factoring out the real part of
the exponentials,
H(z)= (jc/2R)e −bt s /2(e −jRt s −ejRt s)z −1 . (6–97)
1−e −bt s /2(ejRt s +e −jRt s)z −1+e −bt sz −2
We now have H(z) in a form with all the like powers of zcombined into single
terms, and Eq. (6–97) looks something like the desired form of Eq. (6–69).
Knowing that the final coefficients of our IIR filter must be real numbers, the
question is “What do we do with those imaginary j terms in Eq. (6–97)?”
Once again, Euler to the rescue.† Using Euler’s equations for sinusoids, we
can eliminate the imaginary exponentials and Eq. (6–97) becomes
H(z)= (jc/2R)e −bt s /2[−2jsin(Rt s )]z −1
1−e −bt s /2[2cos(Rt )]z −1+e −bt sz −2
s
= (c/R)e −bt s /2[sin(Rt s )]z −1 . (6–98)
1−e −bt s /2[2cos(Rt )]z −1+e −bt sz −2
s
If we plug the values c=17410.145, b=137.94536, R=112.48517, and t =0.01
s
into Eq. (6–98), we get the following IIR filter transfer function:
(154.77724)(0.50171312)(0.902203655)z
−1
H(z)=
1−(0.50171312)(0.86262058)z −1+0.25171605z −2
70.059517z
−1
= . (6–99)
1−0.43278805z −1+0.25171605z −2
Because the transfer function H(z) =Y(z)/X(z), we can again cross-multiply
the denominators to rewrite Eq. (6–99) as
Y(z) ⋅ (1−0.43278805z −1+0.25171605z −2)=X(z) ⋅ (70.059517z −1),
or
Y(z)=70.059517 ⋅ X(z)z −1+0.43278805 ⋅ Y(z)z −1−0.25171605 ⋅ Y(z)z −2 . (6–100)
†From Euler, we know that sin(ø)=(ejø– e–jø)/2j, and cos(ø)=(ejø+ e–jø)/2.

316 Infinite Impulse Response Filters
Now we take the inverse z-transform of Eq. (6–100), by inspection, to get the
time-domain expression for our IIR filter as
y(n)=70.059517 ⋅ x(n−1)+0.43278805 ⋅ y(n−1)−0.25171605 ⋅ y(n−2).
(6–101)
One final step remains. To force the IIR filter gain to be equal to the prototype
analog filter’s gain, we multiply the x(n–1) coefficient by the sample period t
s
as suggested in Method 2, Step 6. In this case, there’s only one x(n) coefficient,
giving us
y(n)=0.01 ⋅ 70.059517 ⋅ x(n−1)+0.43278805 ⋅ y(n−1)−0.25171605 ⋅ y(n−2)
⋅ ⋅ ⋅
=0.70059517 x(n−1)+0.43278805 y(n−1)−0.25171605 y(n−2).
(6–102)
That compares well with the Method 1 result in Eq. (6–86). (Isn’t it comforting
to work a problem two different ways and get the same result?)
Figure 6–35 shows, in graphical form, the result of our IIR design exam-
ple. Thes-plane pole locations of the prototype filter and thez-plane poles of
the IIR filter are shown in Figure 6–35(a). Because thes-plane poles are to the
left of the origin and thez-plane poles are inside the unit circle, both the pro-
totype analog and the discrete IIR filters are stable. We find the prototype fil-
ter’s s-plane pole locations by evaluating H(s) in Eq. (6–92). When
c
s=–b/2–jR,the denominator of the first term in Eq. (6–92) becomes zero and
H (s) is infinitely large. That s = –b/2 – jR value is the location of the lower
c
s-plane z-plane
Imag(s) Imag(z)
Stable Unstable
region region Unstable
region
Stable
+jR region
−1 θ 1
(a) −b/2 Real(s) −θ Real(z)
−jR
|H(f)| in dB
0
−2
(b)
−4
−6
−8
0 20 Hz 50 Hz Freq
(f/2)
s
Figure 6–35 Impulse invariance design example filter characteristics: (a)s-plane pole
locations of prototype analog filter and z-plane pole locations of dis-
crete IIR filter; (b) frequency magnitude response of the discrete IIR filter.

6.10 Impulse Invariance IIR Filter Design Method 317
s-plane pole in Figure 6–35(a). When s = –b/2 + jR, the denominator of the
second term in Eq. (6–92) becomes zero ands=–b/2+jRis the location of the
seconds-plane pole.
The IIR filter’s z-plane pole locations are found from Eq. (6–93). If we
multiply the numerators and denominators of Eq. (6–93) by z,
H(z)
⋅z
=
z(jc/2R)
+
z(−jc/2R)
z z(1−e −(b/2+jR)t sz −1) z(1−e −(b/2−jR)t sz −1)
(jc/2R)z (−jc/2R)z
= + . (6–103)
z−e −(b/2+jR)t
s
z−e −(b/2−jR)t
s
In Eq. (6–103), when z is set equal to e(–b/2+jR)ts, the denominator of the first
term in Eq. (6–103) becomes zero and H(z) becomes infinitely large. The value
of zof
z=e −(b/2+jR)t s =e −bt s /2e −jRt s =e −bt s /2∠ −Rt
s
radians (6–104)
defines the location of the lower z-plane pole in Figure 6–35(a). Specifically,
this lower pole is located at a distance of e–bts/2=0.5017 from the origin, at an
angle of θ=–Rt radians, or –64.45°. Being conjugate poles, the upperz-plane
s
pole is located the same distance from the origin at an angle of θ=Rt radians,
s
or +64.45°. Figure 6–35(b) illustrates the frequency magnitude response of the
IIR filter in Hz.
Two different implementations of our IIR filter are shown in Figure 6–36.
Figure 6–36(a) is an implementation of our 2nd-order IIR filter based on the
general IIR structure given in Figure 6–22, and Figure 6–36(b) shows the 2nd-
order IIR filter implementation based on the alternate structure from Figure
6–21(b). Knowing that the b(0) coefficient on the left side of Figure 6–36(b) is
zero, we arrive at the simplified structure on the right side of Figure 6–36(b).
Looking carefully at Figure 6–36(a) and the right side of Figure 6–36(b), we
can see that they are equivalent.
Although both impulse invariance design methods are covered in the lit-
erature, we might ask, “Which one is preferred?” There’s no definite answer
to that question because it depends on the H(s) of the prototype analog filter.
c
Although our Method 2 example above required more algebra than Method 1,
if the prototype filter’s s-domain poles were located only on the real axis,
Method 2 would have been much simpler because there would be no com-
plex variables to manipulate. In general, Method 2 is more popular for two
reasons: (1) the inverse Laplace and z-transformations, although straightfor-
ward in our Method 1 example, can be very difficult for higher-order filters,
and (2) unlike Method 1, Method 2 can be coded in a software routine or a
computer spreadsheet.

318 Infinite Impulse Response Filters
+
b(1) = 0.70059517
a(1) = 0.43278805
a(1) a(2)
a(2) = –0.25171605
(a)
x(n)
+ z–1 z–1
y(n)
b(1)
x(n) y(n) x(n)
+ + +
z–1 z–1
b(0)
y(n)
(b)
a(1)
z–1
b(1) a(1)
z–1
b(1)
a(2) a(2)
Figure 6–36 Implementations of the impulse invariance design example filter.
Upon examining the frequency magnitude response in Figure 6–35(b),
we can see that this 2nd-order IIR filter’s roll-off is not particularly steep. This
is, admittedly, a simple low-order filter, but its attenuation slope is so gradual
that it doesn’t appear to be of much use as a lowpass filter.† We can also see
that the filter’s passband ripple is greater than the desired value of 1 dB in
Figure 6–34. What we’ll find is that it’s not the low order of the filter that con-
tributes to its poor performance, but the sampling rate used. That 2nd-order
IIR filter response is repeated as the shaded curve in Figure 6–37. If we in-
creased the sampling rate to 200 Hz, we’d get the frequency response shown
by the dashed curve in Figure 6–37. Increasing the sampling rate to 400 Hz re-
sults in the much improved frequency response indicated by the solid line in
the figure. Sampling rate changes do not affect our filter order or implementa-
tion structure. Remember, if we change the sampling rate, only the sample
period t changes in our design equations, resulting in a different set of filter
s
coefficients for each new sampling rate. So we can see that the smaller we
†Apiece of advice: whenever you encounter any frequency representation (be it a digital filter
magnitude response or a signal spectrum) that has nonzero values at +f/2, be suspicious—be
s
very suspicious—that aliasing is taking place.

6.11 Bilinear Transform IIR Filter Design Method 319
|H(f)|
1
0.9 f = 100 Hz
s
0.8
0.7
0.6
f = 200 Hz
0.5 s
0.4
0.3
0.2
0.1 f s = 400 Hz
0
0 20 Hz when 20 Hz when f /2 Freq
s
f s = 400 Hz f s = 100 Hz
Figure 6–37 IIR filter frequency magnitude response, on a linear scale, at three
separate sampling rates. Notice how the filter’s absolute cutoff fre-
quency of 20 Hz shifts relative to the different f sampling rates.
s
make t (larger f), the better the resulting filter when either impulse invari-
s s
ance design method is used because the replicated spectral overlap indicated
in Figure 6–32(b) is reduced due to the larger f sampling rate. The bottom line
s
here is that impulse invariance IIR filter design techniques are most appropri-
ate for narrowband filters, that is, lowpass filters whose cutoff frequencies are
much smaller than the sampling rate.
The second analytical technique for analog filter approximation, the bi-
linear transform method, alleviates the impulse invariance method’s aliasing
problems at the expense of what’s called frequency warping. Specifically,
there’s a nonlinear distortion between the prototype analog filter’s frequency
scale and the frequency scale of the approximating IIR filter designed using
the bilinear transform. Let’s see why.
6.11 BILINEAR TRANSFORM IIR FILTER DESIGN METHOD
There’s a popular analytical IIR filter design technique known as the bilinear
transformmethod. Like the impulse invariance method, this design technique
approximates a prototype analog filter defined by the continuous Laplace
transfer function H(s) with a discrete filter whose transfer function is H(z).
c
However, the bilinear transform method has great utility because
• it allows us simply to substitute a function of z for s in H(s) to get H(z),
c
thankfully eliminating the need for Laplace and z-transformations as
well as any necessity for partial fraction expansion algebra;

320 Infinite Impulse Response Filters
• it maps the entires-plane to thez-plane, enabling us to completely avoid
the frequency-domain aliasing problems we had with the impulse in-
variance design method; and
• it induces a nonlinear distortion of H(z)’s frequency axis, relative to the
original prototype analog filter’s frequency axis, that sharpens the final
roll-off of digital lowpass filters.
Don’t worry. We’ll explain each one of these characteristics and see exactly
what they mean to us as we go about designing an IIR filter.
If the transfer function of a prototype analog filter is H(s), then we can
c
obtain the discrete IIR filter z-domain transfer function H(z) by substituting
the following forsin H(s)
c
2
⎛ 1−z–1⎞
s=
t ⎝ ⎜ 1+z–1⎠ ⎟ (6–105)
s
where, again, t is the discrete filter’s sampling period (1/f). Just as in the im-
s s
pulse invariance design method, when using the bilinear transform method,
we’re interested in where the analog filter’s poles end up on thez-plane after
the transformation. Thiss-plane toz-plane mapping behavior is exactly what
makes the bilinear transform such an attractive design technique.†
Let’s investigate the major characteristics of the bilinear transform’s
s-plane to z-plane mapping. First we’ll show that any pole on the left side
of the s-plane will map to the inside of the unit circle in the z-plane. It’s
easy to show this by solving Eq. (6–105) for z in terms of s. Multiplying
Eq. (6–105) by (t /2)(1 + z–1) and collecting like terms of z leads us to
s
1+(t /2)s
z= s . (6–106)
1−(t /2)s
s
If we designate the real and imaginary parts ofsas
s= σ+ jω , (6–107)
a
where the subscript in the radian frequency ω signifies analog, Eq. (6–106)
a
becomes
1+σt /2+ jωt /2 (1+σt /2)+ jωt /2
z= s a s = s a s . (6–108)
1−σt /2− jωt /2 (1−σt /2)− jωt /2
s a s s a s
†The bilinear transform is a technique in the theory of complex variables for mapping a func-
tion on the complex plane of one variable to the complex plane of another variable. It maps cir-
cles and straight lines to straight lines and circles, respectively.

6.11 Bilinear Transform IIR Filter Design Method 321
We see in Eq. (6–108) that z is complex, comprising the ratio of two complex
expressions. As such, if we denote zas a magnitude at an angle in the form of
z=|z|∠θ , we know that the magnitude of zis given by
z
Mag (1+σt /2)2 +(ωt /2)2
|z|= numerator = s a s . (6–109)
Mag (1−σt /2)2 +(ωt /2)2
denominator s a s
OK, if σ is negative (σ < 0), the numerator of the ratio on the right side of
Eq.(6–109) will be less than the denominator, and |z| will be less than 1. On
the other hand, if σ is positive (σ > 0), the numerator will be larger than the
denominator, and |z| will be greater than 1. This confirms that when using
the bilinear transform defined by Eq. (6–105), any pole located on the left side
of thes-plane (σ<0) will map to az-plane location inside the unit circle. This
characteristic ensures that any stable s-plane pole of a prototype analog filter
will map to a stablez-plane pole for our discrete IIR filter. Likewise, any ana-
log filter pole located on the right side of the s-plane (σ > 0) will map to a
z-plane location outside the unit circle when using the bilinear transform.
This reinforces our notion that to avoid filter instability, during IIR filter de-
sign, we should avoid allowing anyz-plane poles to lie outside the unit circle.
Next, let’s show that the jω axis of thes-plane maps to the perimeter of
a
the unit circle in the z-plane. We can do this by setting σ = 0 in Eq. (6–108) to
get
1+ jωt /2
z= a s . (6–110)
1− jωt /2
a s
Here, again, we see in Eq. (6–110) that zis a complex number comprising
the ratio of two complex numbers, and we know the magnitude of this z is
given by
Mag (1)2 +(ωt /2)2
|z|σ=0 =
Mag
numerator =
(1)2 +(ω
a
t
s
/2)2
. (6–111)
denominator a s
The magnitude of zin Eq. (6–111) is always1. So, as we stated, when using the
bilinear transform, the jω axis of thes-plane maps to the perimeter of the unit
a
circle in thez-plane. However, this frequency mapping from thes-plane to the
unit circle in the z-plane is not linear. It’s important to know why this fre-
quency nonlinearity, or warping, occurs and to understand its effects. So we
shall, by showing the relationship between the s-plane frequency and the
z-plane frequency that we’ll designate as ω .
d
If we define z on the unit circle in polar form as z =
re–jω
d as we did for
Figure 6–13, where r is 1 and ω is the angle, we can substitute z = ejω d in
d
Eq.(6–105) as

322 Infinite Impulse Response Filters
s= 2 ⎛ ⎜ 1−e −jω d ⎞ ⎟. (6–112)
t ⎝1+e −jω d ⎠
s
If we show s in its rectangular form and partition the ratio in brackets into
half-angle expressions,
s=σ+ jω = 2 ⋅e −jω d /2(ejω d /2 −e −jω d /2) . (6–113)
a t e −jω d /2(ejω d /2 +e −jω d /2)
s
Using Euler’s relationships of sin(ø) = (ejø – e–jø)/2j and cos(ø) = (ejø + e–jø)/2,
we can convert the right side of Eq. (6–113) to rectangular form as
s=σ+ jω = 2 ⋅e −jω d /2[2jsin(ω d /2)]
a t e −jω d /2[2cos(ω /2)]
s d
= 2 ⋅2e −jω d /2 ⋅ jsin(ω d /2)
t 2e −jω d /2 cos(ω /2)
s d
j2
= tan(ω /2). (6–114)
t d
s
If we now equate the real and imaginary parts of Eq. (6–114), we see that
σ=0, and
⎛ω ⎞
2
ω = tan⎜ d⎟, −(cid:4)≤ω ≤(cid:4). (6–115)
a t ⎝ 2 ⎠ a
s
The analog frequency ω (radians/second) can have any value and its equiva-
a
lent f cyclic frequency is
a
ω
f = a Hz. (6–115’)
a 2π
Rearranging Eq. (6–115) to give us the useful expression for the z-domain fre-
quency ω , in terms of the s-domain frequency ω , we write
d a
⎛ω t ⎞
ω =2tan −1 ⎜ a s⎟, −π≤ω ≤π. (6–116)
d ⎝ 2 ⎠ d
It’s critical to notice that the range of ω is ±π, and the dimensions of dig-
d
ital frequency ω are radians/sample (not radians/second). The important re-
d
lationship in Eq. (6–116), which accounts for the so-called frequency warping

6.11 Bilinear Transform IIR Filter Design Method 323
ω
d
π
0 ω
a
–π
Figure 6–38 Nonlinear relationship between thez-domain frequency ω and the
d
s-domain frequency ω .
a
due to the bilinear transform, is illustrated in Figure 6–38. Notice that because
tan–1(ω t/2) approaches π/2 as ω becomes large, ω must then approach
a s a d
twice that value, or π. This means that no matter how large the s-plane’s ana-
log ω becomes, the z-plane’s ω will never be greater than π radians/sample
a d
(f/2 Hz).
s
Remember how we considered Figure 6–14 and stated that only the –π f
s
to +π f radians/second frequency range for ω can be accounted for on the
s a
z-plane? Well, our new mapping from the bilinear transform maps the entire
s-plane to the z-plane, and not just the primary strip of the s-plane shown in
Figure 6–14. Now, just as a walk along the jω frequency axis on the s-plane
a
takes us to infinity in either direction, a trip halfway around the unit circle in
a counterclockwise direction takes us from ω =0 to ω =+(cid:4)radians/second.
a a
As such, the bilinear transform maps the s-plane’s entire jω axis onto the unit
a
circle in the z-plane. We illustrate these bilinear transform mapping proper-
ties in Figure 6–39.
s-plane jω a z-plane z imag
ω a = +∞ Unstable
ω
d
=π region
Stable Unstable corresponds to
region region ω a = +∞ S re ta g b io le n ω d = 0
0
σ z
real
ω = –π
d
corresponds to
ω
a
=–∞
ω
a
=–∞
Figure 6–39 Bilinear transform mapping of the s-plane to the z-plane.

324 Infinite Impulse Response Filters
In an attempt to show the practical implications of this frequency warp-
ing, let’s relate the s-plane and z-plane frequencies to a more practical mea-
sure of frequencies in Hz. Because a ω frequency of ω = π radians/sample
d d
corresponds to a cyclic frequency of f/2 Hz, we relate ω and a digital cyclic
s d
frequency f using
d
ω = 2π(f /f) Hz. (6–117)
d d s
Substituting Eq. (6–117) into Eq. (6–115), and recalling that ω =2πf , gives us
a a
f tan
−1(πf
/ f )
f = s a s Hz. (6–118)
d π
Solving Eq. (6–118) for f yields
d
f tan(πf / f )
f = s d s Hz. (6–119)
a π
Equation (6–119) is plotted in Figure 6–40(a). Equations (6–118) and (6–119)
are very useful! They relate the analog s-plane frequency f in Hz to the digital
a
z-plane’s warped frequency f in Hz. This important nonlinear relationship is
d
plotted in Figure 6–40(b). There we see that the f frequency warping (com-
d
pression) becomes more severe as f approaches f/2.
d s
So what does all this f to f mapping rigmarole mean? It means two
a d
things. First, if a bandpass analog filter’s upper cutoff frequency is f Hz, a
a1
bilinear-transform-designed digital bandpass filter operating at a sample rate
of f Hz will have an upper cutoff frequency of f Hz as shown in Figure
s d1
6–40(c). Likewise if a bilinear-transform-designed digital bandpass filter is
desired to have an upper cutoff frequency of f Hz, then the original proto-
d1
type analog bandpass filter must be designed (prewarped) to have an upper
cutoff frequency of f Hz using Eq. (6–118).
a1
Second, no IIR filter response aliasing can occur with the bilinear trans-
form design method. No matter what the shape, or bandwidth, of the |H (f )|
a a
prototype analog filter, none of the |H (f )| magnitude responses can extend
d d
beyond half the sampling rate of f/2 Hz—and that’s what makes the bilinear
s
transform IIR filter design method as popular as it is.
The steps necessary to perform an IIR filter design using the bilinear
transform method are as follows:
Step 1: Obtain the Laplace transfer function H(s) for the prototype analog fil-
c
ter in the form of Eq. (6–43).
Step 2: Determine the digital filter’s equivalent sampling frequency f and es-
s
tablish the sample period t =1/f.
s s

6.11 Bilinear Transform IIR Filter Design Method 325
f
d
0.5f
s
(a)
0
0 f 2f 3f 4f f
s s s s a
f = ftan–1( f/f)/
fd fd d s a s
(b) ( ) H f | d d
|
0
f
a
|H(f)|
a a
0 f
a
|H(f)| |H(f)|
a a d d
(c)
0 f f f 0 f f f/2 f
a0 a1 a d0 d1 s d
Figure 6–40 Nonlinear relationship between the f and f frequencies: (a) fre-
d a
quency warping curve; (b) s-domain frequency response transfor-
mation to a z-domain frequency response; (c) example |H(f)|
a a
and transformed |H (f)|.
d d
Step 3: In the Laplace H(s) transfer function, substitute the expression
c
2 ⎛ 1−z −1⎞
⎜ ⎟ (6–120)
t ⎝1+z −1⎠
s
for the variablesto get the IIR filter’s H(z) transfer function.
Step 4: Multiply the numerator and denominator of H(z) by the appropriate
power of (1 + z–1) and grind through the algebra to collect terms of
like powers of zin the form

326 Infinite Impulse Response Filters
∑N
b(k)z
−k
H(z)= k=0 . (6–121)
∑M
1− a(k)z −k
k=1
Step 5: Just as in the impulse invariance design methods, by inspection, we
can express the IIR filter’s time-domain equation in the general form
of
y(n)=b(0)x(n)+b(1)x(n−1)+b(2)x(n−2)+...+b(N)x(n−N)
+a(1)y(n−1)+a(2)y(n−2)+...+a(M)y(n−M). (6–122)
Although the expression in Eq. (6–122) only applies to the filter struc-
ture in Figure 6–18, to complete our design, we can apply the a(k) and
b(k) coefficients to the improved IIR structure shown in Figure 6–22.
To show just how straightforward the bilinear transform design method
is, let’s use it to solve the IIR filter design problem first presented for the im-
pulse invariance design method.
6.11.1 Bilinear Transform Design Example
Again, our goal is to design an IIR filter that approximates the 2nd-order
Chebyshev prototype analog lowpass filter, shown in Figure 6–26, whose
passband ripple is 1 dB. The f sampling rate is 100 Hz (t = 0.01), and the fil-
s s
ter’s 1 dB cutoff frequency is 20 Hz. As before, given the original prototype
filter’s Laplace transfer function as
17410.145
H (s)= , (6–123)
c s2 +137.94536s+17410.145
and the value of t = 0.01 for the sample period, we’re ready to proceed with
s
Step 3. For convenience, let’s replace the constants in Eq. (6–123) with vari-
ables in the form of
c
H (s)= (6–124)
c s2 +bs+c
where b = 137.94536 and c = 17410.145. Performing the substitution of Eq.
(6–120) in Eq. (6–124),

6.11 Bilinear Transform IIR Filter Design Method 327
c
H(z)= . (6–125)
⎛ 2 ⋅1−z −1⎞2 2 ⎛ 1−z −1⎞
⎜ ⎟
+b
⎜ ⎟
+c
⎝t 1+z −1⎠ t ⎝1+z −1⎠
s s
To simplify our algebra a little, let’s substitute the variable a for the fraction
2/t to give
s
c
H(z)= . (6–126)
⎛ 1−z −1⎞2 ⎛ 1−z −1⎞
a2⎜ ⎟ +ab⎜ ⎟ +c
⎝1+z −1⎠ ⎝1+z −1⎠
Proceeding with Step 4, we multiply Eq. (109)’s numerator and denominator
by (1+z–1)2to yield
c(1+z −1)2
H(z)= . (6–127)
a2(1 − z −1)2 + ab(1 + z −1)(1 − z −1) + c(1 + z −1)2
Multiplying through by the factors in the denominator of Eq. (6–127), and col-
lecting like powers of z,
c(1+2z −1+z −2)
H(z)= . (6–128)
(a2 +ab+c)+(2c−2a2)z −1+(a2 +c−ab)z −2
We’re almost there. To get Eq. (6–128) into the form of Eq. (6–121) with a con-
stant term of one in the denominator, we divide Eq. (6–128)’s numerator and
denominator by (a2+ab+c), giving us
c (1+2z −1+z −2)
H(z)=
(a2 +ab+c)
. (6–129)
1+
(2c−2a2)
z −1+
(a2 +c−ab)
z −2
(a2 +ab+c) (a2 +ab+c)
We now have H(z) in a form with all the like powers of zcombined into single
terms, and Eq. (6–129) looks something like the desired form of Eq. (6–121). If
we plug the values a = 2/t = 200, b = 137.94536, and c = 17410.145 into
s
Eq.(6–129), we get the following IIR filter transfer function:
0.20482712(1+2z −1+z −2)
H(z)=
1−0.53153089z −1+0.35083938z −2
0.20482712+0.40965424z −1+0.20482712z −2
= , (6–130)
1−0.53153089z −1+0.35083938z −2

328 Infinite Impulse Response Filters
and there we are. Now, by inspection of Eq. (6–130), we get the time-domain
expression for our IIR filter as
y(n) = 0.20482712 · x(n) + 0.40965424 ·x(n–1) + 0.20482712 ·x(n–2)
+ 0.53153089 ·y(n–1) – 0.35083938 ·y(n–2). (6–131)
The frequency magnitude response of our bilinear transform IIR design
example is shown as the dark curve in Figure 6–41(a), where, for comparison,
we’ve shown the result of that impulse invariance design example as the
shaded curve. Notice how the bilinear-transform-designed filter’s magnitude
response approaches zero at the folding frequency of f/2=50 Hz. This is as it
s
should be—that’s the whole purpose of the bilinear transform design
method. Figure 6–41(b) illustrates the nonlinear phase response of the bilin-
ear-transform-designed IIR filter.
We might be tempted to think that not only is the bilinear transform de-
sign method easier to perform than the impulse invariance design method,
but that it gives us a much sharper roll-off for our lowpass filter. Well, the fre-
quency warping of the bilinear transform method does compress (sharpen)
the roll-off portion of a lowpass filter, as we saw in Figure 6–40, but an addi-
tional reason for the improved response is the price we pay in terms of the
|H (f )|
d d
1
0.8 Impulse invariance
design
0.6
0.4
Bilinear transform
0.2
design
(a)
50 Hz 20 Hz 0 20 Hz 50 Hz Freq
(fs /2) (fs /2)
Degrees
180
140
100
Phase of bilinear
60
transform design
20
(b)
–50 Hz –20 50 Hz Freq
(–fs /2) –60 (f s /2)
–100
–140
–180
Figure 6–41 Comparison of the bilinear transform and impulse invariance design
IIR filters: (a) frequency magnitude responses; (b) phase of the bilin-
ear transform IIR filter.

6.11 Bilinear Transform IIR Filter Design Method 329
x(n) y(n)
z–1 b(0) z–1 b(0) = 0.20482712
b(1) = 0.40965424
b(2) = 0.20482712
b(1) a(1) a(1) = 0.53153089
z–1 z–1
a(2) = –0.35083938
b(2) a(2)
Figure 6–42 Implementation of the bilinear transform design example filter.
additional complexity of the implementation of our IIR filter. We see this by
examining the implementation of our IIR filter as shown in Figure 6–42. No-
tice that our new filter requires five multiplications per filter output sample
where the impulse invariance design filter in Figure 6–28(a) required only
three multiplications per filter output sample. The additional multiplications
are, of course, required by the additional feedforward zterms in the numera-
tor of Eq. (6–130). These added b(k) coefficient terms in the H(z) transfer func-
tion correspond to zeros in the z-plane created by the bilinear transform that
did not occur in the impulse invariance design method.
Because our example prototype analog lowpass filter had a cutoff fre-
quency that was f/5, we don’t see a great deal of frequency warping in the
s
bilinear transform curve in Figure 6–41. (In fact, Kaiser has shown that when
f is large, the impulse invariance and bilinear transform design methods re-
s
sult in essentially identical H(z) transfer functions[18].) Had our cutoff fre-
quency been a larger percentage of f , bilinear transform warping would have
s
been more serious, and our resultant |H (f )| cutoff frequency would have
d d
been below the desired value. What the pros do to avoid this is to prewarpthe
prototype analog filter’s cutoff frequency requirement before the analog H(s)
c
transfer function is derived in Step 1.
In that way, they compensate for the bilinear transform’s frequency warping
before it happens. We can use Eq. (6–115) to determine the prewarped prototype
analog filter lowpass cutoff frequency that we want mapped to the desired IIR
lowpass cutoff frequency. We plug the desired IIR cutoff frequency ω in
d
Eq.(6–115) to calculate the prototype analog ω cutoff frequency used to derive the
a
prototype analog filter’s H(s) transfer function.
c
Although we explained how the bilinear transform design method avoids
the impulse invariance method’s inherent frequency response aliasing, it’s im-
portant to remember that we still have to avoid filter input data aliasing. No
matter what kind of digital filter or filter design method is used, the original

330 Infinite Impulse Response Filters
input signal data must always be obtained using a sampling scheme that
avoids the aliasing described in Chapter 2. If the original input data contains er-
rors due to sample rate aliasing, no filter can remove those errors.
Our introductions to the impulse invariance and bilinear transform de-
sign techniques have, by necessity, presented only the essentials of those two
design methods. Although rigorous mathematical treatment of the impulse in-
variance and bilinear transform design methods is inappropriate for an intro-
ductory text such as this, more detailed coverage is available to the interested
reader[20,21,25,26]. References [25] and [26], by the way, have excellent mater-
ial on the various prototype analog filter types used as a basis for the analyti-
cal IIR filter design methods. Although our examples of IIR filter design using
the impulse invariance and bilinear transform techniques approximated ana-
log lowpass filters, it’s important to remember that these techniques apply
equally well to designing bandpass and highpass IIR filters. To design a high-
pass IIR filter, for example, we’d merely start our design with a Laplace trans-
fer function for the prototype analog highpass filter. Our IIR digital filter
design would then proceed to approximate that prototype highpass filter.
As we have seen, the impulse invariance and bilinear transform design
techniques are both powerful and a bit difficult to perform. The mathematics is
intricate and the evaluation of the design equations is arduous for all but the
simplest filters. As such, we’ll introduce a third class of IIR filter design meth-
ods based on software routines that take advantage of iterative optimizationcom-
puting techniques. In this case, the designer defines the desired filter frequency
response, and the algorithm begins generating successive approximations until
the IIR filter coefficients converge (ideally) to an optimized design.
6.12 OPTIMIZED IIR FILTER DESIGN METHOD
The final class of IIR filter design methods we’ll introduce is broadly catego-
rized as optimization methods. These IIR filter design techniques were devel-
oped for the situation when the desired IIR filter frequency response was not
of the standard lowpass, bandpass, or highpass form. When the desired
response has an arbitrary shape, closed-form expressions for the filter’s
z-transform do not exist, and we have no explicit equations to work with to
determine the IIR filter’s coefficients. For this general IIR filter design prob-
lem, algorithms were developed to solve sets of linear, or nonlinear, equations
on a computer. These software routines mandate that the designer describe,
in some way, the desired IIR filter frequency response. The algorithms, then,
assume a filter transfer function H(z) as a ratio of polynomials in zand start to
calculate the filter’s frequency response. Based on some error criteria, the al-
gorithm begins iteratively adjusting the filter’s coefficients to minimize the
error between the desired and the actual filter frequency response. The
process ends when the error cannot be further minimized, or a predefined

6.12 Optimized IIR Filter Design Method 331
number of iterations has occurred, and the final filter coefficients are pre-
sented to the filter designer. Although these optimization algorithms are too
mathematically complex to cover in any detail here, descriptions of the most
popular optimization schemes are readily available in the literature [20,21,
29–34].
The reader may ask, “If we’re not going to cover optimization methods
in any detail, why introduce the subject here at all?” The answer is that if we
spend much time designing IIR filters, we’ll end up using optimization tech-
niques in the form of computer software routines most of the time. The vast
majority of commercially available digital signal processing software pack-
ages include one or more IIR filter design routines that are based on optimiza-
tion methods. When a computer-aided design technique is available, filter
designers are inclined to use it to design the simpler lowpass, bandpass, or
highpass forms even though analytical techniques exist. With all due respect
to Laplace, Heaviside, and Kaiser, why plow through all the z-transform de-
sign equations when the desired frequency response can be applied to a soft-
ware routine to yield acceptable filter coefficients in a few seconds?
As it turns out, using commercially available optimized IIR filter de-
sign routines is very straightforward. Although they come in several fla-
vors, most optimization routines only require the designer to specify a few
key amplitude and frequency values, and the desired order of the IIR filter
(the number of feedback taps), and the software computes the final feed-
forward and feedback coefficients. In specifying a lowpass IIR filter, for
example, a software design routine might require us to specify the values
for δ , δ , f , and f shown in Figure 6–43. Some optimization design rou-
p s 1 2
tines require the user to specify the order of the IIR filter. Those routines
then compute the filter coefficients that best approach the required fre-
quency response. Some software routines, on the other hand, don’t require
|H (f)|
d
1
1–δ
p
δ
s
0
0 f 1 f 2 f s /2 Freq
Figure 6–43 Example lowpass IIR filter design parameters required for the opti-
mized IIR filter design method.

332 Infinite Impulse Response Filters
the user to specify the filter order. They compute the minimum order of
the filter that actually meets the desired frequency response.
6.13 A BRIEF COMPARISON OF IIR AND FIR FILTERS
The question naturally arises as to which filter type, IIR or FIR, is best suited
for a given digital filtering application. That’s not an easy question to answer,
but we can point out a few factors that should be kept in mind. First, we can
Table 6–1 IIR and Nonrecursive FIR Filter Characteristics Comparison
Characteristic IIR FIR (nonrecursive)
Number of necessary Least Most
multiplications
Sensitivity to filter Can be high.* (24-bit coefficients Very low (16-bit coeffi-
coefficient quantization needed for high-fidelity audio) cients satisfy most FIR
filter requirements)
Probability of overflow Can be high* Very low
errors
Stability Must be designed in Guaranteed
Linear phase No Guaranteed**
Can simulate prototype Yes No
analog filters
Required coefficient Least Most
memory
Hardware filter control Moderate Simple
complexity
Availability of design Good Very good
software
Ease of design, or Moderately complicated Simple
complexity of design
software
Difficulty of quantization Most complicated Least complicated
noise analysis
Supports adaptive With difficulty Yes
filtering
* These problems can be minimized through cascade or parallel implementations.
** Guaranteed so long as the FIR coefficients are symmetrical (or antisymmetrical).

References 333
assume that the differences in the ease of design between the two filter types
are unimportant. There are usually more important performance and imple-
mentation properties to consider than design difficulty when choosing be-
tween an IIR and an FIR filter. One design consideration that may be
significant is the IIR filter’s ability to simulate a predefined prototype analog
filter. FIR filters do not have this design flexibility.
From a hardware standpoint, with so many fundamental differences be-
tween IIR and FIR filters, our choice must be based on those filter characteris-
tics that are most and least important to us. For example, if we need a filter
with exactly linear phase, then an FIR filter is the only way to go. If, on the
other hand, our design requires a filter to accept very high data rates and
slight phase nonlinearity is tolerable, we might lean toward IIR filters with
their reduced number of necessary multipliers per output sample.
One caveat, though: Just because an FIR filter has, say, three times the
number of multiplies per output sample relative to an IIR filter, that does not
mean the IIR filter will execute faster on a programmable DSP chip. Typical
DSP chips have a zero-overhead looping capability whose parallelism speeds
the execution of multiply and accumulate (MAC) routines, with which FIR fil-
tering is included. The code for IIR filtering has more data/coefficient pointer
bookkeeping to accommodate than FIR filter code. So, if you’re choosing be-
tween an IIR filter requiring Kmultiplies per output sample and an FIR filter
needing 2K (or 3K) multiplies per output sample, code both filters and mea-
sure their execution speeds.
Table 6–1 presents a brief comparison of IIR and FIR filters based on sev-
eral performance and implementation properties.
REFERENCES
[1] Churchill, R. V. Modern Operational Mathematics in Engineering, McGraw-Hill, New York,
1944, pp. 307–334.
[2] Aseltine, J. A. Transform Method in Linear System Analysis, McGraw-Hill, New York, 1958,
pp. 287–292.
[3] Nixon, F. E. Handbook of Laplace Transformation: Tables and Examples, Prentice Hall, Engle-
wood Cliffs, New Jersey, 1960.
[4] Kaiser, J. F. “Digital Filters,” in System Analysis by Digital Computer, ed. by F. F. Kuo and J. F.
Kaiser, John Wiley and Sons, New York, 1966, pp. 218–277.
[5] Kaiser, J. F. “Design Methods for Sampled Data Filters,”Proc. First Annual Allerton Confer-
enceon Circuit and System Theory,1963, Chapter 7, pp. 221–236.
[6] Ragazzini, J. R., and Franklin, G. F. Sampled-Data Control Systems, McGraw-Hill, New York,
1958, pp. 52–83.

334 Infinite Impulse Response Filters
[7] Milne-Thomson, L. M. The Calculus of Finite Differences, Macmillan, London, 1951, pp.
232–251.
[8] Truxal, J. G. Automatic Feedback Control System Synthesis, McGraw-Hill, New York, 1955,
p. 283.
[9] Blackman, R. B. Linear Data-Smoothing and Prediction in Theory and Practice, Addison-
Wesley, Reading, Massachusetts, 1965, pp. 81–84.
[10] Oppenheim, A., Schafer, R., and Buck, J. Discrete-Time Signal Processing, 2nd ed., Prentice
Hall, Upper Saddle River, New Jersey, 1999, pp. 306–307.
[11] Gold, B., and Jordan, K. L., Jr. “ANote on Digital Filter Synthesis,” Proceedings of the IEEE,
Vol. 56, October 1968, p. 1717.
[12] Rabiner, L. R., et al. “Terminology in Digital Signal Processing,” IEEE Trans. on Audio and
Electroacoustics, Vol. AU-20, No. 5, December 1972, p. 327.
[13] Jackson, L. B. “On the Interaction of Roundoff Noise and Dynamic Range and Dynamic
Range in Digital Filters,” Bell System Technical Journal, Vol. 49, February 1970, pp. 159–184.
[14] Jackson, L. B. “Roundoff Noise Analysis for Fixed-Point Digital Filters Realized in Cas-
cade or Parallel Form,” IEEE Trans. Audio Electroacoustics, Vol. AU-18, June 1970,
pp.107–122.
[15] Sandberg, I. W. “ATheorem Concerning Limit Cycles in Digital Filters,” Proc. Seventh An-
nual Allerton Conference on Circuit and System Theory, Monticello, Illinois, October 1969.
[16] Ebert, P. M., et al. “Overflow Oscillations in Digital Filters,” Bell System Technical Journal,
Vol. 48, November 1969, pp. 2999–3020.
[17] Oppenheim, A. V. “Realization of Digital Filters Using Block Floating Point Arithmetic,”
IEEE Trans. Audio Electroacoustics, Vol. AU-18, June 1970, pp. 130–136.
[18] Kaiser, J. F. “Some Practical Considerations in the Realization of Linear Digital Filters,”
Proc. Third Annual Allerton Conference on Circuit and System Theory, 1965, pp. 621–633.
[19] Rabiner, L. R., and Rader, C. M., eds. Digital Signal Processing, IEEE Press, New York, 1972,
p. 361.
[20] Oppenheim, A. V., and Schafer, R. W. Discrete-Time Signal Processing, Prentice Hall, Engle-
wood Cliffs, New Jersey, 1989, p. 406.
[21] Rabiner, L. R., and Gold, B. Theory and Application of Digital Signal Processing, Prentice
Hall, Englewood Cliffs, New Jersey, 1975, p. 216.
[22] Grover, D. “Subject: Re: How to Arrange the (Gain, Pole, Zero) of the Cascaded Biquad
Filter.”Usenet group comp.dsppost, December 28, 2000.
[23] Grover, D., and Deller, J. Digital Signal Processing and the Microcontroller, Prentice Hall,
Upper Saddle River, New Jersey, 1998.

References 335
[24] Stearns, S. D. Digital Signal Analysis, Hayden Book Co., Rochelle Park, New Jersey, 1975,
p. 114.
[25] Stanley, W. D., et al. Digital Signal Processing, Reston Publishing Co., Reston, Virginia,
1984, p. 191.
[26] Williams, C. S. Designing Digital Filters, Prentice Hall, Englewood Cliffs, New Jersey, 1986,
pp. 166–186.
[27] Johnson, M. “Implement Stable IIR Filters Using Minimal Hardware,” EDN, April 14,
1983.
[28] Oppenheim, A. V., Willsky, A. S., and Young, I. T. Signals and Systems, Prentice Hall, Engle-
wood Cliffs, New Jersey, 1983, p. 659.
[29] Deczky, A. G. “Synthesis of Digital Recursive Filters Using the Minimum P Error Crite-
rion,” IEEE Trans. on Audio and Electroacoustics, Vol. AU-20, No. 2, October 1972, p. 257.
[30] Steiglitz, K. “Computer-Aided Design of Recursive Digital Filters,” IEEE Trans. on Audio
and Electroacoustics, Vol. 18, No. 2, 1970, p. 123.
[31] Richards, M. A. “Application of Deczky’s Program for Recursive Filter Design to the De-
sign of Recursive Decimators,” IEEE Trans. on Acoustics, Speech, and Signal Processing, Vol.
ASSP-30, October 1982, p. 811.
[32] Parks, T. W., and Burrus, C. S. Digital Filter Design, John Wiley and Sons, New York, 1987,
p. 244.
[33] Rabiner, L., Graham, Y., and Helms, H. “Linear Programming Design of IIR Digital Filters
with Arbitrary Magnitude Functions,” IEEE Trans. on Acoustics, Speech, and Signal Process-
ing., Vol. ASSP-22, No. 2, April 1974, p. 117.
[34] Friedlander, B., and Porat, B. “The Modified Yule-Walker Method of ARMASpectral Esti-
mation,” IEEE Trans. on Aerospace Electronic Systems, Vol. AES-20, No. 2, March 1984,
pp.158–173.

336 Infinite Impulse Response Filters
CHAPTER 6 PROBLEMS
6.1 Review the z-plane depiction in Figure P6–1. Draw a rough sketch of the
Laplace s-plane showing a shaded area (on the s-plane) that corresponds to
the shaded circular band in Figure P6–1.
Imag.
z-plane
Real
–1 1
Figure P6–1
6.2 Write the H(z) z-domain transfer function equations for the filters described
by the following difference equations:
(a) y(n) = x(n) – y(n–2),
(b) y(n) = x(n) + 3x(n–1) + 2x(n–2) – y(n–3),
(c) y(n) = x(n) + x(n–1) + x(n–3) + x(n–4) – y(n–2).
6.3 Knowing the order of a digital filter is important information. It typically
gives us a direct indication of the computational workload (number of addi-
tions and multiplications) necessary to compute a single filter output sample.
State the order of the filters in Problem 6.2.
6.4 Write the H(ω) frequency response equations, in both polar and rectangular
form, for the filters in Problem 6.2. By “polar form” we mean we want H(ω)
expressed as a ratio of terms using
e–jkω
, where kis an integer. By “rectangular
form” we mean we want H(ω) expressed as a ratio in the form of
a+ jb
c+ jd
where a, b, c, and dare cosine and/or sine functions whose arguments are kω.
(Note:Thisproblemisnot“busywork.”TherectangularformofH(ω)istheex-
pressionyouwouldmodelusinggenericsignalprocessingsoftwaretocompute
andplotafilter’smagnitudeandphaseresponseinthefrequencydomain.)

Chapter 6 Problems 337
6.5 Considering the z-domain transfer function associated with a digital filter:
(a) What does it mean if the filter has one or more poles outside the z-plane’s
unit circle?
(b) What does it mean if the filter has a zero lying exactly on the z-plane’s
unit circle?
6.6 In the literature of DSP, we usually see filter transfer functions expressed in
terms of z where z always has a negative exponent. But sometimes we see
transfer functions in terms of z having positive exponents. For example, you
might encounter an IIR filter’s transfer function expressed as
z2 +0.3z+1
H(z)= .
z2 +0.3z+0.8
(a) What is the transfer function expression equivalent to H(z) in terms of z
with zhaving negative-only exponents?
(b) Is this IIR filter stable? Justify your answer.
(c) Draw the Direct Form I structure (block diagram), showing the filter’s co-
efficients.
(d) Draw the Direct Form II structure, showing the filter’s coefficients.
6.7 Although we didn’t need to use the z-transform to analyze the tapped-delay
line (nonrecursive) FIR filters in Chapter 5, we could have done so. Let’s try
an FIR filter analysis example using the z-transform. For the filter in Figure
P6–7:
x(n) x(n–1) x(n–2)
z–1 z–1
h(0) h(1) h(2)
y(n)
Figure P6–7
(a) Write the time-domain difference equation describing the filter output
y(n) in terms of the x(n) input and the h(k) coefficients.
(b) Write the z-transform of the y(n) difference equation from Part (a).
(c) Write the z-domain transfer function, H(z) = Y(z)/X(z), of the filter.
(d) What is the order of this FIR filter?

338 Infinite Impulse Response Filters
6.8 Thinking about IIR digital filters:
(a) Is it true that to determine the frequency response of an IIR filter, we need
to know both the filter’s time-domain difference equation andthe impulse
response of that filter? Explain your answer.
(b) If we know the H(z) z-domain transfer function equation for a digital fil-
ter, what must we do to determine the frequency response of that filter?
6.9 Draw the Direct Form I and the Direct Form II block diagrams of the filter
represented by the following z-domain transfer function:
H (z)= ⎛ ⎜ 1 ⎞ ⎟ ⋅ ( 1−4z −1+2z −2 ) .
cas ⎝ 1−0.3z −1⎠
6.10 Consider the two filters in Figure P6–10. (Notice the minus sign at the first
adder in Figure P6–10(b).) Determine whether each filter is an IIR or an FIR
filter. Justify your answers.
FilterH FilterH
1 2
x(n) y(n) x(n) y(n)
–
z–1 z–1 z–1 z–1
z–1 z–1
z–1 z–1
(a) (b)
Figure P6–10
6.11 The author once read a design document describing how an engineer was
tasked to implement Network A in Figure P6–11(a), using a programmable
DSP chip, as part of a specialized digital filter. The engineer suggested that,
due to the chip’s internal architecture, for computational speed reasons Net-
work B shown in Figure P6–11(b) should be used instead of Network A. He
also stated that the frequency magnitude responses of the two networks are
identical. Is that last statement true? Justify your answer.

Chapter 6 Problems 339
Network-A
x(n) y(n)
(a) z–1 z–1 z–1
Network-B
x(n) y(n)
(b)
z–1 z–1 z–1
Figure P6–11
6.12 Prove that the z-plane pole locations for the two filters in Figure P6–12 are
identical.
Hint: For Filter #2, write two different equations for U(z) and set those equa-
tions equal to each other.
Filter# 1 Filter# 2
X(z) U(z)
X(z) Y(z)
z–1
z–1 Asin(α)
Acos(α)
Y(z)
2Acos(α) Acos(α)
z–1
z–1
–A2
–Asin(α)
Figure P6–12
6.13 The discrete-sequence output of commercial analog-to-digital (A/D) convert-
ers is often contaminated with a DC bias (a constant-level amplitude offset).
Stated in different words, even though the converter’s analog x(t) input sig-
nal’s average value is zero, the converter’s x(n) output sequence may have a
small nonzero average. As such, depending on the application, A/D convert-
ers are sometimes followed by an IIR filter shown in Figure P6–13 that re-
moves the DC bias level from the filter’s x(n) input sequence. (The coefficient
Ais a positive value slightly less than unity.)

340 Infinite Impulse Response Filters
(a) Derive the z-domain transfer function of the DC bias removal filter.
(b) Prove that the filter has a z-plane zero at z = 1, yielding the desired infi-
nite attenuation at the cyclic frequency of zero Hz.
(c) Draw the block diagram of the Direct Form II version of the DC bias re-
moval filter.
DC bias removal filter
x(t) x(n) y(n)
A/D
z–1 z–1
–1 A
Figure P6–13
6.14 Assume we have the software code to implement a notch filter (a filter that at-
tenuates a very narrow band of frequencies and passes frequencies that are
above and below the notch’s ω center frequency), and the software documen-
c
tation states the filter is defined by the following transfer function:
Y(z) 1−2cos(ω )z −1+z −2 (1−ejω cz −1)(1−e −jω cz −1)
H(z)= = c = .
X(z) 1−2Rcos(ω )z −−1+R2z −2 (1−Rejω cz −11)(1−Re −jω cz −1)
c
(a) If R = 0.9, draw the locations of the notch filter’s poles and zeros on the
z-plane in relation to the notch frequency ω.
c
(b) Let’s say we’re processing the signal from a photodiode light sensor in
our laboratory and our signal’s time samples are arriving at a sample rate
of f = 1.8 kHz. Assume that 120 Hz flicker noise from fluorescent lights is
s
contaminating our photodiode output signal. What would be the correct
value for ω to use in the notch filter code to attenuate the 120 Hz noise?
c
Show your work.
6.15 Show that for a 2nd-order FIR filter, whose z-domain transfer function is
H(z) = 1 + Bz–1+ z–2,
the sum of the locations of the filter’s two z-plane zeros is equal to –B.
6.16 Consider the filter in Figure P6–16.
(a) Determine the z-domain transfer function, H(z) = Y(z)/X(z), of the filter.

Chapter 6 Problems 341
(b) Draw the z-plane pole/zero diagram of the filter.
(c) Using the notion of pole-zero cancellation, draw the block diagram of an
exact equivalent, but simpler, filter having fewer multipliers than shown
in Figure P6–16.
X(z) Y(z)
z–1
2.5 –6
z–1
–1 8
Figure P6–16
6.17 Assume we have a digital filter (having real-valued coefficients) whose com-
plex frequency response is the product of an M(ω) magnitude response and a
θ(ω) phase response as
H(ω)= M(ω)ejθ(ω)
where ω is a normalized frequency variable (in the range of –π to π, corre-
spondingtoacyclicfrequencyrangeof–f/2tof/2Hz)measuredinradians/
s s
sample. Is it possible to have such a real-coefficient filter whose θ(ω) phase re-
sponse is of the form
θ(ω) =C
where Cis a nonzero constant? Explain your answer.
6.18 Determine the H(z) transfer function of the recursive network in Figure P6–18.
2
x(n) w(n) y(n)
z–1 3
0.25 3
Figure P6–18

342 Infinite Impulse Response Filters
6.19 The recursive networks (networks with feedback) that we discussed in this
chapter, if they’re simple enough, can be analyzed with pencil and paper. This
problem gives us practice in such an analysis and prompts us to recall the
process of converting a geometric series into a closed-formequation.
(a) Looking at the discrete network in Figure P6–19, show that the y(n) out-
put is equal to D/Q for large values of time index n when the x(n) input
samples have a constant amplitude of D. (To keep the system stable, as-
sume that Q is a positive number less than one and the network is “at
rest” at time n= 0. That is, w(0) = 0.)
Hint: Write equations for y(n) when n = 0, 1, 2, 3, . . . etc., and develop a
general series expression for the y(n) output in terms of D, Q, and n. Next,
use Appendix B to obtain a closed-form (no summation sign) expression
for the y(n) when nis a large number.
(b) When we arrive at a solution to a problem, it’s reassuring to verify
(double-check) that solution using a different technique. Following this
advice, determine the z-domain H(z) transfer function of the network in
Figure P6–19 and show that its zero Hz (DC) gain is 1/Q, verifying your
solution to Part (a) of this problem.
(c) Prove that the recursive network is stable if Qis in the range 0 < Q≤1.
x(n) + y(n)
+
–
z–1
w(n)
Q
Figure P6–19
6.20 A discrete system that has at least one pole on the z-plane’s unit circle is
called a discrete resonator, such as the system in Figure P6–20. Such resonators
have impulse responses that oscillate indefinitely.
x(n) y(n)
+
–
z–1
Figure P6–20

Chapter 6 Problems 343
(a) Draw the z-plane pole/zero diagram of the resonator in the figure.
(b) At what frequency, measured in terms of the x(n) input f sample rate,
s
does the pole of this system reside?
(c) Draw the time-domain impulse response of the system in Figure P6–20.
(d) Comment on how the frequency of the oscillating impulse response out-
put samples relates to the system’s pole location on the z-plane.
6.21 Given the following six difference equations for various digital filters, deter-
mine which equation is associated with which |H (f)| filter frequency magni-
?
tude response in Figure P6–21. Justify your answers.
y (n) = x(n) + 0.9y (n–1)
1 1
y (n) = x(n) – 0.9y (n–1)
2 2
y (n) = x(n) + 0.81y (n–2)
3 3
y (n) = x(n) – 0.81y (n–2)
4 4
y (n) = x(n) – 1.8cos(π/4)y (n–1) – 0.81y (n–2)
5 5 5
y (n) = x(n) + 1.8cos(π/4)y (n–1) – 0.81y (n–2)
6 6 6
1 1
|H (f)| |H (f)|
A B
0.5 0.5
–f/2 0 f/4 f/2 –f/2 0 f/8 f/2
s s s s s s
1 1
|H (f)| |H (f)|
C D
0.5 0.5
–f/2 0 f/2 –f/2 0 f/2
s 3f/8 s s s
s
1 1
|H (f)| |H (f)|
E F
0.5 0.5
–f/2 0 f/2 –f/2 0 f/2
s s s s
Figure P6–21
6.22 Astandard 2nd-order IIR filter (a biquad) is shown in its Direct Form I struc-
ture in Figure P6–22. Knowing the DC gain (the value H(ω) at ω= 0 radians/
sample) of a filter is critical information when we implement filtering using

344 Infinite Impulse Response Filters
binary arithmetic. What is the DC gain of the filter in terms of the filter’s
coefficients?
Direct Form I biquad
x(n) y(n)
z–1 b(0) z–1
b(1) a(1)
z–1 z–1
b(2) a(2)
Figure P6–22
6.23 Review the brief description of allpassfiltersin Appendix F.
(a) Prove that the 1st-order allpass filter, defined by the following H (z)
ap
transfer function, has an |H (ω)| frequency magnitude response that is
ap
unity over its full operating frequency range of –π≤ω≤πradians/sample
(–f/2 ≤f≤f/2 Hz):
s s
−K+z −1
H (z)= .
ap 1−Kz −1
Variable Kis a real-valued scalar constant.
Hint:Rather than prove |H (ω)| = 1 for all ω, prove that the frequency mag-
ap
nitude response squared, |H (ω)|2, is equal to unity for all ω.
ap
(b) Draw the Direct Form I and Direct Form II block diagrams of the H(z) all-
pass filter.
(c) Explain why the H (z) allpass filter can never have a transfer function
ap
zeroon its z-plane’s unit circle.
6.24 Asimple 1st-order IIR filter, whose z-domain transfer function is
0.9152+0.1889z −1
H (z)= ,
g 1+0.1127z −1
has been proposed for use in synthesizing (simulating) guitar music. Is the
Hg(z) filter a lowpass or a highpass filter? Justify your answer. [Karjalainen,
M., et al. “Towards High-Quality Sound Synthesis of the Guitar and String

Chapter 6 Problems 345
Instruments,” International Computer Music Conference, September 10–15, 1993,
Tokyo, Japan.]
6.25 There are general 2nd-order recursive networks used in practice, such as that
shown in Figure P6–25, where the a(0) coefficient is not unity. Assuming you
need to analyze such a network, determine its z-domain transfer function that
includes the a(0) coefficient. Show your steps.
General 2nd-order A non-unity value
recursive network
x(n) y(n)
z–1 b(0) a(0) z–1
b(1) a(1)
z–1 z–1
b(2) a(2)
Figure P6–25
6.26 Consider the recursive highpass filter shown in Figure P6–26(a).
(a) Derive the H(ω) frequency response equation for the filter.
(b) What is the location of the filter’s single z-plane pole?
(c) The |H(ω)| frequency magnitude response of the filter is shown in Figure
P6–26(b). What are the values of magnitudes M and M ? Show your
0 π
work.
x(n) y(n)
|H( )|
M
2 z–1
M
0
0 2
–e–0.88 Frequency (radians/sample)
(a) (b)
Figure P6–26

346 Infinite Impulse Response Filters
6.27 The recursive network shown in Figure P6–27 can be used to compute the
N-point average of Ninput samples. Although this process works well, it has
the disadvantage that as time index n (where n = 1, 2, 3, 4, ...) increases, it re-
quires the real-time computation of both the 1/nand (n–1)ncoefficients upon
the arrival of each new x(n) input sample.
x(n) y(n)
z–1
1/n
(n–1)/n
Figure P6–27
(a) Aclever DSPengineer always tries to minimize computations. Show how
to modify the network’s diagram so that the real-time coefficient-compu-
tation workload is reduced.
(b) Our N-point averager network has a feedback loop, with possible stabil-
ity problems. Show how your solution to Part (a) of this problem is a sta-
ble network as nincreases starting at n= 1.
6.28 Given the z-plane pole/zero plot, associated with a 2nd-order IIR digital fil-
ter, in Figure P6–28:
(a) What is the H(z) transfer function, in terms of z–1 and z–2, of the Figure
P6–28 filter having two poles and a single zero on the z-plane? Show how
you arrived at your answer.
(b) Draw the Direct Form I block diagram of the H(z) filter that implements
the transfer function arrived at in Part (a) of this problem.
(c) Draw a new block diagram of the H(z) filter that eliminates one of the
multipliers in the Direct Form I block diagram.
Unit circle
z = 0.25 + j0.25
Zero at
z = –1 z = 0.25 – j0.25
Figure P6–28

Chapter 6 Problems 347
6.29 In the text’s Section 6.5 we learned to derive a filter transfer function based on
knowing the locations of the filter’s poles and zeros. We implied that the
roots of polynomial P,
P= z2+ bz+ c,
are equal to the roots of polynomial Q, where
Q= GP= Gz2+ Gbz+ Gc,
with variable G being a real-valued constant. Prove that the roots of P are in-
deed equal to the roots of Q.
6.30 Given the z-plane pole/zero plots in Figure P6–30, associated with the H(z)
transfer functions of four digital filters, draw a rough sketch of the four fil-
ters’ frequency magnitude responses over the frequency range of –f/2 to f/2,
s s
where f is the filter’s input signal sample rate.
s
Note:The two poles, near z= 1 in Figure P6–30(c), are lying exactly on top of
two zeros.
z-plane A z-plane B
1 1
art art
P P
ar y
0
45o ar y
0
45o
gi n –45o gi n –45o
a a
m m
I I
–1 –1
–1 0 1 –1 0 1
(a) (b)
Real Part Real Part
z-plane C z-plane D
1
1
P
art
art
a
gi n
ar y
0
4
–
5
4
o
5o
gi
n ar
y
P
0
m a
I m
I
–1
–1
–1 0 1 –1 0 1
(c) Real Part (d) Real Part
Figure P6–30

348 Infinite Impulse Response Filters
6.31 Assume that you must implement the lowpass H(z) filter shown in Figure
P6–31. Good DSPengineers always simplify their digital networks whenever
possible. Show a simplified block diagram of the filter, without changing the
filter’s frequency response, that has a reduced computational workload and
reduced data storage (number of delay elements).
Hint:Study the filter’s z-plane pole/zero diagram.
x(n) y(n)
–
z–1 z–1
z–1
Figure P6–31
6.32 In Chapter 5 we had a homework problem whose solution revealed that the
3-tap FIR notch filter in Figure P6–32(a) has complex conjugate z-plane zeros
on the unit circle as shown in Figure P6–32(b). That efficient filter, useful for
attenuating narrowband noise located at a normalized frequency of ω (–π ≤
n
ω ≤π), has a frequency magnitude response shown in Figure P6–32(c). If we
n
want the FIR filter’s stopband notches to be narrower, we can implement the
2nd-order IIR filter shown in Figure P6–32(d) that has conjugate z-plane poles
at a radius of Rjust inside the unit circle as shown in Figure P6–32(e). The fre-
quency magnitude response of the IIR notch filter is given in Figure P6–32(f).
Here’s the problem: Express the Figure P6–32(d) IIR filter’s a(1) and a(2) coef-
ficients, in terms of ω and R, that will place the z-plane poles as shown in
n
Figure P6–32(e). Show your work.
Hint:Recall Euler’s identity: 2cos(θ) = (ejθ + e–jθ ).

Chapter 6 Problems 349
3-tap FIR
x(n) y(n)
1
art
z–1 P ω
y n
ar 0
gi
n –ω
n
–2cos(ω) m a
z–1 n I –1
–1 0 1
Real Part
(a) (b)
IIR biquad, Direct Form II
x(n) y(n)
|H (ω)|
0 FIR
B z–1
d
–20
a(1)
–2cos(ω)
n
–40 –π –ω 0 ω π z–1
n n
(–f/2) Freq (f/2)
s s
a(2)
(c) (d)
1 |H (ω)|
0 IIR
art
ar y P 0 R ω n d B
gi
n –ω
n
–20
m a R = 0.9
I
–1
–40
–1 0 1 –π –ω 0 ω π
n n
Real Part (–f/2) Freq (f/2)
s s
(e) (f)
Figure P6–32
6.33 Let’s exercise our IIR filter analysis skills. Suppose your colleague proposes
the 2nd-order IIR filter shown in Figure P6–33(a) to provide narrow passband
filtering as shown in Figure P6–33(b). (The |H(ω)| frequency axis uses the
discrete-signal frequency variable ω(radians/sample) with ω= πcorrespond-
ing to a cyclic frequency of f/2 Hz.)
s

350 Infinite Impulse Response Filters
x(n) y(n)
1
z–1
|H(ω)|
0.5
0.5
z–1
0 0 π/4 π/2 ω π
(f/2)
s
Resonant frequency
A = –0.81
(a) (b)
Figure P6–33
(a) Is this 2nd-order IIR filter unconditionally stable?
(b) OverwhatrangeofnegativevaluesoftheAcoefficientwillthefilterbestable?
(c) For what negative value of A will the filter be conditionally stable (at least
one pole on, and no poles outside, the unit circle)?
(d) What is the resonant frequency (positive frequency) of the filter in terms
of the f sample rate (in Hz) of the x(n) input?
s
Hint: If the z-plane’s positive-frequency pole is near the unit circle, think
about how the angle of that pole is related to the filter’s resonant fre-
quency measured in Hz.
6.34 Think about a 4th-order (5-tap) tapped-delay line finite impulse response
(FIR) filter whose z-domain transfer function is
H(z) = b + b z–1+ b z–2+ b z–3+ b z–4.
0 1 2 3 4
Assume the filter has real-valued b coefficients and that the filter is a linear-
k
phase filter. If one of the filter’s z-plane zeros has a value of z = 0.5657 +
0
j0.5657, what are the values of the other three z-plane zeros of this filter?
6.35 Here’s an interesting problem. As of this writing, in an application note on their
website (www.zilog.com), the skilled folks at Zilog Inc. describe a multistage
digital bandpass filter used to detect the pitch (frequency) of a musical tone. A
two-stage Direct Form II version, where each stage is a 2nd-order IIR filter, of
this detection system is the cascaded bandpass filter shown in Figure P6–35(a).
The frequency magnitude responses of the first and second filters, over the pos-
itive frequency range, are provided in Figure P6–35(b), and the combined (cas-
caded) frequency magnitude response is provided in Figure P6–35(c).
(a) Given that the sample rate of the signal is f = 8000 samples/second, what
s
musical note will the Figure P6–35(a) two-stage bandpass filter detect?
That is, what musical note is closest to the f center frequency of the two-
c

Chapter 6 Problems 351
x(n) y(n)
z–1 0.1032 z–1 0.3034
(a)
1.8275 –0.1837 1.8462 –0.5768
z–1 z–1
–0.9834 0.1032 –0.9843 0.3034
0
dB First filter
–20
(b)
–40
Second filter
–60
0 Freq
0
dB Cascaded filter
–20
(c)
–40
–60
0 Freq
f
c
Figure P6–35
stage filter’s passband in Figure P6–35(c)? Explain how you arrived at
your answer. For your convenience, the frequencies of several musical
notes of an equal-tempered scale are provided in the following table.
Table P6–1 Musical Note Frequencies
Musical note Frequency (Hz) Musical note Frequency (Hz)
C4 261.626 (Middle C) G#4 415.305
C#4 277.183 A4 440.0 (Concert A)
D4 293.665 A#4 466.164
D#4 311.127 B4 493.883
E4 329.628 C5 523.251
F4 349.228 C#5 554.365
F#4 369.994 D5 587.330
G4 391.995 D#5 622.254

352 Infinite Impulse Response Filters
(b) Finally, are the two 2nd-order IIR filters stable? Explain how you arrived
at your answer.
6.36 Consider the Direct Form II IIR filter shown in Figure P6–36, which requires
three multiplies per filter output sample. Smart DSP engineers reduce com-
putations wherever possible. Draw a block diagram of a filter equivalent to
that in Figure P6–36 that requires fewer than three multiplies per filter output
sample.
x(n) y(n)
z–1 B
A B
Figure P6–36
6.37 In high-speed, hardware-only, linear-phase filtering, the transposed structure
of a tapped-delay line FIR filter is often preferred over a traditional tapped-
delay line FIR filter. That’s because the parallel structure of transposed FIR fil-
ters reduces the time required to perform multiple addition operations. Draw
the transposed structure of the traditional FIR filter in Figure P6–37. In your
solution, make sure the x(n) input is on the left-hand side.
x(n)
z–1 z–1
h(0) h(1) h(2)
y(n)
Figure P6–37
6.38 Draw the transposed structures of the networks in Figure P6–38. In your solu-
tions, make sure the x(n) inputs are on the left-hand side.

Chapter 6 Problems 353
x(n) Network I x(n) Network II y(n)
–
A z–1 A
y(n) z–1
B C
(a) (b)
Figure P6–38
6.39 In the text we discussed the problem of limit cycles in IIR filters when finite-
precision values (finite binary word width) are used to represent data values.
To reiterate that concept, the unit-sample impulse response of the 1st-order
IIR filter in Figure P6–39(a) is shown in Figure P6–39(c). That impulse
Floating-point filter x(n) A(n) Round to the y(n)
x(n) y(n) nearest
multiple of q
z–1
z–1
C(n) B(n)
–0.5 –0.5
(a) (b)
1
Floating-pointy(n) when x(n) = 1, 0, 0, 0, 0, ...
0.5
(c)
0
–0.5
0 5 10
n
1
Quantizedy(n) with rounding to nearest q = 0.1,
whenx(n) = 1, 0, 0, 0, 0, ...
0.5
y(6) = 0.1
(d)
0
y(7) = –0.1
–0.5
0 5 10
n
Figure P6–39

354 Infinite Impulse Response Filters
response was computed using the very high precision of a 64-bit floating-
point binary number system within the filter. In Figure P6–39(c) we see that
this stable IIR filter’s y(n) impulse response properly decays toward zero am-
plitude as time advances.
In fixed-point binary filter implementations, if rounding is used to limit
the binary word width (the precision of data sample values) at the output of
the filter’s adder, the ill effects of limit cyclesmay occur. This rounding opera-
tion is shown in Figure P6–39(b) where the y(n) output is rounded to a value
that is a multiple of a rounding precision factor whose value is q. If rounding
to the nearest q = 0.1 value is implemented, the filter’s impulse response ex-
hibits unwanted limit cycles as shown in Figure P6–39(d), where the y(n) im-
pulse response continually oscillates between ±0.1 as time advances.
(a) Reducing the value of the rounding precision factor qis supposed to help
reduce the level of the unwanted limit cycle oscillations. Plot the unit-
sample impulse response of the quantizing filter in Figure P6–39(b) when
q= 0.05.
Note: If an A(n) data value is exactly between two multiples of q, round
away from zero.
(b) Comparing Figure P6–39(c), Figure P6–39(d), and your solution from the
above Part (a), make a statement regarding how the peak-to-peak ampli-
tude of the quantizing filter’s limit cycle behavior is related to the value
of the rounding precision factor q.
6.40 Given the h (k) and h (k) impulse responses of the two filters in Figure P6–40,
1 2
what is the impulse response of the h (k) cascaded combination filter?
Cas
Cascaded filter, h (k)
Cas
x(n) y(n)
h(k) h(k)
1 2
Figure P6–40
6.41 Here’s a problem whose solution may, someday, be useful to the reader. Many
commercial digital filter design software packages require the user to specify
a desired filter’s maximum passband ripple, in terms of a linear peak deviation
parameter represented by R, for a lowpass filter magnitude response in Fig-
ure P6–41.

Chapter 6 Problems 355
|H(ω)|
R P, measured
1
R in dB
The power of 1+R
isP decibels
greater than the
e
ar
al
e
power of 1–R.
nc
Lis
0 f Freq
pass
Figure P6–41
(a) Let’s say that in a lowpass filter design effort, we only know the desired
passband ripple specified in terms of a peak-peak logarithmic (dB) para-
meter Pshown in Figure P6–41. If P= 2 dB, what is R? Stated in different
words, if we only have the P = 2 dB desired passband ripple value avail-
able to us, what R value must we specify in our filter design software?
Show how you arrived at your solution.
(b) Given your solution to the above Part (a), now derive a general equation
that defines the linear R deviation parameter in terms of the logarithmic
(dB) peak-peak passband ripple parameter P.
6.42 Many digital filters are implemented as both cascaded (series) and parallel
combinations of subfilters. Given the four individual H (ω) subfilter fre-
k
quency responses in Figure P6–42, what is the equation for the overall fre-
quency response of this combination of subfilters in terms of H (ω), H (ω),
1 2
H (ω), and H (ω)?
3 4
H(ω)
3
x(n) y(n)
H(ω) H(ω) +
1 2
H(ω)
4
Figure P6–42

356 Infinite Impulse Response Filters
6.43 Many feedback systems can be reduced to the form of the generic feedbacksys-
tem shown in Figure P6–43(a).
x(n) y(n)
x(n) y(n) + A(z) B(z)
+ A(z) – –
–
C(z)
B(z)
D(z)
(a) (b)
Figure P6–43
(a) Prove that the z-domain transfer function of the feedback system in Fig-
ure P6–43(a) is the following expression:
Y(z) A(z)
H(z)= = .
X(z) 1+A(z)B(z)
Note:The above H(z) expression is well known, particularly in the field of
digital control systems, because it is encountered so often in practice.
(b) If we replace the z variable in H(z) with ejω , we obtain an H(ω) equation,
describing the frequency response of the system in Figure P6–43(a),
whose generic form is
Y(ω) A(ω)
H(ω)= = .
X(ω) 1+A(ω)B(ω)
(Notice that we don’t use the
ejω
term, for example
H(ejω
), in our notation
for a frequency response. We use the standard H(ω) notation instead.)
With the above H(ω) equation in mind, what is the expression for the
H(ω) frequency response of the system shown in Figure P6–43(b)?
Hint: Use the principles of cascaded and parallel subsystems to obtain a
simplified network structure.
6.44 In the text we discussed the analysis of digital filters comprising the parallel
combination of two subfilters. Using a highpass filter whose impulse re-
sponse is the h (k) samples in Figure P6–44(a), we can implement a low-
High
pass filter if we’re able to build a parallel network whose impulse response is
the h (k) samples in Figure P6–44(b). The parallel network’s h (k) samples
Par Par
are defined by

Chapter 6 Problems 357
⎧ ⎪ −h (k), when k ≠6
High
h (k)=⎨
Par ⎩⎪ −h (k)+1, when k =66.
High
(a) If the parallel lowpass filter network is that shown in Figure P6–44(c),
what is the impulse response of the h(k) subfilter?
(b) Draw the parallel lowpass filter network showing what processing ele-
ments are in the h(k) subfilter block.
h (k) h (k)
High Par
–h (6) + 1
High
12
0 0
6 k 0 6 12 k
0
(a) (b)
Lowpass filter, h (k)
Par
h(k)
x(n) y(n)
–h (k)
High
(c)
Figure P6–44
6.45 Assume we are given the lowpass filter shown in Figure P6–45 and, based on
the IIR discussion in the text’s Section 6.9, we must scalethe filter to reduce its
passband gain without changing its frequency response shape. Draw a block
diagram of the scaled filter.

358 Infinite Impulse Response Filters
x(n) y(n)
–
z–1 z–M
–
z–1 z–M
–
A B
z–1 z–M
Figure P6–45
6.46 You’re working on a project to upgrade an analog temperature-sensing and
processing system. Your job is to design a digital integrator, to replace an ana-
log integrator whose Laplace s-domain transfer function is
1
H(s)= ,
s
that will receive a new temperature sample once every 2 seconds. Because
ideal integrators have a frequency magnitude response of zero at high fre-
quencies, your digital integrator must have a frequency magnitude response
less than 0.01 at f/2.
s
(a) What is the z-domain transfer function of a digital integrator replacement
for H(s) designed using the impulse invariance Method 2 design tech-
nique?
(b) What is the z-domain transfer function of a digital integrator designed
using the bilinear transform design technique?
(c) Verify that each of your digital integrators has a z-plane pole at the same
frequency at which the H(s) analog integrator had an s-plane pole.
(d) Which of the two digital integrators, from Part (a) and Part (b), will you
submit as your final design, and why?
6.47 Due to its simplicity, the 1st-order analog lowpass filter shown in Figure
P6–47(a) is often used to attenuate high-frequency noise in a v (t) input sig-
in
nal voltage. This lowpass filter’s s-domain transfer function is
1
H(s)= .
1+RCs
(a) Determine a digital filter’s H (z) z-domain transfer function that simu-
ii
lates H(s), using the impulse invariance Method 2 process. Draw the digi-

Chapter 6 Problems 359
tal filter’s Direct Form II block diagram (structure) where the coefficients
are in terms of Rand C. For simplicity, assume that t = 1.
s
(b) Determine a digital filter’s H (z) z-domain transfer function that simu-
bt
lates H(s), using the bilinear transform process. Draw the digital filter’s
Direct Form II block diagram where the coefficients are in terms of Rand
C. Again, assume that t = 1.
s
(c) When properly designed, the filters’ normalized frequency magnitude re-
sponses, |H | and |H |, are those shown in Figure P6–47(b) (plotted on
ii bt
a logarithmic vertical scale). Why does the |H | response have such
bt
large attenuation at high frequencies?
R
(a) v in (t) C v out (t)
0
–20
B
(b) d–40 |H|
ii
|H |
–60 bt
0 f/8 f/4 3f/8 f/2
s s s s
Frequency
Figure P6–47
6.48 A1st-order analog highpass filter’s s-domain transfer function is
s
H(s)= .
s+ω
o
Determine a digital filter’s H(z) z-domain transfer function that simulates
H(s) using the bilinear transform process. Given that frequency ω = 62.832
o
radians/second, assume that the digital filter’s sample rate is f = 100 Hz. Ma-
s
nipulate your final H(z) expression so that it is in the following form:
A+Bz −1
H(z)=
1+Cz −1

360 Infinite Impulse Response Filters
where A, B, and C are constants. The above H(z) form enables convenient
modeling of the digital filter’s transfer function using commercial signal pro-
cessing software.
6.49 Let’s plow through the algebra to design a 2nd-order digital IIR filter that ap-
proximates an analog lowpass filter. Assume the filter’s s-domain transfer
function is
5
H(s)=
s(s−0.8)
and the digital filter’s sample rate is 1000 samples/second. Derive, using the
bilinear transform, the z-domain transfer function equation of the discrete fil-
ter that simulates the above H(s) continuous lowpass filter.
6.50 Let’s say that your colleague has designed a prototype analog lowpass filter
whose cutoff frequency is 3.8 kHz. (By “cutoff frequency” we mean the fre-
quency where the lowpass filter’s magnitude response is 3 dB below its aver-
age passband magnitude response.) Next, assume your colleague wants you
to use the bilinear transform method to design a digital filter whose perfor-
mance is equivalent to that of the analog filter when the sample rate is f = 11
s
kHz.
(a) Given that the analog lowpass filter’s f cutoff frequency is 3.8 kHz, what
a
will be the f cutoff frequency of the digital lowpass filter in Hz?
d
(b) Given that we want the digital lowpass filter’s cutoff frequency to be ex-
actly3.8 kHz, the prototype analog filter will have to be redesigned. What
should be the f cutoff frequency of the new analog lowpass filter?
a

CHAPTER SEVEN
Specialized
Digital
Networks z -N z -1
and Filters
-1 e
jω
r
We begin this chapter by presenting three useful digital networks—differen-
tiators, integrators, and matched filters—that are common in the world of
DSP. Beyond generic applications that require derivatives to be computed,
differentiators are a key component of FM (frequency modulation) demodu-
lation. Acommon application of integration is computing the integral of stock
market prices over some period of days to determine trends in stock price
data. Matched filters are used to detect the arrival of a specific discrete signal
sequence, such as a radar return signal.
Later in this chapter we introduce two specialized implementations of
finite impulse response (FIR) filters: interpolated lowpass FIR filters and fre-
quency sampling filters. The common thread between these two FIR filter
types is that they’re lean mean filtering machines. They wring every last drop
of computational efficiency from a guaranteed-stable linear-phase filter. In
many lowpass filtering applications these FIR filter types can attain greatly
reduced computational workloads compared to the traditional Parks-
McClellan-designed FIR filters discussed in Chapter 5.
We discuss this chapter’s specialized digital networks and FIR filters
now because their behavior will be easier to understand using the
z-transform concepts introduced in the last chapter.
7.1 DIFFERENTIATORS
This section focuses on simple tapped-delay line (FIR) differentiators. The
idea of differentiation is well defined in the world of continuous (analog) sig-
nals, but the notion of derivatives is not strictly defined for discrete signals.
However, fortunately we can approximate the calculus of a derivative
361

362 Specialized Digital Networks and Filters
operation in DSP. To briefly review the notion of differentiation, think about a
continuous sinewave, whose frequency is ωradians/second, represented by
x(t) = sin(2πft) = sin(ωt). (7–1)
The derivative of that sinewave is
dx(t)
=ωcos(ωt).
dt
(7–1’)
So the derivative of a sinewave is a cosine wave whose amplitude is pro-
portional to the original sinewave’s frequency. Equation (7–1) tells us that an
ideal digital differentiator’s frequency magnitude response is a straight line
linearly increasing with frequency ω as shown in Figure 7–1(a). The differen-
tiator’s phase is that shown in Figure 7–1(b), where the digital frequency ω=π
radians/sample is equivalent to half the signal data sample rate in Hz (f/2).
s
π
|Hideal (ω)| = |ω|
π/2
(a)
0
–π –0.5π 0 0.5π π
(–f s /2) Freq (ω) (f s /2)
π
Phase of
π/2
H (ω)
ideal
(b) 0
–π/2
–π
–π –0.5π 0 0.5π π
Freq (ω)
π
Real part of
π/2 H (ω)
ideal
(c) 0
–π/2
–π
–π –0.5π 0 0.5π π
Freq (ω)
π
Imaginary part
π/2
ofH (ω)
ideal
(d) 0
–π/2
–π
–π –0.5π 0 0.5π π
Freq (ω)
Figure 7–1 Ideal differentiator frequency response: (a) magnitude; (b) phase in
radians; (c) real part; (d) imaginary part.

7.1 Differentiators 363
Given the magnitude and phase response of our ideal digital differentia-
tor, we can draw the real and imaginary parts of its frequency response as
shown in Figures 7–1(c) and 7–1(d). (The real part of the response is identi-
cally zero.) What we can say is that our ideal differentiator has the simple fre-
quency response described, in rectangular form, by
H (ω) = jω. (7–2)
ideal
With these thoughts in mind, let’s see how we can build a digital differ-
entiator. We start by exploring two simple discrete-time FIR (nonrecursive)
differentiators: a first-difference and a central-difference differentiator. They are
computationally simple schemes for approximating the derivative of an x(n)
time-domain sequence with respect to time.
7.1.1 Simple Differentiators
With respect to the x(n) samples in Figure 7–2(a), the first-differencedifferentia-
tor is simply the process of computing the difference between successive x(n)
First-difference differentiator
x(n)
y (2) = x(2) –x(1)
Fd
y (1) = x(1) –x(0)
Fd
(a)
. . .
0
0 1 2 n
Central-difference differentiator
x(n)
y (3) = [x(3) –x(1)]/2
(b) Cd
y (2) = [x(2) –x(0)]/2
Cd
0
0 1 2 3 n
First-difference differentiator Central-difference differentiator
x(n) x(n)
z–1 z–1 z–1
(c)
– –
y (n)
y (n) Cd
Fd
0.5
Figure 7–2 Simple differentiators.

364 Specialized Digital Networks and Filters
samples. (While DSP purists prefer to use the terminology digital differencer,
we’ll use the popular term differentiatorfor our purposes.) If we call y (n) the
Fd
output of a first-difference differentiator, then y (n) is
Fd
y (n) = x(n) – x(n–1). (7–3)
Fd
For the x(n) samples in Figure 7–2(b), the central-differencedifferentiator is the
process of computing the average difference between alternate pairs of x(n)
samples. If we call y (n) the output of a central-difference differentiator, then
Cd
y (n) is
Cd
[x(n)−x(n−1)]+[x(n−1)−x(n−2)]
y (n)=
Cd 2
= [x(n) – x(n–2)]/2. (7–4)
The two simple differentiators are implemented with tapped-delay line struc-
tures, just like our standard FIR filters in Chapter 5, as shown in Figure 7–2(c).
(In fact, the two differentiators are merely two different forms of a comb filter,
as discussed in detail in Section 7.5.1, and this is why differentiators are often
called differentiating filters.) So what’s the difference (no pun intended) be-
tween these two simple differentiators? They are different with respect to
their frequency responses, which we now investigate.
The first-difference differentiator is the most fundamental notion of digi-
tal differentiation, i.e., computing the difference between successive samples
of a discrete sequence. The problem with this differentiator is that many real-
world signals have high-frequency spectral components consisting of noise,
and the first-difference differentiator amplifies that noise. The frequency
magnitude response of a first-difference differentiator is
|H (ω)| = 2|sin(ω/2)| (7–5)
Fd
as shown by the dashed curve in Figure 7–3, where it has the characteristic of
a highpass filter. (For comparison, we show an ideal differentiator’s straight-
line |H (ω)| = ω magnitude response in Figure 7–3.) Looking at that
Ideal
dashed curve, we see how the first-difference differentiator tends to amplify
high-frequency spectral components, and this may be detrimental because
real-world signals often contain high-frequency noise.
The central-difference differentiator’s |H (ω)| frequency magnitude
Cd
response, on the other hand, is
|H (ω)| = |sin(ω)| (7–6)
Cd
as shown by the dotted curve in Figure 7–3, and this differentiator can be use-
ful in that it tends to attenuate high-frequency (noise) spectral components.
Looking at the |H (ω)| curve, we see that the price we pay for that high-
Cd
frequency attenuation is a reduction in the frequency range over which the
central-difference differentiator approaches an ideal differentiator’s linear

7.1 Differentiators 365
0
Figure 7–3 Frequency magnitude responses of simple differentiators.
|H (ω)|. The central-difference differentiator’s linear range is from 0 to
Ideal
only, say, 0.2π radians/sample (0.1f Hz). The useful operating frequency
s
ranges of the first-difference and central-difference differentiators are fairly
narrow. This means the differentiators are only accurate when the spectral
content of the input signal is low in frequency with respect to the input sig-
nal’sf sample rate.
s
Another dissimilarity between the Figure 7–2(c) differentiators is their
group delay. Because the impulse response (coefficients) of these tapped-
delay line networks are antisymmetrical, both differentiators have linear
phase responses, and thus both networks have a constant time delay (delay
between the input and output, also called group delay). Like the tapped-delay
line FIR filters in Chapter 5, antisymmetrical-coefficient differentiators have a
group delay (measured in samples) determined by
D
G = samples
diff 2 (7–7)
where D is the number of unit-delay elements in their tapped-delay lines. (D
can also be viewed as one less than the length of the impulse response of a
differentiator.) Hence the first-difference differentiator, where D = 1, has an
input-to-output delay of 1/2 = 0.5 samples. The central-difference differentia-
tor, where D= 2, has a group delay of 2/2 = 1 sample. Whether or not a differ-
entiator’s time delay is an integer number of samples is very important in
applications where multiple-signal sequences must be aligned (synchronized)
in time. (An example of this integer-delay differentiation issue is the FM de-
modulator discussion in Section 13.22.)
DSP folk have improved, in certain respects, upon the above two com-
putationally simple differentiators in an attempt to (1) extend the linear oper-
ating frequency range, (2) continue to attenuate high-frequency spectral
.gaM
tuptuO
2.5
|H (ω)| = ω
Ideal |H (ω)|
Fd
2
1.5
1 |H (ω)|
Cd
0.5
0
0.2π 0.4π 0.6π 0.8π π
(0.1f s ) Freqω (f s /2)

366 Specialized Digital Networks and Filters
components, and (3) keep the number of arithmetic computations as low as
possible. It is to those specialized differentiators that we now turn our at-
tention.
7.1.2 Specialized Narrowband Differentiators
DSPpioneer Richard Hamming provided the following
−3k
h(k)=
M(M+1)(2M+1)
(7–8)
as an expression to compute the coefficients of what he called “low-noise
Lanczos,” differentiating filters having 2M+1 coefficients[1]. Variable k, the in-
teger index of those coefficients, ranges from –M to M. If we set M = 1 in Eq.
(7–8), we obtain the coefficients of the standard central-difference differentia-
tor in Figure 7–2(c). Assigning M= 2 to Eq. (7–8) yields the coefficients
h (k) = 0.2, 0.1, 0, –0.1, –0.2 (7–9)
L
for a five-coefficient differentiator whose |H (ω)| magnitude response is the
L
dotted curve in Figure 7–4. The h (k) differentiator in Eq. (7–9) is of interest
L
because if we’re willing to multiply those coefficients by 10, we have a high-
gain differentiator requiring only two multiplies per output sample. (Happily,
those multiplications can be implemented with a binary arithmetic left shift,
thus eliminating the multiplications altogether.) The disadvantage of this
h (k) differentiator is that its linear operating frequency range is the smallest
L
of any differentiator we’ve considered so far.
Hamming presented two expressions for what he called “super Lanczos
low-noise differentiators.” The first expression yielded the five-coefficient dif-
ferentiator defined by
−1 8 −8 1
h (k)= , , 0, ,
SL1 6 6 6 6 (7–10)
whose normalized |H (ω)| magnitude response is the long-dash curve in
SL1
Figure 7–4. The h (k) differentiator has a wider linear operating frequency
SL1
range than the h (k) differentiator, but at the expense of degraded high-
L
frequency attenuation. However, h (k) is also of interest because if we’re
SL1
willing to multiply the coefficients by 6, we again have a high-gain differen-
tiator requiring only two multiplies per output sample. (Again, those multi-
plications by ±8 can be implemented with binary arithmetic left shifts to
eliminate the multiplication operations.)
Hamming’s second expression for a super Lanczos low-noise differen-
tiator generated the seven-coefficient differentiator defined by
−22 67 58 −58 −67 22
h (k)= , , , 0, , ,
SL2 126 126 126 126 126 126 (7–11)

7.1 Differentiators 367
0
Figure 7–4 Frequency magnitude responses of Lanczos differentiators.
whose normalized |H (ω)| magnitude response is the short-dash curve in
SL2
Figure 7–4. In terms of linear operating frequency range and high-frequency
attenuation, the h (k) differentiator is a reasonable compromise between the
SL2
h (k) and h (k) differentiators. Notice how the h (k) differentiator has a
L SL1 SL2
good high-frequency noise attenuation characteristic. Then again, in one re-
spect, the h (k) differentiator is not all thatsuperbecause it requires six mul-
SL2
tiplies per output sample. (We can do better. Section 13.38 presents a very
computationally efficient narrowband differentiator whose linear operating
frequency range exceeds that of the h (k) differentiator.)
SL1
With the exception of the first-difference differentiator, after account-
ing for their constant integer group delays, all of the above differentiators
achieve the ideal H (ω) phase response in Figure 7–1(b). In the next section
ideal
we introduce high-performance wideband differentiators.
7.1.3 Wideband Differentiators
Nonrecursive discrete-time differentiators having wider linear operating fre-
quency ranges than the above simple differentiators can be built. All we must
do is find the coefficients of a general widebanddifferentiator whose frequency
magnitude response is shown in Figure 7–5(a), having a cutoff frequency of ω .
c
We can derive an equation defining the h (k) coefficients of a general
gen
wideband differentiator by defining those coefficients to be the inverse
Fourier transform of our desired frequency response from Eq. (7–2) of
H (ω) = jωfor continuous ωdefined over the range of –ω ≤ω≤ω . Following
ideal c c
this strategy, the coefficients of our general differentiator are given by
ω ω ω
h (k)= 1 (cid:2) c H (ω)ejωk dω = 1 (cid:2) c j ω e j ωk dω = j (cid:2) c ωe j ωkdω.
gen 2π
−ω
ideal 2π
−ω
2π
−ω
c c c (7–12)
.gaM
tuptuO
2.5
|H (ω)| = ω
Ideal
2
1.5 |H (ω)|
SL1
1
|H (ω)|
SL2
0.5
|H(ω)|
L
0
0.2π 0.4π 0.6π 0.8π π
Freqω
(f/2)
s

368 Specialized Digital Networks and Filters
3
Desired |H (ω)|
gen
2
(a) 1
0
–π 0 π
Freqω
(–f s /2) (f s /2)
–ω ω
c c
1
h (k)
gen
(b) 0
–1
–15 –10 –5 1 5 10 15
k
3
Actual |H (ω)|
gen
2
(c)
1
0
–π 0 π
Freqω
(–f s /2) (f s /2)
–ω ω
c c
Figure 7–5 Frequency response of a generalwideband differentiator: (a) desired
magnitude response; (b) 30 h (k) coefficients; (c) actual magnitude
gen
response.
We can perform the integration in Eq. (7–12) using the dreaded (but useful)
integration by parts method, or by searching our math reference books for a
closed-form integral expression in the form of Eq. (7–12)[2]. Being successful
in this second approach, we find that the integral of
(ωejωk)dω
is
(ejωk)
(jωk–1)/(jk)2. Using this information, we can write
ω
j ⎡ (ejωk)(jωk−1) ⎤ c j ⎡ ejω ck jω ejω ck e − j ω k c − j ω e − j ω ck⎤
h (k)= ⎢ ⎥ = ⎢ − c − + c ⎥
gen 2π ⎣ (jk)2 ⎦ 2π ⎣ k 2 k k2 k ⎦
−ω
c
j ⎡ j2sin(ω k) j2ω cos(ω k) ⎤ ω cos(ω k) sin(ωk)
= ⎢ c − c c ⎥ = c c − c
2π ⎣ k2 k ⎦ π k π k 2
(7–13)
where integer index kis –∞≤k≤∞, and k≠0.
The real-valued h (k) in Eq. (7–13) can be used to compute the coeffi-
gen
cients of a tapped-delay line digital differentiator. This expression, however,
is based on the notion that we need an infinite number of differentiator coeffi-
cients to achieve the desired response in Figure 7–5(a). Because implementing

7.1 Differentiators 369
an infinite-tap differentiator is not possible in our universe, Figure 7–5(b)
shows Eq. (7–13) limited (truncated) to a manageable 30 coefficients, and Fig-
ure 7–5(c) provides the frequency magnitude response of that 30-tap differen-
tiator with ω = 0.85π. (The ripples in that magnitude response are to be
c
expected once we think about it. Truncation in one domain causes ripples in
the other domain, right?)
As a brief aside, if we set ω = π in Eq. (7–13), the coefficients of an
c
N-coefficient differentiator become
(−1)k
h (k)=
ω c=π
k (7–14)
where –(N–1)/2≤k≤(N–1)/2, and k≠0. When index k= 0, h (0) is set to zero.
ω c=π
Equation (7–14) is by far the most popular form given in the standard DSP
textbooks for computing digital differentiator coefficients. Using Eq. (7–14),
however, is only valid for even-order (N is odd) differentiators, and it is ap-
plicable only when the cutoff frequency is ω =π(f/2 Hz).
c s
So where do we stand regarding these wideband differentiators? We’ve
obtained Eq. (7–13) for computing the coefficients of a general wideband dif-
ferentiator. Unfortunately that expression has a time-domain index (k) having
negative values, which can be inconvenient to model using commercial signal
processing software. We’ve discussed the widely disseminated Eq. (7–14) and
mentioned its limitations. Again, we can do better.
For a more useful form of an h (k) expression for an arbitrary-length
gen
N-tap differentiator we propose the following:
ω cos(ω [k−M]) sin(ω [k−M])
h (k)= c c − c
gen π[k−M] π[k−M]2
(7–15)
where M= (N–1)/2, and 0≤k≤N–1. For odd Nwe set h ((N–1)/2), the center
gen
coefficient, to zero. Eq. (7–15) looks a bit messy, but it’s quite practical because
• the differentiator passband width, ω , is a design variable, and not fixed
c
atω =πas in Eq. (7–14);
c
• the number of taps, N, can be odd or even; and
• the coefficient index kis never negative.
Fortunately, because of the range of index k, Eq. (7–15) is straightforward to
model using commercially available signal processing software.
7.1.4 Optimized Wideband Differentiators
For completeness, we point out that the widely available Parks-McClellan al-
gorithm can be used to design wideband digital differentiators whose perfor-
mance is superior to those produced by Eq. (7–15) when the number of taps N

370 Specialized Digital Networks and Filters
Parks-McClellan design
Eq. (7-15) design
0
Figure 7–6 Frequency magnitude responses of 30-tap wideband differentiators.
is greater than, say, 25. That behavior is illustrated in Figure 7–6, where the
solid curve shows the frequency magnitude response of an N = 30 Parks-
McClellan-designed differentiator for ω = 0.85π, and the bold dashed curve is
c
anN= 30 differentiator designed using Eq. (7–15).
What the DSP pioneers found, in studying the Parks-McClellan algo-
rithm, is that it computes coefficients that provide more accurate differentia-
tion when N is even as opposed to when N is odd. (However, we must keep
in mind that the group delay through an even-tap differentiator is not an inte-
ger number of samples, and this could be troublesome in systems that require
time synchronization among multiple signals.) Design curves showing the
relative error for various-length even- and odd-N Parks-McClellan differen-
tiators versus ω are available[3,4].
c
Of course, windowing a wideband differentiator’s coefficients, using one
of the common window sequences described in Chapters 3 and 5, will greatly
reduce the ripples in a differentiator’s magnitude response. (Windowing in
one domain reduces ripples in the other domain, right?) Improved magnitude
response linearity, through time-domain windowing, comes at the expense of
degrading the sharpness of the response’s transition region near ω .
c
7.2 INTEGRATORS
The idea of integration is well defined in the domain of continuous (analog)
signals, but not so clearly defined in the world of discrete signals. With that
said, here we discuss approximating continuous integration by using digital
filters that perform numerical integration of sampled signals. We’ll discuss
digital integration networks whose outputs estimate the area under a contin-
uous curve such as the x(t) function shown in Figure 7–7(a).
.gaM
tuptuO
3
Ideal
2
1
0
π/2 π
Freqω
(f/2)
s
ω = 0.85π
c

7.2 Integrators 371
7.2.1 Rectangular Rule Integrator
One simple way to estimate, to approximate, the area under the x(t) curve is
to merely sum the x(n) samples. Such a rectangular rule integrator computes
the sum of the shaded rectangles shown in Figure 7–7(b). In the time domain
we define the rectangular rule integrator, a running summation, as
y (n) = x(n) + y (n–1), (7–16)
Re Re
where the current sum, y (n), is the previous y (n–1) sum plus the current
Re Re
input sample x(n). When n = 2, for example, Eq. (7–16) adds the area under
the right-side shaded rectangle shown in Figure 7–7(b) to the previous sum
y (1) to compute y (2). The height and width of that right-side shaded rec-
Re Re
tangle are x(2) and one, respectively.
x(n) Continuousx(t)
(a)
0
0 1 2 n
x(n)
Rectangular rule
(b) ~~ 1.5
~~
0.5 0.5 ~~
1
2
.5
.5
–0.5
0
0 1 2 n
x(n)
Trapezoidal rule
(c) 1 2
~~ ~~
0 1
0
0 1 2 n
x(n)
Simpson's rule
2
(d) ~
0
0
0 1 2 n
Figure 7–7 Integrator areas of summation.

372 Specialized Digital Networks and Filters
The frequency response of this rectangular rule integrator is
1
H (ω)= .
Re 1−e −jω
(7–16’)
7.2.2 Trapezoidal Rule Integrator
Auseful area integration estimation scheme is the trapezoidal ruledefined by
y (n) = 0.5x(n) + 0.5x(n–1) + y (n–1). (7–17)
Tr Tr
When n = 2, for example, Eq. (7–17) computes the area (the average of x(2) +
x(1)) under the right-side shaded trapezoid shown in Figure 7–7(c) and adds
that value to the previous y (1) to compute y (2). The frequency response of
Tr Tr
the trapezoidal rule integrator is
0.5+0.5e −jω
H (ω) = .
Tr 1−e −jω
(7–17’)
7.2.3 Simpson’s Rule Integrator
Apopular discrete-time integration scheme is Simpson’sruledefined by
y (n) = [x(n) + 4x(n–1) + x(n–2)]/3 + y (n–2), (7–18)
Si Si
where three samples are used to compute the area under the single shaded
curve in Figure 7–7(d). The frequency response of the Simpson’s rule integra-
tor is
1+4e −jω +e −j2ω
H (ω)= .
Si 3(1−e −j2ω )
(7–18’)
(Simpson’s rule is named after the eighteenth-century English mathematician
Thomas Simpson. Oddly enough, Simpson’s rule was actually developed by
Sir Isaac Newton rather than Simpson. But don’t hold that against Simpson
because the famous iterative method for finding the roots of a polynomial,
calledNewton’s method, was developed by Simpson!)
The above three time-domain integration approximations were devel-
oped using the principles of polynomial curve fitting where Simpson’s rule fits
three signal samples to a second-order polynomial in x, the trapezoidal rule
fits two samples to a first-order polynomial in x, and the rectangular rule uses
a single sample in a zero-order polynomial in x.

7.2 Integrators 373
7.2.4 Tick’s Rule Integrator
For completeness, we point out an integration approximation similar to
Simpson’s rule that you may encounter in the literature of DSP called Tick’s
rule. It’s defined as
y (n) = 0.3584x(n) + 1.2832x(n–1) + 0.3584x(n–2) + y (n–2) (7–19)
Ti Ti
having a frequency response given by
0.3584+1.2832e −jω +0.3584e −j2ω
H (ω)= .
Ti 1−e −j2ω
(7–19’)
The Tick’s rule integrator was designed to be especially accurate over the
low-frequency range of 0≤ω≤π/2 radians/sample (zero to f/4 Hz) with little
s
concern for its accuracy at higher frequencies[5].
7.2.5 Integrator Performance Comparison
OK, so how well do the above discrete integrators perform? We can measure
their performance by comparing their behavior to an ideal continuous (ana-
log) integrator. Doing so, we first recall that the integral of the continuous
function cos(ωt) is
∫ cos(ωt)dt=
sin(ωt)
,
ω
(7–20)
telling us that if we apply a sinusoid to an ideal integrator, the output of the
integrator will be a sinusoid, phase-shifted by –π/2 radians (–90o), whose
amplitude is reduced by a factor of 1/ω. Thus the frequency magnitude re-
sponse of an ideal integrator is |H | = |1/ω| as shown in Figure 7–8(a),
ideal
and the integrator’s phase is that shown in Figure 7–8(b), where the digital
frequency ω = π radians/sample is equivalent to half the signal data sample
rate in Hz (f/2).
s
The frequency magnitude responses of an ideal integrator and our four
digital integrators are shown in Figure 7–9 over various bandwidths in the
positive-frequency range of 0≤ω≤π. For ease of comparison, the magnitude
curves are all normalized so that their peak values, in the vicinity of ω= 0, are
unity. (Note that the ideal integrator’s response curve in Figure 7–9(b) is ob-
scured by the Simpson’s rule and Tick’s rule curves.) What we see from Fig-
ure 7–9 is that the various integrators have very little difference over the ω
frequency range of 0 to π/2 radians/sample (zero to f/4 Hz), but above that
s
range there are meaningful differences that we’ll discuss in a moment.
The magnitude response curves in Figure 7–9 are a bit difficult to inter-
pret when a linear magnitude axis is used. With that thought in mind, Figure

374 Specialized Digital Networks and Filters
100
|H (ω)| = |1/ω)
ideal
50
(a)
0
–π –0.5π 0 0.5π π
(–f s /2) Freq ω (f s /2)
π
π/2
(b) 0
Phase of
–π/2
H (ω)
–π ideal
–π –0.5π 0 0.5π π
Freq ω
100
Real part of
50 H (ω)
ideal
(c) 0
–50
–100
–π –0.5π 0 0.5π π
Freq ω
100
Imaginary part
50 of H (ω)
ideal
(d) 0
–50
–100
–π –0.5π 0 0.5π π
Freq ω
Figure 7–8 Ideal integrator frequency response: (a) magnitude; (b) phase in radi-
ans; (c) real part; (d) imaginary part.
7–10 shows the various integrators’ percent absolute errors using logarithmic
axis scaling. We defined the percent absolute error as
H (ω) − H (ω)
integrator ideal
Percent absolute error= 100 .
H (ω)
ideal (7–21)
Looking at the error curves in Figure 7–10 might cause you to think,
“These integrators aren’t very accurate. For example, the Simpson’s rule inte-
grator has roughly a 7 percent error at ω= 0.5π(f/4 Hz).” Well, the situation
s
is not as bad as it first appears. Looking at the ideal integrator’s response in
Figure 7–9(a), we must realize that a 7 percent error of the small magnitude
response values near ω = 0.5π is not nearly as significant as a 7 percent error
for the larger magnitude response values below ω = 0.1π. So this means our

7.2 Integrators 375
1
Ideal
0.036
0.8 Rectangular rule
Trapezoidal rule
0.6 Simpson’s rule 0.032
Tick’s rule
0.4 0.028
0.2
0.024
0
0 0.25π 0.5π 0.75π π 0.225π 0.25π 0.275π 0.3π
Freqω (f s /2) Freqω (0.15f s )
(a) (b)
0.03 0.04
0.025
0.03
0.02
0.02
0.015
0.01
0.01
0.3π 0.4π 0.5π 0.6π 0.7π 0.7π 0.8π 0.9π
(0.15f
s
) Freqω (0.35f
s
) (0.35f
s
) Freqω (0.45f
s
)
(c) (d)
Figure 7–9 Normalized frequency magnitude responses of four integrators.
simple integrators are quite accurate at low frequencies where we most need
high accuracy.
What we learn from Figure 7–10 is that all of the digital integrators have
good accuracy at low frequencies, with the Tick’s rule and Simpson’s rule in-
tegrators being the most accurate. (The phrase “low frequencies” means that
the spectral components of the function, the signal, we are trying to integrate
are low in frequency relative to the f sample rate.) However, if the integra-
s
tors’ input signals have appreciable noise spectral components near f/2 Hz,
s
100
10
0.1
0.01
0
Figure 7–10 Integrator absolute errors in percent.
|rorrE|
tnecreP 1 Rectangular rule
Trapezoidal rule
Simpson’s rule
Tick’s rule
0.2π 0.4π 0.5π 0.6π 0.8π π
(f s /4) Freqω (f s /2)

376 Specialized Digital Networks and Filters
the Tick’s rule and Simpson’s rule integrators will amplify that noise because
those integrators have z-domain transfer function poles (infinite gain) at z =
–1, corresponding to a cyclic frequency of f/2 Hz. In such high-frequency
s
noise scenarios the rectangular or trapezoidal rule integrators should be used
because they provide improved attenuation of spectral components in the
vicinity of f/2 Hz.
s
The integrators that we’ve discussed are interesting because they are re-
cursive networks and they all have linear phase. However, only the trape-
zoidal, Simpson’s, and Tick’s rule integrators achieve the ideal H (ω) phase
ideal
response in Figure 7–8(b).
The above integrators all have z-domain transfer function poles at z = 1,
corresponding to a cyclic frequency of zero Hz, and this has an important con-
sequence when we implement integrators in digital hardware. Those poles
force us to ensure that the numerical format of our integrator hardware can ac-
commodate summation results when the x(n) input sequence has a nonzero
average value (a constant amplitude bias). Stated in different words, the
widths of our binary data registers must be large enough to guarantee that any
nonzero amplitude bias on x(n) will not cause numerical overflow and corrupt
the data within an integrator. Chapter 10’s discussion of cascaded integrator-
comb (CIC) filters elaborates on this integrator data register width issue.
7.3 MATCHED FILTERS
In this section we introduce a signal processing operation known as matched
filtering. A matched filter is a process that maximizes the signal-power-to-
noise-power ratio (SNR) of its y(t) output when a specified x (t) signal of in-
s
terest arrives at its input. Such a process is widely used in radar, sonar, oil
exploration, digital communications systems, and frequency-domain process-
ing of two-dimensional images. Those systems are designed to reliably detect
(recognize) if, and at what instant in time or position in space, a well-defined
s(t) signal of interest arrived at their inputs.
Matched filtering, for continuous signals, is depicted in Figure 7–11(a).
In that figure the system’s x (t) input signal is an x (t) signal of interest,
in s
which may be a radar signal or perhaps a small portion of a digital communi-
cations signal, contaminated with an x (t) noise signal. The matched filter’s
n
task is to maximize the SNR of the y(t) signal so that reliable detection of x (t)
s
can be performed.
7.3.1 Matched Filter Properties
So the question is “What should the frequency response of the matched filter
be to maximize our chances of detecting the presence of x (t) in the noisy x (t)
s in

7.3 Matched Filters 377
x(t) signal-of-interest
s
was detected
x (t) =
in
x s (t) + x n (t) Analog y(t) Threshold Yes
matched
detection
(a) filter No
x(t) signal-of-interest,
s x(t) signal-of-interest
contaminated with x(t) noise s
n was not detected
and other interference
x (n) =
in
(b) x s (n) + x n (n) Discrete matched y(n) thre T s o hold
filter, h(k) detection
Figure 7–11 Matched filtering: (a) continuous signal implementation; (b) digital
implementation with h(k) impulse response.
signal?” The answer can be found in most communications textbooks[6], and
here’s how we interpret that answer. Given the S(f) spectrum of x (t), the de-
s
sire to maximize the SNR of y(t), lots of calculus, plus an application of
Schwarz’s inequality, the optimum H(f) frequency response of the matched
filter can be shown to be
H(f) =
S*(f)e–j2πfT
(7–22)
where Tis the time duration of the x (t) signal measured in seconds, and * sig-
s
nifies conjugation. Equation (7–22) tells us that the optimum H(f) frequency
response of the continuous matched filter should be the complex conjugate of
the spectrum of our signal of interest multiplied by a phase shift that’s a lin-
ear function of frequency. Stated in different words, the time-domain impulse
response of the optimum matched filter is the inverse Fourier transform of
S*(f) shifted in the negative-time direction by T seconds. We now provide a
physical meaning of all of this as we determine how to implement a matched
filter in discrete-time sampled systems.
To show how to build a digital (discrete-time) matched filter, as shown
in Figure 7–11(b), first we need to determine the h(k) impulse response of the
filter. Let’s make the following assumptions:
• Our discrete signal of interest is an N-samplex (n) sequence.
s
• S(m) is the N-point discrete Fourier transform (DFT) of x (n).
s
• mis the discrete frequency index 0 ≤m≤N–1.
• The x (n) data input sample rate is f samples/second.
in s

378 Specialized Digital Networks and Filters
Under these assumptions we convert the continuous frequency fin Eq. (7–22)
to the DFT’s discrete frequencies of mf/N to express the digital matched fil-
s
ter’s H(m) discrete frequency response as
H(m) =
S*(m)e–j2πmfsT/N
(7–23)
where Tis the time duration of the x (n) signal. OK, our next step is to define
s
T such that the inverse DFT of H(m), our desired h(k), is the inverse DFT of
S*(m) shifted in the negative-time direction by an amount equal to the time
duration of x (n). This sounds a bit complicated, but it’s really not so bad, as
s
we shall see.
To determine T in Eq. (7–23) we need to know how many sample peri-
ods (with 1/f being one period) comprise the time duration of an N-sample
s
x (n) sequence. The answer is: The time duration of an N-sample discrete se-
s
quence is N–1 sample periods. (Readers should convince themselves that this
is true.) So T in Eq. (7–23) is (N–1)/f seconds, and we write the discrete fre-
s
quency response of our discrete matched filter as
H(m) =
S*(m)e–j2πm(N–1)/N.
(7–24)
Finally, our discrete matched filter’s h(k) impulse response is the N-point in-
verse DFT of H(m), which, from Appendix C, is merely the straight time re-
versal (left-to-right flip) of x (n). And there you have it—we express our
s
optimumh(k) as
h(k) = x (N–k–1) (7–25)
s
where kranges from 0 to N–1. We struggled through the above process of de-
veloping Eq. (7–25) so the reader would understand the origin of our expres-
sion for h(k).
7.3.2 Matched Filter Example
As an example of matched filtering, Figure 7–12(a) shows an N = 9 sample
x (n) signal-of-interest sequence and the optimum h(k) matched filter impulse
s
response. If the matched filter’s x (n) input contains two occurrences of x (n),
in s
as shown in Figure 7–12(b), the filter’s y(n) output will be the two pulses
(each one symmetrical) shown in Figure 7–12(c). Our signal recognition
process is then making sure the threshold detection process in Figure 7–11(a)
detects the high-level peaks in y(n). It’s useful to remind ourselves that the
x (n) sequence enters the filter in a reversed order from that shown in Figure
in
7–12(b). That is, sample x (0) enters the filter first, followed by the x (1) sam-
in in
ple, and so on. So the x (n) sequences within x (n), arriving at the filter’s
s in
input, are in the same left-right orientation as the filter’s h(k) impulse re-
sponse.
To show the value of matched filtering, Figure 7–13(a) shows an x (n)
in
input sequence, having two occurrences of the previous x (n), but this time
s

7.3 Matched Filters 379
4 4
x s (n) h(k)
2 2
(a)
0 0
–2 –2
0 2 4 6 8 0 2 4 6 8
n k
6
x (n)
in
4
2
(b)
0
–2
–10 –5 0 5 10 15 20 25 30 35
n
100 y(n)
(c) 50
0
0 5 10 15 20 25 30 35
n
Figure 7–12 Matched filtering example: (a) signal of interest x(n) and h(k); (b) fil-
s
terx (n) input; (c) filter y(n) output.
in
badly contaminated with random noise. It’s very difficult to see the two x (n)
s
sequences in x (n). In this noisy-signal case the filter’s y(n) output, shown in
in
Figure 7–13(b), still distinctly exhibits the two peaks similar to the noise-free
example in Figure 7–12(c). Actually, we should call the two peaks in Figure
7–12(c) “correlation peaks” because our matched filter is performing a
x (n), noisy
in
5
(a) 0
–5
–10 –5 0 5 10 15 20 25 30 35
n
y(n)
100
50
(b)
0
0 5 10 15 20 25 30 35
n
Figure 7–13 Matched filtering example: (a) filter x (n) input contaminated with
in
noise; (b) filter y(n) output.

380 Specialized Digital Networks and Filters
correlation between the x (n) input signal and the predefined x (n) signal of
in s
interest. The y(n) output is not the x (n) signal of interest—y(n) is a quantita-
s
tive measure of the similarity between the x (n) input signal and x (n).
in s
7.3.3 Matched Filter Implementation Considerations
There are a number of important topics to consider when implementing
matched filters. They are:
• Matched filters are most conveniently implemented with tapped-delay
line FIR filters like those we studied in Chapter 5. The h(k) sequence
merely becomes the coefficients of the FIR filter. Again, our digital
matched filter performs convolution of x (n) and h(k), which is equiva-
in
lent to performing correlation between x (n) and x (n). In the 1980s the
in s
TRW LSI Products organization produced an integrated circuit that con-
tained a 32-tap FIR architecture used for matched filtering. The chip was
justifiably called a digital correlator.
• As we discussed in Section 5.9.2, time-domain convolution can be imple-
mented by way of frequency-domain multiplication. When the lengths
ofx (n) and h(k) are large (say, N> 80) and forward and inverse FFTs are
s
used, frequency-domain multiplication may be more computationally
efficient than traditional time-domain convolution.
• The H(m) frequency response given in Eq. (7–23) is based on two as-
sumptions: (1) that the x (n) noise is truly random, having a flat-level
n
broadband power spectrum, which means there is no correlation be-
tween one x (n) noise sample and any subsequent x (n+k) sample in Fig-
n n
ure 7–11(b); and (2) the x (n) noise’s probability density function (PDF)
n
is Gaussian in shape. Such noise is referred to as additive white noise
(AWN). If the x (n) noise is not AWN, for example, when x (n) is a radio
n s
signal and x (n) is a high-level intentional jamming signal, or when x (n)
n s
is a single data symbol of a digital communications signal contaminated
with some previous-in-time data symbol, then Eq. (7–22) must be modi-
fied. References [7–9] provide additional information regarding this
non-AWN scenario.
• Matched filtering is easy to perform. However, the detection threshold
operation in Figure 7–11, to detect the peaks in Figure 7–13(b), can be-
come difficult to implement reliably depending on the nature of x (n),
s
x (n), and the SNR of x (n). If we set the threshold too high, then we re-
n in
duce our probability of detection by risking failure to detect x (n). If we
s
set the threshold too low, then we increase our probability of false alarm
by incorrectly identifying a noise spike in y(n) as an occurrence of our
desired x (n). Advanced signal processing textbooks, by way of statistics
s
and an abundance of probability theory, cover these topics. Representa-
tive descriptions of these concepts are provided in references[10,11].

7.4 Interpolated Lowpass FIRFilters 381
7.4 INTERPOLATED LOWPASS FIR FILTERS
In this section we cover a class of digital filters, called interpolated FIR filters,
used to build narrowband lowpass FIR filters that can be more computation-
ally efficient than the traditional Parks-McClellan-designed tapped-delay line
FIR filters that we studied in Chapter 5. Interpolated FIR filters can reduce
traditional narrowband lowpass FIR filter computational workloads by more
than 80 percent. In their description, we’ll introduce interpolated FIR filters
with a simple example, discuss how filter parameter selection is made, pro-
vide filter performance curves, and go through a simple lowpass filter design
example showing their computational savings over traditional FIR fil-
ters[12,13].
Interpolated FIR (IFIR) filters are based upon the behavior of an N-tap
nonrecursive linear-phase FIR filter when each of its unit delays is replaced
with M unit delays, with the expansion factor M being an integer, as shown in
Figure 7–14(a). If the h (k) impulse response of a 9-tap FIR filter is that shown
p
in Figure 7–14(b), the impulse response of an expandedFIR filter, where for ex-
ample M = 3, is the h (k) in Figure 7–14(c). The M unit delays result in the
sh
x(n)
z–M z–M z–M . . . z–M
h(0) h (1) h(2) h(N–2) h(N–1)
p p p p p
(a)
y(n)
1
Prototypeh(k)
p
(b)
0
0 2 4 6 8
k
1
Shapingh (k)
sh
(c)
0
0 5 10 15 20 25
k
Figure 7–14 Filter relationships: (a) shaping FIR filter with Munit delays between
the taps; (b) impulse response of a prototype FIR filter; (c) impulse re-
sponse of an expanded-delay shaping FIR filter with M= 3.

382 Specialized Digital Networks and Filters
zero-valued samples, the white dots, in the h (k) impulse response. Our vari-
sh
able k is merely an integer time-domain index where 0 ≤ k ≤ N–1. To define
our terminology, we’ll call the original FIR filter the prototype filter—that’s
why we used the subscript “p” in h (k)—and we’ll call the filter with ex-
p
panded delays the shaping subfilter. Soon we’ll see why this terminology is
sensible.
We can express a prototype FIR filter’s z-domain transfer function as
H (z) =
N ∑p −1
h (k)z
−k
p p
k=0 (7–26)
where N is the length of h . The transfer function of a general shaping FIR
p p
filter, with zin Eq. (7–26) replaced with zM, is
H (zM) =
N ∑p −1
h (k)z −kM.
sh p
k=0 (7–27)
Later we’ll see why we chose to provide Eqs. (7–26) and (7–27). If the number
of coefficients in the prototype filter is N , the shaping filter has N nonzero
p p
coefficients and an expanded impulse response length of
N =M(N – 1) + 1. (7–28)
sh p
Later we’ll see how N has an important effect on the implementation of IFIR
sh
filters.
The frequency-domain effect of those M unit delays is shown in Figure
7–15. As we should expect, an M-fold expansion of the time-domain filter im-
pulse response causes an M-fold compression (and repetition) of the frequency-
domain |H (f)| magnitude response as in Figure 7–15(b). While H (f) has a
p p
single passband, H (f) has M passbands. (The frequency axis of these curves
sh
is normalized to the f filter input signal sample rate. For example, the nor-
s
malized frequency f is equivalent to a frequency of f f Hz.) Those repeti-
pass pass s
tive passbands in |H (f)| centered about integer multiples of 1/M(f/MHz)
sh s
are called images, and on them we now focus our attention.
If we follow the shaping subfilter with a lowpass image-reject subfilter,
Figure 7–15(c), whose task is to attenuate the image passbands, we can realize
a multistage filter whose frequency response is shown in Figure 7–15(d). The
resultant |H (f)| frequency magnitude response is, of course, the product
ifir
|H (f)| = |H (f)|·|H (f)|. (7–29)
ifir sh ir
The structure of the cascaded subfilters is the so-called IFIR filter shown
in Figure 7–16(a), with its interpolated impulse response given in Figure
7–16(b).

7.4 Interpolated Lowpass FIRFilters 383
M.f
trans
(a) |H p (f)|
0 M.f pass M.f stop f s –M.f stop 1 Freq
(f)
s
|H (f)|
(b) sh Image Image
0 f pass f stop 1/M 2/M f s –f stop 1 Freq
(f)
1/M–f s
stop
|H(f)|
ir
(c)
0 f pass 1/M–f stop f s –(1/M–f stop ) 1 Freq
(f)
s
f
trans
|H (f)|
(d) ifir
0 f pass f stop f s –f stop 1 Freq
(f)
s
Figure 7–15 IFIR filter magnitude responses: (a) the prototype filter; (b) shaping
subfilter; (c) image-reject subfilter; (d) final IFIR filter.
Interpolated FIR filter, H (f)
ifir
x(n) Image-reject, y(n)
(a) Shaping,H (f)
sh H(f)
ir
1
h (k)
ifir
(b) 0
0 k
Figure 7–16 Lowpass interpolated FIR filter: (a) cascade structure; (b) resultant
impulse response.

384 Specialized Digital Networks and Filters
If the original desired lowpass filter’s passband width is f , its stop-
pass
band begins at f , and the transition region width is f = f –f , then
stop trans stop pass
the prototype subfilter’s normalized frequency parameters are defined as
f =Mf . (7–30)
p-pass pass
f =Mf . (7–30’)
p-stop stop
f =Mf =M(f –f ). (7–30’’)
p-trans trans stop pass
The image-reject subfilter’s frequency parameters are
f =f . (7–31)
ir-pass pass
1
f = − f .
ir-stop M stop
(7–31’)
The stopband attenuations of the prototype filter and image-reject sub-
filter are identical and set equal to the desired IFIR filter stopband attenua-
tion. The word interpolated in the acronym IFIR is used because the
image-reject subfilter interpolates samples in the prototype filter’s h (k) im-
p
pulse response, making the overall IFIR filter’s impulse response equal to the
h (k) sequence in Figure 7-34(b). Note that h (k) does not represent the coef-
ifir ifir
ficients used in any FIR subfilter filter. Sequence h (k) is the convolution of
ifir
the shaping and image-reject subfilters’ impulse responses (coefficients).
Some authors emphasize this attribute by referring to the image-reject subfil-
ter as an interpolator. The f sample rate remains unchanged within an IFIR fil-
s
ter, so no actual signal interpolation takes place.
To give you an incentive to continue reading, the following example
shows the terrific computational advantage of using IFIR filters. Consider the
design of a desired linear-phase FIR filter whose normalized passband width
isf = 0.1, its passband ripple is 0.1 dB, the transition region width is f =
pass trans
0.02, and the stopband attenuation is 60 dB. (The passband ripple is a
peak-peak specification measured in dB.) With an expansion factor of M = 3,
the |H (f)| frequency magnitude response of the prototype filter is shown in
p
Figure 7–17(a). The normalized frequency axis for these curves is such that a
value of 0.5 on the abscissa represents the cyclic frequency f/2 Hz, half the
s
sample rate. The frequency response of the shaping subfilter, for M= 3, is pro-
vided in Figure 7–17(b) with an image passband centered about (1/M) Hz.
The response of the image-reject subfilter is the solid curve in Figure 7–17(c),
and the response of the overall IFIR filter is provided in Figure 7–17(d).
Satisfying the original desired filter specifications in Figure 7–17(d)
would require a traditional tapped-delay FIR filter with N = 137 taps,
tfir
where the “tfir” subscript means traditional FIR. In our IFIR filter, the shaping

7.4 Interpolated Lowpass FIRFilters 385
-60 -60
(a) (b)
-60 -60
(c) (d)
Figure 7–17 Example lowpass IFIR filter magnitude responses: (a) the prototype
filter; (b) shaping subfilter; (c) image-reject subfilter; (d) final IFIR filter.
and the image-reject subfilters require N = 45 and N = 25 taps respectively,
p ir
for a total of N = 70 taps. We can define the percent reduction in computa-
ifir
tional workload (number of multiplies per filter output sample) of an IFIR fil-
ter, over a traditional tapped-delay line FIR filter, as
N − N −N
tfir p ir
% computation reduction = 100 · .
N
tfir (7–32)
As such, the above example IFIR filter has achieved a multiplication compu-
tational workload reduction, over a traditional FIR filter, of
137−45−25
% computation reduction = 100. =49%.
137 (7–32’)
Figure 7–17 shows how the transition region width (the shape) of
|H (f)| is determined by the transition region width of |H (f)|, and this
ifir sh
justifies the decision to call h (k) the shapingsubfilter.
sh
7.4.1 Choosing the Optimum Expansion Factor M
The expansion factor M deserves our attention because it can have a pro-
found effect on the computational efficiency of IFIR filters. To show this, had
we used M = 2 in our Figure 7–17 example, we would have realized an IFIR
filter described by the M = 2 row in Table 7–1. In that case the computation
Bd
Bd
Bd
Bd
0 0
|H(f)| |H (f)|
p sh
0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5
Frequency Frequency
0 0
|H (f)|
ifir
|H sh (f)| |H ir (f)|
0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5
Frequency Frequency

386 Specialized Digital Networks and Filters
Table 7–1 IFIR Filter Computation Reduction versus M
Number of taps
Expansion Computation
factorM h (k) h (k) IFIR total reduction
sh ir
2 69 8 77 43%
3 45 25 70 49%
4 35 95 130 8%
reduction over a conventional FIR filter is 43 percent. With M = 2, a reduced
amount of frequency-domain compression occurred in H (f), which man-
sh
dated more taps in h (k) than were needed in the M= 3 case.
sh
Now had M= 4 been used, the computation reduction, over a single tra-
ditional tapped-delay line FIR filter, would only be 8 percent as shown in
Table 7–1. This is because the H (f) passband images would be so close to-
sh
gether that a high-performance (increased number of taps) image-reject sub-
filter would be required. As so often happens in signal processing designs,
there is a trade-off to be made. We would like to use a large value for M to
compress the H (f)’s transition region width as much as possible, but a large
sh
M reduces the transition region width of the image-reject subfilter, which in-
creases the number of taps in h (k) and its computational workload. In our
ir
Figure 7–17 IFIR filter example an expansion factor of M = 3 is optimum be-
cause it yields the greatest computation reduction over a traditional tapped-
delay line FIR filter.
The optimum IFIR filter expansion factor was found by Mehrnia and
Willson[14] to be
1
M = .
opt f + f + f − f
pass stop stop pass
(7–33)
We’ll explore the meaning, and effects, of Eq. (7–33) in the next few pages, but
first let’s determine the percent computation reduction afforded to us by IFIR
filters.
7.4.2 Estimating the Number of FIR Filter Taps
To estimate the computation reduction achieved by using IFIR filters, an algo-
rithm is needed to compute the number of taps, N , in a traditional tapped-
tfir
delay line FIR filter. Several authors have proposed empirical relationships
for estimating N for traditional tapped-delay line FIR filters based on pass-
tfir
band ripple, stopband attenuation, and transition region width[15–17]. Apar-

7.4 Interpolated Lowpass FIRFilters 387
ticularly simple expression for N , giving results consistent with other esti-
tfir
mates for passband ripple values near 0.1 dB, is
≈ Atten
N
tfir 22(f − f )
stop pass (7–34)
where Attenis the stopband attenuation measured in dB, and f andf are
pass stop
the normalized frequencies in Figure 7–15(d)[17]. (Again, by “normalized”
we mean that the f and f frequency values are normalized to the filter
pass stop
input sample rate, f, in Hz. For example, f = 0.1 is equivalent to a continuous-
s pass
time frequency of f = 0.1f Hz.) Likewise, the number of taps in the proto-
pass s
type and image-reject subfilters can be estimated using
≈ Atten
N
p 22(M)( f − f )
stop pass (7–34’)
≈ Atten
N .
ir 22(1/M− f − f )
. stop pass (7–34’’)
7.4.3 Modeling IFIR Filter Performance
As it turns out, IFIR filter computational workload reduction depends on the
expansion factor M, the passband width, and the transition region width of
the desired IFIR filter. To show this, we substitute the above expressions for
N ,N , and N into Eq. (7–32) and write
tfir p ir
% computation reduction = 100 . ⎡ ⎢ M−1 − Mf trans ⎤ ⎥,
⎣⎢ M 1−Mf −2Mf ⎦⎥
trans pass (7–35)
where f =f –f .
trans stop pass
Having Eqs. (7–33) and (7–35) available to us, we can now see the per-
formance of IFIR filters. The optimum expansion factor curves from Eq.
(7–33) are plotted, versus desired IFIR filter transition region width, for vari-
ous values of passband width in Figure 7–18(a). When various optimum ex-
pansion factors are used in an IFIR filter design, the percent computation
reduction, when an M value is plugged into Eq. (7–35), is that shown in
opt
Figure 7–18(b).
So in IFIR filter design, we use our desired filter transition region width
and passband width values to determine the M optimum expansion factor
opt
using either Eq. (7–33) or the curves in Figure 7–18(a). Given that M value,
opt
we estimate our IFIR filter’s percent computation reduction from either Eq.
(7–35) or the curves in Figure 7–18(b). We’ll go through an IFIR filter design
example shortly.

388 Specialized Digital Networks and Filters
Optimum expansion factor (M ) % computation reduction
opt
50 100
f = 0.005
pass
45 90 0.04
0.06
40 80 0.08 f = 0.01
pass
0.1
35 70
0.01 0.02
30 60
M opt %
25 50
20 0.02 40
15 30
10 0.04 20
0.06
5 0.08 10
0.0001 0.001 0.01 0.1 0.0001 0.001 0.01 0.1
Transition region bandwidth,f trans Transition region bandwidth, f trans
(a) (b)
Figure 7–18 IFIR filter performance versus desired transition region width for vari-
ous passband widths: (a) optimum expansion factors; (b) percent
computation reduction.
7.4.4 IFIR Filter Implementation Issues
The computation reduction of IFIR filters is based on the assumption that
they are implemented as two separate subfilters as in Figure 7–16. We have
resisted the temptation to combine the two subfilters into a single filter whose
coefficients are the convolution of the subfilters’ impulse responses. Such a
maneuver would eliminate the zero-valued coefficients of the shaping subfil-
ter, and we’d lose all our desired computation reduction.
The curves in Figure 7–18(a) indicate an important implementation issue
when using IFIR filters. With decreasing IFIR filter passband width, larger ex-
pansion factors, M, can be used. When using programmable DSPchips, larger
values of M require that a larger block of hardware data memory, in the form
of a circular buffer, be allocated to hold a sufficient number of input x(n) sam-
ples for the shaping subfilter. The size of this data memory must be equal to
at least N as indicated in Eq. (7–28). Some authors refer to this data memory
sh
allocation requirement, to accommodate all the stuffed zeros in the h (k) im-
sh
pulse response, as a disadvantage of IFIR filters. This is a misleading view-

7.4 Interpolated Lowpass FIRFilters 389
point because, as it turns out, the N length of h (k) is only a few percent
sh sh
larger than the length of the impulse response of a traditional FIR filter hav-
ing the same performance as an IFIR filter. So from a data storage standpoint
the price we pay to use IFIR filters is a slight increase in the size of memory to
accommodate N , plus the data memory of size K needed for the image-
sh ir
reject subfilter. In practice, for narrowband lowpass IFIR filters, K is typically
ir
less than 10 percent of N .
sh
When implementing an IFIR filter with a programmable DSP chip, the
filter’s computation reduction gain can only be realized if the chip’s architec-
ture enables zero-overhead looping through the circular data memory using an
increment equal to the expansion factor M. That looping capability ensures
that only the nonzero-valued coefficients of h (k) are used in the shaping sub-
sh
filter computations.
In practice the shaping and image-reject subfilters should be imple-
mented with a folded tapped-delay line FIR structure, exploiting their im-
pulse response symmetry, to reduce the number of necessary multiplications
by a factor of two. (See Section 13.7.) Using a folded structure does not alter
the performance curves provided in Figure 7–18. Regarding an IFIR filter’s
implementation in fixed-point hardware, its sensitivity to coefficient quanti-
zation errors is no greater than the errors exhibited by traditional FIR fil-
ters[12].
7.4.5 IFIR Filter Design Example
The design of practical lowpass IFIR filters is straightforward and comprises
four steps:
1. Define the desired lowpass filter performance requirements.
2. Determine a candidate value for the expansion factor M.
3. Design and evaluate the shaping and image-reject subfilters.
4. Investigate IFIR performance for alternate expansion factors near the
initialMvalue.
As a design example, refer to Figure 7–15(d) and assume we want to
build a lowpass IFIR filter with f = 0.02, a peak-peak passband ripple of 0.5
pass
dB, a transition region bandwidth of f = 0.01 (thus f = 0.03), and 50 dB
trans stop
of stopband attenuation. First, we find the f = 0.01 point on the abscissa of
trans
Figure 7–18(a) and follow it up to the point where it intersects the f = 0.02
pass
curve. This intersection indicates that we should start our design with an ex-
pansion factor of M = 7. (The same intersection point in Figure 7–18(b) sug-
gests that we can achieve a computational workload reduction of roughly 75
percent.)

390 Specialized Digital Networks and Filters
With M= 7, and applying Eq. (7–30), we use our favorite traditional FIR
filter design software to design a linear-phase prototype FIR filter with the
following parameters:
f =M(0.02) = 0.14,
p-pass
passband ripple = (0.5)/2 dB = 0.25 dB,
f =M(0.03) = 0.21, and
p-stop
stopband attenuation = 50 dB.
(Notice how we used our cascaded filters’ passband ripple rule of thumb
from Section 6.8.1 to specify the prototype filter’s passband ripple to be half
our final desired ripple, and we’ll do the same for the image-reject subfilter.)
Such a prototype FIR filter will have N = 33 taps and, from Eq. (7–28), when
p
expanded by M = 7 the shaping subfilter will have an impulse response
length of N = 225 samples.
sh
Next, using Eq. (7–31), we design an image-reject subfilter having the
following parameters:
f =f = 0.02,
ir-pass pass
passband ripple = (0.5)/2 dB = 0.25 dB,
f = 1/M–f = 1/7 – 0.03 = 0.113, and
ir-stop stop
stopband attenuation = 50 dB.
This image-reject subfilter will have N = 27 taps and when cascaded with the
ir
shaping subfilter will yield an IFIR filter requiring 60 multiplications per filter
output sample. The frequency response of the IFIR filter is shown in Figure
7–19(a), with passband response detail provided in Figure 7–19(b).
A traditional FIR filter satisfying our design example specifications
would require approximately N = 240 taps. Because the IFIR filter requires
tfir
only 60 multiplications per output sample, using Eq. (7–32), we have realized
a computational workload reduction of 75 percent. The final IFIR filter design
step is to sit back and enjoy a job well done.
Figure 7–19 IFIR filter design example magnitude responses: (a) full response;
(b) passband response detail.
Bd
Bd
0 0
-0.2
|H (f)|
|H (f)| ifir
ifir -0.4
-0.6
-50 -0.8
0 0.1 0.2 0.3 0.4 0.5 0 0.005 0.01 0.015 0.02
Frequency Frequency
(a) (b)

7.4 Interpolated Lowpass FIRFilters 391
Further modeling of our design example for alternate expansion factors
yields the IFIR filter performance results in Table 7–2. There we see how the
M expansion factors of 5 through 8 provide very similar computational re-
ductions and N -sized data storage requirements for the shaping subfilter.
sh
IFIR filters are suitable whenever narrowband lowpass linear-phase fil-
tering is required, for example, the filtering prior to decimation for narrow-
band channel selection within wireless communications receivers, or in digital
television. IFIR filters are essential components in sharp-transition wideband
frequency-response masking FIR filters[18,19]. In addition, IFIR filters can also
be employed in narrowband two-dimensional filtering applications.
Additional, and more complicated, IFIR design methods have been de-
scribed in the literature. Improved computational workload reduction, on the
order of 30 to 40 percent beyond that presented here, has been reported using
an intricate design scheme when the Figure 7–16 image-reject subfilter is re-
placed with multiple stages of filtering[20].
If you “feel the need for speed,” there are additional ways to reduce the
computational workload of IFIR filters. Those techniques are available in ref-
erences [21] and [22]. We will revisit IFIR filters in Chapter 10 to see how they
are used in sample rate conversion (decimation or interpolation) applications.
To conclude our linear-phase narrowband IFIR filter material, we reiter-
ate that they can achieve significant computational workload reduction (as
large as 90 percent) relative to traditional tapped-delay line FIR filters, at the
cost of less than a 10 percent increase in hardware data memory require-
ments. Happily, IFIR implementation is a straightforward cascade of filters
designed using readily available traditional FIR filter design software.
Table 7–2 Design Example Computation Reduction versus M
Expansion Number of taps N data Computation
sh
factorM storage reduction
h (k) h (k) IFIR total
sh ir
3 76 8 84 226 65%
4 58 12 70 229 71%
5 46 17 63 226 74%
6 39 22 61 229 75%
7 33 27 60 225 75%
8 29 33 62 225 74%
9 26 41 67 226 72%
10 24 49 73 231 70%
11 21 60 81 221 66%

392 Specialized Digital Networks and Filters
7.5 FREQUENCY SAMPLING FILTERS: THE LOST ART
This section describes a class of digital filters, called frequency sampling filters,
used to implement linear-phase FIR filter designs. Although frequency sam-
pling filters were developed over 35 years ago, the advent of the powerful
Parks-McClellan tapped-delay line FIR filter design method has driven them
to near obscurity. Thus in the 1970s frequency sampling filter implementa-
tions lost favor to the point where their coverage in today’s DSP classrooms
and textbooks ranges from very brief to nonexistent. However, we’ll show
how frequency sampling filters remain more computationally efficient than
Parks-McClellan-designed filters for certain applications where the desired
passband width is less than roughly one-fifth the sample rate. The purpose of
this material is to introduce the DSP practitioner to the structure, perfor-
mance, and design of frequency sampling filters, and to present a detailed
comparison between a proposed high-performance frequency sampling filter
implementation and its tapped-delay line FIR filter equivalent. In addition,
we’ll clarify and expand the literature of frequency sampling filters concern-
ing the practical issues of phase linearity, filter stability, gain normalization,
and computational workload using design examples.
Frequency sampling filters were founded upon the fact that a traditional
N-tap nonrecursive tapped-delay line (direct convolution) FIR filter as shown
in Figure 7–20(a) can be implemented as a comb filter in cascade with a bank
of N complex resonators as shown in Figure 7–20(b). We call the filter in Fig-
ure 7–20(b) a general frequency sampling filter (FSF), and its equivalence to the
nonrecursive FIR filter has been verified[23–25]. While the h(k) coefficients,
where 0 < k<N–1, of N-tap nonrecursive FIR filters are typically real-valued,
in general they can be complex, and that’s the initial assumption made in
equating the two filters in Figure 7–20. The H(k) gain factors, the discrete
Fourier transform of the h(k) time-domain coefficients, are, in the general
case, complex values represented by
|H(k)|ejφ(k).
The basis of FSF design is the definition of a desired FIR filter frequency
response in the form of H(k) frequency-domain samples, whose magnitudes
are depicted as dots in Figure 7–21. Next, those complex H(k) sample values
are used as gain factors following the resonators in the FSF structure (block
diagram). If you haven’t seen it before, please don’t be intimidated by this ap-
parently complicated FSF structure. We’ll soon understand every part of it,
and how those parts work together.
Later we’ll develop the math to determine the interpolated (actual) fre-
quency magnitude response
|H(ejω
)| of an FSF shown by the continuous
curve in Figure 7–21. In this figure, the frequency axis labeling convention is a
normalized angle measured in π radians/sample with the depicted ω fre-
quency range covering 0 to 2π radians/sample, corresponding to a cyclic fre-
quency range of 0 to f, where f is the sample rate in Hz.
s s

7.5 Frequency Sampling Filters: The Lost Art 393
x(n) . . .
z–1 z–1 z–1 z–1
h(0) h(1) h(2) h(N–2) h(N–1)
(a)
y(n)
Resonators
1
H(z) =
0 1 –ej2π0/Nz–1
H(0)/N
1
H(z) =
1 1 –ej2π1/Nz–1
Comb
x(n) H(1)/N y(n)
1 –z–N 1
H(z) =
2 1 –ej2π2/Nz–1
(b)
. . . . H(2)/N
. .
1
H (z) =
N-1 1 –ej2π(N–1)/Nz–1
H(N-1)/N
Figure 7–20 FIR filters: (a) N-tap nonrecursive tapped-delay line; (b) equivalent
N-section frequency sampling filter.
To avoid confusion, we remind the reader that there is a popular nonre-
cursive FIR filter design technique known as the frequency sampling design
method described in the DSPliterature. That design scheme begins (in a man-
ner similar to an FSF design) with the definition of desired H(k) frequency re-
sponse samples, then an inverse discrete Fourier transform is performed on
those samples to obtain a time-domain impulse response sequence that’s
1
0.5
0
0 0.5 1 1.5 2
Frequency
Figure 7–21 Defining a desired filter response by frequency sampling.
edutingaM
|H(k)|, |H(ejω)|
k= 0 k=N/2 k=N-1
sample sample sample

394 Specialized Digital Networks and Filters
used as the h(k) coefficients in the nonrecursive N-tap FIR structure of Figure
7–20(a). In the FSF design method described here, the desired frequency-
domain H(k) sample values are the coefficients used in the FSF structure of
Figure 7–20(b) which is typically called the frequency sampling implementation
of an FIR filter.
Although more complicated than nonrecursive FIR filters, FSFs deserve
study because in many narrowband filtering situations they can implement a
linear-phase FIR filter at a reduced computational workload relative to an
N-tap nonrecursive FIR filter. The computation reduction occurs because,
while all of the h(k) coefficients are used in the nonrecursive FIR filter imple-
mentation, most of the H(k) values will be zero-valued, corresponding to the
stopband, and need not be implemented. To understand the function and
benefits of FSFs, we start by considering the behavior of the comb filter and
then review the performance of a single digital resonator.
7.5.1 Comb Filter and Complex Resonator in Cascade
Asingle section of a complex FSF is a comb filter followed by a single com-
plex digital resonator as shown in Figure 7–22.
The 1/N gain factor following a resonator in Figure 7–20(b) is omitted,
for simplicity, from the single-section complex FSF. (The effect of including
the 1/Nfactor will be discussed later.) To understand the single-section FSF’s
operation, we first review the characteristics of the nonrecursive comb filter
whose time-domain difference equation is
v(n) = x(n) – x(n–N), (7–36)
with its output equal to the input sequence minus the input delayed by N
samples. The comb filter’s z-domain transfer function is
V(z)
H (z) = = 1 –z–N.
comb X(z) (7–37)
Comb Complex resonator
x(n) v(n) y(n)
z–N z–1 H(k) = 1
–1 ejωr
Figure 7–22 A single section of a complex FSF.

7.5 Frequency Sampling Filters: The Lost Art 395
Impulse response
0
Figure 7–23 Time- and frequency-domain characteristics of an N= 8 comb filter.
The frequency response of a comb filter, derived in Section G.1 of Appendix
G, is
H
(ejω
) =
e–j(ωN–π)/22sin(ωN/2),
(7–38)
comb
with a magnitude response of |H (ejω )| = 2|sin(ωN/2)| whose maximum
comb
value is 2. It’s meaningful to view the comb filter’s time-domain impulse re-
sponse and frequency-domain magnitude response as shown in Figure 7–23
forN= 8. The magnitude response makes it clear why the term combis used.
Equation (7–37) leads to a key feature of this comb filter; its transfer
function has N periodically spaced zeros around the z-plane’s unit circle as
shown in Figure 7–23(c). Each of those zeros, located at z(k) =
ej2πk/N,
where k=
0, 1, 2, . . ., N–1, corresponds to a magnitude null in Figure 7–23(b), where the
normalized frequency axis is labeled from –π to +π radians/sample. Those
z(k) values are the N roots of unity when we set Eq. (7–37) equal to zero,
yielding z(k)N = (ej2πk/N)N = 1. We can combine the magnitude response (on a
linear scale) and z-plane information in the three-dimensional z-plane depic-
tion shown in Figure 7–24, where we see the intersection of the |H (z)|
comb
surface and the unit circle. Breaking the curve at the z= –1 point, and laying it
flat, corresponds to the magnitude curve in Figure 7–23(b).
To preview where we’re going, soon we’ll build an FSF by cascading the
comb filter with a digital resonator having a transfer function pole lying on
top of one of the comb’s z-plane zeros, resulting in a linear-phase bandpass
filter. With this thought in mind, let’s characterize the digital resonator in Fig-
ure 7–22.
The complex resonator’s time-domain difference equation is
y(n) = v(n) +
ejω
ry(n–1), (7–39)
trap
yranigamI
Magnitude response z-plane
Bd
1 0
1
–5
0.5
–10
0 0
–15
–0.5
–20
–1
–1
–25
0 5 10 15 –1 –0.5 0 0.5 1 –1 1
Time Frequency Real part
(a) (b) (c)

396 Specialized Digital Networks and Filters
2.5
2
1.5
|H comb (z)| 1 –1
0.5
–0.5
0
–1 0
–0.5 Real[z]
0 0.5
Imag[z] 0.5 1 1
This point, z= 1 + j0,
corresponds to zero Hz.
Figure 7–24 Thez-plane frequency magnitude response of the N= 8 comb filter.
where the angle ω, –π≤ω ≤πdetermines the resonant frequency of our res-
r r
onator. We show this by considering the resonator’s z-domain transfer func-
tion
Y(z) 1
H (z) = = (7–40)
res V(z) 1−ejω rz −1
and the resonator’s complex time-domain impulse response, for ω = π/4, in
r
Figure 7–25.
The ω = π/4 resonator’s impulse response is a complex sinusoid, the
r
real part (a cosine sequence) of which is plotted in Figure 7–26(a), and can be
Impulse response
sequence
1
0.5
0
–0.5
–1
0
1
15 0.5
0
Time 30 –0.5
–1 Real Part
Figure 7–25 Single complex digital resonator impulse response with ω =π/4.
r
traP
gamI

7.5 Frequency Sampling Filters: The Lost Art 397
1
0
–1
–1 0 1
Figure 7–26 Time- and frequency-domain characteristics of a single complex
digital resonator with ω =π/4.
r
considered infinite in duration. (The imaginary part of the impulse response
is, as we would expect, a sinewave sequence.) The frequency magnitude re-
sponse is very narrow and centered at ω. The resonator’s H (z) has a single
r res
zero at z= 0, but what concerns us most is its pole, at
z=ejω
r, on the unit circle
at an angle of ω as shown in Figure 7–26(c). We can think of the resonator as
r
an infinite impulse response (IIR) filter that’s conditionally stable because its
pole is neither inside nor outside the unit circle.
We now analyze the single-section complex FSF in Figure 7–22. The
z-domain transfer function of this FSF is the product of the individual transfer
functions and H(k), or
H(k)
H(z) = H (z)H (z)H(k) = (1 –z–N) .
comb res 1−ejω rz −1 (7–41)
If we restrict the resonator’s resonant frequency ω to be 2πk/N, where k
r
= 0, 1, 2, . . ., N–1, then the resonator’s z-domain pole will be located atop one
of the comb’s zeros and we’ll have an FSF transfer function of
H(k)
H ss (z) = (1 –z–N) 1−ej2πk/Nz −1 , (7–42)
where the “ss” subscript means a single-section complex FSF. We can under-
stand a single-section FSF by reviewing its time- and frequency-domain be-
havior for N= 32, k= 2, and H(2) = 1 as shown in Figure 7–27.
Figure 7–27 is rich in information. We see that the complex FSF’s im-
pulse response is a truncated complex sinusoid whose real part is shown in
Figure 7–27(a). The positive impulse from the comb filter started the res-
onator oscillation at zero time. Then at just the right sample, N = 32 samples
later, which is k = 2 cycles of the sinusoid, the negative impulse from the
comb arrives at the resonator to cancel all further oscillation. The frequency
magnitude response, being the Fourier transform of the truncated sinusoidal
impulse response, takes the form of a sin(x)/x-like function. In the z-plane
Bd
trap
yranigamI
Impulse response (real) Magnitude response z-plane
1 0
–5
0.5
–10
ω r
0 –15
–20
–0.5
–25
–1 –30–
0 10 20 30 –0.5 0 0.5 1
Time 1 Frequency ω r Real part
(a) (b) (c)

398 Specialized Digital Networks and Filters
0
–5
–10
–15
–20
–25
–30
–1 –0.5 0 0.5 1
Figure 7–27 Time- and frequency-domain characteristics of a single-section
complex FSF where N= 32, k= 2, and H(2) = 1.
plot of Figure 7–27, the resonator’s pole is indeed located atop the comb fil-
ter’s k= 2 zero on the unit circle, canceling the frequency magnitude response
null at 2πk/N = π/8 radians. (Let’s remind ourselves that a normalized angu-
lar frequency of 2πk/N radians/sample corresponds to a cyclic frequency of
kf /N, where f is the sample rate in Hz. Thus the filter in Figure 7–27 res-
s s
onates at f/16 Hz.)
s
We can determine the FSF’s interpolated frequency response by evaluat-
ing the H (z) transfer function on the unit circle. Substituting
ejω
forzinH (z)
ss ss
in Eq. (7–42), as detailed in Appendix G, Section G.2, we obtain an H
(ejω
) fre-
ss
quency response of
sin(ωN/2)
H (ejω) = H (z)| = e–jω(N–1)/2e–jπk/NH(k) .
ss ss z=ejω sin(ω/2 − πk/N ) (7–43)
Evaluating |H (ejω )| over the frequency range of –π < ω < π yields the
ss
curve in Figure 7–27(b). Our single-section FSF has linear phase because the
e–jπk/N
term in Eq. (7–43) is a fixed phase angle based on constants N and k,
the angle of H(k) is fixed, and the
e–jω(N–1)/2
phase term is a linear function of
frequency (ω). As derived in Appendix G, Section G.2, the maximum magni-
tude response of a single-section complex FSF is N when |H(k)| = 1, and we
illustrate this fact in Figure 7–28.
7.5.2 Multisection Complex FSFs
In order to build useful FSFs we use multiple resonator sections, as indicated
in Figure 7–20(b), to provide bandpass FIR filtering. For example, let’s build a
three-section complex bandpass FSF by establishing the following parame-
ters:N= 32, and the nonzero frequency samples are H(2),H(3), and H(4). The
desired frequency magnitude response is shown in Figure 7–29(a) with the
bandpass FSF structure provided in Figure 7–29(b).
Bd
1
0
–1
–1 0 1
trap
yranigamI
Impulse response (real) Magnitude response z-plane
1
0.5
π/8
0
–0.5
–1
0 10 20 30
Time Frequency 2πk/N=π/8 Real part
(a) (b) (c)

7.5 Frequency Sampling Filters: The Lost Art 399
40
30
|H (ejω)| 20 N= 32
ss 10 –1
0 –0.5
–1 0
–0.5 Real[z]
0 0.5
Imag[z] 0.5 1 1 π/8 (f s /16 Hz)
Zero Hz
Figure 7–28 Thez-plane frequency magnitude response of a single-section com-
plex FSF with N= 32 and k= 2.
Bandpass FSF |H(k)| samples
1
0.75
0.5
(a) 0.25
0
–1 –0.75 –0.5 –0.25 0 0.25 0.5 0.75 1
Frequency
z–1
H(2)
e j2π2/32
x(n) y(n)
(b)
z–N
z–1
H(3)
e j2π3/32
–1
z–1
H(4)
e j2π4/32
Figure 7–29 Three-sectionN= 32 complex FSF: (a) desired frequency magnitude
response; (b) implementation.
edutingaM
|H(2)| |H(4)|
|H(17)| |H(31)| |H(16)|

400 Specialized Digital Networks and Filters
Exploring this scenario, recall that the z-domain transfer function of par-
allel filters is the sum of the individual transfer functions. So, the transfer
function of an N-section complex FSF from Eq. (7–42) is
∑N-1 H(k)
H (z) = (1 –z–N) ,
cplx 1−ej2πk/Nz −1 (7–44)
k=0
where the subscript “cplx” means a complex multisection FSF.
Let’s pause for a moment to understand Eq. (7–44); the first factor on the
right side represents a comb filter, and the comb is in cascade (multiplication)
with the sum of ratio terms. The summation of the ratios (each ratio is a res-
onator) means those resonators are connected in parallel. Recall from Section
6.8.1 that the combined transfer function of filters connected in parallel is the
sum of the individual transfer functions. It’s important to be comfortable with
the form of Eq. (7–44) because we’ll be seeing many similar expressions in the
material to come.
So a comb filter is driving a bank of resonators. For an N = 32 complex
FSF we could have up to 32 resonators, but in practice only a few resonators
are needed for narrowband filters. In Figure 7–29, we used only three res-
onators. That’s the beauty of FSFs; most of the H(k) gain values in Eq. (7–44)
are zero-valued and those resonators are not implemented, keeping the FSF
computationally efficient.
Using the same steps as in Appendix G, Section G.2, we can write the
frequency response of a multisection complex FSF, such as in Figure 7–29, as
∑N-1H(k)e −jπk/Nsin(ω N/2)
H (ejω) = e–jω(N–1)/2 sin(ω/2−πk /N ) . (7–45)
cplx k=0
The designer of a multisection complex FSF can achieve any desired filter
phase response by specifying the φ(k) phase angle value of each nonzero com-
plex H(k) =
|H(k)|ejφ(k)
gain factor. However, to build a linear-phase complex
FSF, the designer must (1) specify the φ(k) phase values to be a linear function
of frequency, and (2) define the φ(k) phase sequence so its slope is –(N-1)/2.
This second condition forces the FSF to have a positive time delay of (N–1)/2
samples, as would the N-tap nonrecursive FIR filter in Figure 7–20(a). The fol-
lowing expressions for φ(k), with Nbeing even, satisfy those two conditions.
2π −(N−1) −kπ(N−1) N
φ(k) = k = , k = 0, 1, 2, ..., –1. (7–46)
N 2 N 2
φ(N/2) = 0. (7–46’)
2π (N−1) π(N−k)(N−1) N
φ(k) = (N – k) ⋅ = , k = + 1, ..., N –1. (7–46’’)
N 2 N 2

7.5 Frequency Sampling Filters: The Lost Art 401
IfNis odd, the linear-phase H(k) phase values are
2π −(N−1) −kπ(N−1) N−1
φ(k) = k⋅ ⋅ = , k = 0, 1, 2, ..., . (7–47)
N 2 N 2
2π (N−1) π(N−k)(N−1) N+1
φ(k) = (N−k)⋅ ⋅ = , k = , ..., N –1. (7–47’)
N 2 N 2
Two example linear-phase φ(k) sequences, for N = 19 and N = 20, are
shown in Figure 7–30. The φ(0) = 0 values set the phase to be zero at zero Hz,
and the φ(N/2) = 0, at the cyclic frequency of f/2 in Figure 7–30(b), ensures a
s
symmetrical time-domain impulse response.
Assigning the appropriate phase for the nonzero H(k) gain factors is,
however, only half the story in building a multisection FSF. There’s good
news to be told. Examination of the frequency response in Eq. (7–45) shows
us a simple way to achieve phase linearity in practice. Substituting
|H(k)|ejφ(k), with φ(k) defined by Eq. (7–46) above, for H(k) in Eq. (7–45) pro-
vides the expression for the frequency response of an even-N multisection
linear-phase complex FSF,
H
(ejω
) =
e–jω(N–1)/2sin(ωN/2)
cplx,lp
[ |H(N/2)|e −jπ/2 (N∑/2)-1 |H(k)|(−1)k ∑N-1 |H(k)|(−1)k ]
x + – ,
sin(ω/2−π/2) sin(ω/2−πk/N) sin(ω/2−πk/N) (7–48)
k=0 k=(N/2)+1
where the “lp” subscript indicates linear phase.
Equation (7–48) is not as complicated as it looks. It merely says the total
FSF frequency response is the sum of individual resonators’ sin(x)/x-like
frequency responses. The first term within the brackets represents the res-
Figure 7–30 Linear phase of H(k) for a single-section FSF: (a) N= 19; (b) N= 20.
snaidaR
f /2 Hz s (atk= 10)
Frequency (k)
snaidaR
Pos. frequency Neg. frequency Pos. frequency Neg. frequency
30 30
20 20 φ(k) for N=
φ(k) for N= 19
20
10 10
0 0
–10 –10
f /2 Hz s –20 (atk= 9.5) –20
0 Hz (DC) 0 Hz (DC)
–30 –30
0 2 4 6 8 10 12 14 16 18 0 2 4 6 8 10 12 14 16 18 20
Frequency (k)
(a) (b)

402 Specialized Digital Networks and Filters
onator centered at k = N/2 (f/2). The first summation is the positive-
s
frequency resonators and the second summation represents the negative-
frequency resonators.
The (–1)kterms in the numerators of Eq. (7–48) deserve our attention be-
cause they are an alternating sequence of plus and minus ones. Thus a single-
section frequency response will be 180o out of phase relative to its
neighboring section. That is, the outputs of neighboring single-section FSFs
will have a fixed π-radians phase difference over the passband common to
both filters as shown in Figure 7–31. (The occurrence of the (–1)kfactors in Eq.
(7–48) is established in Appendix G, Section G.3.)
The effect of those (–1)k factors is profound and not emphasized nearly
enough in the literature of FSFs. Rather than defining each nonzero complex
H(k) gain factor with its linearly increasing phase angles φ(k), we can build a
linear-phase multisection FSF by using just the |H(k)| magnitude values and
incorporating the alternating signs for those real-valued gain factors. In addi-
tion, if the nonzero |H(k)| gain factors are all equal to one, we avoid Figure
7–29’s gain factor multiplications altogether as shown in Figure 7–32(a).
The unity-valued |H(k)| gain factors and the alternating-signed sum-
mation allow the complex gain multiplies in Figure 7–29(b) to be replaced by
simple adds and subtracts as in Figure 7–32(a). We add the even-k and sub-
tract the odd-k resonator outputs. Figure 7–32(b) confirms the linear phase,
with phase discontinuities at the magnitude nulls, of these multisection com-
plex FSFs. The transfer function of the simplified complex linear-phase FSF is
32
k= 3 k= 4 Magnitude responses
16
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
π
k= 4 Phase responses
0
k= 3
–π
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
π
Phase difference
0
–π
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Frequency
Figure 7–31 Comparison of the magnitude and phase responses, and phase
difference, between the k= 3 and the k= 4 FSFs, when N= 32.

7.5 Frequency Sampling Filters: The Lost Art 403
z–1
e j2π2/32
x(n) + y(n)
Frequency
–
z–N
z–1 +
Phase response
e j2π3/32
–1
z–1
e j2π4/32 –1 –0.8 –0.4 0 0.4 0.8 1
(a) (b)
Figure 7–32 SimplifiedN= 32 three-section linear-phase complex bandpass FSF:
(a) implementation; (b) frequency response.
∑N-1 (−1)k
H (z) = (1 –z–N) .
cplx 1−ej2πk/Nz −1 (7–49)
k=0
(We didn’t use the “lp” subscript, meaning linear phase, in Eq. (7–49) be-
cause, from here on, all our complex FSFs will be linear phase.)
7.5.3 Ensuring FSF Stability
So far we’ve discussed complex FSFs with pole/zero cancellation on the unit
circle. However, in practice exact cancellation requires infinite-precision arith-
metic, and real-world binary word quantization errors in the FSF’s coeffi-
cients can make the filter poles lie outside the unit circle. The result would be
an unstable filter, whose impulse response is no longer finite in duration,
which must be avoided. (This is a beautiful example of the time-honored
axiom “In theory, there’s no difference between theory and practice. In prac-
tice, sometimes the theory doesn’t work.”) Even if a pole is located only very
slightly outside the unit circle, roundoff noise will grow as time increases,
corrupting the output samples of the filter. We prevent this problem by mov-
ing the comb filter’s zeros and the resonators’ poles just inside the unit circle
as depicted in Figure 7–33(a). Now the zeros and a pole are located on a circle
of radius r, where the damping factor ris just slightly less than 1.
snaidaR
Magnitude response
0
–10
–20
–30
–40
–1 –0.8 –0.4 0 0.4 0.8 1
5
0
–5
–10
Frequency
Bd

404 Specialized Digital Networks and Filters
We call r the damping factor because a single-stage FSF impulse re-
sponse becomes a damped sinusoid. For example, the real part of the impulse
response of a single-stage complex FSF, where N= 32, k= 2, H(2) = 2, and r=
0.95, is shown in Figure 7–33(b). Compare that impulse response to Figure
7–27(a). The structure of a single-section FSF with zeros and a pole inside the
unit circle is shown in Figure 7–33(c).
The comb filter’s feedforward coefficient is –rN because the new
z-domain transfer function of this comb filter is
V(z)
H (z) = = 1 – rNz–N, (7–50)
comb,r<1 X(z)
with the Nzeros for this comb being located at z (k) =
rej2πk/N,
where k= 0, 1,
r<1
2,. . .,N–1.
Thosez (k) values are the Nroots of rNwhen we set Eq. (7–50) equal to
r<1
zero, yielding z (k)N = (rej2πk/N)N = rN. The z-domain transfer function of the
r<1
resonator in Figure 7–33(b), with its pole located at a radius of r, at an angle of
2πk/N, is
1
H (z) = , (7–51)
res,r<1 1−rej2πk/Nz −1
leading us to the transfer function of a guaranteed-stable single-section com-
plex FSF of
H(k)
H (z) = H (z)H (z)H(k) = (1 – rNz–N)
gs,ss comb,r<1 res,r<1 1−[rej2πk/N]z −1 (7–52)
whose implementation is shown in Figure 7–33(c). The subscript “gs,ss”
means a guaranteed-stable single-section FSF. The z-domain transfer function
of a guaranteed-stable N-section complex FSF is
∑N-1 H(k)
H
gs,cplx
(z) = (1 – rNz–N), 1−[rej2 k/N]z−1 (7–53)
k=0
Impulse response (real)
1 N= 32, k= 2,
r H(2) = 1, x(n) v(n) y(n)
0.5 r= 0.95
z–N z–1 H(k)
0
–0.5
–rN re jωr
Unit circle 0 10 20 30
Time
(a) (b) (c)
Figure 7–33 Ensuring FSF stability: (a) poles and zeros are inside the unit circle; (b) real
part of a stable single-section FSF impulse response; (c) FSF structure.

7.5 Frequency Sampling Filters: The Lost Art 405
where the subscript “gs,cplx” means a guaranteed-stable complex multisec-
tion FSF. The frequency response of a guaranteed-stable multisection complex
FSF (derived in Appendix G, Section G.4) is
∑N-1H(k)e −jπk/Nsinh[Nln(r)/2−jNω/2]
H (ejω) = rN−1 e–jω(N–1)/2 sinh[ln(r)/2−j(ω −2πk/N)/2] . (7–54)
gs,cplx k=0
If we modify the bandpass FSF structure in Figure 7–32(a) to force the
zeros and poles inside the unit circle, we have the structure shown in Figure
7–34(a). The frequency-domain effects of zeros and poles inside the unit circle
are significant, as can be seen in Figure 7–34(b) for the two cases where r =
0.95 and r= 0.9999.
Figure 7–34(b) shows how a value of r = 0.95 badly corrupts our com-
plex bandpass FSF performance; the stopband attenuation is degraded, and
significant phase nonlinearity is apparent. Damping factor r values of less
than unity cause phase nonlinearity because the filter is in a nonreciprocalzero
condition. Recall a key characteristic of FIR filters: To maintain linear phase,
anyz-plane zero located inside the unit circle at z=z (k), where z (k) is not
r<1 r<1
equal to 0, must be accompanied by a zero at a reciprocal location, namely, z=
1/z (k) outside the unit circle. We do not satisfy this condition here, leading
r<1
to phase nonlinearity. (The reader should have anticipated nonlinear phase
due to the asymmetrical impulse response in Figure 7–33(b).) The closer we
can place the zeros to the unit circle, the more linear the phase response. So
Magnitude response
0
z–1 –10
–20
re j2π2/32
x(n) + y(n)
–30
– 0 0.1 0.2 0.3 0.4
z–N
z–1 +
re j2π3/32
–rN
z–1
re j2π4/32
(a) (b)
Figure 7–34 Guaranteed-stable N = 32 three-section linear-phase complex
bandpass FSF: (a) implementation; (b) frequency response for two
values of damping factor r.
Bd
Frequency
Phase response
5
0
–5
–10
0 0.1 0.2 0.3 0.4
snaidaR
r= 0.95
r= 0.9999
0.5
r= 0.9999
r= 0.95
0.5
Frequency

406 Specialized Digital Networks and Filters
the recommendation is to define rto be as close to unity as your binary num-
ber format allows[26]. If integer arithmetic is used, set r = 1–1/2B, where B is
the number of bits used to represent a filter coefficient magnitude.
Another stabilization method worth considering is decrementing the
largest component (either real or imaginary) of a resonator’s
ej2πk/N
feedback
coefficient by one least significant bit. This technique can be applied selec-
tively to problematic resonators and is effective in combating instability due
to rounding errors, which results in finite-precision
ej2πk/N
coefficients having
magnitudes greater than unity.
Thus far we’ve reviewed FSFs with complex coefficients and frequency
magnitude responses not symmetrical about zero Hz. Next we explore FSFs
with real-only coefficients having conjugate-symmetric frequency magnitude
and phase responses.
7.5.4 Multisection Real-Valued FSFs
We can obtain real-FSF structures (real-valued coefficients) by forcing our
complexN-section FSF, where Nis even, to have conjugate poles, by ensuring
that all nonzero H(k) gain factors are accompanied by conjugate H(N–k) gain
factors, so that H(N–k) = H*(k). That is, we can build real-valued FSFs if we
use conjugate pole pairs located at angles of ±2πk/N radians. The transfer
function of such an FSF (derived in Appendix G, Section G.5) is
[
|H(0)| |H(N/2)|
H (z)= (1−rNz −N) +
gs,real 1−rz −1 1+rz −1
N∑/2-12|H(k)|[cos(φ )−rcos(φ −2πk/N)z −1]
k k ,
1−[2rcos(2πk/N)]z −1+r2z −2
(7–55)
k=1
where the subscript “gs,real” means a guaranteed-stable real-valued multi-
section FSF, and φ is the desired phase angle of the kth section. Eq. (7–55) de-
k
fines the structure of a Type-I real FSF to be as shown in Figure 7–35(a),
requiring five multiplies per resonator output sample. The implementation of
a real pole-pair resonator, using real-only arithmetic, is shown in Figure
7–35(b).
Of course, for lowpass FSFs the stage associated with the H(N/2) gain
factor in Figure 7–35 would not be implemented, and for bandpass FSFs nei-
ther stage associated with the H(0) and H(N/2) gain factors would be imple-
mented. The behavior of a single-section Type-I real FSF with N = 32, k = 3,
H(3) = 1, r= 0.99999, and φ = 0 is provided in Figure 7–36.
3
An alternate version of the Type-I FSF, with a simplified resonator struc-
ture, can be developed by setting all φ values equal to zero and moving the
k
gain factor of two inside the resonators. Next we incorporate the alternating

7.5 Frequency Sampling Filters: The Lost Art 407
r
z–1
H(0)
–r z–1
H(N/2)
Type-I real-only resonator
x(n) Type-I y(n) z–1 cos(φ k )
resonator,
k= 1
z–N 2|H(1)|
Type-I
2r.cos(2πk/N)
z–1
–r.cos(φ
k
–2πk/N)
resonator,
k= 2
–rN . .
.
.
.
.
2|H(2)| –r2
Type-I
resonator,
k= (N/2)–1
2|H(N/2-1)|
(a) (b)
Figure 7–35 Guaranteed-stable, even-N, Type-I real FSF: (a) structure; (b) using
real-only resonator coefficients.
signs in the final summation as shown in Figure 7–37 to achieve linear phase,
just as we did to arrive at the linear-phase multisection complex FSF in Figure
7–32(a), by adding the even-k and subtracting the odd-k resonator outputs.
The “±” symbol in Figure 7–37(a) warns us that when Nis even, k= (N/2) –1
can be odd or even.
If the nonzero |H(k)| gain factors are unity, this Type-II real FSF re-
quires only three multiplies per section output sample. When a resonator’s
multiply by 2 can be performed by a hardware binary arithmetic left shift,
Impulse response Magnitude response z-plane
1
0
Isolated
zero
–1
0 10 20 30 –1 0 1
Figure 7–36 Time- and frequency-domain characteristics of a single-section
Type-I FSF when N= 32, k= 3, H(3) = 1, r= 0.99999, and φ3 = 0.
trap
yranigamI
0
–10
–20
–30
–1 –0.5 0 0.5 1
Bd
2
1
0
–1
–2
Time Frequency Real part
(a) (b) (c)

only two multiplies are needed per output sample. The transfer function of
this real-valued FSF is
[
|H(0)| |H(N/2)|
H (z)= (1−rNz −N)
Type-II 1 rz −1 1 rz −1
(7–56)
Neither the Type-I nor the Type-II FSF has exactly linear phase. While
the phase nonlinearity is relatively small, their passband group delays can
have a peak-to-peak fluctuation of up to two sample periods (2/f) when used
s
in multisection applications. This phase nonlinearity is unavoidable because
those FSFs have isolated zeros located at z = rcos(2πk/N), when φ = 0, as
k
shown in Figure 7–36(c). Because the isolated zeros inside the unit circle have
no accompanying reciprocal zeros located outside the unit circle at z =
1/[rcos(2πk/N)], sadly, this causes phase nonlinearity.
While the Type-I and -II FSFs are the most common types described in
the literature of FSFs, their inability to yield exact linear phase has not re-
[
408 Specialized Digital Networks and Filters
r
z–1
|H(0)|
Type-II real-only resonator
–r
z–1
|H(N/2)|
–
z–1 2
x(n) Type-II y(n)
resonator,
–
k= 1
±
z–N |H(1)|
2r.cos(2πk/N)
Type-II
resonator,
z–1
k= 2
–rN . .
. . |H(2)|
. .
–r2
Type-II
resonator,
k= (N/2)–1
|H(N/2–1)|
(a) (b)
Figure 7–37 Linear-phase, even-N, Type-II real FSF: (a) structure; (b) real-only res-
onator implementation.
− +
+ N ∑ /2-1(−1) k |H(k)|[2−2rcos(2πk/N )z −1 ] .
1−[2rcos(2πk/N )]z −1+r 2 z −2
k=1

ceived sufficient attention or analysis. In the next section we take steps to ob-
tain linear phase by repositioning the isolated zero.
7.5.5 Linear-Phase Multisection Real-Valued FSFs
We can achieve exact real-FSF phase linearity by modifying the Type-I real
FSF resonator’s feedforward coefficients, in Figure 7–35(b), moving the iso-
lated zero on top of the comb filter’s zero located at z = r. We do this by set-
ting φ = πk/N. The numerator of the transfer function of one section of the
k
real FSF, from Eq. (7–55), is
cos(φ ) –rcos(φ –2πk/N)z–1.
k k
If we set this expression equal to zero, and let φ = πk/N, we find that the
k
shifted isolated zero location z is
0
cos(φ ) –rcos(φ –2πk/N)z –1= cos(πk/N) –rcos(πk/N–2πk/N)z –1= 0
k k 0 0
or
z = = r.
0
Substituting πk/N for φ in Eq. (7–55) yields the transfer function of a
k
linear-phase Type-III real FSF as
[
H (z) = (1 – rNz–N)
|H(0)|
+
N∑/2-12(−1)k |H(k)|cos(πk/N)[1−rz −1]
.
Type-III 1−rz −1 1−[2rcos(2πk/N)]z −1+r2z −2 (7–57)
k=1
The implementation of the linear-phase Type-III real FSF is shown in
Figure 7–38, requiring four multiplies per section output sample.
[
7.5 Frequency Sampling Filters: The Lost Art 409
rcos(πk/N)
cos(πk/N)
r
z–1
H(0) Type-III real-only resonator
x(n) Type-III
resonator, z–1
k= 1 –
y(n)
z–N 2|H(1)|cos(π/N)
Type-III
resonator, ± 2r.cos(2πk/N) z–1 –r
k= 2
–rN .
.
.
.
2|H(2)|cos(2π/N)
. .
–r2
Type-III
resonator,
k= (N/2)–1
2|H(N/2–1)|cos[(N/2–1)π/N]
(a) (b)
Figure 7–38 Even-N, linear-phase, Type-III real FSF: (a) structure; (b) real-only res-
onator implementation.

410 Specialized Digital Networks and Filters
Notice that the H(N/2), f/2, section is absent in Figure 7–38(a). We jus-
s
tify this as follows: The even-NType-I, -II, and -III real-FSF sections have im-
pulse responses comprising N nonzero samples. As such, their k = N/2
sections’ impulse responses, comprising even-length sequences of alternating
plus and minus ones, are not symmetrical. This asymmetry property would
corrupt the exact linear phase should a k = N/2 section be included. Conse-
quently, as with even-length nonrecursive FIR filters, even-N Type-I, -II, and
-III real FSFs cannot be used to implement linear-phase highpass filters.
Figure 7–39 shows the frequency-domain performance of an eight-
section Type-III FSF, for N = 32 where the eight sections begin at DC (0 ≤ k ≤
7). Figure 7–39(c) provides a group delay comparison between the Type-III
FSF and an equivalent eight-section Type-II FSF showing the improved Type-
III phase linearity having a constant group delay of (N–1)/2 samples over the
passband.
Magnitude response
0 0.2 0.4 0.6 0.8 1
Figure 7–39 Interpolated frequency-domain response of a Type-III FSF having
eight sections with N= 32: (a) magnitude response; (b) phase re-
sponse; (c) group delay compared with an equivalent Type-II FSF.
Bd
Phase response
snaidaR
0
–10
–20
–30
0
–5
–10
–15
–20
–25
0 0.2 0.4 0.6 0.8 1
Group delay
16.5
15.5
14.5
sdoirep
elpmaS
(a)
(b)
Type-II FSF
(c)
Type-III real FSF
13.5
0 0.2 0.4 0.6 0.8 1
Frequency

7.5 Frequency Sampling Filters: The Lost Art 411
7.5.6 Where We’ve Been and Where We’re Going with FSFs
We’ve reviewed the structure and behavior of a complex FSF whose complex
resonator stages had poles residing atop the zeros of a comb filter, resulting in
a recursive FIR filter. Next, to ensure filter implementation stability, we forced
the pole/zero locations to be just inside the unit circle. We examined a
guaranteed-stable even-N Type-I real FSF having resonators with conjugate
pole pairs, resulting in an FSF with real-valued coefficients. Next, we modi-
fied the Type-I real-FSF structure, yielding the more computationally efficient
but only moderately linear-phase Type-II real FSF. Finally, we modified the
coefficients of the Type-I real FSF and added post-resonator gain factors, re-
sulting in the exact linear-phase Type-III real FSF. During this development,
we realized that the even-N Type-I, -II, and -III real FSFs cannot be used to
implement linear-phase highpass filters.
In the remainder of this section we introduce a proposed resonator
structure that provides superior filtering properties compared to the Type-I,
-II, and -III resonators. Next we’ll examine the use of nonzero transition band
filter sections to improve overall FSF passband ripple and stopband attenua-
tion, followed by a discussion of several issues regarding modeling and de-
signing FSFs. We’ll compare the performance of real FSFs to their equivalent
Parks-McClellan-designed N-tap nonrecursive FIR filters. Finally, a detailed
FSF design procedure is presented.
7.5.7 An Efficient Real-Valued FSF
There are many real-valued resonators that can be used in FSFs, but of partic-
ular interest to us is the Type-IV resonator presented in Figure 7–40(a). This
resonator deserves attention because it
• is guaranteed stable,
• exhibits highly linear phase,
• uses real-valued coefficients,
• is computationally efficient,
• can implement highpass FIR filters, and
• yields stopband attenuation performance superior to the Type-I, -II, and
-III FSFs.
From here on, the Type IV will be our FSF of choice.
Cascading Type-IV resonators with a comb filter provide a Type-IV real-
only FSF with a transfer function of
∑N/2 (−1)k|H(k)|(1−r2z2)
H (z) = (1 – rNz–N)
Type-IV 1−2rcos(2πk/N)z −1+r2z −2 (7–58)
k=0

412 Specialized Digital Networks and Filters
Type-IV resonator SimplifiedType-IV resonator
z–1
z–1
2r.cos(2πk/N)
2r.cos(2πk/N) z–1
z–1
–r2
–r2 –r2
(a) (b)
Figure 7–40 Type-IV resonator: (a) original structure; (b) simplified version.
where N is even. (Note: When N is odd, k = N/2 is not an integer and the
|H(N/2)| term does not exist.) As derived in Appendix G, Section G.6, the
Type-IV FSF frequency response is
H Type-IV (ejω) = e–jωN/2
∑N/2(−1)k|H(k)|
c
[c
o
o
s
s
(2
(ω
π
N
k/
/
N
2
)
−
−
ω
c
)
o
−
s(
c
ω
o
)
s(ωN/2+ω)]
. (7–59)
k=0
The even-Nfrequency and resonance magnitude responses for a single Type-
IV FSF section are
cos(ωN/2−ω)−cos(ωN/2+ ω)
H (ejω) = e–jωN/2
Type-IV cos(2πk/N)−cos(ω) (7–60)
and
N
|H (ejω)| = N, k = 1, 2, ..., –1,
Type-IV ω=2πk/N 2 (7–61)
and its resonant magnitude gains at k= 0 (DC), and k=N/2 (f/2), are
s
|H
(ejω
)| = |H
(ejω
)| = 2N. (7–62)
Type-IV ω=0 Type-IV ω=π
To reduce the number of resonator multiply operations, it is tempting to
combine the dual –r2 factors in Figure 7–40(a) to simplify the Type-IV res-
onator as shown in Figure 7–40(b). However, a modification reducing both
the number of multiply and addition operations in each FSF resonator is to
implement the (1–r2z–2) term in the numerator of Eq. (7–58) as a second-order
comb filter as shown in Figure 7–41(a)[27]. This transforms the Type-IV res-
onator to that shown in Figure 7–41(b). The |H(0)|/2 and |H(N/2)|/2 gain
factors compensate for the gain of 2Nfor a Type-IV resonator in Eq. (7–62).
The cascaded-comb subfilter combination has N zeros spaced around
the unit circle with dual z-plane zeros at z= 1 (0 Hz) and dual zeros at z= –1

7.5 Frequency Sampling Filters: The Lost Art 413
Modified Type-IV resonators
k= 0
|H(0)|/2
Modified Type-IV
k= 1
resonator
|H(1)|
x(n) – y(n)
k= 2 z–1
±
. . ±
z–N z–2 .
.
.
.
|H(2)|
2r.cos(2πk/N)
z–1
k= (N/2)-1
–rN –r2
|H(N/2–1)| –r2
k= (N/2)
|H(N/2)|/2
(a) (b)
Figure 7–41 Type-IV, even-N, real FSF: (a) structure; (b) its modified resonator.
(f/2 Hz). However, the k= 0 and k=N/2 Type-IV resonators have dual poles
s
atz= ±1, respectively, enabling the necessary pole/zero cancellation.
The “±” symbols in Figure 7–41(a) remind us, again, that when Nis even,
k=N/2 could be odd or even. This FSF has the very agreeable characteristic of
having an impulse response whose length is N+1 samples. Thus, unlike the
Type-I, -II, and -III FSFs, an even-NType-IV FSF’s k=N/2 section impulse re-
sponse (alternating ±1s) is symmetrical, and Type-IV FSFs can be used to build
linear-phase highpass filters. When N is odd, the k = N/2 section is absent in
Figure 7–41, and the odd-NType-IV transfer function is identical to Eq. (7–58)
with the summation’s upper limit being (N–1)/2 instead of N/2.
We’ve covered a lot of ground thus far, so Table 7–3 summarizes what
we’ve discussed concerning real-valued FSFs. The table lists the average pass-
band group delay measured in sample periods, the number of multiplies and
additions necessary per output sample for a single FSF section, and a few re-
marks regarding the behavior of the various real FSFs. The section gains at
their resonant frequencies, assuming the desired |H(k)| gain factors are
unity, is Nfor all four of the real-valued FSFs.
7.5.8 Modeling FSFs
We derived several different
H(ejω
) frequency response equations here, not so
much to use them for modeling FSF performance but to examine them in
order to help define the structure of our FSF block diagrams. For example, it

414 Specialized Digital Networks and Filters
Table 7–3 Summary of Even-N Real FSF Properties
Real FSF type Group delay Multiplies Adds Remarks
Type I N 5 3 Real-coefficient FSF. Phase
(Figure 7–35) 2 is only moderately linear.
Type II N 3 3 Modified, and more efficient,
(Figure 7–37) 2 version of the Type-I FSF.
Phase is only moderately
linear. Abinary left shift
may eliminate one resonator
multiply.
Type III
N−1
4 3 Very linear phase. Cannot
(Figure 7–38) 2 implement a linear-phase
highpass filter.
Type IV N 2 2 Very linear phase. Improved
(Figure 7–41) 2 stopband attenuation. Usable
for lowpass, bandpass, or
highpass filtering.
was analyzing the properties of H
(ejω
) that motivated the use of the 1/2
Type-IV
gain factors in the Type-IV FSF structure.
When modeling the FSFs, fortunately it’s not necessary to write code to
calculate frequency-domain responses using the various
H(ejω
) equations pro-
vided here. All that’s necessary is code to compute the time-domain impulse
response of the FSF being modeled, pad the impulse response with enough
zeros so the padded sequence is 10 to 20 times the length of the original, per-
form a discrete Fourier transform (DFT) on the padded sequence, and plot the
resulting interpolated frequency-domain magnitude and phase. Of course,
forcing your padded sequence length to be an integer power of two enables
the use of the efficient FFT to calculate frequency-domain responses. Alterna-
tively, many commercial signal processing software packages have built-in
functions to calculate filter frequency response curves based solely on the fil-
ter’s transfer function coefficients.
7.5.9 Improving Performance with Transition
Band Coefficients
We can increase FSF stopband attenuation if we carefully define |H(k)| mag-
nitude samples in the transition region, between the passband and stopband.
For example, consider a lowpass Type-IV FSF having seven sections, of unity
gain, with N = 32, whose desired performance is shown in Figure 7–42(a),
and whose interpolated (actual) frequency magnitude response is provided in
Figure 7–42(b).

7.5 Frequency Sampling Filters: The Lost Art 415
Lowpass FSF |H(k)| samples
1
0.75
0.5
0.25
0
0 0.2 0.4 0.6 0.8
Frequency
(a)
Figure 7–42 Seven-section, N= 32, Type-IV FSF: (a) desired frequency response;
(b) actual performance; (c) using one transition sample; (d) im-
proved stopband attenuation.
The assignment of a transition magnitude sample, coefficient T whose
1
value is between 0 and 1 as in Figure 7–42(c), reduces the abruptness in the
transition region between the passband and the stopband of our desired mag-
nitude response. Setting T = 0.389 results in reduced passband ripple and the
1
improved stopband sidelobe suppression shown in Figure 7–42(d). The price
we pay for the improved performance is the computational cost of an addi-
tional FSF section and increased width of the transition region.
Assigning a coefficient value of T = 0.389 was not arbitrary or magic.
1
Measuring the maximum stopband sidelobe level for various values of 0 ≤T
1
≤ 1 reveals the existence of an optimum value for T . Figure 7–43 shows that
1
the maximum stopband sidelobe level is minimized when T = 0.389. The
1
minimum sidelobe level of –46 dB (when T = 0.389) corresponds to the
1
height of the maximum stopband sidelobe in Figure 7–42(d). This venerable
and well-known method of employing a transition region coefficient to
edutingaM
|H(6)|
Bd
|H(7)|
Lowpass FSF |H(k)| samples
1
0.75
0.5
0.25
0
0 0.2 0.4 0.6 0.8
(c)
edutingaM
|H(6)|
|H(7)| = T
1
Frequency
Bd
Magnitude response
0
–10
–20
–30
–40
–50
0 0.2 0.4 0.6 0.8 1
Frequency
(b)
Magnitude response
0
–10
–20
–30
–40
–50
0 0.2 0.4 0.6 0.8 1
Frequency
(d)

416 Specialized Digital Networks and Filters
0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 0.46
Figure 7–43 Maximum stopband sidelobe level as a function of the transition region
coefficient value for a seven-section Type-IV real FSF when N= 32.
reduce passband ripple and minimize stopband sidelobe levels also applies to
bandpass FSF design where the transition sample is used just before and just
after the filter’s passband unity-magnitude |H(k)| samples.
Further stopband sidelobe level suppression is possible if two transition
coefficients, T and T , are used such that 0 ≤ T ≤ T ≤ 1. (Note: For lowpass
1 2 2 1
filters, if T is the |H(k)| sample, then T is the |H(k+1)| sample.) Each addi-
1 2
tional transition coefficient used improves the stopband attenuation by ap-
proximately 25 dB. However, finding the optimum values for the T
coefficients is a daunting task. Optimum transition region coefficient values
depend on the number of unity-gain FSF sections, the value of N, and the
number of coefficients used; and unfortunately there’s no closed-form equa-
tion available to calculate optimum transition coefficient values. We must
search for them empirically.
If one coefficient (T ) is used, finding its optimum value can be consid-
1
ered a one-dimensional search. If two coefficients are used, the search be-
comes two-dimensional, and so on. Fortunately, descriptions of linear algebra
search techniques are available[23,28–30], and commercial mathematical soft-
ware packages have built-in optimization functions to facilitate this computa-
tionally intensive search. For Type-I/II FSFs, tables of optimum transition
region coefficients for various N, and number of FSF sections used, have been
published[23], and a subset of those tables is provided as an appendix in a
textbook[24]. For the higher-performance Type-IV FSFs, tables of optimum
coefficient values have been compiled by the author and are provided in Ap-
pendix H. With this good news in mind, shortly we’ll look at a real-world FSF
design example to appreciate the benefits of FSFs.
7.5.10 Alternate FSF Structures
With the primary comb filter in Figure 7–41 having a feedforward coefficient
of –rN, its even-N transfer function zeros are equally spaced around and in-
)Bd(
eboledis
dnabpots
xaM
–35
–40
–45
–50
T Transition sample magnitude
1

7.5 Frequency Sampling Filters: The Lost Art 417
N even N odd
Case I: comb
feedforward
coefficient
is –rN
(a) (b)
1
Case II: comb
feedforward
0
coefficient
is +rN
–1
–1 0 1
Real part
(c) (d)
Figure 7–44 Four possible orientations of comb filter zeros near the unit circle.
side the unit circle, as shown in Figures 7–23(c) and 7–44(a), so the k= 1 zero
is at an angle of 2π/Nradians. In this Case I, if Nis odd, the comb’s zeros are
spaced as shown in Figure 7–44(b). An alternate situation, Case II, exists
when the comb filter’s feedforward coefficient is +rN. In this mode, an even-N
comb filter’s zeros are rotated counterclockwise on the unit circle by π/N ra-
dians as shown in Figure 7–44(c), where the comb’s k= 1 zero is at an angle of
3π/Nradians[27]. The k= 0 zeros are shown as solid dots.
The structure of a Case II FSF is identical to that of a Case I FSF; how-
ever, the Case II resonators’ coefficients must be altered to rotate the poles by
π/N radians to ensure pole/zero cancellation. As such, the transfer function
of a Case II, even-N, Type-IV FSF is
∑N/2 (−1) k|H(k)|(1−r 2 z −2)
H (z) = (1 + rNz–N) .
Case II,Type-IV 1−2rcos[2π(k + 1/2)/N]z −1+r 2 z −2 (7–63)
k=0
Because a Case II FSF cannot have a pole or a zero at z = 1 (DC), these
FSFs are unable to build lowpass filters, in which case the z–2-delay comb fil-
ter in Figure 7–41(a) is not implemented. Table 7–4 lists the filtering capabili-
ties of the various Case I and Case II Type-IV FSFs.
trap
yranigamI
1
0
–1
–1 0 1
Real part
trap
yranigamI
1
0
–1
–1 0 1
Real part
trap
yranigamI
1
0
-1
–1 0 1
Real part
trap
yranigamI
N= 8 N= 9
4π/N
2π/N
N= 8 N= 9
3π/N
5π/N

418 Specialized Digital Networks and Filters
Table 7–4 Type-IV FSF Modes of Operation
Type-IV FSF Lowpass Bandpass Highpass
Case I, Neven Yes Yes Yes
Case I, Nodd Yes Yes No
Case II, Neven No Yes No
Case II, Nodd No Yes Yes
Their inability to implement lowpass filtering limits the utility of Case II
FSFs, but their resonators’ shifted center frequencies do provide some addi-
tional flexibility in defining the cutoff frequencies of bandpass and highpass
filters. The remainder of this material considers only the Case I Type-IV FSFs.
7.5.11 The Merits of FSFs
During the formative years of FIR filter theory (early 1970s) a computer pro-
gram was made available for designing nonrecursive FIR filters using the
Parks-McClellan (PM) method[31]. This filter design technique provided
complete control in defining desired passband ripple, stopband attenuation,
and transition region width. (FIR filters designed using the PM method are
often called optimal FIR, Remez Exchange, or equiripple filters.) Many of the
early descriptions of the PM design method included a graphical comparison
of various lowpass FIR filter design methods similar to that redrawn in Fig-
ure 7–45[29,32,33]. In this figure, a given filter’s normalized (to impulse re-
sponse length N, and the sample rate f) transition bandwidth is plotted as a
s
function of its minimum stopband attenuation. (Passband peak-to-peak rip-
ple values, measured in dB, are provided numerically within the figure.)
Because the normalized transition bandwidth measure Din Figure 7–45
is roughly independent of N, this diagram is considered to be a valid perfor-
mance comparison of the three FIR filter design methods. For a given pass-
band ripple and stopband attenuation, the PM-designed FIR filter could
exhibit the narrowest transition bandwidth, thus the highest performance fil-
ter. The wide dissemination of Figure 7–45 justifiably biased filter designers
toward the PM design method. (During his discussion of Figure 7–45, one au-
thor boldly declared, “The smaller is D, the better is the filter”[32].) Given its
flexibility, improved performance, and ease of use, the PM method quickly
became the predominant FIR filter design technique. Consequently, in the
1970s FSF implementations lost favor to the point where they’re rarely men-
tioned in today’s DSPclassrooms or textbooks.
However, from a computational workload standpoint, Figure 7–45 is
like modern swimwear: what it shows is important; what it does not show is
vital. That design comparison does not account for those situations where

7.5 Frequency Sampling Filters: The Lost Art 419
Figure 7–45 Comparison of the Kaiser window, frequency sampling, and Parks-
McClellan-designed FIR filters.
both frequency sampling and PM filters meet the filter performance require-
ments but the frequency sampling implementation is more computationally
efficient. The following FIR filter design example illustrates this issue and
shows why FSFs should be included in our bag of FIR filter design tools.
7.5.12 Type-IV FSF Example
Consider the design of a linear-phase lowpass FIR filter whose cutoff fre-
quency is 0.05 times the sample rate f, the stopband must begin at 0.095 times
s
f, the maximum passband ripple is 0.3 dB peak to peak, and the minimum
s
stopband attenuation must be 65 dB. If we designed a six-section Type-IV FSF
with N = 62 and r = 0.99999, its frequency-domain performance satisfies our
requirements and is the solid curve in Figure 7–46(a).
For this FSF, two transition region coefficients of |H(4)| = T = 0.589921
1
and |H(5)| = T = 0.104964 were used. Included in Figure 7–46(a), the dashed
2
curve, is the magnitude response of a PM-designed 60-tap nonrecursive FIR
filter. Both filters meet the performance requirements and have linear phase.
The structure of the Type-IV FSF is provided in Figure 7–47. APM-designed
filter implemented using a folded nonrecursive FIR structure (as discussed in
Section 13.7), exploiting its impulse response symmetry to halve the number
of multipliers, requires 30 multiplies and 59 adds per output sample. We see
the computational savings of the Type-IV FSF, which requires only 17 multi-
plies and 19 adds per output sample. (Note: The FSF’s H(k) gain factors are all
zero for 6 ≤k≤31.)
)erusaem
htdiw
noitisnart(D
6 Kaiser
window
0.0017
5
Frequency
sampling 0.243
4
0.0174
0.486
3 Parks-
0.97
McClellan
2
0.174
0.347
0.87
1
1.74
–80 –60 –40
Stopband attenuation (dB)

420 Specialized Digital Networks and Filters
Figure 7–46 An N = 62, six-section Type-IV real FSF (solid) versus a 60-tap PM-
designed filter (dashed): (a) frequency magnitude response; (b)
passband response detail.
7.5.13 When to Use an FSF
In this section we answer the burning question “When is it smarter (more
computationally efficient) to use a Type-IV FSF instead of a PM-designed FIR
filter?” The computational workload of a PM-designed FIR filter is directly
related to the length of its time-domain impulse response, which in turn is in-
versely proportional to the filter transition bandwidth. The narrower the tran-
sition bandwidth, the greater the nonrecursive FIR filter’s computational
workload measured in arithmetic operations per output sample. On the other
Bd
0 0
–0.5
–20
–1.0
–40 –1.5
–60
–2.5
–80 –3.0
Bd
Magnitude response Magnitude response
–2.0
Passband
0 0.1f s 0.2f s 0.3f s 0.4f s 0 0.025f s 0.05f s 0.075f s 0.1f s
Frequency Frequency
(a) (b)
Modified Type-IV resonators
k= 0
0.5
k= 1
k= 2 –
x(n) y(n)
–
k= 3 –
z–N z–2
0.589921
k= 4
–rN –r2
0.104964
k= 5
Figure 7–47 Structure of the N = 62, six-section, lowpass, Type-IV FSF with two
transition region coefficients.

7.5 Frequency Sampling Filters: The Lost Art 421
hand, the computational workload of an FSF is roughly proportional to its
passband width. The more FSF sections needed for wider bandwidths and
the more transition region coefficients used, the higher the FSF’s computa-
tional workload. So in comparing the computational workload of FSFs to PM-
designed FIR filters, both transition bandwidth and passband width must be
considered.
Figure 7–48 provides a computational comparison between even-N
Type-IV FSFs and PM-designed nonrecursive lowpass FIR filters. The curves
represent desired filter performance parameters where a Type-IV FSF and a
PM-designed filter have approximately equal computational workloads per
output sample. (The bandwidth values in the figure axis are normalized to
the sample rate, so, for example, a frequency value of 0.1 equals f/10.) If the
s
desired FIR filter transition region width and passband width combination (a
point) lies beneath the curves in Figure 7–48, then a Type-IV FSF is computa-
tionally more efficient than a PM-designed filter. Figure 7–48(a) is a compari-
son accounting only for multiply operations. For filter implementations
where multiplies and adds require equal processor clock cycles, Figure
7–48(b) provides the appropriate comparison. The solitary black dot in the
figure indicates the comparison curves location for the above Figure 7–46
Type-IV FSF filter example. (AType-IV FSF design example using the curves
in Figure 7–48 will be presented later.)
Multiplication only efficiency Multiplication and addition efficiency
0.3 0.3
N= 16 N= 64
1 transition coefficient
N= 24 N= 128
0.25 2 transition coefficents 0.25
N= 32 N= 256
3 transition coefficients
0.2 0.2
PM more efficient
PM more efficient
0.15 0.15
0.1 0.1
Type-IV FSF
example
0.05 0.05
FSF more efficient FSF more efficient
0 0
0 0.02 0.06 0.1 0.13 0 0.04 0.08 0.12 0.
Transition bandwidth /f Transition bandwidth / f
s s
(a) (b)
Figure 7–48 Computational workload comparison between even-N lowpass
Type-IV FSFs and nonrecursive PM FIR filters: (a) only multiplications
considered; (b) multiplications and additions considered. (No 1/N
gain factor used.)
f
/
htdiw
dnabssaP
s

422 Specialized Digital Networks and Filters
The assumptions made in creating Figure 7–48 are an even-N Type-IV
FSF, damping factor r= 0.99999, and no 1/Ngain scaling from Figure 7–20(b).
The PM filter implementation used in the comparison assumes a symmetrical
impulse response, an M-tap folded structure requiring M/2 multiplies, and
M–1 additions. The performance criteria levied on the PM filter are typical
Type-IV FSF properties (when floating-point coefficients are used) given in
Table 7–5.
Table 7–5 Typical Even-NType-IV FSF Properties
Parameter 1-coefficient 2-coefficient 3-coefficient
Passband peak-peak
ripple (dB) 0.7 0.35 0.16
Minimum stopband
attenuation (dB) –45 –68 –95
The Type-IV resonators have a gain of N, so the subsequent gain of the
FSF in Figure 7–47 is N. For FSF implementations using floating-point num-
bers, this gain may not be a major design issue. In filters implemented with
fixed-point numbers, the gain of Ncould cause overflow errors, and a gain re-
duction by a scaling factor of 1/Nmay be necessary. Occasionally, in the liter-
ature, a 1/Nscaling factor is shown as a single multiply just prior to an FSF’s
comb filter[24,34]. This scale factor implementation is likely to cause an intol-
erable increase in the input signal quantization noise. A more practical ap-
proach is to apply the 1/N scaling at the output of each FSF section, prior to
the final summation, as indicated in Figure 7–20(b). Thus every FSF resonator
output is multiplied by a non-unity value, increasing the computational
workload of the FSF. In this situation, the computational comparison between
even-NType-IV FSFs and PM filters becomes as shown in Figure 7–49.
Of course, if N is an integer power of two, some hardware implementa-
tions may enable resonator output 1/N scaling with hardwired binary arith-
metic right shifts to avoid explicit multiply operations. In this situation, the
computational workload comparison in Figure 7–48 applies.
As of this writing, programmable DSP chips typically cannot take ad-
vantage of the folded FIR filter structure assumed in the creation of Figures
7–29 and 7–30. In this case, an M-tap direct convolution FIR filter has the dis-
advantage that it must perform the full M multiplies per output sample.
However, DSP chips have the advantage of zero-overhead looping and single-
clock-cyclemultiply and accumulate(MAC) capabilities, making them more ef-
ficient for direct convolution FIR filtering than for filtering using the recursive
FSF structures. Thus, a DSPchip implementation’s disadvantage of more nec-

7.5 Frequency Sampling Filters: The Lost Art 423
Multiplication only efficiency Multiplication and addition efficiency
0.25
0.14
With 1/N scaling With 1/N scaling
0.12 0.2
PM more efficient PM more efficient
0.1
0.15
0.08
0.06
0.1
0.04
FSF more efficient FSF more efficient
0.02 0.05
0 0.04 0.08 0.12 0 0.04 0.08 0.12
Transition bandwidth / f
s
(a) (b)
Figure 7–49 Computational workload comparison between even-N lowpass
Type-IV FSFs, with resonator output 1/Nscaling included, and nonre-
cursive PM FIR filters: (a) only multiplications considered; (b) multipli-
cations and additions considered.
essary computations and its advantage of faster execution of those computa-
tions roughly cancel each other. With those thoughts in mind, we’re safe to
use Figures 7–48 and 7–49 as guides in deciding whether to use a Type-IV
FSF or a PM-designed FIR filter in a programmable DSP chip filtering appli-
cation.
Finally, we conclude, from the last two figures, that Type-IV FSFs are
more computationally efficient than PM-designed FIR filters in lowpass ap-
plications where the passband is less than approximately f/5 and the transi-
s
tion bandwidth is less than roughly f/8.
s
7.5.14 Designing FSFs
The design of a practical Type-IV FSF comprises three stages: (1) determine if
an FSF can meet the desired filter performance requirements, (2) determine if
a candidate FSF is more, or less, computationally efficient than an equivalent
PM-designed filter, and (3) design the FSF and verify its performance. To aid
in the first stage of the design, Figure 7–50 shows the typical minimum stop-
band attenuation of Type-IV FSFs, as a function of transition bandwidth, for
various numbers of transition region coefficients. (Various values for N, and
typical passband ripple performance, are given within Figure 7–50 as general
reference parameters.)
f
/
htdiw
dnabssaP
s
Transition bandwidth / f
s
f/htdiwdnabssaP s

424 Specialized Digital Networks and Filters
256 No transition coefficients
64 32 16
One transition coefficient
128
256 64
16 Two transition coefficients
128
32
Three transition
256 64 16 coefficients
0 0.05f 0.1f 0.15f 0.2f
s s s s
Transition bandwidth
Figure 7–50 Typical Type-IV lowpass FSF stopband attenuation performance as a
function of transition bandwidth.
In designing an FSF, we find the value N to be a function of the desired
filter transition bandwidth, and the number of FSF sections required is deter-
mined by both Nand the desired filter bandwidth. Specifically, given a linear-
phase FIR filter’s desired passband width, passband ripple, transition
bandwidth, and minimum stopband attenuation, the design of a linear-phase
lowpass Type-IV FSF proceeds as follows:
1. Using Figure 7–50, determine which shaded performance band in the
figure satisfies the desired minimum stopband attenuation. This step de-
fines the number of transition coefficients necessary to meet stopband
attenuation requirements.
2. Ensure that your desired transition bandwidth value resides within the
chosen shaded band. (If the transition bandwidth value lies to the right
of a shaded band, a PM-designed filter will be more computationally ef-
ficient than a Type-IV FSF and the FSF design proceeds no further.)
3. Determine if the typical passband ripple performance, provided in Fig-
ure 7–50, for the candidate shaded band is acceptable. If so, a Type-IV
FSF can meet the desired lowpass filter performance requirements. If
not, a PM design should be investigated.
4. Based on arithmetic implementation priorities, determine the appropri-
ate computational workload comparison chart to be used from Figure
)Bd(
noitaunetta
dnabpots
niM
0
R = passband
(R≈ 1.6 dB) peak-peak ripple
–20
(R≈ 0.7 dB)
–40
32
–60
(R≈ 0.35 dB)
–80
(R≈ 0.16 dB)
–100 256 128 64
32
16
–120
0.25f
s

7.5 Frequency Sampling Filters: The Lost Art 425
7–48 or Figure 7–49 in determining if an FSF is more efficient than a PM-
designed nonrecursive filter.
5. Using the desired filter’s transition bandwidth and passband width as
coordinates of a point on the selected computational workload compari-
son chart, determine if that point lies beneath the appropriate curve. If so,
an FSF can meet the filter performance requirements with fewer compu-
tations per filter output sample than a PM-designed filter. (If the transi-
tion bandwidth/passband point lies above the appropriate curve, a PM
design should be investigated and the FSF design proceeds no further.)
6. Assuming the designer has reached this step of the evaluation process,
the design of a Type-IV FSF proceeds by selecting the filter order N. The
value of Ndepends on the desired transition bandwidth and the number
of transition coefficients from Step 1 and, when the transition bandwidth
is normalized to f, can be estimated using
s
f (number of transition coefficients + 1)
N≈ s .
(7–64)
transition bandwidth
7. The required number of unity-gain FSF sections, integer M, is roughly
proportional to the desired normalized double-sided passband width
divided by the FSF’s frequency resolution (f/N) and is estimated using
s
2N(passband width)
M≈ .
f (7–65)
s
8. Given the initial integer values for N and M, find the appropriate opti-
mized transition coefficient gain values from the compiled tables in Ap-
pendix H. If the tables do not contain optimized coefficients for the
given N and M values, the designer may calculate approximate coeffi-
cients by linear interpolation of the tabulated values. Alternatively, an
optimization software program may be used to find optimized transi-
tion coefficients.
9. Using the values for N,M, and the optimized transition coefficients, de-
termine the interpolated (actual) frequency response of the filter as a
function of those filter parameters. This frequency response can be ob-
tained by evaluating Eq. (7–59). More conveniently, the frequency re-
sponse can be determined with a commercial signal processing software
package to perform the DFT of the FSF’s impulse response sequence, or
by using the transfer function coefficients in Eq. (7–58).
10. Next the fun begins as the values for NandMare modified slightly, and
Steps 8 and 9 are repeated, as the design process converges to a mini-
mum value of M to minimize the computational workload, and opti-
mized transition coefficients maximizing stopband attenuation.

426 Specialized Digital Networks and Filters
11. When optimized filter parameters are obtained, they are then used in a
Type-IV FSF implementation, as in Figure 7–47.
12. The final FSF design step is to sit back and enjoy a job well done.
The Type-IV FSF example presented in Figures 7–46 and 7–47 provides
an illustration of Steps 6 and 7. The initial design estimates for NandMare
f (2+1) (2)(62)(0.05f )
N init. ≈ (0.09 s 5−0.05)f ≈66, and M init. ≈ f s ≈6.2.
s s
Repeated iterations of Steps 8 through 11 converge to the parameters of N =
62 and M= 6 that satisfy the desired filter performance specifications.
7.5.15 FSF Summary
We’ve introduced the structure, performance, and design of frequency sam-
pling FIR filters. Special emphasis was given to the practical issues of phase
linearity, stability, and computational workload of these filters. In addition,
we presented a detailed comparison of the high-performance Type-IV FSF
and its nonrecursive FIR equivalent. Performance curves were presented to
aid the designer in choosing between a Type-IV FSF and a Parks-McClellan-
designed FIR filter for a given narrowband linear-phase filtering application.
We found that
• Type-IV FSFs are more computationally efficient, for certain stopband
attenuation levels, than Parks-McClellan-designed nonrecursive FIR fil-
ters in lowpass (or highpass) applications where the passband is less
thanf/5 and the transition bandwidth is less than f/8 (see Figures 7–29
s s
and 7–30);
• FSFs are modular; their components (sections) are computationally iden-
tical and well understood;
• tables of optimum transition region coefficients, used to improve Type-IV
FSF performance, can be generated (as was done in Appendix H); and
• although FSFs use recursive structures, they can be designed to be guar-
anteed stable and have linear phase.
REFERENCES
[1] Hamming, R. Digital Filters, 3rd ed., Dover, Mineola, New York, 1998, pp. 133–137.
[2] Selby, S. CRC Standard Mathematical Tables, 21st ed., CRC Press, Boca Raton, Florida, 1973, p. 453.
[3] Proakis, J., and Manolakis, D. Digital Signal Processing: Principles, Algorithms, and Applica-
tions, 3rd ed., Prentice Hall, Upper Saddle River, New Jersey, 1996, pp. 656–657.

References 427
[4] Rabiner, L., and Gold, B. Theory and Application of Digital Signal Processing, Prentice Hall,
Upper Saddle River, New Jersey, 1975, p. 169.
[5] Hamming, R. Numerical Methods for Scientists and Engineers, 2nd ed., Dover, Mineola, New York,
1986, pp. 590–591.
[6] Sklar, B. Digital Communications: Fundamentals and Applications, 2nd ed., Prentice Hall, Engle-
wood Cliffs, New Jersey, 1988, pp. 122–127.
[7] Engelberg, S. Random Signals and Noise: A Mathematical Introduction, CRC Press, Boca
Raton, Florida, 2007, Chapter 6.
[8] Shanmugam, K. Digital and Analog Communication Systems, John Wiley and Sons, New
York, 1979, Chapter 8.
[9] Mahafza, B. Introduction to Radar Analysis, CRC Press, Boca Raton, Florida, 1998, Chapter
6.
[10] Poor, H. Vincent. An Introduction to Signal Detection and Estimation, Springer Publishing,
New York, 1998.
[11] Van Trees, H. Detection, Estimation, and Modulation Theory, Part I, Wiley-Interscience Pub-
lishing, Malden, Massachusetts, 2001.
[12] Mitra, S. K., et al. “Interpolated Finite Impulse Response Filters,” IEEE Trans. Acoust.
Speech, and Signal Proc., Vol. ASSP-32, June 1984, pp. 563–570.
[13] Vaidyanathan, P. Multirate Systems and Filter Banks, Prentice Hall, Englewood Cliffs, New
Jersey, 1993.
[14] Mehrnia, A., and Willson, A., Jr. “On Optimal IFIR Filter Design,”Proc. of the 2004 Interna-
tional Symp. on Circuits and Systems (ISCAS), Vol. 3, May 23–26, 2004, pp. 133–136.
[15] Crochiere, R., and Rabiner, L. “Interpolation and Decimation of Digital Signals—ATutor-
ial Review,” Proceedings of the IEEE, Vol. 69, No. 3, March 1981, pp. 300–331.
[16] Kaiser, J. “Nonrecursive Digital Filter Design Using Io-Sinh Window Function,” Proc. 1974
IEEE Int. Symp. Circuits Systems, April 1974, pp. 20–23.
[17] Harris, F. “Digital Signal Processing for Digital Modems,” DSPWorld Spring Design Con-
ference, Santa Clara, California, April 1999.
[18] Lim, Y. “Frequency-Response Masking Approach for the Synthesis of Sharp Linear Phase
Digital Filters,”IEEE Trans. Circuits Syst., Vol. 33, April 1986, pp. 357–364.
[19] Yang, R., et al. “A New Structure of Sharp Transition FIR Filters Using Frequency-
Response Masking,” IEEE Trans. Circuits Syst., Vol. 35, August 1988, pp. 955–966.
[20] Saramaki, T., et al. “Design of Computationally Efficient Interpolated FIR Filters,” IEEE
Trans. Circuits Syst., Vol. 35, January 1988, pp. 70–88.
[21] Lyons, R. “Turbocharging Interpolated FIR Filters,” IEEE Signal Processing Magazine,“DSPTips
and Tricks” column, Vol. 24, No. 5, September 2007.

428 Specialized Digital Networks and Filters
[22] Lyons, R. Streamlining DigitalSignalProcessing, IEEE Press, Piscataway, New Jersey, 2007,
pp. 73–84.
[23] Rabiner, L., et al. “An Approach to the Approximation Problem for Nonrecursive Digital
Filters,”IEEETrans.AudioElectroacoust., Vol. AU-18, June 1970, pp. 83–106.
[24] Proakis, J., and Manolakis, D. Digital Signal Processing: Principles, Algorithms, and Applica-
tions, 3rd ed., Prentice Hall, Upper Saddle River, New Jersey, 1996, pp. 506–507.
[25] Taylor, F., and Mellott, J. Hands-onDigitalSignalProcessing, McGraw-Hill, New York, 1998,
pp. 325–331.
[26] Rader, C., and Gold, B. “Digital Filter Design Techniques in the Frequency Domain,” Pro-
ceedingsoftheIEEE, Vol. 55, February 1967, pp. 149–171.
[27] Rabiner, L., and Schafer, R. “Recursive and Nonrecursive Realizations of Digital Filters
Designed by Frequency Sampling Techniques,” IEEE Trans. Audio Electroacoust., Vol.
AU-19, September 1971, pp. 200–207.
[28] Gold, B., and Jordan, K. “A Direct Search Procedure for Designing Finite Duration Im-
pulse Response Filters,” IEEE Trans. Audio Electroacoust., Vol. AU-17, March 1969, pp.
33–36.
[29] Rabiner, L., and Gold, B. Theory and Application of Digital Signal Processing, Prentice Hall,
Upper Saddle River, New Jersey, 1975, pp. 105–112.
[30] Rorabaugh, C. DSPPrimer, McGraw-Hill, New York, 1999, pp. 278–279.
[31] Parks, T., and McClellan, J. “AProgram for the Design of Linear Phase Finite Impulse Re-
sponse Digital Filters,” IEEE Trans. Audio Electroacoust., Vol. AU-20, August 1972, pp.
195–199.
[32] Herrmann, O. “Design of Nonrecursive Digital Filters with Linear Phase,” Electron. Lett.,
Vol. 6, May 28, 1970, pp. 328–329.
[33] Rabiner, L. “Techniques for Designing Finite-Duration-Impulse-Response Digital Filters,”
IEEETrans.onCommunicationTechnology, Vol. COM-19, April 1971, pp. 188–195.
[34] Ingle, V., and Proakis, J. Digital Signal Processing Using MATLAB, Brookes/Cole Publish-
ing, Pacific Grove, California, 2000, pp. 202–208.

Chapter 7 Problems 429
CHAPTER 7 PROBLEMS
7.1 Prove that the frequency magnitude responses of the first-difference and
central-difference differentiators can be represented by
|H (ω)| = 2|sin(ω/2)|
Fd
and
|H (ω)| = |sin(ω)|
Cd
respectively. The ω frequency-domain variable has a range of –π ≤ ω ≤ π, corre-
sponding to a cyclic frequency range of –f/2 to f/2, where f is the differentia-
s s s
tor’s input signal sample rate in Hz.
Hint:Keep a list of trigonometric identities close at hand.
7.2 Redraw the block diagram of the central-difference differentiator, shown in
Figure P7–2, giving a new reduced-computation structure having only one
multiplier.
Central-difference differentiator
x(n)
z–1 z–1
0.5 –0.5
y (n)
cd
Figure P7–2
7.3 I once encountered an Internet web page that stated: “The central-difference
differentiator is equivalent to two first-difference differentiators in series (cas-
caded).” Upon his reading that statement, if Rocky Balboa said, “This is very
true,” would he be correct? Show how you arrived at your answer.
7.4 Assume you are working on a system that contains a servomotor and the
motor’s angular position, A (n), is measured by a shaft encoder coupled di-
pos
rectly to the motor shaft as shown in Figure P7–4. With the A (n) discrete-
pos
signal sequence representing the motor shaft angular position, assume that
the A (n) sequence is contaminated with high-frequency noise and that the
pos
acceleration measurement network is restricted to performing no more than

430 Specialized Digital Networks and Filters
one multiplication per A (n) input sample. What is the block diagram of the
pos
acceleration measurement network that will generate the A (n) signal repre-
acc
senting the acceleration of the motor shaft? Explain how you arrived at your
solution.
Shaft encoder
Motor
Signal A (n) Acceleration A (n)
pos acc
conditioning measurement
hardware network
Figure P7–4
7.5 Here’s a differentiator design problem. Compute the coefficients of an N = 7-
tap wideband differentiator whose cutoff frequency is ω =π.
c
7.6 Differentiators are often used in digital receivers to perform FM (frequency
modulation) demodulation. For a differentiator to be useful for FM demodu-
lation its group delay should be an integer number of samples. One proposed
differentiating filter has the following difference equation:
y (n)=
−x(n)+
x(n−2)−x(n−4) +
x(n−6)
diff 16 16
where y (n) is the differentiator’s output. Prove that this differentiator’s
diff
group delay is an integer number of samples.
7.7 This author once read, on the Internet, the surprising statement that “the real
part of the frequency response of the rectangular rule integrator is 0.5 for all
input frequencies.” Prove the validity of that statement.
7.8 In the text we introduced the rectangular rule integrator whose block dia-
gram is shown in Figure P7–8(a).
(a) Assume that, at time index n= 2, we apply an x(n) cosine sequence whose
frequency is half the input signal sample rate (f /2), shown in Figure
s
P7–8(b), to a rectangular rule integrator. Given this x(n) input sequence,
draw the y(n) output sequence of the integrator.

Chapter 7 Problems 431
(b) Based on the y(n) sequence from Part (a), by what factor does the rectan-
gular rule integrator have an amplitude gain, or an amplitude loss, when
the input is a sinusoid whose frequency is f /2 Hz?
s
(c) Use the text’s equation for the frequency response of the integrator to ver-
ify your answer to Part (b) of this problem.
Rectangular rule integrator
x(n) y(n)
(a)
z–1
x(n)
1
. . .
(b) 3 5 7
0
0 1 2 4 6 8 n
–1
Figure P7–8
7.9 Examine the x(n) samples obtained from digitizing the continuous x(t) signal
shown in Figure P7–9, and assume we must estimate the shaded area, A,
under the x(t) curve during the time interval of 0 to 3t seconds. If we use a
s
trapezoidal rule integrator for our computations, will our estimate of the
shaded area Abe larger or smaller than the true area A? Justify your answer.
x(n)
Continuousx(t)
0
0 1 2 3 n
Time
t
s
Figure P7–9
7.10 In the continuous-signal domain an integrator is defined by the following
Laplace-domain transfer function expression:
1
H (s)= .
int s

432 Specialized Digital Networks and Filters
(a) Using the bilinear transform method (Chapter 6) for converting analog
s-domain transfer functions to discrete z-domain transfer functions, what
is the H(ω) frequency response expression for the bilinear transform-
designed integrator corresponding to the H(s) above?
(b) Which of the integrator types discussed in this chapter’s text (rectangular
rule, trapezoidal rule, Simpson’s rule, and Tick’s rule) is most similar to
the bilinear transform-designed integrator obtained in Part (a)?
7.11 Considering the four discrete integrators discussed in the text, rectangular
rule, trapezoidal rule, Simpson’s rule, and Tick’s rule:
(a) Which integrator type is the most computationally simple?
(b) Which integrator has a transfer function pole (infinite gain) at z = –1?
Show how you arrived at your answer.
(c) Which integrator has a frequency magnitude response exactly equal to
zero (infinite attenuation) at f/2 Hz? Show how you arrived at your an-
s
swer.
7.12 Consider the following four-sample s(n) sequence to be a signal of interest
that we want to detect using a tapped-delay line FIR matched filter:
s(n) = [2, 4, –1, 1].
(a) Draw the block diagram of the FIR matched filter showing the filter coef-
ficients.
(b) What is the y(n) output sequence of the filter as the s(n) sequence passes
completely through the delay line of the matched filter?
7.13 If x (n) is an N-sample, noise-free signal of interest we wish to detect using a
s
tapped-delay line matched filter, what is a simple algebraic expression for the
maximum filter output sample value?
7.14 Considering the signal-to-noise ratio (SNR) discussion at the end of the text’s
matched filter material, think about a signal of interest represented by the
three-sample, triangular-like x (n) shown in Figure P7–14(a).
1
x(n) x(n)
1 2
5 5
3 3
0 0
0 1 2 n 0 1 2 3 4 n
(a) (b)
Figure P7–14

Chapter 7 Problems 433
Next, consider representing the x (n) signal’s triangular envelope with five
1
samples like the x (n) sequence shown in Figure P7–14(b). What is the im-
2
provement in a matched filter’s y (n) output SNR, measured in decibels (dB),
2
by using the x (n) sequence as our signal of interest over the y (n) output SNR
2 1
using the x (n) sequence as our signal of interest? Justify your answer.
1
7.15 In designing lowpass IFIR filters we prefer to use large values for the M ex-
pansion factor because large M reduces the number of taps in the shaping
subfilter. How do large values for M affect the number of taps in the associ-
ated image-reject subfilter?
7.16 Assume we’re designing a lowpass IFIR filter with an expansion factor of M=
5 and our desired IFIR filter has a passband from zero to 4 kHz (f = 4 kHz).
pass
(a) What is the passband width, in Hz, of the prototypeFIR filter used to start
our IFIR filter design effort?
(b) How many passband images will reside between zero and f/2 Hz in the
s
IFIRshapingsubfilter?
7.17 Assume we’re tasked to design a lowpass IFIR filter, operating at a sampling
rate of f = 3 MHz, having the following properties:
s
• Passband width of 60 kHz
• Maximum passband peak-to-peak ripple of 0.2 dB
• Stopband beginning at 61.2 kHz
(a) What is the value of the optimum Mexpansion factor for the shaping sub-
filter?
(b) What is the maximum passband peak-to-peak ripple requirement, in dB,
for the prototype filter?
(c) Using the text’s Figure 7–18(b), what percent computational reduction
should we expect from the IFIR filter relative to a Parks-McClellan-
designed tapped-delay line FIR filter?
7.18 In studying frequency sampling filters (FSFs) we encountered networks con-
taining complex multipliers like that shown in Figure P7–18. Let’s now ex-
plore the details of such a multiplier.
(a) Write the time-domain difference equation of the complex w(n) output
ofthe complex multiplier in terms of v(n) = v (n) + jv(n) and
ejω
where all
R I
the variables are in rectangular form. Variable ωis a fixed radian angle in
the range –π≤ω≤π.

434 Specialized Digital Networks and Filters
(b) Based on the difference equation from Part (a), draw the block diagram of
the complex multiplier where the complex v(n) and w(n) discrete se-
quences, and the constant complex
ejω
multiplicand, are shown in rectan-
gular (real-only variables) form.
(c) How many real multiplies and how many real additions are required to
perform a single complex multiply?
u(n) y(n)
z–1
w(n) v(n)
ejω
Complex multiplier
Figure P7–18
7.19 To exercise your frequency sampling filter (FSF) analysis skills, consider the
three-stage Type-IV FSF in Figure P7–19.
(a) Draw the FSF’s pole and zero locations on the z-plane.
Hint: From Figure P7–19, determine the filter’s N and r parameters to
find the zeros’ locations on the unit circle, and use each resonator’s k
index value to find the FSF’s pole locations on the unit circle.
(b) Is this filter a lowpass, bandpass, or highpass filter? Justify your answer.
(c) Draw a rough sketch of the filter’s frequency magnitude response over
the frequency range of –π≤ω≤πradians/sample (–f/2≤f≤f/2 Hz).
s s

Chapter 7 Problems 435
Type-IV FSF
z–1
1/16
2 z–1
Resonator
index k= 0
–1
z–1
x(n) 1/16 – y(n)
z–8 z–2 1.41421 z–1
Resonator
index k= 1
–1
–1 –1
z–1
1/16
z–1
Resonator
index k= 2
–1
Figure P7–19
7.20 Concerning cascaded subfilters used in frequency sampling filters (FSFs):
(a) What is the impulse response of the “two cascaded comb filters” combi-
nation in a Type-IV FSF? (See the text’s Figure 7–41.) Assume N = 8, and
the damping factor r= 1.
(b) Will this cascaded combination of comb filters have linear phase? Explain
how you arrived at your answer.
Hint: Recall what is an important characteristic of the impulse response
of a linear-phase digital filter.
(c) If we set r = 0.9, will the combination of comb filters have linear phase?
Explain how you arrived at your answer.
7.21 What is the range, roughly, of stopband attenuation that can be achieved with
a Type-IV frequency sampling filter (FSF) having a single non-unity transition
band coefficient?

436 Specialized Digital Networks and Filters
7.22 If a linear-phase lowpass filter needs to be designed to have a minimum of 85
dB of stopband attenuation, is it possible to design a Type-IV frequency sam-
pling filter (FSF) that satisfies this attenuation requirement?
7.23 Assume you’ve designed a six-section, N = 23, lowpass, Type-IV frequency
sampling filter (FSF) (such as that in Figure 7–41) having three low-frequency
passband sections, and the remaining three sections will use transition coeffi-
cients to achieve your desired stopband attenuation.
(a) What are the optimum values for those three coefficients? Show how you
arrived at your solution.
(b) Approximately what level of stopband attenuation (in dB) should we ex-
pect from our six-section, N = 23 FSF? Show how you arrived at your so-
lution.
(c) Draw the block diagram (the structure) of your FSF. No resonator details
need be drawn. Just show a resonator as a block labeled with its appropri-
atekvalue.
7.24 Here is a frequency sampling filter (FSF) problem whose solution is a net-
work useful for computing a single-bin discrete Fourier transform (DFT) out-
put magnitude sample in a way that avoids having to perform an entire DFT.
Such a network can be used for sinusoidal tone detection.
Think about building a tapped-delay line FIR filter, with the complex co-
efficients shown in Figure P7–24–I(a). After the arrival of the x(N–1) input
sample the system’s output is equal to the mth DFT bin’s power sample of an
N-point DFT of the most recent N input samples. What we’re saying is that
we can build a real-time spectral power measurement system for computing
successive single-bin DFT power values (the mth DFT bin power samples
over time). The tapped-delay line filter’s h(k) coefficients are
h(k) =
e–j2πm(N–1–k)/N
where filter coefficient index k is 0 ≤ k ≤ N–1. The output term |X (n)|2 in
m
Figure P7–24–I(a) represents the value of the DFT’s mth bin power at time in-
stantn. (Power samples are computed by multiplying X (n) samples by their
m
conjugatesX (n)*.)
m

Chapter 7 Problems 437
Tone detector (nonrecursive FIR)
x(n) z–1 z–1 z–1 . . . z–1
h(0) = h(1) = h(2) = h(N–2) = h(N–1) =
e–j2πm(N–1)/N e–j2πm(N–2)/N e–j2πm(N–3)/N e–j2πm(1)/N e–j2πm(0)/N
(a)
X (n) |X (n)|2
m X (n)X (n)* m
m m
Real part of h(k) for m= 2
1
0
1 6 8 13 15 k
–1
(b)
Imaginary part of h(k) for m= 2
1
0
1 3 8 10 15 k
–1
Figure P7–24–I
As an example, if we needed to detect the power of the m= 2 bin of an N= 16-
point DFT, the complex h(k) coefficients of the tapped-delay line filter would
beh(k) =
e–j2π2(15–k)/16as
shown in Figure P7–24–I(b). In this case our real-time
spectral power detector would be the filter shown in Figure P7–24–II(a). (No-
tice how those coefficients are a flipped-in-time version of an
e–j2π2k/16
Tone detector (nonrecursive FIR) for m= 2
x(n) z–1 z–1 z–1 . . . z–1
e–j2π(2)15/16 e–j2π(2)14/16 e–j2π(2)13/16 e–j2π(2)1/16 e–j2π(2)0/16
(a)
X(n) |X(n)|2
2 2
X(n)X(n)*
2 2
Tone detector (FSF)
(b) x(n) FSF X 2 (n) X 2 (n)X 2 (n)* |X 2 (n)|2
Figure P7–24–II

438 Specialized Digital Networks and Filters
sequence. That’s necessary because the earlier-in-time x(n) samples are to-
ward the right in the tapped-delay line filter.)
(a) Reviewing the equivalency of a tapped-delay line FIR filter and a com-
plex FSF in the text’s Figure 7–20, draw the FSF block diagram that en-
ables the real-time computation of the single m = 2 bin power for an N =
16-point DFT function performed in Figure P7–24–II(a). That is, show
what would be the network inside the FSF block in Figure P7–24–II(b), in-
cluding the numerical values for the H(k)/N gain factors following the
FSF resonators.
Hint: There are two ways to solve this problem. You could (1) perform a
customary DFT analysis on the tapped-delay line filter’s h(k) coefficients
to obtain the FSF’s H(k)/N gain factors, or (2) you could determine the
H(z)z-domain transfer function of the FSF to determine its block diagram.
(Pick your poison.)
(b) If the f sample rate of the x(n)inputs in Figure P7–24–II is 200 kHz, what
s
is the center frequency of the m= 2 DFT bin of our real-time single-bin 16-
point DFT power measurement system?

CHAPTER EIGHT
Quadrature
Signals
Quadrature signals are based on the notion of complex numbers. Perhaps no
other topic causes more heartache for newcomers to DSPthan these numbers
and their strange terminology of j-operator, complex, analytic, imaginary, real,
and orthogonal. If you’re a little unsure of the physical meaning of complex
numbers and the j= −1 operator, don’t feel bad because you’re in good com-
pany. Sixteenth-century Italian mathematician Girolamo Cardano described
an imaginary number “as subtle as it is useless.” In the seventeenth century
Gottfried Leibniz described imaginary numbers as being “amphibian, halfway
between existence and nonexistence.” (The term imaginary was first used by
the brilliant mathematician/philosopher René Descartes in the seventeenth
century and was meant to be derogatory. That’s because not only was the no-
tion of the square root of a negative number dubious at best, surprisingly there
was no consensus at that time as to the true meaning of negative real num-
bers.) Even Karl Gauss, one the world’s greatest mathematicians, called the
j-operator the “shadow of shadows.” Here we’ll shine some light on that
shadow so you’ll never have to call the QuadraturePsychicHotlinefor help.
Quadrature signals, represented by complex numbers, are used in just
about every field of science and engineering.†Quadrature signals are of inter-
est to us because they describe the effects of Fourier analysis as well as the
quadrature processing and implementations that take place in modern digital
communications systems. In this chapter we’ll review the fundamentals of
complex numbers and get comfortable with how they’re used to represent
quadrature signals. Next we’ll examine the notion of negative frequency as it
relates to quadrature signal algebraic notation and learn to speak the
†That’s because complex sinusoids are solutions to those second-order linear differential equa-
tions used to describe so much of nature.
439

440 Quadrature Signals
language of quadrature processing. In addition, we’ll use three-dimensional
time- and frequency-domain plots to clarify and give physical meaning to
quadrature signals.
8.1 WHY CARE ABOUT QUADRATURE SIGNALS?
Quadrature signal formats, also called complex signals, are used in many
digital signal processing applications, such as
• digital communications systems,
• radar systems,
• time difference of arrival processing in radio direction-finding schemes,
• coherent pulse measurement systems,
• antenna beamforming applications, and
• single sideband modulators.
These applications fall into the general category known as quadrature
processing, and they provide additional processing power through the coher-
ent measurement of the phase of sinusoidal signals.
Aquadrature signal is a two-dimensional signal whose value at some in-
stant in time can be specified by a single complex number having two parts:
what we call the real part and the imaginary part. (The words real and imagi-
nary, although traditional, are unfortunate because of their meanings in our
everyday speech. Communications engineers use the terms in-phaseand quad-
rature phase. More on that later.) Let’s review the mathematical notation of
these complex numbers.
8.2 THE NOTATION OF COMPLEX NUMBERS
To establish our terminology, we define real numbers to be those numbers
we use in everyday life, like a voltage, a temperature on the Fahrenheit
scale, or the balance of your checking account. These one-dimensional num-
bers can be either positive or negative, as depicted in Figure 8–1(a). In that
figure we show a one-dimensional axis and say a single real number can be
represented by a point on the axis. Out of tradition, let’s call this axis the
realaxis.
A complex number c is shown in Figure 8–1(b) where it’s also repre-
sented as a point. Complex numbers are not restricted to lying on a one-
dimensional line but can reside anywhere on a two-dimensional plane. That
plane is called the complex plane(some mathematicians like to call it an Argand

8.2 The Notation of Complex Numbers 441
This point represents the
real number −2.2
(a) −5 −4 −3 −2 −1 0 1 2 3 4 Real
axis
Imaginary
axis (j)
This point represents
2
the complex number
c = 2.5 + j2
0
(b) 2.5 Real axis
Figure 8–1 Graphical interpretations: (a) a real number; (b) a complex number.
diagram), and it enables us to represent complex numbers having both real
and imaginary parts. For example, in Figure 8–1(b), the complex number
c = 2.5 + j2 is a point lying on the complex plane on neither the real nor the
imaginary axis. We locate point c by going +2.5 units along the real axis and
up +2 units along the imaginary axis. Think of those real and imaginary axes
exactly as you think of the east-west and north-south directions on a road
map.
We’ll use a geometric viewpoint to help us understand some of the arith-
metic of complex numbers. Taking a look at Figure 8–2, we can use the
trigonometry of right triangles to define several different ways of represent-
ing the complex number c.
Our complex number c is represented in a number of different ways in
the literature, as shown in Table 8–1.
Eqs. (8–3) and (8–4) remind us that c can also be considered the tip of a
phasor on the complex plane, with magnitude M, in the direction of ø degrees
relative to the positive real axis as shown in Figure 8–2. Keep in mind that cis
a complex number and the variables a, b, M, and ø are all real numbers. The
magnitude of c, sometimes called the modulus of c, is
Imaginary
axis (j)
b
c = a + jb
M φ
0 Real
a axis
Figure 8–2 The phasor representation of complex number c=a+jbon the com-
plex plane.

442 Quadrature Signals
Table 8–1 Complex Number Notation
Notation name Math expression Remarks
Rectangular form c= a+ jb Used for explanatory purposes. (8–1)
Easiest to understand. (Also
called the Cartesian form.)
Trigonometric form c= M[cos(ø) + jsin(ø)] Commonly used to describe (8–2)
quadrature signals in com-
munications systems.
Polar form c= Mejø Most puzzling, but the primary (8–3)
form used in math equations.
(Also called the exponential
form. Sometimes written as
Mexp(jø).)
Magnitude-angle c= M∠ø Used for descriptive purposes, (8–4)
form but too cumbersome for use in
algebraic equations. (Essen-
tially a shorthand version of
Eq. (8–3).)
M= |c| = a 2 +b 2 (8–5)
.
The phase angle ø, the argument of c, is the arctangent of the ratio of the
imaginary part over the real part, or
⎛b⎞
ø=tan–1 ⎜ ⎟ radians. (8–6)
⎝ ⎠
a
If we set Eq. (8–3) equal to Eq. (8–2), Mejø = M[cos(ø) + jsin(ø)], we can
state what’s named in his honor and now called one of Euler’s identities as
ejø= cos(ø) + jsin(ø). (8–7)
The suspicious reader should now be asking, “Why is it valid to repre-
sent a complex number using that strange expression of the base of the nat-
ural logarithms, e, raised to an imaginary power?” We can validate Eq. (8–7)
as did Europe’s wizard of infinite series, Leonhard Euler, by plugging jø in
for zin the series expansion definition of ezin the top line of Figure 8–3.†That
substitution is shown on the second line. Next we evaluate the higher orders
† Leonhard Euler, born in Switzerland in 1707, is considered by many historians to be the
world’s greatest mathematician. By the way, the name Euler is pronounced ‘oy-ler.

8.2 The Notation of Complex Numbers 443
z 2 z 3 z4
e z = 1 + z + + + + ...
2! 3! 4!
2 3 4 5 6
e jφ = 1 + jφ +
(jφ)
+
(jφ)
+
(jφ)
+
(jφ)
+
(jφ)
+ ...
2! 3! 4! 5! 6!
φ2 φ3 φ4 φ5 φ6
e
jφ
= 1 + jφ 2! j 3! + 4! +j 5! 6! + ...
e jφ = cos(φ) + jsin(φ)
Figure 8–3 One derivation of Euler’s equation using series expansions for ez,
cos(ø), and sin(ø).
of jto arrive at the series in the third line in the figure. Those of you with ele-
vated math skills like Euler (or who check some math reference book) will
recognize that the alternating terms in the third line are the series expansion
definitions of the cosine and sine functions.
Figure 8–3 verifies Eq. (8–7) and justifies our representation of a complex
number using the Eq. (8–3) polar form: Mejø. If we substitute –jø for z in the
top line of Figure 8–3, we end up with a slightly different, and very useful,
form of Euler’s identity:
e–jø= cos(ø) – jsin(ø). (8–8)
The polar form of Eqs. (8–7) and (8–8) benefits us because:
• It simplifies mathematical derivations and analysis, turning trigonomet-
ric equations into the simple algebra of exponents. Math operations on
complex numbers follow exactly the same rules as real numbers.
• It makes adding signals merely the addition of complex numbers (vector
addition).
• It’s the most concise notation.
• It’s indicative of how digital communications system are implemented
and described in the literature.
Here’s a quick example of how the polar form of complex numbers can
simplify a mathematical analysis. Let’s say we wanted to understand the
process of multiplying complex number c = cos(ø) + jsin(ø) by another com-
1
plex number, c = cos(2ø) – jsin(2ø), whose angle is the negative of twice c ’s
2 1
angle. The product is

444 Quadrature Signals
c c = [cos(ø) + jsin(ø)][cos(2ø) – jsin(2ø)]
1 2
= cos(ø)cos(2ø) + sin(ø)sin(2ø) + j[sin(ø)cos(2ø) – cos(ø)sin(2ø)]. (8–9)
Using the trigonometricfunction productidentities, we can write Eq. (8–9) as
c c = (1/2)[cos(–ø) + cos(3ø) + cos(–ø) – cos(3ø)]
1 2
+ j(1/2)[sin(3ø) + sin(–ø) – sin(3ø) + sin(–ø)] (8–10)
= cos(–ø) + jsin(–ø) = cos(ø) – jsin(ø).
So the c c product yields the complex conjugate of c . That’s not too
1 2 1
thrilling, but what is interesting is how trivial a polar form c c product analy-
1 2
sis turns out to be. We can complete our polar form analysis in one brief line:
c c = ejøe–j2ø= e–jø, (8–11)
1 2
which is equivalent to Eq. (8–10). For math analysis, polar form is usually the
notation of choice.
Back to quadrature signals. We’ll be using Eqs. (8–7) and (8–8) to under-
stand the nature of time-domain quadrature signals. But first let’s take a deep
breath and enter the Twilight Zone of the joperator.
You’ve seen the definition j= −1 before. Stated in words, we say that j
represents a number that when multiplied by itself results in negative one.
Well, this definition causes difficulty for the beginner because we all know
that any number multiplied by itself always results in a positive number. (Un-
fortunately, engineering textbooks often define j and then, with justified
haste, swiftly carry on with all the ways the joperator can be used to analyze
sinusoidal signals. Readers soon forget about the question “What does
j = −1 actually mean?”) Well, −1 had been on the mathematical scene
forsome time but wasn’t taken seriously until it had to be used to solve cubic
polynomial equations in the sixteenth century[1,2]. Mathematicians reluc-
tantly began to accept the abstract concept of −1 without having to visual-
ize it, because its mathematical properties were consistent with the arithmetic
of normal real numbers.
It was Euler’s equating complex numbers to real sines and cosines, and
Gauss’s brilliant introduction of the complex plane, that finally legitimized
the notion of −1 to Europe’s mathematicians in the eighteenth century.
Euler, going beyond the province of real numbers, showed that complex
numbers had a clean, consistent relationship to the well-known real trigono-
metric functions of sines and cosines. As Einstein showed the equivalence of
mass and energy, Euler showed the equivalence of real sines and cosines to
complex numbers. Just as modern-day physicists don’t know what an elec-
tron is but they understand its properties, we’ll not worry about what jis and
be satisfied with understanding its behavior. We’ll treat jnot as a number, but

8.2 The Notation of Complex Numbers 445
as an operation performed on a number, as we do with negation or multipli-
cation. For our purposes, the j-operator means rotate a complex number by
90 degrees counterclockwise. (For our friends in the UK, counterclockwise
means your anti-clockwise.) Let’s see why.
We’ll get comfortable with the complex plane representation of imagi-
nary numbers by examining the mathematical properties of the j= −1 oper-
ator as shown in Figure 8–4.
Multiplying any number on the real axis by j results in an imaginary
product lying on the imaginary axis. The example on the left in Figure 8–4
shows that if +8 is represented by the dot lying on the positive real axis, mul-
tiplying +8 by jresults in an imaginary number, +j8, whose position has been
rotated 90 degrees counterclockwise (from +8), putting it on the positive
imaginary axis. Similarly, multiplying +j8 by j results in another 90-degree
rotation, yielding the –8 lying on the negative real axis because j2=–1. Multi-
plying –8 by j results in a further 90-degree rotation, giving the –j8 lying on
the negative imaginary axis. Whenever any number represented by a dot is
multiplied by j, the result is a counterclockwise rotation of 90degrees. (Con-
versely, multiplication by –j results in a clockwise rotation of –90degrees on
the complex plane.)
If we let ø=π/2 in Eq. 8–7, we can say
ejπ/2= cos(π/2) + jsin(π/2) = 0 + j1 , or
(8–12)
ejπ/2=
j.
Here’s the point to remember. If you have a single complex number, rep-
resented by a point on the complex plane, multiplying that number by jor by
ejπ/2
will result in a new complex number rotated 90 degrees counterclock-
wise (CCW) on the complex plane. Don’t forget this, as it will be useful as you
begin reading the literature of quadrature processing systems!
Imaginary Imaginary
axis axis
j8 j8
0 0
−8 8 Real −8 8 Real
axis axis
−j8 −j8
= multiply by j = multiply by −j
Figure 8–4 What happens to the real number 8 when multiplied by jand –j.

446 Quadrature Signals
Imaginary Imaginary
j j
e j2πf o t e j2πf o t
φ = 2πft φ = 2πft
o 1 o 1
-1 φ = -2πf o t Real -1 φ = -2πf o t Real
e-j2πf o t e-j2πf o t
-j -j
(a) (b)
Figure 8–5 A snapshot, in time, of two complex numbers whose exponents
change with time: (a) numbers shown as dots; (b) numbers shown as
phasors.
Let’s pause for a moment here to catch our breath. Don’t worry if the
ideas of imaginary numbers and the complex plane seem a little mysterious.
It’s that way for everyone at first—you’ll get comfortable with them the more
you use them. (Remember, the j-operator puzzled Europe’s heavyweight
mathematicians for many years.) Granted, not only is the mathematics of
complex numbers a bit strange at first, but the terminology is almost bizarre.
While the term imaginary is an unfortunate one to use, the term complex is
downright weird. When first encountered, the phrase “complex numbers”
makes us think complicatednumbers. This is regrettable because the concept of
complex numbers is not really so complicated.†Just know that the purpose of
the above mathematical rigmarole was to validate Eqs. (8–2), (8–3), (8–7), and
(8–8). Now, let’s (finally!) talk about time-domain signals.
8.3 REPRESENTING REAL SIGNALS USING COMPLEX PHASORS
We now turn our attention to a complex number that is a function of time.
Consider a number whose magnitude is one, and whose phase angle in-
creases with time. That complex number is the
ej2πfot
point shown in Figure
8–5(a). (Here the 2πf term is frequency in radians/second, and it corresponds
o
to a frequency of f cycles/second where f is measured in Hz.) As time tgets
o o
larger, the complex number’s phase angle increases and our number orbits
†The brilliant American engineer Charles P. Steinmetz, who pioneered the use of real and imag-
inary numbers in electrical circuit analysis in the early twentieth century, refrained from using
the term complex numbers—he called them general numbers.

8.3 Representing Real Signals Using Complex Phasors 447
the origin of the complex plane in a CCW direction. Figure 8–5(a) shows the
number, represented by the solid dot, frozen at some arbitrary instant in time.
If, say, the frequency f = 2 Hz, then the dot would rotate around the circle
o
two times per second. We can also think of another complex number
e–j2πfot
(the white dot) orbiting in a clockwise direction because its phase angle gets
more negative as time increases.
Let’s now call our two complex expressions,
ej2πfotand e–j2πfot,
quadrature
signals. Each has both real and imaginary parts, and they are both functions
of time. Those
ej2πfot
and
e–j2πfot
expressions are often called complex exponen-
tialsin the literature.
We can also think of those two quadrature signals,
ej2πfot
and
e–j2πfot,
as
the tips of two phasors rotating in opposite directions, as shown in Figure
8–5(b). We’re going to stick with this phasor notation for now because it’ll
allow us to achieve our goal of representing real sinusoids in the context of
the complex plane. Don’t touch that dial!
To ensure that we understand the behavior of a simple quadrature sig-
nal, Figure 8–6 shows the three-dimensional path of the
ej2πfot
signal as time
passes. We’ve added the time axis, coming out of the page, to show how
ej2πfot
follows a corkscrew path spiraling along, and centered about, the time axis.
The real and imaginary parts of
ej2πfotare
shown as the sine and cosine projec-
tions in Figure 8–6 and give us additional insight into Eq. 8–7.
ej2 fot
sin(2 ft)
o
2
s
xi
g.
a 1
a
m
I 0
–1
–2
–1
0 2
1
0
1 Real axis
2 –1
Time
3 –2
cos(2 ft)
o
Figure 8–6 The motion of the ej2πfotcomplex signal as time increases.

448 Quadrature Signals
To appreciate the physical meaning of our discussion here, let’s remem-
ber that a continuous quadrature signal ej2πfot = cos(2πf t) +jsin(2πf t) is not
o o
just mathematical mumbo jumbo. We can generate
ej2πfot
in our laboratory
and transmit it to another lab down the hall. All we need is two sinusoidal
signal generators, set to the same frequency f . (However, somehow we have
o
to synchronize those two hardware generators so their relative phase shift is
fixed at 90 degrees.) Next we connect coax cables to the generators’ output
connectors and run those two cables, labeled cos for the cosine signal and sin
for the sinewave signal, to their destination as shown in Figure 8–7.
Now for a two-question pop quiz. First question: In the other lab, what
would we see on the screen of an oscilloscope if the continuous real cos(2πf t)
o
and sin(2πf t) signals were connected to the horizontal and vertical input chan-
o
nels, respectively, of the scope (remembering, of course, to set the scope’s hori-
zontal sweep control to the External position)? That’s right. We’d see the scope’s
electron beam rotating counterclockwise in a circle on the scope’s screen.
Next, what would be seen on the scope’s display if the cables were mis-
labeled and the two signals were inadvertently swapped? We’d see another
circle, but this time it would be orbiting in a clockwise direction. This would
be a neat little real-world demonstration if we set the signal generators’ f fre-
o
quencies to, say, 1 Hz.
This oscilloscope example is meaningful and helps us answer the impor-
tant question “When we work with quadrature signals, how is the j-operator
implemented in hardware?” The j-operator is implemented by how we treat
the two signals relative to each other. We have to treat them orthogonally
such that the real cos(2πf t) signal represents an east-west value, and the real
o
sin(2πf t) signal represents an orthogonal north-south value. (By “orthogo-
o
nal,” I mean the north-south direction is oriented exactly 90 degrees relative
to the east-west direction.) So in our oscilloscope example the j-operator is
implemented merely by how the connections are made to the scope. The real
cosine signal controls horizontal deflection and the real sine signal controls
vertical deflection. The result is a two-dimensional quadrature signal repre-
sented by the instantaneous position of the dot on the scope’s display. We
physically implemented the j-operator in ej2πfot=cos(2πf t)+jsin(2πf t) the mo-
o o
ment we connected the sin(2πf t) signal to the vertical input connector of the
o
oscilloscope. Our Figure 8–7 example reminds us of an important characteris-
j2πft
Oscilloscope e o
Signal
sin(2πft)
gen. o
Vert. In
Signal Horiz. In
cos(2πft)
gen. o
Figure 8–7 Displaying a quadrature signal using an oscilloscope.

8.3 Representing Real Signals Using Complex Phasors 449
tic of quadrature signals: While real signals can be transmitted over a single
cable, two cables are always necessary to transmit a quadrature (complex)
signal.
Returning to Figure 8–5(b), ask yourself: “What’s the vector sum of those
two phasors as they rotate in opposite directions?” Think about this for a mo-
ment. That’s right, the phasors’ real parts will always add constructively, and
their imaginary parts will always cancel. This means the summation of these
ej2πfotand e–j2πfotphasors
will always be a purely real number. Implementations
of modern-day digital communications systems are based on this property!
To emphasize the importance of the real sum of these two complex sinu-
soids we’ll draw yet another picture. Consider the waveform in the three-
dimensional Figure 8–8 generated by the sum of two half-magnitude complex
phasors,
ej2πfot/2
and
e–j2πfot/2,
rotating in opposite directions about, and mov-
ing down along, the time axis.
Thinking about these phasors, it’s clear now why the cosine wave can be
equated to the sum of two complex exponentials by
cos(2(cid:5) f t)= ej2(cid:5)f o t+e −j2(cid:5)f o t = ej2(cid:5)f o t + e −j2(cid:5)f o t . (8–13)
o 2 2 2
Eq. (8–13), a well-known and important expression, is also one of Euler’s
identities. We could have derived this identity by solving Eqs. (8–7) and (8–8)
for jsin(ø), equating those two expressions, and solving that final equation for
cos(ø). Similarly, we could go through the same algebra exercise and show a
real sinewave as also the sum of two complex exponentials as
e −e je je
sin( )= = − . (8–14)
2j 2 2
Imaginary Real
1 axis
axis
t = 0 cos(2 ft)
o
ej2 f
o
t
2
Time
e-j2 f
o
t
2
Figure 8–8 A cosine represented by the sum of two rotating complex phasors.

450 Quadrature Signals
Look at Eqs. (8–13) and (8–14) carefully—they are the standard expres-
sions for a cosine wave and a sinewave, using complex notation, and are seen
throughout the literature of quadrature communications systems. Equation
(8–13) tells us that the two complex exponentials are both oriented toward the
positive real axis when time t=0. The j operators in Eq. (8–14) tell us that the
negative-frequency complex exponential is oriented along the positive imagi-
nary axis, and the positive-frequency complex exponential is oriented along
the negative imaginary axis, when time t=0.
To keep the reader’s mind from spinning like those complex phasors,
please realize that the sole purpose of Figures 8–5 through 8–8 is to validate
the complex expressions of the cosine and sinewave given in Eqs. (8–13) and
(8–14). Those two equations, along with Eqs. (8–7) and (8–8), are the Rosetta
Stone of quadrature signal processing.†We can now easily translate, back and
forth, between real sinusoids and complex exponentials.
Let’s step back now and remind ourselves what we’re doing. We are
learning how real signals that can be transmitted down a coax cable, or digi-
tized and stored in a computer’s memory, can be represented in complex num-
ber notation. Yes, the constituent parts of a complex number are each real, but
we’re treating those parts in a special way—we’re treating them in quadrature.
8.4 A FEW THOUGHTS ON NEGATIVE FREQUENCY
It’s important for us to be comfortable with the concept of negative frequency
because it’s essential in understanding the spectral replication effects of peri-
odic sampling, discrete Fourier transforms, and the various quadrature signal
processing techniques discussed in Chapter 9. The convention of negative fre-
quency serves as both a consistent and powerful mathematical tool in our
analysis of signals. In fact, the use of negative frequency is mandatory when
we represent realsignals, such as a sine or cosine wave, in complex notation.
The difficulty in grasping the idea of negative frequency may be, for
some, similar to the consternation felt in the parlors of mathematicians in the
Middle Ages when they first encountered negative numbers. Until the thir-
teenth century, negative numbers were considered fictitious because numbers
were normally used for counting and measuring. So up to that time, negative
numbers just didn’t make sense. In those days, it was valid to ask, “How can
you hold in your hand something that is less than nothing?” The idea of sub-
tracting six from four must have seemed meaningless. Math historians suggest
that negative numbers were first analyzed in Italy. As the story goes, around
the year 1200 the Italian mathematician Leonardo da Pisa (known as Fibonacci)
† The Rosetta Stone was a basalt slab found in Egypt in 1799. It had the same text written in
three languages, two of them being Greek and Egyptian hieroglyphs. This enabled scholars to,
finally, translate the ancient hieroglyphs.

8.5 Quadrature Signals in the Frequency Domain 451
was working on a financial problem whose only valid solution involved a neg-
ative number. Undaunted, Leo wrote, “This problem, I have shown to be insol-
uble unless it is conceded that the first man had a debt.” Thus negative
numbers arrived on the mathematics scene, never again to be disregarded.
Modern men and women can now appreciate that negative numbers
have a direction associated with them. The direction is backward from zero in
the context that positive numbers point forward from zero. For example, neg-
ative numbers can represent temperatures measured in degrees below zero,
minutes before the present if the present is considered as zero time, or money
we owe the tax collector when our income is considered positive dollars. So,
the notion of negative quantities is perfectly valid if we just define it properly.
As comfortable as we now are with negative numbers, negative frequency re-
mains a troublesome and controversial concept for many engineers[3,4]. This
author once encountered a paper in a technical journal which stated: “since
negative frequencies cannot exist—.” Well, like negative numbers, negative
frequency is a perfectly valid concept as long as we define it properly relative
to what we’re used to thinking of as positive frequency. With this thought in
mind, we’ll call Figure 8–5’s
ej2πfotsignal
a positive-frequencycomplex exponen-
tial because it rotates around the complex plane’s origin in a circle in a
positive-angle direction at a cyclic frequency of f cycles per second. Likewise,
o
we’ll refer to the
e–j2πfot
signal as a negative-frequency complex exponential be-
cause of its negative-angle direction of rotation.
So we’ve defined negative frequency in the frequency domain. If my
DSPpals want to claim negative frequency doesn’t exist in the time domain, I
won’t argue. However, our frequency-domain negative frequency definition
is clean, consistent with real signals, very useful, and here to stay.
8.5 QUADRATURE SIGNALS IN THE FREQUENCY DOMAIN
Now that we know much about the time-domain nature of quadrature signals,
we’re ready to look at their frequency-domain descriptions. We’ll illustrate the
full three-dimensional aspects of the frequency domain so none of the phase re-
lationships of our quadrature signals will be hidden from view. Figure 8–9 tells
us the rules for representing complex exponentials in the frequency domain.
We’ll represent a single complex exponential as a narrow impulse lo-
cated at the frequency specified in the exponent. In addition, we’ll show the
phase relationships between those complex exponentials along the real and
imaginary frequency-domain axes. To illustrate those phase relationships, a
complex frequency domain representation is necessary. With all this said, take
a look at Figure 8–10.
See how a real cosine wave and a real sinewave are depicted in our
complex frequency-domain representation on the right side of Figure 8–10.
Those bold arrows on the right of Figure 8–10 are not rotating phasors but

452 Quadrature Signals
Negative Positive
frequency frequency
-j2πft j2πft
e o e o
j -j
2 2
Direction along the Magnitude is 1/2
imaginary axis
Figure 8–9 Frequency-domain interpretation of complex exponentials.
are frequency-domain impulse symbols indicating a single spectral line for a
single complex exponential such as
ej2πfot.
The directions in which the spec-
tral impulses are pointing merely indicate the relative phases of the spectral
components. The amplitude of those spectral impulses is 1/2. Notice how
the spectrum of cos(2πf t) is real-only. That’s because cos(2πf t) is an even
o o
function in time, its value at negative time t is equal to its value at positive
time t, or
cos[2πf (–t)] = cos(2πf t). (8–15)
o o
The sin(2πf t) function, on the other hand, has an imaginary-only spectrum
o
because it’s an odd function. An odd function’s value at negative time t is
equal to the negative of its value at positive time t, or
sin[2πf (–t)] = –sin(2πf t). (8–16)
o o
Imaginary Imag
. . . Real cos(2πft) = Real
o -f
o
j2πft -j2πft
e o e o 0
+
(a) 2 2 f
cos(2πf o t) . . . Time o Frequency
Imag
Imaginary
. . . Real sin(2πft) = Real
o -f
o
e -j2πf o t e j2πf o t 0 f
j - j o
(b) 2 2
Time Frequency
sin(2πft) . . .
o
Figure 8–10 Complex time- and frequency-domain representations: (a) cosine
wave; (b) a sinewave.

8.5 Quadrature Signals in the Frequency Domain 453
Why are we bothering with this three-dimensional frequency-domain
representation? Because it’s the tool we’ll use to understand the generation
(modulation) and detection (demodulation) of quadrature signals in digital
(and some analog) communications systems, and that’s one of the goals of
this chapter. Before we go there, however, let’s validate this frequency-
domain representation with a little example.
Figure 8–11 is a straightforward example of how we use the complex fre-
quency domain. There we begin with a real sinewave, multiply it by j, and
then add it to a real cosine wave of the same frequency. The result is the single
complex exponential
ej2πfot,
illustrating graphically Euler’s identity that we
stated mathematically in Eq. (8–7).
On the frequency axis, the notion of negative frequency is seen as those
spectral impulses located at –2πf radians/second on the frequency axis. This
o
figure shows the big payoff: when we use complex notation, generic complex
exponentials like
ej2πft
and
e–j2πft
are the fundamental constituents of the real
sinusoids sin(2πft) or cos(2πft). That’s because both sin(2πft) and cos(2πft) are
made up of
ej2πft
and
e–j2πft
components. If you were to take the discrete
Fourier transform (DFT) of discrete time-domain samples of a sin(2πf t)
o
sinewave, a cos(2πf t) cosine wave, or an ej2πfotcomplex sinusoid and plot the
o
complex results, you’d get exactly the narrow frequency-domain impulses in
Figure 8–11.
0.5 Imaginary
multiply Imag
Real
-f by j -f Real
o o
0 f
o 0
f
sin(2πf o t) -0.5 Freq jsin(2πf o t) o
Freq
add
Imag Imag
Real Real
-f
o 1
0 0
f 0.5 f
o o Freq
cos(2πft)
o Freq e j2πf o t = cos(2πf o t) + jsin(2πf o t)
Figure 8–11 Complex frequency-domain view of Euler’s ej2πfot = cos(2πft) +
o
jsin(2πft).
o

454 Quadrature Signals
If you understand the notation and operations in Figure 8–11, pat your-
self on the back, because you now know a great deal about the nature and
mathematics of quadrature signals.
8.6 BANDPASS QUADRATURE SIGNALS IN THE FREQUENCY DOMAIN
In quadrature processing, by convention, the real part of the spectrum is
called the in-phase componentand the imaginary part of the spectrum is called
the quadrature component. The signals whose complex spectra are in Figures
8–12(a), 8–12(b), and 8–12(c) are real, and in the time domain they can be rep-
resented by amplitude values having nonzero real parts and zero-valued
imaginary parts. We’re not forced to use complex notation to represent them
in the time domain—the signals are real-only.
Real signals always have positive- and negative-frequency-spectral com-
ponents. For any real signal, the positive- and negative-frequency compo-
Complex exponential
In-phase, (real) part
Quadrature phase,
(imaginary) part Quadrature
Quadrature
phase
phase
In-phase
In-phase
B
0.5
-f −φ
o φ -f
o
cos(2πft+φ) 0
o 0
f Freq f
o o Freq
(a) (b)
Quadrature Quadrature
phase phase
In-phase In-phase
B B
-f -f
o o
0 0
f f
(c) o Freq (d) o Freq
Figure 8–12 Quadrature representation of signals: (a) real sinusoid cos(2πft+ ø);
o
(b) real bandpass signal containing six sinusoids over bandwidth B;
(c) real bandpass signal containing an infinite number of sinusoids
over bandwidth BHz; (d) complex bandpass signal of bandwidth BHz.

8.6 Bandpass Quadrature Signals in the Frequency Domain 455
nents of its in-phase (real) spectrum always have even symmetry around the
zero-frequency point. That is, the in-phase part’s positive- and negative-
frequency components are mirror images of each other. Conversely, the
positive- and negative-frequency components of its quadrature (imaginary)
spectrum are always negatives of each other. This means that the phase angle
of any given positive quadrature frequency component is the negative of the
phase angle of the corresponding negative quadrature frequency component
as shown by the thin solid arrows in Figure 8–12(a). This conjugatesymmetryis
the invariant nature of real signals and is obvious when their spectra are rep-
resented using complex notation.
A complex-valued time signal, whose spectrum can be that in Figure
8–12(d), is not restricted to the above spectral conjugate symmetry conditions.
We’ll call that special complex signal an analytic signal, signifying that it has
no negative-frequency spectral components.
Let’s remind ourselves again: those bold arrows in Figures 8–12(a) and
8–12(b) are not rotating phasors. They’re frequency-domain impulses indicat-
ing a single complex exponential
ej2πft.
The directions in which the impulses
are pointing show the relative phases of the spectral components.
There’s an important principle to keep in mind before we continue. Multi-
plying a time signal by the complex exponential
ej2πfot,
what we call quadrature
mixing (also called complex mixing), shifts a signal’s spectrum upward in fre-
quency by f Hz, as shown in Figures 8–13(a) and 8–13(b). Likewise, multiply-
o
ing a time signal by
e–j2πfot
(also called complex down-conversion or mixing to
baseband) shifts a signal’s spectrum down to a center frequency of zero Hz as
shown in Figure 8–13(c). The process of quadrature mixing is used in many
DSPapplications as well as most modern-day digital communications systems.
Our entire quadrature signals discussion, thus far, has been based on
continuous signals, but the principles described here apply equally well to
discrete-time signals. Let’s look at the effect of complex down-conversion of a
discrete signal’s spectrum.
Quad. phase Quad. phase Quad. phase
In-phase In-phase In-phase
-f -f -f
o 0 o 0 o 0
f f f
o Freq o 2f o Freq o Freq
(a) (b) (c)
Figure 8–13 Quadrature mixing of a bandpass signal: (a) spectrum of a complex
signalx(t); (b) spectrum of x(t)ej2πfot; (c) spectrum of x(t)e–j2πfot.

456 Quadrature Signals
8.7 COMPLEX DOWN-CONVERSION
Complex down-conversion (also called quadrature demodulation) of discrete sig-
nals is a straightforward process and is best described by an example. Think of
a real-valued discrete sequence x(n) having an |X(m)| spectral magnitude
whose nonzero-valued samples are shown as the solid dots in Figure 8–14(a).
Because of the periodicity of discrete spectral representations we discussed in
Sections 2.1 and 3.17 (as well as the frequency axis of the FFT lying on the unit
circle in the z-plane explained in Section 6.3), we’re justified in also represent-
ing the |X(m)| spectrum as the three-dimensional circular plot given in Figure
8–14(b). There we’ve wrapped the linear frequency axis of Figure 8–14(a)
around a circle whose perimeter length is equal to the sample rate f such that
s
the frequencies f/2 and –f/2 are the same point on the circular axis.
s s
With x(n) being an N-point real sequence, |X(m)|’s spectrum is symmet-
rical about the zero-frequency point. If we now perform complex down-
conversion (by multiplying x(n) by
e–j2πfcnts,
where t=1/f, using either
s s
equivalent scheme shown in Figure 8–15(a)), the result is the complex sequence
3
m)|
2
X(
|
1
(a)
0
–f/2 –f/4 0 f f/4 f/2
s s c s Freq s
|X(m)|
f/2
s
(b) –f/4
s
f/4
s
–Freq 0 +Freq f c
Figure 8–14 Discrete |X(m)| spectra of a real-valued time sequence: (a) tradi-
tional depiction; (b) circular frequency axis depiction.

8.7 Complex Down-Conversion 457
x (n)=
x(n)e–j2πfcnts=i(n)+jq(n)
(8–17)
c
whose spectrum is given in Figure 8-15(b).
The minus sign in the exponent of
e–j2πfcntsshifts
the |X(m)| spectrum f
c
Hz in the negative-frequency direction. Of course, because x(n) is complex,
c
there’s no symmetry about the zero-frequency point in |X(m)|. The circular
c
depiction of |X(m)| is provided in Figure 8–15(c).
c
The purpose of Figures 8–14 and 8–15 is to show how frequency transla-
tion by means of complex down-conversion causes spectral components to
wrap around the f/2 point.
s
i(n)
x(n) x(n) x(n)
c x(n) = i(n) + jq(n)
cos(2 fnt) c
c s
q(n)
(a)
j2 fnt
e c s
sin(2 fnt)
c s
3
m)|
2
( Xc
|
1
(b)
0
/2 /4 0 f/4 f/2
s s Freq s s
|X(m)|
c
f/2
s
(c)
/4
s
f/4
s
Freq 0 +Freq
Figure 8–15 Discrete |X(m)| spectra of a down-converted time sequence:
c
(a) down-conversion symbols; (b) traditional frequency axis depic-
tion; (c) circular frequency axis depiction.

458 Quadrature Signals
+
i(n) i'(n)
-
q(n)
x'(n) = i'(n) + jq'(n)
c
x(n) = i(n) + jq(n)
c
q'(n)
cos(2πfnt) sin(2πfnt) up-conversion
c s c s
-sin(2πfnt) down-conversion
c s
Figure 8–16 Complex multiplier used for up/down-conversion.
Figure 8–15(a) showed the method of down-converting a real x(n) time
sequence. For completeness, Figure 8–16 shows how translating a complex
time sequence x (n)=i(n) + jq(n) up or down by f Hz requires a complex mul-
c c
tiplier.
This complex multiplier computes
i’(n) + jq’(n) = x (n)e±j2πfcnts= [i(n) + jq(n)][cos(2πfnt) ±jsin(2πfnt)]. (8–18)
c c s c s
If you use this multiplier, don’t forget the minus sign at the top adder in Fig-
ure 8–16. (It’s an easy mistake to make. Believe me.)
8.8 A COMPLEX DOWN-CONVERSION EXAMPLE
We can use all we’ve learned so far about quadrature signals by exploring the
process of quadrature sampling. Quadrature sampling is the process of digi-
tizing a continuous (analog) bandpass signal and down-converting its spec-
trum to be centered at zero Hz. Let’s see how this popular process works by
thinking of a continuous bandpass signal, of bandwidth B, centered about a
carrier frequency of f Hz as shown in Figure 8–17(a).
c
Our goal in quadrature sampling is to obtain a digitized version of the
analog bandpass signal, but we want the digitized signal’s discrete spectrum
centered about zero Hz, not f Hz, as in Figure 8–17(b). That is, we want to
c
mix a time signal with
e–j2πfct
to perform complex down-conversion. The fre-
quency f is the digitizer’s sampling rate in samples/second. We show replicated
s
spectra in Figure 8–17(b) to remind ourselves of this effect when A/D conver-
sion takes place.

8.8 A Complex Down-Conversion Example 459
|X(f)|
B
Continuous
(a)
-f 0 f Freq
c c
|X(m)|
Discrete
(b)
-f s 0 f s Freq (m)
Figure 8–17 The “before and after” spectra of a quadrature-sampled signal.
We can solve our sampling problem with the quadrature sampling block
diagram (also known as I/Q demodulation) shown in Figure 8–18(a). That
arrangement of two sinusoidal oscillators, with their relative 90-degree phase,
is often called a quadrature oscillator. First we’ll investigate the in-phase (upper)
path of the quadrature sampler. With the input analog x (t)’s spectrum shown
bp
in Figure 8–18(b), the spectral output of the top mixer is provided in Figure
8–18(c).
Those
ej2πfct
and
e–j2πfct
terms in Figure 8–18 remind us, from Eq. (8–13),
that the constituent complex exponentials comprise a real cosine duplicate
and translate each part of |X (f)|’s spectrum to produce the |X(f)| spec-
bp i
trum. There is a magnitude loss of a factor of two in |X(f)|, but we’re not
i
concerned about that at this point. Figure 8–18(d) shows the output of the
lowpass filter (LPF) in the in-phase path.
Likewise, Figure 8–19 shows how we get the filtered continuous quadra-
ture-phase portion (bottom path) of our desired complex signal by mixing
x (t) with –sin(2πft). From Eq. (8–14) we know that the complex exponentials
bp c
comprising the real –sin(2πft) sinewave are ej2πfct and –e–j2πfct. The minus sign
c
in the
–e–j2πfct
term accounts for the down-converted spectra in |X (f)| being
q
180degreesout of phase with the up-converted spectra.
This depiction of quadrature sampling can be enhanced if we look at the
situation from a three-dimensional standpoint, as in Figure 8–20. There the +j
factor rotates the “imaginary-only” Q(f) by 90 degrees, making it “real-only.”
This jQ(f) is then added to I(f) to yield the spectrum of a complex continuous
signal x(t)=i(t)+jq(t). Applying this signal to two A/D converters gives our
final desired discrete time samples of x (n)=i(n)+jq(n) in Figure 8–18(a) hav-
c
ing the spectrum shown in Figure 8–17(b).
Some advantages of this quadrature sampling scheme are:
• Each A/D converter operates at half the sampling rate of standard real-
signal sampling.

460 Quadrature Signals
x(t) i(t)
i
LPF A/D i(n)
x (t)
bp
x(n) = i(n) + jq(n)
cos(2πft) c
(a) c q(t)
LPF A/D q(n)
x(t)
q
-sin(2πft)
c
|X bp (f)| B
(b)
-f 0 f Freq
c |X(f)| e -j2πf c t c e j2πf c t
i 2 2
(c)
-2f 0 2f Freq
c c
|I(f)|
(d)
-B/2 0 B/2 Freq
Figure 8–18 Quadrature sampling: (a) block diagram; (b) input spectrum; (c) in-
phase mixer output spectrum; (d) in-phase filter output spectrum.
B
|X (f)|
bp
(a)
-f 0 f Freq
c c
Spectral components are
|X(f)| 180o out of phase.
q
-2f
(b) c
Freq
2f
c
|Q(f)|
B/2
(c)
Freq
-B/2
Figure 8–19 Spectra within the quadrature phase (lower) signal path of the block
diagram.

8.8 A Complex Down-Conversion Example 461
Imaginary
axis ( j ) Real
axis
I(f)
Imaginary 0
axis ( j ) Real
axis
Freq
Q(f)
Imaginary 0
axis ( j ) Real
axis Freq
+jQ(f)
Imaginary 0
axis ( j ) Real
axis
Freq
I(f) + jQ(f)
0
Freq
Figure 8–20 Three-dimensional view of combining the I(f) and Q(f) spectra to ob-
tain the I(f)+jQ(f) spectra.
• In many hardware implementations, operating at lower clock rates saves
power.
• For a given f sampling rate, we can capture wider-band analog sig-
s
nals.
• Quadrature sequences make FFT processing more efficient due to a
wider frequency range coverage.
• Quadrature sampling also makes it easier to measure the instantaneous
magnitude and phase of a signal during demodulation.
• Knowing the instantaneous phase of signals enables coherent process-
ing.

462 Quadrature Signals
i(n)
-1
i(n) - jq(n)
q(n) -B/2 0 B/2 Freq
i(n) + jq(n)
-B/2 0 B/2 Freq
Figure 8–21 Using conjugation to control spectral orientation.
While the quadrature sampler in Figure 8–18(a) performed complex
down-conversion, it’s easy to implement complex up-conversion by merely
conjugating the x (n) sequence, effectively inverting x (n)’s spectrum about
c c
zero Hz, as shown in Figure 8–21.
8.9 AN ALTERNATE DOWN-CONVERSION METHOD
The quadrature sampling method of complex down-conversion in Figure
8–18(a) works perfectly on paper, but it’s difficult to maintain the necessary
exact 90-degree phase relationships with high-frequency, or wideband, sig-
nals in practice. One- or two-degree phase errors are common in the labora-
tory. Ideally, we’d need perfectly phase-matched coax cables, two oscillators
exactly 90degreesout of phase, two ideal mixers with identical behavior and
no DC output component, two analog lowpass filters with identical magni-
tude and phase characteristics, and two A/D converters with exactly identi-
cal performance. (Sadly, no such electronic components are available to us.)
Fortunately, there’s an easier-to-implement quadrature sampling method[5].
Consider the process shown in Figure 8–22, where the analog x (t) sig-
bp
nal is initially digitized with the follow-on mixing and filtering being per-
formed digitally. This quadraturesamplingwithdigitalmixingmethod mitigates
the problems with the Figure 8–18(a) quadrature sampling method and elimi-
nates one of the A/D converters.
We use Figure 8–23 to show the spectra of the in-phase path of this
quadrature sampling with digital mixing process. Notice the similarity be-
tween the continuous |I(f)| in Figure 8–18(d) and the discrete |I(m)| in Fig-
ure 8–23(d). A sweet feature of this process is that with f = f/4, the cosine
c s
and sine oscillator outputs are the repetitive four-element cos(πn/2)=1,0,–1,0,
and –sin(πn/2)=0,–1,0,1, sequences, respectively. (See Section 13.1 for details
of these special mixing sequences.) No actual mixers (or multiplies) are
needed to down-convert our desired spectra to zero Hz! After lowpass filter-
ing, the i(n) and q(n) sequences are typically decimated by a factor of two to

8.9 An Alternate Down-Conversion Method 463
x(n)
i
LPF i(n)
x bp (t) x(n)
A/D cos(2πn/4) x c (n) = i(n) + jq(n)
LPF q(n)
f = 4f =1/t x(n)
s c s q
-sin(2πn/4)
Figure 8–22 Quadrature sampling with digital mixing method.
reduce the data rate for following processing. (Decimation is a topic covered
in Section 10.1.)
With all its advantages, you should have noticed one drawback of this
quadrature sampling with digital mixing process: the f sampling rate must be
s
four times the signal’s f center frequency. In practice, 4f could be an unpleas-
c c
antly high value. Happily, we can take advantage of the effects of bandpass
sampling to reduce our sample rate. Here’s an example: Consider a real ana-
log signal whose center frequency is 50 MHz, as shown in Figure 8–24(a).
Rather than sampling this signal at 200 MHz, we perform bandpass sampling
and use Eq. (2–13) with m = 5 to set the f sampling rate at 40 MHz. This
odd s
forces one of the replicated spectra of the sampled |X(m)| to be centered
at f/4, as shown in Figure 8–24(b), which is what we wanted. The A/D
s
|X (f)|
bp B
(a)
-f 0 f Freq
c c
|X(m)|
(b)
-4f -3f -2f -f 0 f = f/4 2f 3f 4f Freq
c c c c c s c c c
|X(m)|
i
(c)
-4f -3f -2f -f 0 f = f/4 2f 3f 4f Freq
c c c c c s c c c
(-f) (f)
s s
|I(m)|
(d)
-f 0 f Freq
s s
Figure 8–23 Spectra of quadrature sampling with digital mixing within the in-
phase (upper) signal path.

464 Quadrature Signals
|X (f)|
bp
(a)
-50 0 50 Freq
(MHz)
|X(m)| f = 40 MHz
s
(b)
-50 -10 0 10 50 Freq
(f/4) (MHz)
s
Figure 8–24 Bandpass sampling effects used to reduce the sampling rate of
quadrature sampling with digital mixing: (a) analog input signal
spectrum; (b) A/D converter spectrum.
converter x(n) output is now ready for complex down-conversion by f/4
s
(10 MHz) and digital lowpass filtering.
Section 13.1 provides a clever trick for reducing the computational
workload of the lowpass filters in Figure 8–22, when this f/4 down-
s
conversion method is used with decimation by two.
REFERENCES
[1] Struik, D. AConcise History of Mathematics, Dover Publications, New York, 1967.
[2] Bergamini, D. Mathematics, Life Science Library, Time Inc., New York, 1963.
[3] Lewis, L. J., et al. Linear Systems Analysis, McGraw-Hill Inc., New York, 1969, p. 193.
[4] Schwartz, M. Information, Transmission, Modulation, and Noise, McGraw-Hill Inc., New York,
1970, p. 35.
[5] Considine, V. “Digital Complex Sampling,” Electronics Letters, 19, August 4, 1983.

Chapter 8 Problems 465
CHAPTER 8 PROBLEMS
8.1 Consider the following expressions:
1. x+ 6 = 0
2. 2x–7 = 0
3. x2–2 = 0
4. x2+ 2 = 0
(a) For those expressions, fill in the table below with “Yes” or “No” describ-
ing the numerical nature of x.
Hint: Arational number is a number that can be expressed as a fraction p/q
where pand qare integers, and q≠0.
Real-only Integer Rational Complex
x + 6 = 0
2x –7 = 0
x2–2 = 0
x2+ 2 = 0
(b) Write the various values for x in both rectangular and polar (magnitude
and phase) notation.
8.2 Because they are so important in understanding quadrature signals and sys-
tems, prove the following:
(a) Multiplying a complex number C by the j-operator rotates that complex
number by 90degrees.
(b) Multiplying a complex number C by the j-operator results in a number
whose magnitude is equal to |C|.
8.3 In quadrature systems, we often need to compute the square of a complex
time-domain sample’s magnitude in order to estimate instantaneous signal
power. Prove that the product of a complex number times its conjugate, CC*,
is equal to the complex number’s magnitude squared.
(a) Use the rectangular form of C= Mcos(φ) + jMsin(φ) for your proof.
(b) Use the polar form of C=
Mejφ
for your proof.
(c) Which form of C, rectangular or polar, was easier to use for this proof?
8.4 Consider a complex number C having nonzero real and imaginary parts. If
the sum of C plus its reciprocal (C+1/C) is real-only, what can we say about
the magnitude of C? Justify your answer.

466 Quadrature Signals
8.5 Assume that we have two complex values:
C = R + jI ,
a a a
C = R + jI .
b b b
Next, suppose we want to compute a real-valued Qdefined as
⎡ ⎤
C
Q=realpartof ⎢ a⎥.
⎣C ⎦
b
For computational efficiency reasons, to avoid unnecessary arithmetic using
the imaginary parts of the complex Cvalues, we might decide to compute Qas
realpartof[C ]
Q = a .
efficient realpartof[C ]
bb
Show the validity (the correctness) of using Q to compute our desired Q
efficient
value.
Hint:Appendix Awill help you solve this problem.
8.6 To show the value of using complex numbers in our derivations of real-
valued functions, use the notation
cos(φ) = Re{ejφ }
where
Re{ejφ
} means “the real part of
ejφ
” to prove the following trigonometric
identity:
cos(α + β) = cos(α)cos(β) – sin(α)sin(β).
8.7 Given a continuous single sinusoidal x(t) signal whose three-dimensional
spectrum is shown in Figure P8–7, write the time-domain equation for x(t) in
trigonometric form. (Assume the magnitudes of the spectral components are
0.5.) Justify your answer.
Imag
–100 Real
0
100
Freq (Hz)
Figure P8–7

Chapter 8 Problems 467
8.8 The sum of two general complex exponentials, of the same frequency, is
Aej(ωt+α)+Bej(ωt+β) = Mej(ωt+θ)
where A, B, and Mare real-valued constants. Continuous frequency ωis in ra-
dians/second, and α, β, and θ are constant phase shifts measured in radians.
Find the expressions for Mand θin terms of A, B, α, and β.
Hint:Recall one of thelaws of exponents: xpxq= xp+q.
8.9 In your future mathematical work you may need to represent a real tangent
function in terms of complex exponentials. Such an expression is
ejα−e −jα
tan(α)= .
j(ejα−e −jα
)
Prove that the above tan(α) equation is correct.
8.10 In the literature of DSP, some authors state that the combined frequency mag-
nitude response of two cascaded systems is the product of their individual
frequency magnitude responses. That is, |H (ω)|=|H(ω)| · |H (ω)|. An-
combined 1 2
other author might say the combined frequency magnitude response of two
cascaded systems is equal to the magnitude of the product of their individual
frequency responses. That is, |H (ω)|=|H(ω)H (ω)|. Can both state-
combined 1 2
ments be correct? We simplify, and restate, this problem as follows: Given two
complex numbers, C and C , prove that the product of their magnitudes is
1 2
equal to the magnitude of their product, i.e.,
|C|·|C|=|C C|.
1 2 1 2
8.11 To gain further experience in working with complex numbers, show that:
(a) The magnitude of any complex number divided by its conjugate, |C/C*|,
is equal to 1 (unity). Assume C≠0.
(b) The conjugate of the product of two complex numbers is equal to the
product of their conjugates. That is:
(C · C )*= C *· C *.
1 2 1 2
8.12 Consider the continuous complex exponential signals c (t) and c (t), shown in
1 2
Figure P8–12 at the time instant t=0. Signal c (t) is orbiting the complex
1
plane’s origin counterclockwise at a frequency of 5 Hz, and signal c (t) is or-
2
biting the plane’s origin clockwise at a frequency of 7 Hz. Write the time-
domain equation for the sum of c (t) and c (t) where the terms in the equation
1 2
are in polar form having exponents measured in radians.

468 Quadrature Signals
Imaginary
Radius = 1.0
c(t=0)
1
= 45o
1
= –60o Real
2
c(t=0)
2
Radius = 0.75
Figure P8–12
8.13 In the mathematics of complex numbers, and quadrature systems, you may
encounter De Moivre’s Theorem. (De Moivre is pronounced duh-‘mwah-
vruh.) That theorem,
[cos(φ) +jsin(φ)]N= cos(Nφ) + jsin(Nφ),
is used to compute the result of raising a complex number to some integer (N)
power. Prove De Moivre’s Theorem.
8.14 Consider the complex time-domain sequence q(n) that is in polar form having
a magnitude factor and a phase factor, defined by
q(n) =
0.9nej2πn/8
where nis our time-domain index. Starting at time n=0:
(a) State, in words, how the magnitude of q(n) changes as time index n
increases.
(b) State, in words, how the phase of q(n) changes as nincreases.
(c) State, in words, how the real part of q(n) changes as nincreases.
(d) State, in words, how the imaginary part of q(n) changes as nincreases.
(e) On a complex plane, draw the q(n) sequence for time index 0≤n≤7.
8.15 Consider the real-valued sequence defined by
x(n) = cos(2πf nt + π/4)
o s
where f = –1 kHz. What is the phase, measured in degrees, of the negative-
o
frequency 1 kHz spectral component of x(n)?

Chapter 8 Problems 469
8.16 Consider a complex x (n) sequence whose X (ω) spectrum is depicted in
C C
Figure P8–16(a). The frequency axis in that figure is labeled in both our ωdigital
frequency (radians/sample) and an f cyclic frequency (Hz). If we take just the
real part of x (n), we have a real-valued x (n) sequence whose X (ω) spectrum is
C R R
depicted in Figure P8–16(b). If the magnitude of the maximum spectral com-
ponent of |X (ω)| is M, what is the magnitude of the maximum spectral compo-
C
nent of |X (ω)|? That is, what is Pin terms of M? Justify your solution.
R
Note: This problem is not “busy work.” Extracting the real part of a complex
time sequence is a common operation in quadrature processing.
|X ( )| Spectral
C replication
M
(a)
/2 0 /2 3 /2 2 5 /2
(f)
f/2) (f/2) (f)
s s s
|X ( )|
R P
(b)
/2 0 /2 3 /2 2 5 /2
(f)
f/2) (f/2) (f)
s s s
Figure P8–16
8.17 Assume you are required to multiply a complex x(n) sequence by
ejπ/2
to ob-
tain the complex y(n) sequence as shown in Figure P8–17. Draw the block dia-
gram, showing real-valued sequences only, that implements the process in
Figure P8–17.
Hint:Ask yourself, “Factor
ejπ/2is
equal to what?”
x(n) = x(n) + jx (n) y(n) = y(n) + jy (n)
i q i q
ej /2
Figure P8–17
8.18 Many digital communications systems (your cell phone, for example) are de-
signed using the following principle regarding sinusoids whose frequencies
are fHz: The product of
sin(2πft)sin(2πft+ θ)
results in a sinusoid of frequency 2fHz superimposed on a constant DC level
whose value is proportional to cos(θ). Prove this principle to be true.

470 Quadrature Signals
Note: The value of this exercise is that we now know how to determine the
phase θ of sin(2πft + θ) by computing the average of sin(2πft)sin(2πft + θ).
Thus if the phase θ of sin(2πft + θ) represents some sort of information, we
can measure (extract) that information.
8.19 Think about programming a DSP chip to generate the complex time-domain
sequence, the sum of two complex exponentials that both oscillate through
one cycle every 20 samples, defined by
x(n) = 5ej(2πn/20 + π/4)+ 3ej(2πn/20 + π/6).
To optimize your code (reduce its number of computations), you can generate
the x(n) sequence using
x(n) = Aej(Bn+C).
To exercise our skills using both the mathematical notation of complex signals
and Euler’s equations, determine the values for the above real-only variables
A, B, and C.
Hint:Recall one of thelaws of exponents: αpαq= αp+q.
8.20 Think about the continuous (analog) signal comprising a constant value of
one minus a complex sinusoid whose frequency is one Hz defined by
x(t) = 1 –
ej2π[1]t.
Draw rough sketches, over the time interval 0≤t≤1 second, of
(a) The real part of x(t),
(b) The imaginary part of x(t),
(c) The magnitude of x(t).
8.21 In many quadrature processing applications we seek to compute the angle θ
of a single complex time-domain sample by using the arctangent operation in
the text’s Eq. (8–6). Given the complex number
C=I+jQ=Mejθ
, three simple al-
gorithms for estimating angle θare the following polynomials, where X=Q/I:
1 1
θ≈θ =X− (X)3 + (X)5 ,
1 3 5
15X+4X3
θ≈θ = ,and
2 15+9X2
θ≈θ = –0.19329X 3 + 0.97327X.
3

Chapter 8 Problems 471
3
Magnitude of
s) 2 arctan error 1
e
e
e
gr
3
d 1
or|
(
Err
0
|
2
-1
-40 -20 0 20 40
True angle in degrees
Figure P8–21
The magnitudes of the angular error associated with these arctangent algo-
rithms, measured in degrees, over the true angle θ range of –45o≤θ≤45o are
shown in Figure P8–21.
(a) Which arctangent algorithm, θ , θ , or θ , has the lowest average error
1 2 3
magnitude over the range –45o≤θ≤45o?
(b) To compare the computational workload associated with these algo-
rithms, create a table (like the following) and fill in the number of multi-
plies, additions, and divisions necessary to compute an estimated
arctangent for each algorithm.
Hint: Do not count redundant multiplies. That is, if X2 has been computed,
use that X2value for computing higher orders of X.
Algorithm Multiplies Additions Divisions
θ
1
θ
2
θ
3
(c) Assume an arctangent computation needs to be implemented on a pro-
grammable DSP chip, and that chip requires 1, 1, and 30 processor clock
cycles to implement a multiply, an addition, and a division, respectively.
Which arctangent algorithm, θ , θ , or θ , would you use to minimize the
1 2 3
computational workload (processor clock cycles) and increase real-time
data throughput?
(d) If you know that the true angle θis always in the range of –22.5o≤θ≤22.5o,
which arctangent algorithm, θ , θ , or θ , would be your first choice? Ex-
1 2 3
plain why. (The assumptions in Part (c) regarding processor clock cycles
still apply.)

472 Quadrature Signals
8.22 An interesting contender in the arctangent race (to calculate reasonably accu-
rate arctangents with as few computations as possible) for estimating the
angle of a complex time-domain sample is the following algorithm:
tan −1 ⎛ ⎜ Q⎞ ⎟ ≈θ′= Q/I radians
⎝ I ⎠ 1+0.28125(Q/I)2
or
tan −1 ⎛ ⎜ Q⎞ ⎟ ≈θ′= Q/I ⋅ 180 deggrees,
⎝ I ⎠ 1+0.28125(Q/I)2 π
where θ‘ is the approximated angle of the complex sample C=I+jQ. This al-
gorithm is useful when the true angle of Cis in the range of –π/2 to π/2 radi-
ans (–45 to 45 degrees).
The error in degrees in using the above θ‘ approximation is shown in Figure
P8–22. To further investigate this arctangent algorithm, and exercise your al-
gebraic function analysis skills, at what angles of the true θis the algorithm’s
error at its peak values of ±0.26 degrees?
Hint: To make the algebra a little simpler, replace Q/I with X and replace
0.28125 with A. With those changes, the error E(X) in this algorithm (in radians)
is the true arctangent of Xminus the approximation of the arctangent of X, or
E(X)=tan
−1(X)− X
radians.
1+AX2
0.4
s)
e e 0.2
gr
error = –0.26o
e
or
( d 0
error = 0.26o
Err
-0.2
-0.4
-40 -20 0 20 40
True angle (degrees)
Figure P8–22
8.23 Consider the process shown in Figure P8–23. The x(n) input sequence, whose
f sample rate is 5000 Hz, is multiplied by a complex exponential m(n) se-
s
quence to produce the complex y(n) output sequence. What is the frequency,
measured in Hz, of the complex exponential m(n) sequence?

Chapter 8 Problems 473
x(n) y(n)
m(n)= ej0.8 n
f = 5000 Hz
s
Figure P8–23
8.24 To gain further understanding of the quadrature sampling (complex down-
conversion) in the text’s Figure 8–18(a), we investigate the quadrature sam-
pler’s behavior when the x (t) input is a continuous (analog) cosine wave
bp
whose frequency is f +10 Hz. That is:
c
x (t) = cos[2π(f +10)t].
bp c
Given the above x (t) input signal:
bp
(a) What is the spectral content of x(t) in Figure 8–18(a)?
i
(b) What is the spectral content of x (t)?
q
(c) What is the phase shift between the low- and high-frequency components
in x (t)?
q
(d) What is the spectral content of i(t) in Figure 8–18(a)?
(e) What is the spectral content of q(t)?
8.25 In the text’s Section 8.6 we said that multiplying a continuous (analog) x(t)
time-domain signal, centered at f Hz, by a complex exponential whose fre-
o
quency is negative f Hz,
e–j2πfct,
results in a signal whose spectrum is shifted
c
down in frequency by f Hz. This property is an exceedingly important con-
c
cept to understand and remember!
(a) Prove that the above statement is true for discrete x(n) time-domain sig-
nals originally centered at f Hz. Assume that x(n)=cos(2πf nt), a real-
o o s
valued sinusoid where f >f .
o c
(b) Draw the spectrum of the original x(n) and the spectrum of the down-
shifted sequence.
(c) What is the spectral amplitude loss when multiplying a discrete x(n) time-
domain signal by
e–j2πfct?
8.26 Here is a problem of great practical importance. In the text we discussed ana-
log signal down-conversion using the process shown in Figure P8–26(a)
where x (t) was an analog bandpass signal centered at a frequency of ω ra-
bp o
dians/second. The ideal mixing (frequency translation) signal is
m(t) = Acos(ω t) –jAsin(ω t) = Ae–jω ot,
o o
whose spectrum is the single spectral component shown in Figure P8–26(b).

474 Quadrature Signals
x(t) Imag
i A
x (t)
bp Real
Acos( t) o
o x (t)
q
IdealM(f)
0
Asin( t)
o Freq
(a) (b)
Imag
x
i
(t) ~~A
x (t) Real
bp o
Acos( t) b
o x (t)
q M (f)
imp
0
o
A(1+ )sin( t + ) c
o
d
Freq
(c) (d)
Figure P8–26
A more realistic situation is shown in Figure P8–26(c) where, due to real-
world analog signal generator imperfections, the quadrature part of m(t) con-
tains a small amplitude error εand a small phase error α(in radians). That is,
the imperfect m(t) becomes
m (t) = Acos(ω t) – jA(1+ε)sin(ω t+ α).
imp o o
When those quadrature errors exist, the spectrum of the imperfect m (t) is
imp
that shown in Figure P8–26(d). In practice, we want to keep the b, c, and d
spectral components of M (f) as small as possible. Your problem is: What
imp
are the values for ε and α such that the magnitude of the unwanted ω fre-
o
quency component of M (f) is equal to 0.1 percent of (60 dB below) the
imp
magnitude of the desired –ω frequency component of M (f)?
o imp
Hint: Start by converting the m (t) expression to polar (exponential) form
imp
and determine the complex-amplitude terms of its positive- and negative-
frequency components. At the end of that exercise, assume that ε and α are
much less than one part in a hundred to simplify your equations.
8.27 I once encountered an Internet website that presented a narrowband quadra-
ture (complex) bandpass filter similar to that shown in Figure P8–27(a). The
input sequence is assumed to be complex, and the filter’s feedback coefficient
is complex, A+jB. (Such filters are useful in both spectrum analysis and quad-

Chapter 8 Problems 475
A
y(n)
i
x(n) +jx (n) y(n) +jy (n) x(n) –
i q i q i z–1
–B
z–1
B
x (n)
q
z–1
y (n)
A + jB – q
A
(a) (b)
Figure P8–27
rature digital filtering applications.) The website stated that the real-valued
coefficient implementation of this complex filter is that shown in Figure
P8–27(b).
(a) As it turns out, Figure P8–27(b) is not a correct implementation of the
quadrature filter. (Don’t believe everything you read on the Internet!)
Draw the correct real-coefficient implementation of the filter in Figure
P8–27(a).
(b) Putting on our filter analysis hat, represent the complex input to the nar-
rowband quadrature filter in Figure P8–27(a) as x(n), the filter’s complex
output as y(n), and the feedback coefficient as
ej2πfr/fs.
(Variable f is the
r
narrowband filter’s resonant frequency in Hz, and f is the system’s sam-
s
ple rate in Hz.) What is the time-domain difference equation, in complex
notation, describing the quadrature bandpass filter?
(c) What is the z-domain equation for the filter’s H(z)=Y(z)/X(z) transfer
function? (Transfer functions help us determine both the frequency re-
sponse and stability of the filter.)
(d) Is this complex filter stable? Justify your answer.
(e) What is the frequency-domain equation for the filter’s H(f) frequency re-
sponse? (The frequency response expression allows us to plot the filter’s
frequency-domain magnitude and phase behavior using signal process-
ing software.)

476 Quadrature Signals
8.28 There are many signal processing applications that require a signal to be
translated in frequency (particularly in digital communications systems). In
the text we referred to frequency translation as complex up-conversion, complex
down-conversion, and complex mixing. With this frequency translation notion in
mind:
(a) Assuming we want to frequency translate a real-only time sequence,
draw the block diagram of a discrete complex frequency translation net-
work whose input is a real-only-valued discrete sequence, and whose
output is a complex-valued discrete sequence.
(b) Assuming we only need the real part of a frequency-translated complex
time sequence, draw the block diagram of a discrete complex frequency
translation network whose input is a complex-valued discrete sequence
and whose output sequence is real-valued.
8.29 In the literature of quadrature processing, we often encounter a network
called a complex digital resonator whose block diagram is shown in Figure
P8–29. The feedback coefficient is a complex number, where the ω frequency
r
value is a normalized angle measured in radians in the range of 0 to 2π radi-
ans, corresponding to a resonator cyclic frequency range of 0 to f, where f is
s s
the sample rate in Hz. Should we want to build a resonator (oscillator) whose
cyclic frequency is f/4, then we’d merely set ω equal to π/2 and apply a
s r
single unity-valued sample, x(n), to the input of the resonator to initiate its
oscillations.
Complex digital resonator (oscillator)
x(n) y(n)
z–1
ej r
Figure P8–29
Here’s the homework problem: If you had to build this complex resonator in
hardware (because you needed a complex
ejωr
quadrature sequence for some
follow-on processing), what would be your resonator’s block diagram where,
of course, all of the discrete sequence values and feedback coefficients are
real-only values?

Chapter 8 Problems 477
|H (f)| H (z) z-plane
0 real real
1
art
–10 p
d B ar y 0
n
–20 gi
a
m
I
–1
–30
–f/2 0 f/2 f 3f/2 –1 0 1
s s s s Real part
Frequency
(a) (b)
|H (f)| H (z) z-plane
cmplx cmplx
0
1
art
–10 p
y
d
B
n
ar0
–20
gi
a
m
I
–1
–30
–f/2 0 f/2 f 3f/2 –1 0 1
s s s s Real part
–f/4 f/4
s s
Frequency
(c) (d)
Figure P8–30
8.30 Assume that, on the job, you encounter the real-coefficient recursive lowpass
filter whose z-domain transfer function is
H (z) = 1 + z–1.
real
The filter’s frequency magnitude response and z-plane pole/zero plot are
given in Figures P8–30(a) and P8–30(b). Your problem is to design a complex-
coefficient H (z) bandpass filter whose frequency magnitude response is
cmplx
shifted up in frequency by f/4 Hz relative to |H (f)|. That is, design a
s real
complex filter whose magnitude response is that given in Figure P8–30(c),
having the pole/zero plot shown in Figure P8–30(d).
(a) What is the transfer function equation of the complex-coefficient H (z)
cmplx
filter?
(b) Draw the block diagram of the H (z) filter.
cmplx
8.31 The process of quadrature modulation and demodulation has become very
important and popular in modern communications systems. This means we

478 Quadrature Signals
I(t)
cos( t) Quadrature
c +
t) modulator
c
Q(t)sin( t)
Q(t) c
Note:
= 2 f
c c
Antenna
LPF Output1
cos( t) Quadrature
c
demodulator
LPF Output2
t)
c
Figure P8–31
can transmit two totally independent signals, I(t) and Q(t), at the same RF car-
rier frequency (f ) and still receive and demodulate those two signals sepa-
c
rately as shown in Figure P8–31. In addition, we only have to use one
transmitting antenna! To convince ourselves this communications method
works, show algebraically what will be the outputs (in terms of I(t) and Q(t))
of the lowpass filters of the quadrature demodulator in Figure P8–31. (The I(t)
and Q(t) signals are binary and have the values of either +1 or –1. The “LPF”
stages are identical lowpass filters whose cutoff frequencies are ω /2 radi-
c
ans/second.) Justify your solution.

CHAPTER NINE
The Discrete
Hilbert
Transform
The discrete Hilbert transform is a process used to generate complex-valued
signals from real-valued signals. Using complex signals in lieu of the real sig-
nals simplifies and improves the performance of many signal processing op-
erations. If you’ve read about the discrete Hilbert transform in the DSP
literature, you’ve probably plowed through the mathematical descriptions of
analytic functions, with the constraints on their z-transforms in their regions
of convergence, and perhaps you’ve encountered the Cauchy integral theo-
rem used in the definition of the Hilbert transform.†Well, the discrete Hilbert
transform is not as complicated as it first appears; this chapter attempts to
support that claim.
Here we gently introduce the Hilbert transform from a practical stand-
point, explain the mathematics behind its description, and show how it’s
used in DSP systems. In addition to providing some of the algebraic steps
missing from some textbooks, we’ll illustrate the time- and frequency-domain
characteristics of the transform, with an emphasis on the physical meaning of
the quadrature (complex) signals associated with Hilbert transform applica-
tions. Finally, nonrecursive Hilbert transformer design examples and tech-
niques for generating complex, so-called analytic signals are presented. (If
you’re not well versed in the notation and behavior of complex signals at this
point, a review of Chapter 8 would be useful.)
†The Hilbert transform is named in honor of the great German mathematician David Hilbert
(1862–1943). On his tomb in Göttingen, Germany, is inscribed, “Wir müssen wissen, wir werden
wissen.” (We need to know, we shall know.)
479

480 The Discrete Hilbert Transform
9.1 HILBERT TRANSFORM DEFINITION
The Hilbert transform (HT) is a mathematical process performed on a real
signal x(t), yielding a new real signal x (t), as shown in Figure 9–1.
r ht
Our goal here is to ensure that x (t) is a 90-degree phase-shifted version
ht
of x(t). So, before we carry on, let’s make sure we understand the notation
r
used in Figure 9–1. The variables are defined as follows:
x(t) = a real continuous time-domain input signal
r
h(t) = the time impulse response of a Hilbert transformer
x (t) = the HT of x(t), (x (t) is also a real time-domain signal)
ht r ht
X(ω) = the Fourier transform of real input x(t)
r r
H(ω) = the frequency response (complex) of a Hilbert transformer
X (ω) = the Fourier transform of output x (t)
ht ht
ω = continuous frequency measured in radians/second
t = continuous time measured in seconds
We’ll clarify that x (t) =h(t) x(t), where the symbol means convolu-
ht * r * ⋅
tion. In addition, we can define the spectrum of x (t) as X (ω) =H(ω) X(ω).
ht ht r
(These relationships sure make the HT look like a filter, don’t they? We’ll cog-
itate on this notion again later in this chapter.)
Describing how the new x (t) signal, the HT of x(t), differs from the origi-
ht r
nal x(t) is most succinctly done by relating their Fourier transforms, X(ω) and
r r
X (ω). In words, we can say that all of x (t)’s positive-frequency components
ht ht
are equal to x(t)’s positive-frequency components shifted in phase by –90 de-
r
grees. Also, all of x (t)’s negative-frequency components are equal to x(t)’s
ht r
negative-frequency components shifted in phase by +90 degrees. Mathemati-
cally, we recall
X (ω) = H(ω)X(ω) (9–1)
ht r
where H(ω) = –j over the positive-frequency range, and H(ω) = +j over the
negative-frequency range. We show the nonzero imaginary part of H(ω) in
Figure 9–2(a).
To fully depict the complex H(ω), we show it as floating in a three-
dimensional space in Figure 9–2(b). The bold curve is our complex H(ω). On
the right side is an upright plane on which we can project the imaginary part
x(t) Hilbert transform x (t)
r ht
X(ω) h(t),H(ω) X (ω)
r ht
Figure 9–1 The notation used to define the continuous Hilbert transform.

9.1 Hilbert Transform Definition 481
Imag part of
H( )
H( )
Imag
Imag part ofH( ) 2
1
. . . +1 0
–1
–2
0 Freq
+2
–1 . . . –Freq 0 0 Real
+Freq –2
Real part
ofH( )
(a) (b)
Figure 9–2 The complex frequency response of H(ω).
of H(ω). At the bottom of Figure 9–2(b) is a flat plane on which we can project
the real part of H(ω). In rectangular notation, we say that H(ω)=0+j1 for neg-
ative frequencies and H(ω)=0–j1 for positive frequencies. (We introduce the
three-dimensional axes of Figure 9–2(b) now because we’ll be using them to
look at other complex frequency-domain functions later in this discussion.)
To show a simple example of an HT, and to reinforce our graphical
viewpoint, Figure 9–3(a) shows the three-dimensional time and frequency
representations of a real cosine wave cos(ωt). Figure 9–3(b) shows the HT of
cos(ωt) to be the sinewave sin(ωt).
The complex spectrum on the right side of Figure 9–3(b) shows how the
HT rotates the cosine wave’s positive-frequency spectral component by –j,
and the cosine wave’s negative-frequency spectral component by +j. You can
.
.
.
Imaginary Imaginary
Real Real
cosine Real
wave 0
(a)
. .
.
Time Freq
Imaginary
.
. .
Imaginary
Part
Real Real
Real
sinewave
(b) 0
. .
.
Time Freq
Figure 9–3 The Hilbert transform: (a) cos(ωt); (b) its transform is sin(ωt).

482 The Discrete Hilbert Transform
see on the right side of Figure 9–3 that our definition of the +j multiplication
operation is a +90-degree rotation of a spectral component counterclockwise
about the frequency axis. (The length of those spectral components is half the
peak amplitude of the original cosine wave.) We’re assuming those sinusoids
on the left in Figure 9–3 exist for all time, and this allows us to show their
spectra as infinitely narrow impulses in the frequency domain.
Now that we have this frequency response of the HT defined, it’s rea-
sonable for the beginner to ask: “Why would anyone want a process whose
frequency-domain response is that weird H(ω) in Figure 9–2(b)?”
9.2 WHY CARE ABOUT THE HILBERT TRANSFORM?
The answer is: We need to understand the HT because it’s useful in so many
complex-signal (quadrature) processing applications. Abrief search on the In-
ternet reveals HT-related signal processing techniques being used in the fol-
lowing applications:
• Quadrature modulation and demodulation (communications)
• Automatic gain control (AGC)
• Analysis of two- and three-dimensional complex signals
• Medical imaging, seismic data and ocean wave analysis
• Instantaneous frequency estimation
• Radar/sonar signal processing, and time-domain signal analysis using
wavelets
• Time difference of arrival (TDOA) measurements
• High-definition television (HDTV) receivers
• Loudspeaker, room acoustics, and mechanical vibration analysis
• Audio and color image compression
• Nonlinear and nonstationary system analysis
All of these applications employ the HT either to generate or to measure
complex time-domain signals, and that’s where the HT’s power lies. The HT
delivers to us, literally, another dimension of signal processing capabilities as
we move from two-dimensional real signals to three-dimensional complex
signals. Here’s how.
Let’s consider a few mathematical definitions. If we start with a real
time-domain signal x(t), we can associate with it a complex signal x (t), de-
r c
fined as
x (t) = x(t) + jx(t). (9–2)
c r i

9.2 Why Care About the Hilbert Transform? 483
The complex x (t) signal is known as an analyticsignal(because it has no
c
negative-frequency spectral components), and its real part is equal to the
original real input signal x(t). The key here is that x (t)’s imaginary part, x(t),
r c i
is the HT of the original x(t) as shown in Figure 9–4.
r
As we’ll see shortly, in many real-world signal processing situations x (t)
c
is easier, or more meaningful, to work with than the original x(t). Before we
r
see why that is true, we’ll explore x (t) further to attempt to give it some
c
physical meaning. Consider a real x(t)=cos(ω t) signal that’s simply four cy-
r o
cles of a cosine wave and its HT x(t) sinewave as shown in Figure 9–5. The
i
x (t) analytic signal is the bold corkscrewfunction.
c
We can describe x (t) as a complex exponential using one of Euler’s
c
equations. That is:
x (t) = x(t) + jx(t) = cos(ω t) + jsin(ω t) = ejω ot. (9–3)
c r i o o
The spectra of those signals in Eq. (9–3) are shown in Figure 9–6. Notice
three things in Figure 9–6. First, following the relationships in Eq. (9–3), if we
rotate X(ω) by +90 degrees counterclockwise (+j) and add it to X(ω), we get
i r
X (ω) =X (ω)+jX(ω). Second, note how the magnitude of the component in
c r i
X (ω) is double the magnitudes in X(ω). Third, notice how X (ω) is zero over
c r c
the negative-frequency range. This property of zero spectral content for nega-
tive frequencies is why X (ω) is called an analytic signal. Some people call
c
X (ω) a one-sidedspectrum.
c
To appreciate the physical meaning of our discussion here, let’s remem-
ber that the x (t) signal is not just a mathematical abstraction. We can generate
c
x (t) in our laboratory and route it via cables to the laboratory down the hall.
c
(This idea is described in Section 8.3.)
To illustrate the utility of this analytic signal x (t) notion, let’s see how ana-
c
lytic signals are very useful in measuring instantaneous characteristics of a
time-domain signal: measuring the magnitude, phase, or frequency of a signal
at some given instant in time. This idea of instantaneous measurements doesn’t
seem so profound when we think of characterizing, say, a pure sinewave. But if
we think of a more complicated signal, like a modulated sinewave, instanta-
neous measurements can be very meaningful. If a real sinewave x(t) is ampli-
r
tude modulated so its envelopecontains information, from an analytic version of
the signal we can measure the instantaneous envelope E(t) value using
x(t)
r
x(t)
r
x(t) = x(t) + jx(t)
Hilbert c r i
x(t)
transformer, h(t) i
Figure 9–4 Functional relationship between the x(t) and x(t) signals.
c r

484 The Discrete Hilbert Transform
x(t) i
Figure 9–5 The Hilbert transform and the analytic signal of cos(ω t).
o
E(t) = |x c (t)| = x r (t) 2 +x i (t) 2 . (9–4)
That is, the envelope of the signal is equal to the magnitude of x (t). We show
c
a simple example of this AM demodulation idea in Figure 9–7(a), where a si-
nusoidal signal is amplitude modulated by a low-frequency sinusoid (dashed
curve). To recover the modulating waveform, traditional AM demodulation
gamI
x(t)
c (Hilbert
transform
ofx(t))
2 r
1
0
-1
-2
-1
0 2
1 1 Real
0
Time 2 -1
3 -2 x(t)
r
X( ) X( ) X ( ) = X( ) + jX( )
r i c r i
Imag Imag Imag
0.5
0.5
– Real Real Real
o –
o 1
0 0.5 0 0
o
o o
Freq Freq Freq
–0.5
Figure 9–6 HT spectra: (a) spectrum of cos(ω t); (b) spectrum of the Hilbert trans-
ο
form of cos(ω t), sin(ω t); (c) spectrum of the analytic signal of
ο ο
cos(ω t),ejωot.
ο

9.2 Why Care About the Hilbert Transform? 485
1 Amplitude-modulated
sinewavex(t)
0.5 r
(a) 0
-0.5
-1
0 0.05 0.1 0.15 0.2 0.25
Time
1
0.8 |x(t)|
r
0.6
(b)
0.4
0.2
0 0.05 0.1 0.15 0.2 0.25
Time
1
E(t) = |x(t)|
0.6 c
0.2
(c)
-0.2
-0.6 x r (t) x i (t)
-1
0 0.05 0.1 0.15 0.2 0.25
Time
Figure 9–7 Envelope detection: (a) input x(t) signal; (b) traditional lowpass filter-
r
ing of |x(t)|; (c) complex envelope detection result |x(t)|.
r c
would rectify the amplitude-modulated sinewave, x(t), and pass the result
r
through a lowpass filter. The filter’s output is represented by the solid curve
in Figure 9–7(b) representing the modulating waveform. Instead, we can
compute the HT of x(t), yielding x(t), and use x(t) to generate the
r i i
x (t)=x (t)+jx (t) analytic version of x(t). Finally, we compute the magnitude
c r i r
of x (t) using Eq. (9–4) to extract the modulating waveform shown as the bold
c
solid curve in Figure 9–7(c). The |x (t)| function is a muchmore accurate rep-
c
resentation of the modulating waveform than the solid curve in Figure 9–7(b).
Suppose, on the other hand, some real x(t) sinewave is phase modu-
r
lated. We can estimate x (t)’s instantaneous phase ø(t), using
c
⎛ ⎞
x (t)
ø(t) = tan–1 ⎜ i ⎟. (9–5)
⎝x (t)⎠
r
Computing ø(t) is equivalent to phase demodulation of x(t). Likewise
r
(and more often implemented), should a real sinewave carrier be frequency
modulated, we can measure its instantaneous frequency F(t) by calculating
the instantaneous time rate of change of x (t)’s instantaneous phase using
c
edutilpmA
edutilpmA
edutilpmA

486 The Discrete Hilbert Transform
⎛ ⎞
x (t)
F(t) = d ø(t) = d tan –1 ⎜ i ⎟. (9–6)
⎝x (t)⎠
dt dt r
The process in Eq. (9–6) is a form of digital differentiation, a topic we discuss
in some detail in Chapter 7.
Calculating F(t) is equivalent to frequency demodulation of x(t). By the
r
way, if ø(t) is measured in radians, then F(t) in Eq. (9–6) is measured in radi-
ans/second. Dividing F(t) by 2π will give it dimensions of Hz. (Another fre-
quency demodulation method is discussed in Section 13.22.)
For another HT application, consider a real x(t) signal whose |X(ω)|
r r
spectral magnitude is centered at 25 kHz as shown in Figure 9–8(a). Suppose
we wanted to translate that spectrum to be centered at 20 kHz. We could mul-
tiply x(t) by the real sinusoid cos(2π5000t) to obtain a real signal whose spec-
r
trum is shown in Figure 9–8(b). The problem with this approach is we’d need
an impractically high-performance filter (the dashed curve) to eliminate those
unwanted high-frequency spectral images.
On the other hand, if we compute the HT of x(t) to obtain x(t), and com-
r i
bine the two signals to form the analytic signal x (t) =x (t)+jx(t), we’ll have
c r i
the complex x (t) whose one-sided spectrum is given in Figure 9–8(c). Next
c
we multiply the complex x (t) by the complex
e–j2π5000t,
yielding a frequency-
c
translated x (t) complex signal whose spectrum is shown in Figure 9–8(d).
out
Our final step is to take the real part of x (t) to obtain a real signal with the
out
desired spectrum centered about 20 kHz, as shown in Figure 9–8(e).
|X( )|
r
(a)
-50 kHz 0 50 kHz Freq
(b)
-50 kHz 0 50 kHz Freq
|X( )|
c
(c)
-50 kHz 0 50 kHz Freq
|X ( )|
out
(d)
-50 kHz 0 50 kHz Freq
Magnitude of real
part of X ( )
(e) out
-50 kHz 0 50 kHz Freq
Figure 9–8 Spectra associated with frequency translation of a real signal x(t).
r

9.3 Impulse Response of a Hilbert Transformer 487
Now that we’re convinced of the utility of the HT, let’s determine the
HT’s time-domain impulse response and use it to build Hilbert transformers.
9.3 IMPULSE RESPONSE OF A HILBERT TRANSFORMER
Instead of following tradition and just writing down the impulse response
equation for a device that performs the HT, we’ll show how to arrive at that
expression. To determine the HT’s impulse response expression, we take the
inverse Fourier transform of the HT’s frequency response H(ω). The garden-
variety continuous inverse Fourier transform of an arbitrary frequency func-
tion X(f) is defined as
∞
x(t) =
∫
X(f)e
j2πft
df, (9–7)
−∞
where f is frequency measured in cycles/second (hertz). We’ll make three
changes to Eq. (9–7). First, in terms of our original frequency variable ω=2πf
radians/second, and because df = dω/2π, we substitute dω/2π for the df
term. Second, because we know our discrete frequency response will be peri-
odic with a repetition interval of the sampling frequency ω, we’ll evaluate
s
Eq. (9–7) over the frequency limits of –ω/2 to +ω/2. Third, we partition the
s s
original integral into two separate integrals. This algebraic massaging gives
us the following:
h(t) =
2
1
π −
ω
ω
s ∫
s
/
/
2
2
H(ω)e jωt dω=
2
1
π −ω
∫ 0
s /2
je jωt dω+
2
1
π
ω s ∫
0
/2 −je jωt dω
= 1 ⎛ ⎜ [ e jωt ] 0 − [ e jωt ]ω s /2⎞ ⎟ = 1 ( e j0 −e −jω s t/2 −e jω s t/2 +e j0 )
2πt⎝ −ω /2 0 ⎠ 2πt
s
= 1 [2 –2cos(ωt/2)] = 1 [1 –cos(ωt/2)]. (9–8)
2πt s πt s
Whew! OK, we’re going to plot this impulse response shortly, but first
we have one hurdle to overcome. Heartache occurs when we plug t = 0 into
Eq. (9–8) because we end up with the indeterminate ratio 0/0. Hardcore
mathematics to the rescue here. We merely pull out the Marquis de L’Hopi-
tal’s Rule, take the time derivatives of the numerator and denominator in Eq.
(9–8), and then set t=0 to determine h(0). Following through on this:

488 The Discrete Hilbert Transform
d
h(0) = dt (1−cos(ω s t/2)) | = ω s sin(ω s t/2)| = 0. (9–9)
d t→0 2π t→0
πt
dt
So now we know that h(0)=0. Let’s find the discrete version of Eq. (9–8)
because that’s the form we can model in software and actually use in our DSP
work. We can go digital by substituting the discrete-time variable nt for the
s
continuous-time variable tin Eq. (9–8). Using the following definitions
n = discrete time-domain integer index (...,–3,–2,–1,0,1,2,3,...)
f = sample rate measured in samples/second
s
t = time between samples, measured in seconds (t =1/f)
s s s
ω = 2πf
s s
we can rewrite Eq. (9–8) in discrete form as
1
h(n) = [1 – cos(ωnt/2)]. (9–10)
πnt s s
s
Substituting 2πf for ω, and 1/f for t, we have
s s s s
h(n) = 1 [1 – cos(2πfn/2f)] = f s [1 – cos(πn)],
πnt
s
s s πn
≠
for n 0, and [h(n) = 0, for n= 0]. (9–11)
Finally, we plot HT’s h(n) impulse response in Figure 9–9. The f term in
s
Eq. (9–11) is simply a scale factor; its value does not affect the shape of h(n).
An informed reader might, at this time, say, “Wait a minute. Equation (9–11)
doesn‘t look at all like the equation for the HT’s impulse response that’s in
my other DSP textbook. What gives?” The reader would be correct because
one popular expression in the literature for h(n) is
2sin2(πn/2) (9–12)
alternateform:h(n)= .
πn
Here’s the answer: The derivation of Eq. (9–12) is based on the assump-
tion that the f sampling rate is normalized to unity. If we set f = 1 in Eq.
s s
(9–11), then that new expression will be equal to Eq. (9–12). We leave the
proof of that equality as a homework problem.

9.4 Designing a Discrete Hilbert Transformer 489
1
2
π 2
h(n) 5π
2
0.5
3π 2
7π
. . .
0
. . .
-0.5
-1
-15 -10 -5 0 5 10 15
n
Figure 9–9 The Hilbert transform’s discrete impulse response when f =1.
s
Looking again at Figure 9–9, we can reinforce the validity of our h(n) de-
rivation. Notice that for n > 0 the values of h(n) are nonzero only when n is
odd. In addition, the amplitudes of those nonzero values decrease by factors
of 1/1, 1/3, 1/5, 1/7, etc. Of what does that remind you? That’s right, the
Fourier series of a periodic squarewave! This makes sense because our h(n) is
the inverse Fourier transform of the squarewave-like H(ω) in Figure 9–2. Fur-
thermore, our h(n) is antisymmetric, and this is consistent with a purely imag-
inary H(ω). (If we were to make h(n) symmetrical by inverting its values for
all n < 0, the new sequence would be proportional to the Fourier series of a
periodic real squarewave.)
Now that we have the expression for the HT’s impulse response h(n),
let’s use it to build a discrete Hilbert transformer.
9.4 DESIGNING A DISCRETE HILBERT TRANSFORMER
Discrete Hilbert transformations can be implemented in either the time or fre-
quency domains. Let’s look at time-domain Hilbert transformers first.
9.4.1 Time-Domain Hilbert Transformation:
FIR Filter Implementation
Looking back at Figure 9–4, and having h(n) available, we want to know how
to generate the discrete x(n). Recalling the frequency-domain product in
i
Eq.(9–1), we can say x(n) is the convolution of x(n) and h(k). Mathematically,
i r
this is

490 The Discrete Hilbert Transform
x(n)
r . . .
z -1 z -1 z -1 z -1
h(0) h(1) h(2) h(K-2) h(K-1)
x(n)
i
Figure 9–10 FIR implementation of a K-tap Hilbert transformer.
∞
∑
x(n)= h(k) x(n–k). (9–13)
i r
k=−∞
So this means we can implement a Hilbert transformer as a discrete nonrecur-
sive finite impulse response (FIR) filter structure as shown in Figure 9–10.
Designing a traditional time-domain FIR Hilbert transformer amounts to
determining those h(k) values so the functional block diagram in Figure 9–4
can be implemented. Our first thought is merely to take the h(n) coefficient
values from Eq. (9–11), or Figure 9–9, and use them for the h(k)s in Figure 9–10.
That’s almost the right answer. Unfortunately, the Figure 9–9 h(n) sequence is
infinite in length, so we have to truncate the sequence. Figuring out what the
truncated h(n) should be is where the true design activity takes place.
To start with, we have to decide if our truncated h(n) sequence will have
an odd or even length. We make this decision by recalling that FIR implemen-
tations having antisymmetric coefficients and an odd, or even, number of
taps are called a Type-III, or a Type-IV, system respectively[1–3]. These two
antisymmetric filter types have the following unavoidable restrictions with
respect to their frequency magnitude responses |H(ω)|:
h(n) length:→ Odd (Type-III) Even (Type-IV)
|H(0)|=0 |H(0)|=0
|H(ω/2)|=0 |H(ω/2)| no restriction
s s
What this little table tells us is odd-tap Hilbert transformers always have
a zero magnitude response at both zero Hz and at half the sample rate. Even-
tap Hilbert transformers always have a zero magnitude response at zero Hz.
Let’s look at some examples.
Figure 9–11 shows the frequency response of a 15-tap (Type-III, odd-tap)
FIR Hilbert transformer whose coefficients are designated as h (k). These plots
1
have much to teach us.

9.4 Designing a Discrete Hilbert Transformer 491
1
h(k)
1
0
-1
0 2 4 6 8 10 12 14
k
0
-5
B Magnitude of H(ω)
d 1
-10
-15
-f/2 -f/4 0 f/4 f/2
s s s s
0
-10 Phase of H (ω)
s 1
n -20
a
di -30
a
r -40
-f/2 -f/4 0 f/4 f/2
s s s s
Figure 9–11 H(ω) frequency response of h(k), a 15-tap Hilbert transformer.
1 1
1. For example, an odd-tap FIR implementation does indeed have a zero
magnitude response at 0 Hz and ±f/2 Hz. This means odd-tap (Type-
s
III) FIR implementations turn out to be bandpass in performance.
2. There’s ripple in the H (ω) passband. We should have expected this be-
1
cause we were unable to use an infinite number of h (k) coefficients.
1
Here, just as it does when we’re designing standard lowpass FIR filters,
truncating the length of the time-domain coefficients causes ripples in
the frequency domain. (When we abruptly truncate a function in one do-
main, Mother Nature pays us back by invoking the Gibbs phenomenon,
resulting in ripples in the other domain.) You guessed it. We can reduce
the ripple in |H (ω)| by windowing the truncated h (k) sequence. How-
1 1
ever, windowing the coefficients will narrow the bandwidth of H (ω)
1
somewhat, so using more coefficients may be necessary after window-
ing is applied. You’ll find windowing the truncated h (k) sequence to be
1
to your advantage.
3. It’s exceedingly difficult to compute the HT of low-frequency signals.
We can widen the passband and reduce the transition region width of
H (ω)’s magnitude response, but that requires many filter taps.
1
4. The phase response of H (ω) is linear, as it should be when the coeffi-
1
cients’ absolute values are symmetrical. The slope of the phase curve

492 The Discrete Hilbert Transform
(that is constant in our case) is proportional to the time delay a signal se-
quence experiences traversing the FIR filter. More on this in a moment.
That discontinuity in the phase response at 0 Hz corresponds to π radi-
ans, as Figure 9–2 tells us it should. Whew, good thing. That’s what we
were after in the first place!
In our relentless pursuit of correct results, we’re forced to compensate
for the linear phase shift of H (ω)—that constant time value equal to the
1
group delay of the filter—when we generate our analytic x (n). We do this by
c
delaying, in time, the original x(n) by an amount equal to the group delay of
r
the h (k) FIR Hilbert transformer. Recall that the group delay, G, of a tapped-
1
delay line FIR filter, having antisymmetrical coefficients, is G=D/2 samples
where D is the number of unit-delay elements in the delay line. So our block
diagram for generating a complex x (n) signal, using an FIR structure, is given
c
in Figure 9–12(a). In this example, the 7-tap Hilbert filter has D=6 delay ele-
ments as shown in Figure 9–12(b). There we delay x(n) by G=6/2=3 samples,
r
generating the delayed sequence x’(n). This delayed sequence now aligns
r
properly in time with x(n).
i
If you’re building your odd-tap FIR Hilbert transform in hardware, an
easy way to obtain x’(n) is to tap off the original x(n) sequence at the center
r r
tap of the FIR Hilbert transformer structure as in Figure 9–12(b). If you’re
modeling Figure 9–12(a) in software, the x’(n) sequence can be had by insert-
r
ing G=3 zeros at the beginning of the original x(n) sequence.
r
x(n)
r 3-sample delay x'(n)
r
x(n) = x'(n) + jx(n)
(a) c r i
7-tap FIR
x(n)
Hilbert transformer i
x(n) x'(n)
r z–1 z–1 z–1 z–1 z–1 z–1 r
(b)
h(0) h(2) h(4) h(6)
+
x(n)
i
Figure 9–12 Generating an x(n) sequence when h(k) is a 7-tap FIR Hilbert filter:
c
(a) processing steps; (b) filter structure.

9.4 Designing a Discrete Hilbert Transformer 493
1
h(k)
2
0
-1
0 2 4 6 8 10 12 14
k
0
-5
-10 Magnitude of H( )
B 2
d-15
-20
-25
-f/2 -f/4 0 f/4 f/2
s s s s
Figure 9–13 H(ω) frequency response of h(k), a 14-tap Hilbert transformer.
2 2
We can, for example, implement an FIR Hilbert transformer using a
Type-IV FIR structure, with its even number of taps. Figure 9–13 shows this
notion where the coefficients are, say, h (k). See how the frequency magnitude
2
response is nonzero at ±f/2 Hz. Thus this even-tap filter approximates an
s
ideal Hilbert transformer somewhat better than an odd-tap implementation.
One of the problems with this traditional Hilbert transformer is that the
passband gain in |H (ω)| is not unity for all frequencies, as is the x’(n) path
2 r
in Figure 9–12. So to minimize errors, we must use many h (k) coefficients (or
2
window the coefficients) to make |H (ω)|’s passband as flat as possible.
2
Although not shown here, the negative slope of the phase response of
H (ω) corresponds to a filter group delay of G = (14–1)/2 = 6.5 samples. This
2
requires us to delay the original x(n) sequence by a noninteger (fractional)
r
number of samples in order to achieve time alignment with x(n). Fractional
i
time-delay filters are beyond the scope of this material, but reference [4] is a
source of further information on the topic.
Let’s recall that alternate coefficients of a Type-III (odd-tap) FIR are
zeros. Thus the odd-tap Hilbert transformer is more attractive than an even-
tap version from a computational workload standpoint. Almost half of the
multiplications in Figure 9–10 can be eliminated for a Type-III FIR Hilbert
transformer. Designers might even be able to further reduce the number of
multiplications by a factor of two by using the foldedFIR structure (discussed
in Section 13.7) that’s possible with symmetric coefficients (keeping in mind
that half the coefficients are negative).
Abrief warning: Here’s a mistake sometimes even the professionalsmake.
When we design standard linear-phase FIR filters, we calculate the coeffi-
cients and then use them in our hardware or software designs. Sometimes we
forget to flipthe coefficients before we use them in an FIR filter. This forgetful-
ness usually doesn’t hurt us because typical FIR coefficients are symmetrical.

494 The Discrete Hilbert Transform
Not so with FIR Hilbert filters, so please don’t forget to reverse the order of
your coefficients before you use them for convolutional filtering. Failing to
flip the coefficients will distort the desired HT phase response.
As an aside, Hilbert transformers can be built with IIR filter structures,
and in some cases they’re more computationally efficient than FIR Hilbert
transformers at the expense of a slight degradation in the 90-degree phase
difference between x(n) and x(n)[5,6].
r i
9.4.2 Frequency-Domain Hilbert Transformation
Here’s a frequency-domain Hilbert processing scheme deserving mention be-
cause the HT of x(n) and the analytic x (n) sequence can be generated simul-
r c
taneously. We merely take an N-point DFT of a real even-length-Nx(n) signal
r
sequence, obtaining the discrete X(m) spectrum given in Figure 9–14(a).
r
Next, create a new spectrum X (m)=2X(m). Set the negative-frequency X (m)
c r c
samples, that’s (N/2)+1 ≤ m ≤ N–1, to zero, leaving us with a new one-sided
X (m) spectrum as in Figure 9–14(b). Next, divide the X (0) (the DC term) and
c c
the X (N/2) spectral samples by 2. Finally, we perform an N-point inverse
c
DFT of the new X (m), the result being the desired analytic x (n) time-domain
c c
sequence. The real part of x (n) is the original x(n), and the imaginary part of
c r
x (n) is the HT of x(n). Done!
c r
There are several issues to keep in mind concerning this straightforward
frequency-domain analytic signal generation scheme:
1. If possible, restrict the x(n) input sequence length to an integer power of
r
two so the radix-2 FFT algorithm can be used to efficiently compute the
DFT.
2. Make sure the X (m) sequence has the same length as the original X(m)
c r
sequence. Remember, you zero out the negative-frequency X (m) sam-
c
ples; you don’t discard them.
3. The factor of two in the above X (m) = 2X(m) assignment compensates
c r
for the amplitude loss by a factor of two in losing the negative-frequency
spectral energy.
|X(m)| |X(m)| |X'(m)|
r c c
-f s /2 -ω o 0 ω o f s /2 Freq -f s /2 0 ω o f s /2 Freq -f' s -ω o 0 ω o f' s Freq
(a) (b) (c)
Figure 9–14 Spectrum of the original x(n) sequence, and the one-sided spec-
r
trum of the analytic x(n) sequence.
c

9.5 Time-Domain Analytic Signal Generation 495
4. If your HT application is block-oriented in the sense that you only have
to generate the analytic sequence from a fixed-length real-time sequence,
this technique is sure worth thinking about because there’s no time
delay heartache associated with time-domain FIR implementations to
worry about. With the advent of fast hardware DSPchips and pipelined
FFT techniques, the above analytic signal generation scheme may be vi-
able for a number of applications. One scenario to consider is using the
efficient 2N-Point Real FFT technique, described in Section 13.5.2, to
compute the forward DFT of the real-valued x(n). Of course, the
r
thoughtful engineer would conduct a literature search to see what algo-
rithms are available for efficiently performing inverse FFTs when many
of the frequency-domain samples are zeros.
Should you desire a decimated-by-two analytic x’ (n) sequence based on
c
x(n), it’s easy to do, thanks to reference [7]. First, compute the N-point X(m).
r r
Next, create a new spectral sequence X’ (k) = 2X(k) for 1 ≤ k ≤ (N/2)-1. Set
c r
X’ (0) equal to X(0)+X(N/2). Finally, compute the (N/2)-point inverse DFT
c r r
of X’ (m), yielding the decimated-by-two analytic x’ (n). The x’ (n) sequence
c c c
has a sample rate of f’ =f/2 and the spectrum shown in Figure 9–14(c).
s s
In Section 13.28.2 we discuss a scheme to generate interpolated analytic
signals from x(n).
r
9.5 TIME-DOMAIN ANALYTIC SIGNAL GENERATION
In digital communications applications, the FIR implementation of the HT
(such as that in Figure 9–12(b)) is used to generate a complex analytic signal
x (n). Some practitioners now use a time-domain complex filtering technique
c
to achieve analytic signal generation when dealing with real bandpass sig-
nals[8]. This scheme, which does not specifically perform the HT of an input
sequence x(n), uses a complex filter implemented with two real FIR filters
r
with essentially equal magnitude responses, but whose phase responses dif-
fer by exactly 90 degrees, as shown in Figure 9–15.
Here’s how it’s done. A standard K-tap FIR lowpass filter is designed,
using your favorite FIR design software, to have a two-sided bandwidth
slightly wider than the original real bandpass signal of interest. The real coef-
ficients of the lowpass filter, h (k), are then multiplied by the complex expo-
LP
nential m (k) sequence specified by
ix
m (k)= ej2π(k-D)fc/fs= cos[2π(k-D)f/f] + jsin[2π(k-D)f/f] (9-14)
ix c s c s
using the following definitions:
k= time index term of the exponential mixing sequence (k=0, 1 , 2, ..., K–1)
D= time delay of the K-tap lowpass filter, D=(K–1)/2

496 The Discrete Hilbert Transform
|X(m)| |H (m)| |X(m)|
r BP c
f/2 –f 0 f f/2 Freq –f/2 0 f f/2 Freq –f/2 –f 0 f f/2 Freq
s c c s s c s s c c s
x(n) h (k), real FIR filter
r cos x(n)
I
[Real parts ofh (k)]
BP
x(n) =x(n) +jx (n)
c I Q
h (k), real FIR filter
sin x (n)
Q
[Imag. parts ofh (k)]
BP
Figure 9–15 Generating an x(n) sequence with a complex filter (two real FIR fil-
c
ters).
f = bandpass signal’s center frequency, in Hz
c
f = sample rate of the original x(n) bandpass signal sequence in Hz
s r
The results of the h (k) times m (k) multiplication are complex coeffi-
LP ix
cients whose rectangular-form representation is
h (k)= h (k)+jh (k), (9-14’)
BP cos sin
creating a complex bandpass filter centered at f Hz. The delay value D in
c
Eq. (9–14) ensures that the h (k) and h (k) coefficients are symmetrical (or
cos sin
antisymmetrical) to obtain a linear-phase complex filter[9].
Next we use the real and imaginary parts of the filter’s h (k) coefficients
BP
in two separate real-valued-coefficient FIR filters as shown in Figure 9–15. In
DSP parlance, the filter producing the x(n) sequence is called the I-channel
I
for in-phase, and the filter generating x (n) is called the Q-channel for quadra-
Q
ture phase. There are several interesting aspects of this mixing analytic signal
generation scheme in Figure 9–15:
1. The mixing of the h (k) coefficients by m (k) induces a loss, by a factor
LP ix
of two, in the frequency magnitude responses of the two real FIR filters.
Doubling the h (k) values before mixing will eliminate the loss.
LP
2. We can window the h (k) coefficients before mixing to reduce the pass-
LP
band ripple in, and to minimize differences between, the magnitude re-
sponses of the two real filters. (We’d like to keep the passband gains as
similar as possible.) Of course, windowing will degrade the lowpass fil-
ter’s roll-off somewhat, so using more coefficients may be necessary be-
fore windowing is applied.

9.6 Comparing Analytic Signal Generation Methods 497
3. Odd- or even-tap lowpass filters can be used, with equal ease, in this
technique.
4. Because the h (k) or h (k) coefficients are exactly symmetrical, or ex-
cos sin
actly antisymmetrical, the folded FIR structure (see Section 13.7) can be
used to reduce the number of multiplies per filter output sample by
roughly a factor of two.
5. If the original bandpass signal and the complex filter are centered at
one-fourth the sample rate (f = f/4), count your blessings. In this case
o s
we set D= 0 in Eq. (9–14), making nearly half the coefficients of each real
filter zero-valued, maintaining linear phase, and reduce our FIR compu-
tational workload further by a factor of two.
6. A particularly efficient complex bandpass filter is a lowpass half-band
FIR filter whose center frequency has been translated to f = f/4. The re-
c s
sult is that half the h (k) coefficients are zeros, and all but one of the
sin
h (k) coefficients are zeros! (Two specialized analytic signal generation
cos
schemes using half-band filters are described in Sections 13.1 and 13.37.)
7. For hardware applications, both of the two real FIR filters in Figure 9–15
must be implemented. If your analytic signal generation is strictly a
high-level software language exercise, such as using MATLAB, the lan-
guage being used may allow h (k) to be implemented as a single com-
BP
plex filter.
Keep in mind, now, that the x (n) sequence in Figure 9–15 is not the
Q
Hilbert transform of the x(n) input. That wasn’t our goal here. Our intent was
r
to generate an analytic x (n) sequence whose x (n) quadrature component is
c Q
the Hilbert transform of the x(n) in-phase sequence.
I
9.6 COMPARING ANALYTIC SIGNAL GENERATION METHODS
Time-domain FIR Hilbert transformer design is essentially an exercise in low-
pass filter design. As such, an ideal discrete FIR Hilbert transformer, like an
ideal lowpass FIR filter, cannot be achieved in practice. Fortunately, we can
usually ensure that the bandpass of our Figure 9–12 Hilbert transformer cov-
ers the bandwidth of the x(n) input signal. The Figure 9–12 Hilbert trans-
r
former has magnitude roll-off in the x(n) channel but not in the x’(n)
i r
channel. Using more filter taps improves the transformer’s performance by
• minimizing passband ripple in the x(n) channel,
i
• minimizing passband gain differences between the two channels, and
• broadening the transformer’s passband width.

498 The Discrete Hilbert Transform
The choice of an odd or even number of taps depends on whether the gain
at f/2 should be zero or not, and whether an integer-sample delay is needed.
s
For roughly the same passband ripple performance, the complex filter in
Figure 9–15 requires a longer impulse response (more filter taps) than the Fig-
ure 9–12 Hilbert transformer. However, the complex filter has the nice feature
that it actually implements bandpass filtering where the Figure 9–12 Hilbert
transformer performs no filtering. Apossible drawback of the complex filter
method is that the real part of the generated complex signal is not equal to the
original real input signal. (That is, x(n) does not equal x(n).)
i r
For very high-performance analytic signal generation (where the equiv-
alent Hilbert transformer bandwidth must be very close to the full f/2 band-
s
width, and passband ripple must be very small), the DFT method should be
considered. With the DFT method the real part of x (n) is equal to the real
c
input x(n). Choosing which of the above analytic signal generation methods
r
to use (or perhaps a complex down-conversion scheme from Chapter 8) in
any given application requires modeling with your target computing hard-
ware, using your expected number format (fixed point versus floating point),
against the typical input signals you intend to process.
REFERENCES
[1] Proakis, J., and Manolakis, D. Digital Signal Processing: Principles, Algorithms and Applica-
tions, Prentice Hall, Upper Saddle River, New Jersey, 1996, pp. 618, 657.
[2] Rabiner, L., and Gold, B. The Theory and Application of Digital Signal Processing, Prentice
Hall, Englewood Cliffs, New Jersey, 1975, pp. 67, 168.
[3] Oppenheim, A., and Schafer, R. Discrete-Time Signal Processing, Prentice Hall, Englewood
Cliffs, New Jersey, 1st ed. 1989, 2nd ed. 1999.
[4] Laasko, T., et al. “Splitting the Unit Delay,” IEEE Signal Processing Magazine,January 1996.
[5] Gomes, J., and Petraglia, A. “An Analog Sampled-Data DSB to SSB Converter Using Recur-
sive Hilbert Transformer for Accurate I and Q Channel Matching,” IEEE Trans. on Circuits
and Sys.-II, Analog and Digital Signal Processing, Vol. 39. No. 3, March 2002, pp. 177–187.
[6] Ansari, R. “IIR Hilbert Transformers,” IEEE Trans. Acoust., Speech Signal Processing, Vol. 35,
August 1987, pp. 1116–1119.
[7] Marple, S., Jr. “Computing the Discrete-Time ‘Analytic’ Signal via FFT,” IEEE Trans. on Sig-
nal Proc., Vol. 47, No. 9, September 1999, pp. 2600–2603.
[8] Reilly, A., et al. “Analytic Signal Generation—Tips and Traps,” IEEE Trans. on Signal Proc.,
Vol. 42, No. 11, November 1994.
[9] Bell, D. Subject: “Hilbert Transform Using FFT Approach,” Internet Newsgroup: comp.dsp,
September 28, 2005.

Chapter 9 Problems 499
CHAPTER 9 PROBLEMS
9.1 Consider the real-valued continuous x(t) = cos(ωt) sinusoidal signal applied
r
to the network in Figure P9–1. The “HT” notation means Hilbert transform.
Write the algebraic expressions for the signals x (t) and x (t). Justify your
a b
solutions.
x(t) = cos( t)
r
+
x(t)
a
–
HT HT
+
x (t)
b
+
Figure P9–1
9.2 Consider the real-valued continuous x(t) sinusoidal signal defined by
r
x(t) = Asin(2πf t)
r o
where Ais a scalar constant, and f is a fixed frequency measured in Hz.
o
(a) Draw the three-dimensional spectrum of the Hilbert transform of x(t).
r
(b) What is the equation for the analytic signal whose real part is x(t), in
r
terms of sin() and cos() functions?
(c) What is the equation for the analytic signal whose real part is x(t), in
r
complex exponential notation? Show your work.
(d) Is the analytic signal whose real part is x(t) a positive- or negative-
r
frequency complex exponential? Justify your answer.
(e) On a complex plane, draw the location of the analytic signal whose real
part is x(t), at time t=0.
r
9.3 Consider the effect of repeated Hilbert transformations of a real-valued con-
tinuous x(t) = cos(ω t) sinusoidal signal, expressed as
o
u(t) = HT[x(t)]
v(t) = HT[u(t)]
w(t) = HT[v(t)]
y(t) = HT[w(t)]
where “HT[x(t)]” means the Hilbert transform of x(t).

500 The Discrete Hilbert Transform
x(t) u(t)
HT
Hilbert
transform
ofx(t)
Figure P9–3
(a) Which of u(t), v(t), w(t), or y(t) equals x(t)? Justify your answer.
(b) Which of u(t), v(t), w(t), or y(t) equals –x(t)? Justify your answer.
(c) Which of u(t), v(t), w(t), or y(t) equals the inverse Hilbert transform of
x(t)?
(d) Draw a block diagram of the system that implements the inverse Hilbert
transform using just one forward Hilbert transform operation. Use a
simple block, as shown in Figure P9–3, to represent a forward Hilbert
transform.
9.4 Looking at the odd- and even-length Hilbert transformers’ time-domain im-
pulse responses in the text’s Figures 9–11 and 9–13:
(a) Why should we expect that their frequency magnitude responses are zero
(a magnitude null) at zero Hz?
(b) Write the z-domain transfer function for a 6-tap tapped-delay line Hilbert
transform filter having coefficients h(0), h(1), h(2), ..., h(5). Evaluate that
function to determine the filter’s frequency response at zero Hz.
9.5 Draw the block diagram (the structure) of a real-coefficient 11-tap FIR Hilbert
transformer that will compute an x (n)=x’(n)+jx(n) analytic signal associated
a r i
with a real x(n) input signal.
r
9.6 Think about the tapped-delay line FIR Hilbert transformer whose coefficients
are shown in Figure P9–6. The coefficients are h(0), h(1), h(2), ..., h(10).
1
h(k)
0
–1
0 2 4 6 8 10
k
Figure P9–6

Chapter 9 Problems 501
(a) Write the z-domain transfer function polynomial, H(z), for this 11-coeffi-
cient Hilbert transformer in terms of the h(k) coefficients and z.
(b) We often describe a Hilbert transformer by the order of the transformer’s
z-domain transfer function polynomial. What is the order of the trans-
former whose coefficients are shown in Figure P9–6?
(c) How many unit-delay elements are needed in the tapped-delay line to
implement the Hilbert transformer whose coefficients are shown in Fig-
ure P9–6?
(d) How many multipliers are needed in the standard tapped-delay line im-
plementation of this Hilbert transformer?
9.7 The text’s Eq. (9–11) provided the expression for the impulse response of a
discrete Hilbert transformer. That expression is repeated here as
f [1−cos(πn)]
h(n)= s .
πn
The text also presented an alternate expression for the impulse response of a
discrete Hilbert transformer, Eq. (9–12), under the assumption that f is nor-
s
malized to unity. That alternate expression is repeated here as
2sin2(πn/2)
h′(n)= .
πn
Show that the expressions for h(n) and h’(n) are equal.
9.8 Assume that we’ve performed the discrete Fourier transform (DFT) of an
N-sample x(n) time-domain sequence to obtain a frequency-domain X(m)
sequence. X(m)’s integer frequency index is 0 ≤ m ≤ N–1. If N is even and
X(0) = X(N/2) = 0, describe what operations must be performed on X(m) to
create a new frequency-domain X (m) sequence whose inverse DFT is the
new
Hilbert transform of x(n). (Stated in different words: How do we perform the
Hilbert transform in the frequency domain?)
9.9 In the literature of DSP you may learn that we can design an N-tap Hilbert
transformer by first designing an N-tap half-band nonrecursive FIR filter
(where N+1 is an integer multiple of four), yielding coefficients h(n). Next, we
multiply the half-band filter’s h(n) coefficients by 2sin(πn/2) to obtain the de-
sired h (n) Hilbert transformer coefficients. That is,
hilb
h (n) = 2sin(πn/2)h(n).
hilb
Not emphasized in the literature, however, is the fact that the above h (n)
hilb
expression is only valid when the h(n) coefficients’ nindex is in the range

502 The Discrete Hilbert Transform
N−1 N−1
− ≤n≤ ,
2 2
where h(0) is the middle coefficient of h(n) as shown for an 11-tap half-band
FIR filter in Figure P9–9(a). In this textbook we typically use the more tradi-
tional FIR filter coefficient indexing notation of k=0, 1, 2, ..., N–1 as shown in
Figure P9–9(b). What is the correct equation for the h (k) coefficients when
hilb
the half-band filter’s coefficients are h(k) with index kdefined using our stan-
dard notation of k=0, 1, 2, ..., N–1 as shown in Figure P9–9(b)?
n indexing
0.5
h(n)
(a) 0
–0.5
–5 –3 –1 0 1 3 5
FPO n
k indexing
0.5
h(k)
(b) 0
–0.5
0 2 4 6 8 10
k
Figure P9–9
9.10 In Section 9.4 we presented a 7-tap (6th-order) FIR Hilbert transformer filter,
used to compute the complex sequence x (n)=x’(n)+x(n) based on the real
c r i
input sequence x(n), as shown in Figure P9–10.
r
x'(n)
r
x r (n) z–1 z–1 x r (n–2) z–1 z–1 x r (n–4) z–1 z–1 x r (n–6)
h(0) h(2) h(4) h(6)
+
x(n)
i
Figure P9–10

Chapter 9 Problems 503
Clever DSP engineers always seek to reduce the number of multipliers in
their systems. Redesign the Hilbert transformer in Figure P9–10 to a form that
reduces the number of necessary multiplications per output sample. Draw
the block diagram of your new design.
Hint: Write the difference equation for the x(n) output sequence, and recall
i
the symmetry relationships between the Hilbert transformer filter’s coeffi-
cients.
9.11 Similar to the structure in the text’s Figure 9–12:
(a) Draw the block diagram of a tapped-delay line FIR Hilbert transformer
filter, having 6 taps, that will generate both x’(n) and x(n).
r i
(b) Comment on the practicality of this even-tap implementation relative to
building a Hilbert transformer having an odd number of taps. (That is, is
an even-tap or an odd-tap Hilbert transformer easier to implement?)
Someday, if you’re designing a Hilbert transformer for a real-world
project, the answer to this question may be veryimportant!
9.12 In this chapter we discussed the notion of building a tapped-delay line FIR
filter whose coefficients are complex-valued. Explain why, or why not, such a
filter’s frequency magnitude response is periodic.
9.13 Because an FIR Hilbert transformer is a linear time-invariant (LTI) system,
there are two ways to determine its frequency response. The first way is to de-
rive an equation for the transformer’s frequency response as a function of a
continuous frequency variable, just as we did for IIR filters in Section 6.4.
Given that, we could plot that equation’s magnitude and phase, using soft-
ware, on our computer screen. The second method to determine a Hilbert
transformer’s frequency response is to perform the discrete Fourier transform
(DFT) of the Hilbert transformer’s unit impulse response, and plot the magni-
tude and phase of the DFT results.
(a) Derive the expression for the frequency response of a (K–1)th-order
(K taps) Hilbert transformer as a function of a continuous frequency
variable ωin the range of –πto +πradians/sample.
(b) If you were to model your equation from Part (a) using software, over
what range would you define your continuous frequency variable?
(c) The above Parts (a) and (b) were related to the algebraic method for find-
ing a Hilbert transformer’s frequency response in the form of an equation
that is a function of a continuous frequency variable. We can also use the
discrete Fourier transform (DFT) to estimate a Hilbert transformer’s fre-
quency response. Provide the equation for the K-point DFT of a (K–1)th-
order Hilbert transformer’s unit impulse response.

504 The Discrete Hilbert Transform
(d) For a 43rd-order Hilbert transformer, how many frequency-domain sam-
ples would result from your Part (c) DFT equation? How can you increase
the number of DFT frequency-domain samples to see finer detail (im-
proved granularity) of the Hilbert transformer’s frequency response?
9.14 Determine the closed-form equation, as a function of the number of coeffi-
cients K, of the number of multipliers needed by a tapped-delay line FIR
Hilbert transformer when K is an odd number. That is, write the expression
for N = some function of K. Do not consider the case of a folded delay line
mults
structure alluded to in the text.
Hint: Due to the unusual situation when K–1 is an integer multiple of four,
you’ll have to use the mathematical notation of ⎣X⎦which means the “integer
part of number X.”
9.15 Suppose we wish to implement the quadrature processing system in Figure
P9–15 where x(n) is a real-valued sequence whose discrete-time Fourier trans-
r
form is X(ω), and ω is our digital frequency ranging from –π to π
r
radians/sample. The sequences x(n) and x(n) are interpreted as the real and
r i
imaginary parts of a complex sequence x (n)=x(n)+jx(n). Under the restriction
c r i
that we want the discrete-time Fourier transform of x (n) to be defined by
c
⎧X (ω), for −π<ω≤0
X (ω)=⎨ r
c ⎩ 0, for 0<ω<π ,
fill in the following blank lines defining the H(ω) network’s frequency response:
⎧ ____, for −π<ω≤0
H(ω)=⎨
⎩ ____, for 0<ω<π.
That is, what is H(ω) so that X (ω) is equal to X(ω) for negative frequencies,
c r
and equal to zero for positive frequencies. Show your work.
Hint:Begin by writing an expression for X (ω) in terms of X(ω) and H(ω).
c r
x(n) x(n)
r r
x(n) = x(n) + jx(n)
x(n) c r i
H( ) i
Figure P9–15
9.16 Consider the network given in Figure P9–16(a) whose input is the complex
x(t)=x(t)+jx(t) signal, and whose output is the complex y(t)=y(t)+jy(t) sig-
r i r i

Chapter 9 Problems 505
nal. The block labeled “H” represents a Hilbert transform. The x(t) input has the
spectrum shown by the bold arrows in Figure P9–16(b). Using a format similar
to Figure P9–16(b), draw the three-dimensional spectrum of the y(t) output.
x(t) y(t)
r r
X(f)
Imag
x(t) y(t)
i i
Real
0.5
1
HT –5
0
10
–1
Hz
HT
(a) (b)
Figure P9–16
9.17 Consider the real-valued continuous x(t) cosine signal shown in Figure
r
P9–17(a). That cosine wave’s peak amplitude fluctuates as follows:
Peak amp.: Time interval:
1.0 0 ≤t < 100 illiseconds
3.0 100 ≤t < 150 milliseconds
0.5 150 ≤t < 200 milliseconds
1.0 200 ≤t ≤250 milliseconds
If we generate the analytic x (t) signal, using a Hilbert transformer as shown
a
in Figure P9–17(b), draw the time-domain magnitude of x (t).
a
4
x(t)
2 r
(a) 0
–2
–4
0 50 100 150 200 250
Time (milliseconds)
x(t) x(t)
r r
x(t) = x(t) + jx(t)
(b) x(t) a r i
HT i
Figure P9–17

This page intentionally left blank

CHAPTER TEN
Sample Rate
Conversion
The useful, and fascinating, process of sample rate conversion is a scheme for
changing the effective sampling rate of a discrete time-domain signal se-
quence. We’ll fully explain that puzzling notion in a moment. But first, know
that sample rate conversion has many applications; it’s primarily used to
minimize computations by reducing signal data rates when a signal of inter-
est’s bandwidth has been narrowed by lowpass filtering. Sample rate conver-
sion is mandatory in real-time processing when two separate hardware
processors operating at two different sample rates must exchange digital sig-
nal data. In satellite and medical image processing, sample rate conversion is
necessary for image enhancement, scale change, and image rotation. Sample
rate conversion is also used to reduce the computational complexity of certain
narrowband digital filters.
In this chapter we’ll explore sample rate conversion by first looking at
the process by way of a few examples. Then we introduce what are known as
polyphase filters. With some knowledge under our belts, next we’ll review
the standard mathematical notation used to describe sample rate conversion.
Finally, we’ll examine the behavior of specialized digital filters that have
found wide use in sample rate conversion applications.
We can define sample rate conversion as follows: Consider the process
where a continuous signal x(t) has been sampled at a rate of f = 1/T , and
s,old old
the discrete samples are x (n) = x(nT ). Sample rate conversion is necessary
old old
when we need x (n) = x(nT ), and direct sampling of the continuous x(t)
new new
at the rate of f = 1/T is not possible. For example, imagine we have an
s,new new
analog-to-digital (A/D) conversion system supplying a sample value every
T seconds. But our processor can only accept data at a rate of one sample
old
every T seconds. How do we obtain x (n) directly from x (n)? One
new new old
507

508 Sample Rate Conversion
possibility is to digital-to-analog (D/A) convert the x (n) sequence to regen-
old
erate the continuous x(t) and then A/D convert x(t) at a sampling rate of f
s,new
to obtain x (n). Due to the spectral distortions induced by D/Afollowed by
new
A/D conversion, this technique limits our effective dynamic range and is typ-
ically avoided in practice. Fortunately, accurate all-digital sample rate conver-
sion schemes have been developed, as we shall see.
Sampling rate changes come in two flavors: rate decreases and rate in-
creases. Decreasing the sampling rate is typically called decimation. When the
sampling rate is being increased, the process is known as interpolation, i.e.,
computing intermediate sample values. Because decimation is the simpler of
the two sample rate change operations, let’s examine it first.
10.1 DECIMATION
Decimation is the two-step process of lowpass filtering followed by an opera-
tion known as downsampling. Let’s first consider the notion of downsampling.
We can downsample a sequence of sampled signal values by a factor of Mby
retaining every Mth sample and discarding all the remaining samples. Rela-
tive to the original sample rate, f , the sample rate of the downsampled se-
s,old
quence is
f
f = s,old.
s,new M
(10–1)
For example, assume that an analog sinewave x(t) has been sampled to
produce the x (n) sequence shown in Figure 10–1(a). To downsample x (n)
old old
by a factor of M = 3, we retain x (0) and discard x (1) and x (2), retain
old old old
x (n)
old
1
(a) 0
n
−1
x (m)
new
1 x(t)
(b) 0
m
−1
Figure 10–1 Sample rate conversion: (a) original sequence; (b) downsampled by
M= 3 sequence.

10.1 Decimation 509
x (3) and discard x (4) and x (5), retain x (6), and so on as shown in Fig-
old old old old
ure 10–1(b). Mathematically, we describe the downsampled sequence as
x (m) = x (Mm) (10–1’)
new old
where M= 3, and m= 0, 1, 2, 3, etc.
Notice in Eq. (10–1’) that we’re using an alternate time index variable m,
rather than n, in x (m) to remind us that the time period between the
new
x (m) samples is different from the time period between the x (n) samples.
new old
That is, the absolute time instant corresponding to m = 3 is not equal to the
absolute time instant corresponding to n= 3.
The spectral implications of downsampling are what we should expect
as shown in Figure 10–2, where the spectrum of an original band-limited
sampled x (n) signal is indicated by the solid lines, and the spectral replica-
old
tions are indicated by the dashed lines. With x (m) = x (3n),x (m)’s spec-
new old new
trum, X (f), is shown in Figure 10–2(b). Two important features are
new
|X (f)|
old
B
P
(a)
−f 0 f Freq
s,old s,old
|X (f)|
new P
3
(b)
−3f −2f −f 0 f 2f 3f Freq
s,new s,new s,new s,new s,new s,new
=f
s,old
|X (f)|
old
B'
(c) B
−f
s,old
−f
s,new
0
f /2
f
s,new
f
s,old
Freq
s,new
|X (f)|
old B' B'
(d)
0 B' f s,new /2 B f stop f s,new Freq
=f −B'
Aliasing s,new
Figure 10–2 Decimation by a factor of three: (a) spectrum of original x (n) sig-
old
nal; (b) spectrum after downsampling by three; (c) bandwidth B’ is
to be retained; (d) lowpass filter’s frequency response relative to
bandwidthB’.

510 Sample Rate Conversion
illustrated in Figure 10–2. First, X (f) could have been obtained directly by
new
sampling the original continuous x(t) signal at a rate of f , as opposed to
s,new
downsampling x (n) by a factor of three. Second, there is a limit to the
old
amount of downsampling that can be performed relative to the bandwidth B
of the original signal. We must ensure that f > 2B to prevent overlapped
s,new
spectral replications (aliasing errors) after downsampling.
If a decimation application requires f to be less than 2B, then x (n)
s,new old
must be lowpass filtered before the downsampling process is performed, as
shown in Figure 10–2(c). (To clarify our terminology, we refer to decimation
as the two-step process of lowpass filtering followed by downsampling.) If
the original signal has a bandwidth B, and we’re interested in retaining only
the band Bv, the signal spectrum above B‘ must be lowpass filtered, with full
attenuation in the stopband beginning at f , before the downsample-by-M
stop
process is performed. Figure 10–2(d) shows this in more detail where the fre-
quency response of the lowpass filter, the bold lines, must attenuate the signal
spectral components whose frequencies are greater than B‘. Review the busy
Figure 10–2(d) carefully and notice how the lowpass filter’s f frequency
stop
can be as high as f = f –B‘ and no spectral aliasing will occur in the B‘
stop s,new
band of interest.
In practice, the nonrecursive tapped-delay line FIR filter structure in Fig-
ure 5–13 is the prevailing choice for decimationfiltersdue to its linear phase re-
sponse[1]. However, we need not apply x (n) samples, one at a time, to an
old
FIR lowpass filter and discard M–1 out of every M filter output samples. In-
stead, we could apply one x (n) sample to the filter and compute an output
old
sample, apply the next Mconsecutivex (n) samples to the filter’s delay line
old
and compute the next output, and continue applying M consecutive x (n)
old
samples for each new filter output sample. That way we do not compute filter
output samples that are discarded. In Section 10.7 we’ll learn how to mini-
mize the number of filter multiplications needed.
10.2 TWO-STAGE DECIMATION
When the desired decimation factor Mis large, say M> 20, there is an impor-
tant feature of the filter/decimation process to keep in mind. Significant low-
pass filter (LPF) computational savings may be had by implementing the
single-stage decimation, shown in Figure 10–3(a), in two stages as shown in
Figure 10–3(b). There we decimate sequence x (n) by integer factor M to
old 1
produce the intermediate x (n‘) sequence, which is then decimated by inte-
int
ger factor M . The downsampling, sample rate decrease operation “↓M ” in
2 1
Figure 10–3(b) means discard all but every M th sample. The product of M
1 1
andM is our desired decimation factor; that is, M=M M .
2 1 2

10.2 Two-Stage Decimation 511
x (n) x (m)
(a) old LPF M new
f f =f /M
s,old s,new s,old
x (n) x (n') x (m)
old LPF M int LPF M new
(b) 1 1 2 2
f f =f /M f =f /MM
s,old s,int s,old 1 s,new s,old 1 2
M =MM
1 2
Figure 10–3 Decimation: (a) single-stage; (b) two-stage.
As a brief aside, the systems in Figure 10–3 are called multirate systems
because there are two or more different data sample rates within a single
system.
10.2.1 Two-Stage Decimation Concepts
Considering Figure 10–3(b), an important question is “Given a desired total
downsampling factor M, what should be the values of M and M to mini-
1 2
mize the number of taps in lowpass filters LPF and LPF ?” If, for example, M
1 2
= 100, should M M be 5 · 20, 20 · 5, 25 · 4, or maybe 10 · 10? Thankfully,
1 2
thoughtful DSPpioneers answered this question for us[1]. For two-stage deci-
mation, the optimum value for M is
1
1− M F/(2 − F)
≈ .
M 2M
1,opt 2− F(M+1)
(10–2)
where Fis the ratio of Figure 10–3(a)’s single-stage lowpass filter’s transition
region width to that filter’s stopband frequency. That is,
f −B'
stop
F= .
f
stop (10–2’)
After using Eq. (10–2) to determine the optimum M factor, and setting M
1,opt 1
equal to the integer submultiple of M that is closest to M , the second
1,opt
downsampling factor is
M
M = .
2 M
1 (10–2’’)

512 Sample Rate Conversion
10.2.2 Two-Stage Decimation Example
By way of example, let’s assume we have an x (n) input signal arriving at a
old
sample rate of 400 kHz, and we must decimate that signal by a factor of M =
100 to obtain a final sample rate of 4 kHz. Also, let’s assume the baseband fre-
quency range of interest is from 0 to B‘ = 1.8 kHz, and we want 60 dB of filter
stopband attenuation. As such, a single-stage-decimation lowpass filter’s fre-
quency magnitude response is shown in Figure 10–4(a).
So, with f = 4 kHz, we must filter out all x (n)’s signal energy above
s,new old
f by having our filter transition region extend from 1.8 kHz to f = 4–1.8
stop stop
= 2.2 kHz. Now let’s estimate the number of taps, N, required of a single-
stage decimation-by-100 process. Using Chapter 5’s Eq. (5–49), and the nota-
tionf =B‘ = 1.8 kHz, we estimate the filter tap length to be
pass
Atten 60
Nª = ª 2727
− −
22(f f ) 22(2.2/400 1.8/400)
stop pass (10–3)
Single-stage lowpass filter response
(a) B'
0 1 1.8 2.2 Freq
(f ) (kHz)
stop
(b) x old (n) LPF 25 x int (n') LPF 4 x new (m)
1 2
f = 400 kHz f = 16 kHz f = 4 kHz
s,old s,int s,new
|X (f)|
old
(c)
0 100 200 300 400 Freq
(f s,old ) (kHz)
|X (f)|
int
(d)
0 1.8 4 8 12 14.2 16 Freq
(f ) (kHz)
|X (f)| s,int
new
(e) B'
0 1 1.8 2.2 3 4 Freq
(f s,new ) (kHz)
Figure 10–4 Two-stage decimation: (a) single-stage filter response; (b) decima-
tion by 100; (c) spectrum of original signal; (d) output spectrum of
the M= 25 downsampler; (e) output spectrum of the M= 4 down-
sampler.

10.2 Two-Stage Decimation 513
taps. That’s a painfully large number! (Resist all temptation to propose using
a 2727-tap FIR filter in any system design review meeting at your company,
or else you may be forced to update your résumé.)
Happily, to reduce the number of necessary filter taps we can partition
our decimation problem into two stages. With M = 100 and F =
(2200–1800)/2200 = 4/22, Eq. (10–2) yields an optimum M downsample
1,opt
factor of 26.4. The integer submultiple of 100 closest to 26.4 is 25, so we set M
1
= 25. Next, from Eq. (10–2’’), M = 4 as shown in Figure 10–4(b).
2
In this two-stage decimation example we’ll assume the original X (f)
old
input signal spectrum extends from zero Hz to something greater than 100
kHz as shown in Figure 10–4(c). If the first lowpass filter LPF has a passband
1
cutoff frequency of 1.8 kHz and its f is defined as f –B‘ = 16–1.8 = 14.2
stop s,int
kHz, the output of the M = 25 decimator will have the spectrum shown in
1
Figure 10–4(d). When filter LPF has a passband cutoff frequency of 1.8 kHz
2
and its f is set equal to 4–1.8 = 2.2 kHz, the output of the M = 4 decimator
stop 2
will have our desired spectrum shown in Figure 10–4(e). The point is, the
total number of taps in the two lowpass filters, N , is greatly reduced from
total
the 2727 taps needed by a single filter stage. From the expression in Eq. (10–3)
for the combined LPF and LPF filters, the total number of two-stage filter
1 2
taps is roughly
+
N = N N
total LPF1 LPF2
≈ 60 + 60 ≈
197.
− −
22(14.2/400 1.8/400) 22(2.2/16 1.8/16)
(10–3‘)
This is an impressive computational savings, and it shows the kind of
processing efficiency afforded by two-stage decimation[1,2]. Had we used M
1
= 50 and M = 2 (or M = 10 and M = 10) in our decimation-by-100 example,
2 1 2
the total number of two-stage filter taps would have been greater than 250.
ThusM = 25 and M = 4 is the better choice.
1 2
10.2.3 Two-Stage Decimation Considerations
The multistage decimation design curves in reference [1] tell us that, for com-
putational efficiency reasons, it’s always to our benefit to decimate in order
from the largest to the smallest factor. That is, we make sure that M is greater
1
thanM .
2
In two-stage decimation applications it is advantageous to consider set-
ting the M andM decimation factors equal to integer powers of two because
1 2
we can use computationally efficient half-band filters for the lowpass filters
in Figure 10–4(b). We discuss the use of multirate half-band filtering later in
Section 10.11.

514 Sample Rate Conversion
There are two practical issues to consider for two-stage decimation.
First, as we discussed regarding cascaded filters in Section 6.8.1, if the dual-
filter system in Figure 10–4(b) is required to have a passband peak-peak rip-
ple of R dB (R decibels), then both filters must be designed to have a
passband peak-peak ripple of no greater than R/2 dB. Second, the number of
multiplications needed to compute each x (m) output sample in Figure
new
10–4(b) is much larger than N because we must compute so many LPF
total 1
and LPF output samples destined to be discarded. Later we’ll introduce an
2
efficient decimation filter implementation scheme called polyphase decomposi-
tionthat only requires N multiplications per x (m) output sample.
total new
The advantages of two-stage decimation, over single-stage decima-
tion, are
• an overall reduction in computational workload,
• reduced signal and filter coefficient data storage,
• simpler filter designs, and
• a decrease in the ill effects of finite binary-word-length filter coefficients.
These advantages become more pronounced as the overall desired decima-
tion factor M becomes larger. To conclude our two-stage decimation discus-
sion, be aware that reference [3] discusses aspects of multistage decimation
where the number of stages is greater than two.
10.3 PROPERTIES OF DOWNSAMPLING
Let us now quickly review several interesting aspects of downsampling a dis-
crete sequence (retaining every Mth sample and discarding all the remaining
samples).
10.3.1 Time and Frequency Properties of Downsampling
First, we realize that downsampling is one of those rare processes that is not
time invariant. From the very nature of its operation, we know if we delay the
input sequence by one sample, a downsampler will generate an entirely dif-
ferent output sequence. For example, if we apply an input sequence x(n) =
x(0), x(1), x(2), x(3), x(4), etc., to a downsampler and M = 3, the output y(m)
will be the sequence x(0), x(3), x(6), etc. Should we delay the input sequence
by one sample, our delayed x (n) input would be x(1),x(2),x(3),x(4),x(5), etc.
d
In this case the downsampled output sequence y (m) would be x(1),x(4),x(7),
d
etc., which is not a delayed version of y(m). Thus a downsampler is not time
invariant. What this means is that if a downsampling operation is in cascade
with other operations, we are not permitted to swap the order of any of those

10.3 Properties of Downsampling 515
operations and the downsampling process without modifying those opera-
tions in some way. We first discussed this notion of time invariance in Section
1.7, and we’ll see an example of it in Section 10.13.
Second, downsampling does not cause time-domain signal amplitude
loss. Asinusoid with a peak-peak amplitude of 10 retains this peak-peak am-
plitude after downsampling. However, downsampling by M does induce a
magnitude loss by a factor of M in the frequency domain. That’s because, as
we learned in Chapter 3, DFT magnitudes are proportional to the number of
time-domain samples used in the transformation.
10.3.2 Drawing Downsampled Spectra
To illustrate the frequency properties of downsampling, let’s review an algo-
rithm (a recipe) that tells us how to draw the spectrum of a downsampled sig-
nal. Drawing the spectrum of a downsampled lowpass signal is easy; we saw
that in Figures 10–2(a) and 10–2(b). However, drawing the spectra of bandpass
and highpass signals that have been downsampled can be a bit tricky. Here’s
the process I use to draw the spectra of any type of downsampled signal.
We begin by looking at the spectral magnitude, |X(ω)| in Figure
10–5(a), of an x(n) time signal containing spectral energy at both low and high
|X(ω)|
P
. . . . . .
(a) ω
−2π −π 0 π 2π 3π old
(−f ) (f ) (Hz)
s,old s,old
. . . . . .
(b) ω
−2π −π 0 π 2π 3π old
(−f ) (f ) (Hz)
s,old 2π/3 s,old
(f /3) 4π/3
s,old
(2f /3)
s,old
|Y(ω)|
P/M= P/3
. . . . . .
(c) ω
−6π −3π 0 3π 6π 9π new
(−3f ) (3f ) (Hz)
s,old s,old
|Y(ω)|
P/3
. . . . . .
(d) ω
−3π −2π −π 0 π 2π 3π new
(−f ) (f ) (Hz)
s,new s,new
Figure 10–5 Spectra associated with downsampling by M= 3.

516 Sample Rate Conversion
frequencies. To help clarify our discussion by making the associated spectra
(we hope) easier to interpret, we use a complex-valued lowpass x(n) for this
example. Regarding Figure 10–5(a), notice the following:
• The baseband spectral envelope of |X(ω)| is centered at zero Hz cover-
ing the frequency range of –π ≤ ω ≤ πradians/sample (–f /2 to
old s,old
f /2 Hz), shown by the bold solid curve. Frequency f is the original
s,old s,old
sample rate of x(n), measured in Hz.
• For clarity, and reference, we label the frequency axis in both
radians/sample and Hz.
• The spectral replications in |X(ω)| are shown by the short-dashed
curves, spaced at integer multiples of 2πradians/sample (f Hz).
s,old
• |X(ω)| has a peak magnitude of P.
Assuming we want to downsample x(n) by a factor of M = 3 to create a
y(m) sequence, the following steps show how to determine the |Y(ω)| spec-
trum based on the known |X(ω)|:
1. Draw the |X(ω)| spectrum of sequence x(n) showing at least one spec-
tral replication in both the positive- and negative-frequency directions.
We did that in Figure 10–5(a).
2. Insert M–1 equally spaced copies of the primary spectral envelope be-
tween the primary spectral envelope and the spectral replications cen-
tered at ω = ±2π. The spectral spacing of the M–1 inserted copies
old
should be multiples of 2π/M radians/sample as shown by the long-
dashed curves in Figure 10–5(b).
3. Scale upward the frequency axis values of |Y(ω)| by a factor of M,
yielding the new ω frequency axis variable as shown in Figure
new
10–5(c).
4. Finally, scale downward the vertical axis of |Y(ω)| by a factor of 1/M. This
produces a peak magnitude for |Y(ω)| of P/Mas shown in Figure 10–5(c).
We zoom in on the |Y(ω)| spectrum in Figure 10–5(d) to show enhanced
detail.
10.4 INTERPOLATION
As we said before, downsampling is only part of the sample rate conversion
story—let’s now consider interpolation. Sample rate increase by interpolation
is a bit more involved than decimation because with interpolation new sam-
ple values need to be calculated. Conceptually, interpolation comprises the

10.4 Interpolation 517
generation of a continuous x(t) curve passing through our x (n) sampled
old
values, as shown in Figure 10–6(a), followed by sampling that curve at the
new sample rate f to obtain the interpolated sequence x (m) in Figure
s,new new
10–6(b). Of course, continuous curves cannot exist inside a digital machine,
so we’re forced to obtain x (m) directly from x (n). To increase a given f
new old s,old
sample rate by an integer factor of Lwe must insert L–1 zero-valued samples
between each sample in x (n), creating a longer-length sequence. To the end
old
of that longer sequence we append L–1 zero-valued samples. Those two steps
are what we call upsampling, indicated by the “↓L” operation in Figure
10–6(c). Next, we apply the upsampled sequence to a lowpass filter whose
output is the interpolated sequence in Figure 10–6(b).
We formally refer to interpolation as the two-step process of upsampling
followed by lowpass filtering. The process of interpolation is beautifully
straightforward and best understood by way of an example.
Let’s assume we have the sequence x (n), part of which is shown in
old
Figure 10–7(a), and we want to increase its sample rate by a factor of L = 4.
The x (n) sequence’s spectrum is provided in Figure 10–7(a) where the sig-
old
nal spectrum between zero Hz and 4f is shown. Please notice that the
s,old
dashed curves in X (f) are spectral replications. To upsample x (n) by a fac-
old old
tor of four, we insert three zeros between each sample of x (n) and append
old
the last three zeros, as shown in Figure 10–7(b), to create the new intermedi-
ate sequence x (m). Notice that the old sequence is embedded in the new
int
x (n)
old
1 x(t)
(a) 0
n
−1
x (m)
new
1/L
(b) 0
m
−1/L
x (n) x (m) x (m)
old L int LPF new
(c)
f f = Lf
s,old s,new s,old
Figure 10–6 Interpolation: (a) original time sequence; (b) interpolated by L= 3
sequence; (c) interpolation functional notation.

518 Sample Rate Conversion
|X (f)|
x (n) old
old P
(a)
T Time 0 f 2f 3f 4f Freq
old s,old s,old s,old s,old
Lowpass filter
|X (f)|
int Images
x (m)
int
(b)
T new T old Time 0 (f f s,old /L) 2f s,old 3f s,old f s,new Freq
s,new
x (m) |X new (f)| Residual
new images P
(c)
T T Time 0 f 2f 3f f Freq
new old s,old s,old s,old s,new
(f /L)
s,new
Figure 10–7 Interpolation by four: (a) original sampled sequence and its spec-
trum; (b) zeros inserted in original sequence and resulting spectrum;
(c) output sequence of interpolation filter and final spectrum.
sequence. The insertion of the zeros (a process often called zero stuffing) estab-
lishes the sample index for the intermediate sequence x (m) where the inter-
int
polated values will be assigned.
The spectrum of x (m), X (f), is shown in Figure 10–7(b) where f =
int int s,new
4f . The solid curves in X (f), centered at multiples of f , are called im-
s,old int s,old
ages. What we’ve done by adding the zeros is merely increase the effective
sample frequency to f =f in Figure 10–7(b). The final step in interpolation
s s,new
is to filter the x (m) sequence with the lowpass filter shown in Figure 10–6(c).
int
That filter’s frequency magnitude response is crudely shown as the dashed
lines centered at zero Hz, and f Hz, in Figure 10–7(b). The lowpass filter’s
s,new
job is to attenuate the spectral images shown in Figure 10–7(b). This lowpass
filter is called an interpolation filter, and its output sequence is the desired
x (m) sequence in Figure 10–7(c) having the spectrum X (f) containing
new new
residual spectral images. We’ll discuss those residual images in a moment.
10.5 PROPERTIES OF INTERPOLATION
Here we discuss several important aspects of the interpolation (upsampling
followed by lowpass filtering) process depicted in Figure 10–7.

10.5 Properties of Interpolation 519
10.5.1 Time and Frequency Properties of Interpolation
Because we cannot implement an ideal lowpass interpolation filter, x (m)
new
will not be an exact interpolation of x (n). The error manifests itself as the
old
residual spectral images in X (f) as indicated in Figure 10–7(c). With an
new
ideal filter, these images would not exist, but we can only approximate an
ideal lowpass interpolation filter. The issue to remember is that the accuracy
of our entire interpolation process depends on the stopband attenuation of
our lowpass filter. The greater the stopband attenuation, the more accurate
the interpolation. As with decimation, interpolation can be thought of as an
exercise in lowpass filter design.
Note that our interpolation process, because of the zero-valued samples,
has an inherent amplitude loss factor of L when a unity-gain lowpass filter is
used. That is, the peak sample value of x (m) is equal to the peak sample
new
value of x (n) divided by L. Thus, to achieve unity gain between sequences
old
x (n) and x (m), the lowpass interpolation filter must have a gain of L at
old new
zero Hz.
Although there is a time-domain gain (amplitude) loss of L by upsam-
pling and filtering, that loss is canceled in the discrete frequency domain by
the L-fold gain in the magnitudes of the discrete Fourier transform (DFT) of
anx (m) sequence that is Ltimes longer in duration than the original x (n)
new old
time sequence. (We’re repeating a fact we learned in Chapter 3—DFT magni-
tudes are proportional to the length of the time sequence being transformed.)
Rather than perform the upsampling in Figure 10–7(b), we might be in-
clined to merely repeat each x (n) sample three times to generate the new
old
upsampled x (m) sequence. Such a maneuver would indeed help attenuate
int
the unwanted spectral images, but sadly the resulting low-frequency X (m)
int
spectral magnitude shape will be the original desired X (m) spectrum multi-
old
plied by a sin(x)/xfunction. If this happens, then the follow-on lowpass filter
must compensate for that spectral magnitude roll-off distortion. Such non-flat
passband sin(x)/x-compensation filters require so many additional taps that
the “repeat each x (n) sample” scheme is unwise. In fact, later we’ll discuss
old
an efficient interpolation filtering scheme called polyphase filteringwherein we
don’t bother to create the upsampled x (m) sequence at all.
int
There’s one further issue regarding interpolation. You might tend to
think that interpolation was born of our modern-day signal processing appli-
cations such as cell phones and compact disc players. Please don’t. Ancient
astronomical cuneiform tablets, originating from Uruk and Babylon (200
years before the birth of Jesus), indicate that linear interpolation was used to
fill in the missing tabulated positions of celestial bodies for those times when
atmospheric conditions prevented direct observation[4]. Interpolation has
been used ever since, for filling inmissing data.

520 Sample Rate Conversion
10.5.2 Drawing Upsampled Spectra
To illustrate the frequency properties of upsampling (insertion of zero-valued
samples), and to demonstrate the method for drawing the spectra of upsam-
pled signals, consider the spectral magnitude, |X(ω)| in Figure 10–8(a), of a
lowpassx(n) time signal. Regarding Figure 10–8(a), notice the following:
• The baseband spectral envelope of |X(ω)| is centered at zero Hz cover-
ing the frequency range of roughly –π ≤ ω ≤ πradians/sample
old
(–f /2 to f /2 Hz), shown by the solid lines. Frequency f is the
s,old s,old s,old
original sample rate of x(n), measured in Hz.
• For clarity, and reference, we label the frequency axis in both
radians/sample and Hz.
• The spectral replications in |X(ω)| are shown by the dashed-line spec-
tral envelopes spaced at integer multiples of 2π radians/sample
(f Hz).
s,old
• |X(ω)| has a peak magnitude of P.
Assuming we want to upsample x(n) by a factor of L= 3, for example, to
create a y(m) sequence, the following steps show how to determine the
|Y(ω)| spectrum of y(m):
1. Draw the |X(ω)| spectrum of sequence x(n) showing at least L= 3 spec-
tral replications in both the positive- and negative-frequency directions.
We did that in Figure 10–8(a).
2. Scale downward the frequency axis values of X(ω) by a factor of L, yield-
ing the new ω frequency variable as shown in Figure 10–8(b).
new
Spectral replications
|X(ω)|
P
(a) −6π −5π −4π −3π −2π −π 0 π 2π 3π 4π 5π 6π 7π ω old
(−3f ) (−2f ) (−f ) (f ) (2f ) (3f ) (Hz)
s,old s,old s,old s,old s,old s,old
Spectral Spectral
|Y(ω)| P images replication
(b) −5π/3 −π −π/3 0 π/3 π 5π/3 ω new
−2π −4π/3 −2π/3 2π/3 4π/3 2π (Hz)
(−f ) (f )
s,new s,new
Figure 10–8 Spectra associated with upsampling by L= 3.

10.6 Combining Decimation and Interpolation 521
3. Finally, indicate the spectral images (destined to be attenuated by subse-
quent lowpass filtering) by using solid lines to represent their spectral
envelopes, as we did in Figure 10–8(b).
10.6 COMBINING DECIMATION AND INTERPOLATION
Although changing sampling rates, through decimation or interpolation, by
integer factors is quite common in practice, what can we do if we need a sam-
ple rate change that is not an integer? The good news is that we can imple-
ment sample rate conversion by any rational fraction L/Mwith interpolation
by an integer factor Lfollowed by decimation by an integer factor M. Because
the ratio L/M can be obtained as accurately as we want, with the correct
choice of integers L and M, we can change sample rates by almost any factor
in practice. For example, a sample rate increase by a factor of 7.125 can be per-
formed by an interpolation by L= 57 followed by a decimation by M= 8, be-
cause 7.125 = 57/8.
ThisL/Msample rate change is illustrated as the processes shown in Fig-
ure 10–9(a). The neat part here is that the computational burden of changing
the sample rate by the ratio of L/Mis less than the sum of an individual inter-
polation followed by an individual decimation. That’s because we can combine
the interpolation filter LPF and the decimation filter LPF into a single filter
L M
shown as LPF in Figure 10–9(b). The process in Figure 10–9(b) is normally
L/M
called a sample rate converterbecause if L>Mwe have interpolation, and when
M>Lwe have decimation. (The filter LPF is often called a multiratefilter.)
L/M
Filter LPF must sufficiently attenuate the interpolation spectral im-
L/M
ages so they don’t contaminate our desired signal beyond acceptable limits
after decimation. To accomplish this task, lowpass filter LPF must attenu-
L/M
ate all spectral components whose frequencies are above f /2 or (f /2)
s,old s,old
· (L/M), whichever is smaller, where f is x (n)’s sample rate in Hz. The
s,old old
Interpolation Decimation
x (n) x (m)
old new
(a) L LPF LPF M
L M
.
L f
f = s,old
s,new M
Sample Rate Converter
x (n) x (m)
(b) old L LPF M new
L/M
f f
s,old s,new
Figure 10–9 Sample rate conversion by a rational factor: (a) combination inter-
polation/decimation; (b) single lowpass filter method.

522 Sample Rate Conversion
stopband attenuation of LPF must be great enough that the attenuated up-
L/M
sampled images do not induce intolerable levels of noise when they’re aliased
by downsampling by M into the final band of 0 to f /2 Hz, where f is
s,new s,new
the filter’s data rate, in Hz.
Again, our interpolator/decimator designs are exercises in lowpass filter
design, and all the knowledge and tools we have to design lowpass filters can be
applied to this task. In software interpolator/decimator design, we want our
lowpass filter algorithm to prevent aliasing images and be fast in execution time.
For hardware interpolator/decimators, we strive to implement designs optimiz-
ing the conflicting goals of high performance (minimum spectral aliasing), sim-
ple architecture, high data throughput speed, and low power consumption.
The filtering computational workload in rational-factor sample rate con-
version, as we’ve presented it here, is sadly inefficient. Think about interpo-
lating a signal sequence by a factor of 4/3; we’d insert the zero-valued
samples into the original time sequence and apply it to a lowpass filter.
Three-fourths of the filter multiplication products would necessarily be zero.
Next, we’d discard two-thirds of our computed filter output values. Very in-
efficient! Fortunately, we are now prepared to introduce special sample rate
conversion filters, called digital polyphase filters, that avoid these computa-
tional inefficiencies.
10.7 POLYPHASE FILTERS
In this section we introduce the fascinating, and exceedingly useful, subject of
digital polyphase FIR filters. These filters have the ability to eliminate all mul-
tiply by zero operations in interpolation, as well as avoid the wasteful com-
putation of filter output samples that are subsequently discarded in
decimation applications.
Let’s assume that a linear-phase FIR interpolation filter design requires
an N = 12-tap filter; our initial plan is to pass the upsampled by L = 4 x (m)
int
sequence in Figure 10–10(a) through the 12-tap FIR filter coefficients shown
in Figure 10–10(b) to obtain the desired x (m) sequence. (This filter, whose
new
coefficients are the h(k) sequence, is often called the prototypeFIR filter. That’s
because later we’re going to modify it.) Notice that with time advancing to
the right in Figure 10–10(a), the filter coefficients are in reversed order as
shown in Figure 10–10(b). This filtering requires 12 multiplications for each
x (m) output sample, with 9 of the products always being zero. As it turns
new
out, we need not perform all 12 multiplications.
To show this by way of an example, Figure 10–11(a) shows the x (m)
int
samples just filling the filter’s delay line so that we can compute the
x (m=11) output sample. The 12 filter coefficients are indicated by the (cid:381)
new
symbols.

10.7 Polyphase Filters 523
x (m)
int
x (1) x (3)
old x (2) old
(a) x (0) old
old
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (m)
h(k)
(b) h(11) h(0)
9 2
11 8 7 6 5 4 3 0 k
Figure 10–10 Interpolation by four with a 12-tap lowpass FIR filter: (a) filter input
samples; (b) filter coefficients, (cid:381)s, used to compute x (m).
new
With the dots in Figure 10–11(a) representing the x (m) sequence, we
int
see that although there are nine (cid:381)s and three (cid:382)s, only the three (cid:382)s generate
nonzero products contributing to the convolution sum x (11). Those three
new
(cid:382)s represent FIR filter coefficients h(3), h(7), and h(11). The issue here is that
we need not perform the multiplications associated with the zero-valued
samples in x (m). We only need to perform three multiplications to obtain
int
x (11). To see the polyphase concept, remember that we use the prototype
new
filter coefficients indicated by the (cid:382)s to compute x (12). When we slide the
new
filter’s impulse response to the right one sample, we use the coefficients indi-
cated by the circles, in Figure 10–11(b), to calculate x (12) because the
new
nonzero values of x (m) will line up under the circled coefficients. Those cir-
int
cles represent filter coefficients h(0),h(4), and h(8).
x (0) x (1)
new new h(4)
h(7) x (1)
x (1) h(3) x (3) old x (3)
x (0) old old x (2) h(0) old
old x (2) h(8) old
old
Time Time
(a) (b)
x (2) x (3)
new new
x old (1) x old (2) x old (3) x old (1) x old (2) x old (3)
Time Time
(c) (d)
Figure 10–11 Filter coefficients used to calculate various x (m) samples.
new

524 Sample Rate Conversion
Likewise, when we slide the impulse response to the right one more
sample to compute x (13), we use the coefficients indicated by the dia-
new
monds in Figure 10–11(c). Finally, we slide the impulse response to the right
once more and use the coefficients indicated by the triangles in Figure
10–11(d) to compute x (14). Sliding the filter’s impulse response once more
new
to the right, we would return to using the coefficients indicated by the (cid:382)s to
calculate x (15). You can see the pattern here—there are L = 4 different sets
new
of coefficients used to compute x (m) from the x (n) samples. Each time a
new old
new x (m) sample value is to be computed, we rotate one step through the
new
four sets of coefficients and calculate as
x (11) = h(3)x (2) + h(7)x (1) + h(11)x (0) ←uses the (cid:382) coefficients
new old old old
x (12) = h(0)x (3) + h(4)x (2) + h(8)x (1) ←uses the o coefficients
new old old old
x (13) = h(1)x (3) + h(5)x (2) + h(9)x (1) ←uses the (cid:3)coefficients
new old old old
x (14) = h(2)x (3) + h(6)x (2) + h(10)x (1) ←uses the Δcoefficient
new old old old
x (15) = h(3)x (3) + h(7)x (2) + h(11)x (1) ←uses the (cid:382) coefficients
new old old old
x (16) = h(0)x (4) + h(4)x (3) + h(8)x (2) ←uses the o coefficients
new old old old
x (17) = h(1)x (4) + h(5)x (3) + h(9)x (2) ←uses the (cid:3)coefficients
new old old old
x (18) = h(2)x (4) + h(6)x (3) + h(10)x (2) ←uses the Δcoefficients
new old old old
and so on. The beautiful parts here are that we don’t actually have to create
thex (m) sequence at all, and we perform no multiply by zero computations.
int
Thatis polyphase filtering.
The above list of calculations not only shows us what filtering to do, it
shows us how to do it. We can implement our polyphase interpolation filter-
ing technique with a bank of four subfilters as shown in Figure 10–12. This
depiction is called the commutator model for polyphase interpolation filters.
We have a commutator switch rotating one complete cycle after the arrival of
each new x (n) sample. This way, four x (m) samples are computed for
old new
eachx (n) input sample.
old
In the typical case, if our polyphase filter is interpolating by a factor of L,
then we’ll have L subfilters. As such, for convenience the number of taps in
(the impulse response length of) the original prototype lowpass FIR filter, N,
is chosen to be an integer multiple of L. Again, the passband width of the pro-
totype lowpass FIR filter must not be greater than f /2 where f is
s,old s,old
x (n)’s sample rate in Hz.
old
Aminimum data storage structure for the polyphase interpolation filter
is shown in Figure 10–13, where three commutators rotate (in unison) coun-
terclockwise through four sets of filter coefficients upon the arrival of each
newx (n) sample. Again, four x (m) samples are computed for each x (n)
old new old
sample.
This commutated-coefficients scheme has the advantage of reducing the
number of storage registers for the x (n) input samples. If our polyphase fil-
old
ter is interpolating by a factor of L, then we have Lsets of coefficients.

10.7 Polyphase Filters 525
x (n)
old z−1 z−1
h(3) h(7) h(11)
+
z−1 z−1
h(0) h(4) h(8)
+
z−1 z−1
x (m)
new
h(1) h(5) h(9)
+
z−1 z−1
h(2) h(6) h(10)
+
Figure 10–12 Polyphase interpolation by L= 4 filter structure as a bank of FIR sub-
filters.
We can validate our polyphase FIR filter block diagrams with z-transform
equations. We start by describing our Figure 10–12 polyphase FIR filter with
H(z) = h(0) + h(4)z –1+h(8)z –2
in in
+ [h(1) + h(5)z –1+h(9)z –2]z –1
in in out
+ [h(2) + h(6)z –1+h(10)z –2]z –2
in in out
+ [h(3) + h(7)z –1+h(11)z –2]z –3, (10–4)
in in out
x (n)
old z−1 z−1
h(3) h(7) h(11)
h(0) h(4) h(8)
h(1) h(5) h(9)
h(2) h(6) h(10)
+ x (m)
new
Figure 10–13 Minimum-storage polyphase interpolation filter structure using com-
mutated coefficients.

526 Sample Rate Conversion
where z –1is a unit delay at the input sample rate, and z –1is a unit delay at
in out
the output sample rate implemented with the commutator. Because z –1 =
in
z –4, and z –2=z –8, we can write
out in out
H(z) = h(0) + h(4)z –4+h(8)z –8
out out
+ [h(1) + h(5)z –4+h(9)z –8]z –1
out out out
+ [h(2) + h(6)z –4+h(10)z –8]z –2
out out out
+ [h(3) + h(7)z –4+h(11)z –8]z –3
out out out
=h(0) + h(4)z –4+h(8)z –8
out out
+h(1)z –1+h(5)z –5+h(9)z –9
out out out
+h(2)z –2+h(6)z –6+h(10)z –10
out out out
+h(3)z –3+h(7)z –7+h(11)z –11
out out out
= ∑ 11 h(k)z -k ,
out
k=0
(10–4’)
which is the classic z-domain transfer function for a 12-tap FIR filter. Equation
(10–4) is called a polyphase decompositionof Eq. (10–4’).
Concerning our Figure 10–11 example, there are several issues to keep in
mind:
• For an interpolation factor of L, most people make sure the prototype
FIR has an integer multiple of Lnumber of stages for ease of implemen-
tation.
• As with the zeros-insertion and filtering method of interpolation, the
polyphase method has a gain loss equal to the interpolation factor L. To
compensate for this amplitude loss we can increase the filter’s coeffi-
cients by a factor of L, or perhaps multiply the x (m) output sequence
new
byL.
• Our Figure 10–11 example used a prototype filter with an even number
of taps, but an odd-tap prototype FIR interpolation filter can also be
used[5]. For example, you could have a 15-tap prototype FIR and inter-
polate by 5.
• Because the subfilter coefficient sets in Figure 10–13 are not necessarily
symmetrical, we can’t reduce the number of multiplications by means of
thefolded FIRstructure discussed in Section 13.7.
With the commutating switch structure of Figure 10–12 in mind, we can
build a decimation-by-four polyphase filter using a commutating switch as
shown in Figure 10–14. The switch rotates through its four positions (M = 4),
applying four x (n) input samples to the subfilters, then the four subfilters’
old
outputs are accumulated to provide a single x (m) output sample. In this fil-
new
ter the commutating switch rotates in the counterclockwise direction.

10.7 Polyphase Filters 527
z−1 z−1
h(3) h(7) h(11)
+
z−1 z−1
x (n)
old h(0) h(4) h(8)
atf rate
s
+
z−1 z−1
x (m) at
new
f /4 rate
h(1) h(5) h(9) + s
+
z−1 z−1
h(2) h(6) h(10)
+
Figure 10–14 Polyphase decimation by M= 4 filter structure as a bank of FIR sub-
filters.
Notice that the subfilters in Figure 10–14 are unchanged from the inter-
polation filter in Figure 10–12. Again, the benefit of polyphase decimation fil-
tering means no unnecessary computations are performed. We’re decimating
before filtering, so no filter computational results are discarded.
In the typical case, if our polyphase filter is decimating by a factor of M,
then we’ll have M subfilters. As such, for convenience the number of taps in
(the impulse response length of) the original prototype lowpass FIR filter, N,
is chosen to be an integer multiple of M. The passband width of the prototype
lowpass filter must not be greater than (f /2) · (L/M) where f isx (n)’s
s,old s,old old
sample rate in Hz.
Again, in practice, large changes in sampling rate are accomplished with
multiple stages (where Figure 10–14, for example, is a single decimation
stage) of cascaded smaller rate change operations of decimation and interpo-
lation as discussed in Sections 10.2 and 10.8. With that thought in mind, now
is the appropriate time to discuss two-stage interpolation.
This concludes our brief introduction to the important topic of discrete
polyphase filters. (For my money, the development of polyphase filters ar-
guably resides in the stratosphere of brilliant DSPinnovations, along with the

528 Sample Rate Conversion
radix-2 FFT algorithm and the Parks-McClellan FIR filter design algorithm.)
More detailed information on polyphase filters can be found in references
[6–8] and my favorite, reference [9].
10.8 TWO-STAGE INTERPOLATION
Because we’re now familiar with the notion of polyphase filtering, we’re
ready to consider the process of two-stage interpolation. When a desired in-
terpolation factor Lis large, say L> 20, significant interpolation filter compu-
tational savings may be had by implementing the interpolation in Figure
10–15(a) in two stages as shown in Figure 10–15(c). In the later figure we in-
terpolate input sequence x (n) by integer factor L followed by interpolation
old 1
by integer factor L , where L=L L .
2 1 2
10.8.1 Two-Stage Interpolation Concepts
Let’s assume we want to interpolate Figure 10–15(a)’s input x (n) sequence
old
byL, so we insert the L–1 zero-valued samples appropriately in x (n) to cre-
old
ate the x (m) sequence whose spectral magnitude is shown as the dashed
int
lines in Figure 10–15(b). The lowpass filter (LPF) in Figure 10–15(a) must
x (n) x (m) x (m)
old L int LPF new
(a)
f f =Lf
s,old s,new s,old
|H (f)| Images in |X (f)|
LPF int
(b)
0 B f f /L 2f /L Freq
stop s,new s,new
f /2 (f )
s,old s,old
x (n) x (m)
old new
L LPF L LPF
(c) 1 1 2 2
f Lf f =LL f =Lf
s,old 1s,old s,new 1 2 s,old s,old
x (n) x (m)
old new
PLPF PLPF
(d) 1 2
f L f f =LL f =Lf
s,old 1 s,old s,new 1 2 s,old s,old
Figure 10–15 Interpolation: (a) single-stage; (b) lowpass filter (LPF) magnitude re-
sponse and upsampled x (m) spectrum; (c) two-stage interpola-
int
tion; (d) two-stage polyphase interpolation.

10.8 Two-Stage Interpolation 529
have a frequency magnitude response, shown as the solid lines in Figure
10–15(b), that eliminates the spectral images in the x (m) sequence’s X (f)
int int
spectrum. As such, the filter LPF’s transition region extends from BHz to f
stop
= f –B = f /L–B Hz. (Frequency f is the beginning of the lowpass fil-
s,old s,new stop
ter’s stopband.) Given that frequency response requirement, we could now
begin to design the lowpass filter LPF.
However, using Figure 10–15(c)’s two-stage interpolation, we can ac-
complish our overall interpolation by Lwhere the combined number of com-
putations in filters LPF and LPF is much smaller than the computations
1 2
needed in the single Figure 10–15(a) LPF filter. This computational workload
reduction can be achieved by determining the optimum L and L factors for
1 2
our two-stage interpolation in Figure 10–15(c), just as we did in finding the
optimum downsampling factors in two-stage decimation.
Given the desired upsampling factor Lin Figure 10–15(a), we can deter-
mine the L and L upsampling factors that minimize the number of overall
1 2
two-stage filtering multiplications per input sample using
1− LF/(2−F)
L ≈2L⋅ ,
2,opt 2−F(L+1)
(10–5)
where Fis the ratio of the LPF filter’s transition region width over the filter’s
stopband frequency, as shown in Figure 10–15(b). That is,
f −B f −2B
F= stop = s,old .
f f
stop stop (10–5’)
Upon using Eq. (10–5) to compute L , and setting L equal to the integer
2,opt 2
submultiple of L that is closest to L , the first interpolation factor L is
2,opt 1
found using
L
L = .
1 L
2 (10–5’’)
So, once we know the optimum values for L and L , we proceed by de-
1 2
signing the LPF and LPF lowpass filters in Figure 10–15(c). Finally, we im-
1 2
plement the two-stage interpolation using two polyphase interpolation filters,
PLPF and PLPF , as shown in Figure 10–15(d). Let’s illustrate this two-stage
1 2
interpolation concept with an example.
10.8.2 Two-Stage Interpolation Example
Assume we must convert a compact disc (CD) audio signal, having a signal
bandwidth of 15 kHz and a sample rate of 44.1 kHz, to the sample rate of 96

530 Sample Rate Conversion
kHz used by a high-performance audio system. In addition, let’s assume that
our interpolation filtering requires a stopband attenuation of 60 dB. We can
accomplish this sample rate conversion by interpolating the CD signal by a
factor of L = 320, and then decimate the interpolated signal by M = 147. So
this two-stage interpolation example will show how to efficiently interpolate
an input signal sequence by L = 320, yielding an interpolated sequence hav-
ing a sample rate of f =L·f = 320 · 44.1 = 14112 kHz.
s,new s,old
The bold lines in Figure 10–16(a) show the frequency requirements of
the lowpass filter that we need for a single-stage L = 320 interpolation
process. It is that magnitude response that we will implement using two cas-
caded polyphase interpolation filter stages. The sample rate in Figure
10–16(a) is f = 14112 kHz.
s,new
Spectral image
|H (f)|
LPF
0
d B B
(a) −60
0 15 29.1 44.1 Freq
(B) (f ) (f ) (kHz)
stop s,old
22.05
(f /2)
s,old
|H (f)|
LPF1
0
d B B
(b)
−60
0
15 29.1 44.1 352.8 Freq
.
(B) (f ) (f ) (L f ) (kHz)
stop s,old 1 s,old
|H (f)|
LPF2
0
B B
d
(c)
−60
0
15 337.8 3.52.8
.
141
.
12 Freq
(B) (f ) (L f ) (L L f ) (kHz)
stop 1 s,old 1 .2 s,old
(L f )
s,old
72 taps 120 taps
x (n) x (m)
old Interpolation by8, Interpolation by 40, new
(d)
polyphase LPF polyphase LPF
1 2
44.1 kHz 352.8 kHz 14112 kHz
Figure 10–16 Two-stage interpolation: (a) single-stage filter frequency parame-
ters; (b) LPF filter parameters; (c) LPF filter parameters;
1 2
(d) polyphase implementation.

10.8 Two-Stage Interpolation 531
First we determine the optimum L and L interpolation factors. With
1 2
f = 29.1 kHz and B= 15 kHz, we use Eq. (10–5’) to compute ratio Fas
stop
− −
f 2B 44.1 30
F= s,old = =0.4845.
f 29.1
stop (10–6)
Next, we compute L using Eq. (10–5) as
2,opt
1− LF/(2−F)
L ≈2L
2,opt 2−F(L+1)
1− 320⋅0.485/(2−0.485)
=2⋅320 =377.98.
2−0.485(320+1)
(10–6’)
The integer submultiple of L = 320 that’s closest to L = 37.98 is 40. So we
2,opt
setL = 40, and using Eq. (10–5’’), we compute L = 320/40 = 8.
2 1
So the first polyphase lowpass filter, LPF , must have the frequency
1
magnitude response shown in Figure 10–16(b) when its operating sample rate
is L · f = 8 · 44.1 = 352.8 kHz. (That 352.8 kHz sample rate would have
1 s,old
been the LPF filter’s input rate had we inserted the L –1 zero-valued samples
1 1
between each of the original CD samples. Recall that with polyphase filtering
we don’t actually insert any zero-valued samples, but we must design a
polyphase filter assuming the upsampled 352.8 kHz sample rate.)
Using Eq. (10–3) to estimate the number of taps in LPF , N , with
1 LPF1
Atten= 60, we compute
Atten
N ≈
LPF1 22(f − f )
stop pass
60
= ≈68 taps.
22(29.1/352.8−15/352.8)
(10–7)
Because we must partition the LPF coefficients into a polyphase bank of L =
1 1
8 subfilters, N must be an integer multiple of 8. So we’ll set N = 72
LPF1 LPF1
taps, and the polyphase LPF filter will have 8 subfilters.
1
The second polyphase lowpass filter, LPF , must have the frequency
2
magnitude response shown in Figure 10–16(c) when its operating sample rate
isL ·L ·f = 14112 kHz. Using Eq. (10–3) to estimate the number of taps in
1 2 s,old
LPF ,N , with Atten= 60, we compute
2 LPF2
60
N ≈ ≈119taps.
LPF2 22(337.8/14112−15/14112)
(10–8)

532 Sample Rate Conversion
Because we must partition the LPF coefficients into a polyphase bank of L =
2 2
40 subfilters, N must be an integer multiple of 40. So we’ll set N = 120
LPF2 LPF2
taps, and the polyphase LPF filter will have 40 subfilters. We implement our
2
two-stage interpolation as shown in Figure 10–16(d), and that completes our
two-stage interpolation example.
The number of multiplies in our two-stage polyphase interpolation
process is N + L · N = 1032 multiplies per x (n) input sample. If we
LPF1 1 LPF2 old
had implemented our interpolation by L= 320 using a single polyphase filter
having 320 subfilters, we would have had to perform 2880 multiplies per
x (n) input sample. So, happily, our two-stage interpolation process reduced
old
the number of necessary filter multiplies by almost a factor of three relative to
a single-stage interpolation.
10.8.3 Two-Stage Interpolation Considerations
Due to the duality between decimation and interpolation, for computational
efficiency reasons as presented in reference [3], it’s beneficial to interpolate in
order from the smallest to the largest factor. That is, we make sure that L is
1
smaller than L .
2
Also, it is advantageous to consider setting the L and L interpolation
1 2
factors equal to integer powers of two because we can use computationally
efficient half-band filters for the lowpass filters in Figure 10–15(c). We discuss
the use of multirate half-band filtering later in Section 10.11.
As with dual-stage decimation, if the single-filter system in Figure
10–15(a) is required to have a passband peak-peak ripple of RdB (Rdecibels),
then each filter in Figure 10–15(c) must be designed to have passband peak-
peak ripple of no greater than R/2 dB. We have previously mentioned that in-
terpolation has an inherent amplitude loss. Thus, to achieve unity gain
between sequences x (n) and x (m) in Figure 10–15(c), the product of the
old new
DC (zero Hz) gains of the LPF and LPF filters must be equal to L.
1 2
The advantages of two-stage interpolation, over single-stage interpola-
tion, are identical to the advantages of two-stage decimation listed at the end
of Section 10.2. Be aware that references [1] and [3] discuss aspects of multi-
stage interpolation where the number of stages is greater than two.
In concluding this section, we mention that Chapter 13 contains three
DSPtricks regarding interpolation of time-domain signals. Now that we have
some familiarity with sample rate conversion, for completeness let’s review
the standard mathematical notation used to describe these operations using
polyphase filters. Learning that notation will aid readers as they encounter
other descriptions of sample rate conversion in the literature of DSP.

10.9 z-Transform Analysis of Multirate Systems 533
10.9 z-TRANSFORM ANALYSIS OF MULTIRATE SYSTEMS
In preparation for the multirate filter material in the following sections, here
we formalize both our terminology and notation of sample rate conversion
operations.
First, there is a fair amount of variety (some would call it “ambiguity”) in
the literature of DSPregarding the language of sample rate conversion. If you’ve
been reading the literature, you may have noticed that the terminology used has
been, unfortunately, very inconsistent—sometimes downright confusing. A
wide variety of terms are used in the literature as shown in Figure 10–17 where
“LPF” means lowpass filter. In the spirit of consistency, from here on we’ll use
the terminology indicated by the bold underlined font in Figure 10–17.
10.9.1 Signal Mathematical Notation
Compared to the written language of sample rate conversion, the mathematical
notation of sample rate conversion is quite consistent if we use z-transform repre-
sentations. For example, if a time-domain sequence x(n), having a z-transform of
∞
∑
X(z)= x(n)z
−n,
n=−∞
(10–9)
is upsampled by two (L = 2, a single zero-valued sample is inserted between
each x(n) sample), producing a w(m) sequence as shown in Figure 10–17(a),
then we can describe w(m) as
( )
⎧x(m/2), m=0,±2,±4...
w m =⎨
⎩0, whenmis±odd.
(10–10)
"Expander", "Compressor",
"interpolator", "Image-reject "Anti-aliasing "decimator",
"interpolation", filter", filter", "decimation",
"upsampler", "interpolation "decimation "downsampler",
"upsampling" filter" filter" "downsampling"
x(n) w(m) y(m) x(n) w(n) y(m)
L LPF LPF M
"Upsampler", "Downsampler",
"upsampling", "downsampling",
"interpolator", "decimator",
"interpolation" "decimation"
(a) (b)
Figure 10–17 Sample rate conversion terminology: (a) sample rate increase;
(b) sample rate reduction.

534 Sample Rate Conversion
Equation (10–10) indicates that every other w(m) sample is zero. Considering
only the nonzero values of w(m), the z-transform of w(m) is expressed as
∞ ∞
∑ ∑
W(z)= w(m)z
−m
= w(2k)z
−2k
m=−∞ k=−∞
∞ ∞
∑ ∑
= x(2k/2)z −2k = x(k)z −2k =X(z2)
k=−∞ k=−∞ (10–11)
where m represents even-valued integers and k represents all integers. If the
w(m) sequence is an upsampled-by-integer-L version of x(n) (inserting L–1
zero-valued samples between each x(n) sample), then w(m)’s z-transform is
expressed as
W(z) = X(zL). (10–12)
In a similar manner, some authors express the z-transform of sequence x(n) as
X(z) = W(z1/L). (10–13)
So here is the point: When we see expressions like Eqs. (10–11), (10–12), or
(10–13), they merely mean that sequence w(m) is an upsampled-by-L version
of sequence x(n), and sequence x(n) is a decimated-by-L version of sequence
w(m).
10.9.2 Filter Mathematical Notation
With the above multirate notation fresh in our minds, let’s consider how we
can use that notation to describe digital polyphase filters. If we have a
tapped-delay line FIR filter, having N taps, whose impulse response is h(k),
then we can represent the filter’s z-domain transfer function as
H(z) = N ∑ −1 h(k)z − k.
k=0
(10–14)
For an N= 9-tap FIR filter, for example, from Eq. (10–14) its z-domain transfer
function is
H(z) = h(0) + h(1)z–1+h(2)z–2+h(3)z–3+ ... + h(8)z–8. (10–15)
In both up- and downsample-by-integer-factor-Q filtering applications, for
computational efficiency reasons, we partition H(z) into Q separate subfilters

10.10 Polyphase Filter Implementations 535
using the polyphase decomposition process. For example, if Q = 3, we can
writeH(z) as
H(z) = h(0) + h(3)z–3+h(6)z–6+ [h(1) + h(4)z–3+h(7)z–6]z–1
+ [h(2) + h(5)z–3+h(8)z–6]z–2. (10–16)
(Read no further until you convince yourself that Eqs. (10–15) and (10–16) are
equivalent.) Due to the exponents of zin Eq. (10–16) we can write
H(z) = h(0)(z–0)3+h(3)(z–1)3+h(6)(z–2)3
+ [h(1)(z–0)3+h(4)(z–1)3+h(7)(z–2)3]z–1
+ [h(2)(z–0)3+h(5)(z–1)3+h(8)(z–2)3]z–2
=H (z3) + H (z3)z–1+H (z3)z–2, (10–17)
0 1 2
where
H (z3) = h(0) + 0z–1+ 0z–2+h(3)z–3+ 0z–4+ 0z–5+h(6)z–6
0
H (z3) = h(1) + 0z–1+ 0z–2+h(4)z–3+ 0z–4+ 0z–5+h(7)z–6
1
H (z3) = h(2) + 0z–1+ 0z–2+h(5)z–3+ 0z–4+ 0z–5+h(8)z–6. (10–17’)
2
The notation in the last line of Eq. (10–17) seems, at first, like a needless com-
plication in describing the 9-tap h(k) filter, but shortly we will see why such
notation is veryuseful.
10.10 POLYPHASE FILTER IMPLEMENTATIONS
Let’s now use the above z-domain transfer functions to help us understand
the most popular forms of polyphase filtering in multirate systems. Equation
(10–17), when followed by downsampling by M = 3, is depicted graphically
in Figure 10–18, showing the three subfilters. We interpret the notation of the
top subfilter, H (z3) in Figure 10–18, as a tapped-delay line wherein there are
0
x(n)
H(z3)
0
(f s sample y(m)
rate) z−1 H(z3) + 3
1
z−1 z−1 H(z3) (f s /3 sample
2 rate)
Figure 10–18 Polyphase decomposition of H(z) prior to downsampling by M= 3.

536 Sample Rate Conversion
H(z3)
0
x(n)
z−1 z−1 z−1 z−1 z−1 z−1
(f) h(0) h(3) h(6)
s
+
z−1 z−1 z−1 z−1 z−1 z−1 z−1
h(1) h(4) h(7)
+ y(m)
+ 3
z−1 z−1 z−1 z−1 z−1 z−1 z−1 z−1
(f/3)
s
h(2) h(5) h(8)
+
Figure 10–19 Details of the polyphase decomposition of H(z) for decimation M= 3.
M = 3 delay elements between each tap. To pause for a moment, what we’re
doing here is showing the algebraic and graphical notation used to describe
the polyphase decomposition of a 9-tap prototype FIR filter used in a
decimation-by-three application.
The detailed structure of the FIR filter in Figure 10–18 is shown in Figure
10–19, where we see the polyphase decomposition of h(k) into three subfilters,
creating a polyphase filter.
When the multirate concepts described above were first applied to the
impulse responses of digital filters, DSP pioneers quickly arrived at the im-
pressive-sounding “noble identities” graphically depicted in Figure 10–20.
Those complementary identities, showing the equivalency of swapping the
order of filters and up/downsamplers, are exceedingly useful in the analysis
and implementation of multirate systems as we shall see in the next section.
In Figure 10–20 the H(z) term is the z-transform of a filter’s h(n) impulse re-
sponse, and the H(zQ) term is the z-transform of h(n) upsampled by integer Q,
similar in form to Eqs. (10–17) and (10–17’).
Using the noble identities, we can move the downsampling by M= 3 op-
eration in front of the subfilters in Figure 10–18 as shown in Figure 10–21(a).
A detailed depiction of the polyphase filter is provided in Figure 10–21(b),
where we also rearranged the initial delay elements at the input of the filter.

10.10 Polyphase Filter Implementations 537
x(n) y(m) Equivalent to x(n) y(m)
Q H(zQ) H(z) Q
(a)
x(n) y(m) Equivalent to x(n) y(m)
H(zQ) Q Q H(z)
(b)
Figure 10–20 Noble identities of multirate systems: (a) sample rate increase;
(b) sample rate reduction.
In that figure we see that the delay lines between the filter coefficients now
contain only a single delay element and the subfilters can be described by
H (z) = h(0) + h(3)z–1+h(6)z–2
0
H (z) = h(1) + h(4)z–1+h(7)z–2
1
H (z) = h(2) + h(5)z–1+h(8)z–2. (10–18)
2
The upper subfilters in Figures 10–19 and 10–21(b) make obvious the mean-
ing of our notation regarding H (z3) and H (z), for example. That is, H (z3) is
0 0 0
merely an upsampled-by-three version of H (z).
0
One final simplification available for polyphase decimation filters is
shown in Figure 10–22, where the two initial delay elements and the down-
sampling by M = 3 operations in Figure 10–21(b) are replaced by a three-
position commutating (multiplexing) switch. One y(m) output sample is pro-
duced each time the switch completes a single full (three-step) rotation.
In an identical manner, interpolation by L = 3 (upsampling by three fol-
lowed by lowpass filtering) by way of polyphase decomposition is depicted
in Figure 10–23(a). The subfilters in that figure are identical to the subfilters
from Eq. (10–17) and Figure 10–19. Looking at Figure 10–23(b), we see that
the upsamplers insert two zero-valued samples between each output sample
of the three subfilters. The delay elements delay those upsampled sequences
by various delay times such that at each output time instant only one of the
inputs to the final summation is nonzero. So instead of performing a summa-
tion of mostly zero-valued samples, we can select only the path to the sum-
mer that contains a nonzero sample.
Thinking about this path selection process (multiplexing), happily we
can use the three-path commutating switch in Figure 10–23(c) for multipath
selection and eliminate the delay elements, the upsamplers, and the final
summation. As each new x(n) input sample is available, the switch completes
a single full (three-step) rotation, producing three y(m) output samples.
Again, the purpose of the material in this section is to show the algebraic
and graphical notation typically used to describe FIR polyphase filters used
in sample rate conversion applications.

x(n)
3 H(z)
0
(f sample
s y(m)
rate) z−1 3 H(z) +
1
(a)
z−1 z−1 3 H 2 (z) (f s /3 r a s t a e m ) ple
H(z)
0
x(n)
3 z−1 z−1
(f) h(0) h(3) h(6)
s
z−1
+
(b) 3 z−1 z−1
h(1) h(4) h(7)
z−1
+
y(m)
+
3 z−1 z−1
(f /3)
s
h(2) h(5) h(8)
+
Figure 10–21 Polyphase decomposition of h(k), for decimation by M= 3: (a) sim-
plified depiction; (b) detailed depiction.
z−1 z−1
h(0) h(3) h(6)
+
(f/3)
s
x(n)
z−1 z−1
(f) h(1) h(4) h(7)
s
(f /3) + y(m)
s +
z−1 z−1
(f/3)
s
h(2) h(5) h(8)
+
Figure 10–22 Simplified polyphase decomposition of h(k), for decimation by M= 3.
538

10.10 Polyphase Filter Implementations 539
H(z3)
0
x(n) y(m)
3 H(z3) z−1 +
(a) 1
(f) (3f)
s H(z3) z−1 z−1 s
2
H(z) 3
0
x(n) y(m)
H(z) 3 z−1 +
1
(b)
(f) (3f)
s H(z) 3 z−1 z−1 s
2
x(n)
z−1 z−1
(f) h(0) h(3) h(6)
s
+
z−1 z−1
(c) h(1) h(4) h(7)
+
y(m)
z−1 z−1
(3f)
s
h(2) h(5) h(8)
+
Figure 10–23 Polyphase decomposition of h(k), for interpolation by L= 3: (a) sim-
ple depiction; (b) reduced-length subfilters; (c) final structure.
The major benefits of using polyphase filters for sample rate conversion
are:
• Signal data storage requirements are minimized.
• No multiply by zero computations are performed (for interpolation).
• No computational results are discarded (for decimation).
• Akey benefit is that the computations are performed at the lower sam-
ple rate. For an N-tap FIR filter, polyphase decimation implementations

540 Sample Rate Conversion
reduce the number of multiplications per unit time to 1/M times the
number of multiplications per unit time with no polyphase decomposi-
tion. This advantage may be critical in high-data-rate applications and
leads to lower power consumption in battery-powered devices.
In the following sections we introduce several specialized digital filters
developed specifically to minimize the computational workload encountered
in sample rate conversion applications. As such, let’s have a look at rational-
factor sample rate change filters first.
10.11 SAMPLE RATE CONVERSION BY RATIONAL FACTORS
In the event that we wish to resample a signal by a rational factor L/M(as in
Figure 10-9(b)), we can interpolate by integer factor Lfollowed by downsam-
pling by integer factor M. Our single lowpass filter comprises the Lpolyphase
subfilters shown in Figure 10–24(a), where the input and output sample rates
are related by
L⋅ f
s,in
f = .
s,out M (10–19)
However, this naive approach would not be sensible because we’d be com-
puting some w(p) samples that are destined to be discarded by the downsample-
by-Mprocess.
Attempting to avoid that computational inefficiency, we can omit the
downsampling process altogether and merely control the position of the inter-
polator’s output commutating switch position as depicted in Figure 10–24(b).
For example, if we rotate the switch but skip alternate switch output ports, we
achieve resampling by a factor of L/2. If we advance the switch to every third
output position, for each y(m) output sample, we’ll have resampling by a factor
of L/3, and so on. This commutating switch control mechanism idea means
that we need only compute the output of a single subfilter for each y(m) output
sample. In resampling by a rational factor L/M, the switch output port (index
of a single subfilter) used to compute a y(m) output sample is found using
k= <mM> (10–20)
L
where <mM> means compute the product mMmodulo-L. So the switch out-
L
put port counter in Figure 10–24(b) is a binary modulo-L counter. As the re-
sampler operates, the index n, of the most recent input x(n) applied to the
subfilters, is given by
n=1mM/L2 (10–20’)

10.11 Sample Rate Conversion by Rational Factors 541
H(z)
0
x(n)
H 1 (z) 0
1
w(p) y(m)
(a) f s,in H(z) 2 M
2 −
... L 1 f
s,out
H−(z)
L 1
H 0 (z) Switch position
isk,where
x(n) 0 < k < L−1
H 1 (z) 0
1
(b) y(m)
H(z) 2
2 − k
... L 1
Commutator
switch output
H−(z) port counter
L 1
Input Switch
x(n) y(m)
H(z)
k
(c) f k f
s,in s,out
Modulo-L ROM
address counter
Figure 10–24 Resampling by rational-factor L/M: (a) fundamental process; (b) no
downsampling; (c) addressed-ROM scheme.
where 1mM/L2 means the integer part of mM/L. The actual resampler dif-
ference equation is
(N/L)−1
∑
y(m)= h(pL+k)⋅x(n−p)
p=0 (10–20’’)
where N is the number of taps in the prototype FIR filter from which the
polyphaseH (z) subfilters in Figure 10–24(b) were obtained.
k
For a numerical example, the left side of Table 10–1 shows the commu-
tating switch output port index k (index of a single subfilter), and the input
x(n) index n, as a function of a resampler’s mth y(m) output sample index for
resampling by a factor of L/M = 4/3. In this case, the switch counter counts

542 Sample Rate Conversion
ask= 0,3,2,1,0,3,2,1, and so on. The right side of Table 10–1 shows the switch
indexing for resampling by 3/4. In that scenario, the switch counter counts as
k= 0,1,2,0,1,2, and so on.
Table 10–1 Indexing for Resampling by 4/3 and 3/4
Interpolation by 4/3 Decimation by 3/4
L = 4, M = 3 L = 3, M = 4
Apply Apply
m k n new x(n) k n new x(n)
0 0 0 0 0
1 3 0 1 1
2 2 1 2 2
3 1 2 0 3,4
4 0 3 1 5
5 3 3 2 6
6 2 4 0 7,8
7 1 5 1 9
8 0 6 2 10
9 3 6 0 11,12
10 2 7 1 13
In our daily lives we hear the phrase “Timing is everything.” Well, that’s
certainly true in our resampling schemes. In Figure 10–24(b) we must remem-
ber that when the commutating switch resides at position k = 0, and when
during its cycling it crosses the k= 0 position, we must input a new x(n) sam-
plebeforewe compute a y(m) output sample. The times when a new x(n) input
sample is applied to the subfilters, before a y(m) sample is computed, are indi-
cated by the left-pointing arrows in Table 10–1.
Be aware that it’s possible that more than one x(n) input sample must be
applied to the resampler prior to an output y(m) computation for decimation
applications. For example, on the right side of Table 10–1, when m= 3, we are
forced to apply both the x(3) and x(4) input samples to the resampler before
computingy(3).
OK, let’s stop and catch our breath here. If we were to substitute the ex-
pressions for k and n, from Eqs. (10–20) and (10–20’), into Eq. (10–20’’), we
would produce a rather complicated algebraic expression for y(m). However,
we will not let such an equation for y(m) intimidate us because the h(pL + k)
term in Eq. (10–20’’) merely specifies the coefficients of the kth subfilter, and

10.12 Sample Rate Conversion with Half-Band Filters 543
the x(n – p) term simply defines the x(n) input samples residing in that kth
subfilter. As such, we see that Eq. (10–20’’) is no more than a convolution
equation where the summation index p accounts for each of the N/L coeffi-
cients in a subfilter. (N/Lis an integer.)
Notice that the tapped-delay lines of each subfilter in Figure 10–24(b)
contain the same x(n) time samples. To reduce input signal data storage re-
quirements, we can use a single tapped-delay line as we described for Figure
10–13. So in our rational-factor resampling implementation, shown in Figure
10–24(c), the modulo-Lcounter output index know becomes a pointer point-
ing to a bank of read-only memory (ROM) locations that contain the N/L co-
efficients of the kth subfilter. For each updated value of kin Table 10–1 we use
thekth set of subfilter coefficients to compute y(m). The control of applying a
newx(n) input sample, or samples, to the resampler before computing a y(m)
output sample is indicated by the dashed line to the Input Switch in Figure
10–24(c). As such, each time the modulo-L ROM address counter overflows,
we apply new x(n) input samples to the resampler.
To conclude this rational-factor resampling discussion, there are three
practical issues we must keep in mind. First, if we want the DC (zero Hz) gain
of our resampling process to be unity, then the original prototype lowpass FIR
filter must have a DC gain of Lto compensate for the amplitude loss by a fac-
tor of L caused by interpolation. (The downsampling by M causes no ampli-
tude change.) To achieve a DC gain of L, the sum of the prototype filter’s h(k)
coefficients must equal L.
Second, to avoid aliasing errors after downsampling, in designing the
original prototype lowpass FIR filter, the filter’s passband width must not be
greater than f /2 or (f /2) · (L/M), whichever is smaller, where f isx(n)’s
s,in s,in s,in
sample rate, and f is the filter’s data rate, in Hz. The stopband attenuation
s,out
of the prototype filter must be such that the attenuated upsampled images do
not induce intolerable levels of noise when they’re aliased by downsampling
byMinto the final band of 0 to f /2 Hz.
s,out
Third, from a computational efficiency standpoint, the rational-factor re-
sampling scheme described in this section has the power of George Fore-
man’s right hand.
10.12 SAMPLE RATE CONVERSION WITH HALF-BAND FILTERS
Recall that the half-band filters we introduced in Section 5.7 have a frequency
magnitude response with transition regions centered at ±f/4 as shown in Fig-
s
ure 10–25(a). Those filters are linear-phase lowpass tapped-delay line FIR fil-
ters in which every other filter coefficient is zero, except the center coefficient.
We discuss half-band filters here because their sparse nonzero coefficient sets
make them ideal for use in sample rate conversion applications where the re-
sampling factor is an integer power of two (2, 4, 8, etc.).

544 Sample Rate Conversion
10.12.1 Half-band Filtering Fundamentals
An example of sample rate change by an integer power of two is shown in
Figure 10–25(b) where the same h(k) half-band filter is used three times to
achieve decimation by eight. If the sample rate at the input of the three-stage
decimation is f , the sample rate at the output is f /8.
s,in s,in
We remind the reader that due to the nature of half-band filters there
will be some amount of spectral overlap, and thus some aliasing, after each
downsample-by-two operation. This is shown in Figure 10–25(c) for the first
decimation-by-two stage, where the spectral replications are shown as dotted
lines centered at integer multiples of the sample rate f /2. The amount of
s,in
spectral overlap is proportional to the transition region width of the filters
(inversely proportional to the number of h(k) half-band filter taps).
It’s normal to use the same half-band filter in multistage decimation by
two as was done in Figure 10–25(b). However, in multistage interpolation by
factors of two it would be computationally inefficient to use the same half-
band filter in each stage. Figure 10–26 helps explain why this is true. Con-
sider the x(n) signal in Figure 10–26(a) that we wish to interpolate by a factor
of eight. The x(n) signal’s spectrum is that shown in Figure 10–26(b) where
the spectral replications are shown as dotted lines centered at integer multi-
ples of the input sample rate f . The signal at node A, after x(n) has been up-
s,in
sampled by two via zeros insertion, has the |A(f)| spectrum shown in Figure
10–26(c) where the new sample rate is 2f .
s,in
|H(f)|
1.0
0.5
(a)
−f/2 −f/4 0 f /4 f /2 Freq
s s s s
Decimation by 8
x(n) w(n') y(m)
(b) h(k) 2 h(k) 2 h(k) 2
f f /2 f /4 f /8
s,in s,in s,in s,in
|W(f)|
(c)
−f −f /2 0 f /2 f Freq
s,in s,in s,in s,in
Figure 10–25 Half-band filters: (a) filter frequency magnitude response; (b) deci-
mation by eight; (c) spectral overlap after decimation by two.

10.12 Sample Rate Conversion with Half-Band Filters 545
Interpolation by 8
x(n) A B y(m)
2 h(k) 2 h(k) 2 h(k)
(a) 1 2 3
f 2f 4f 8f
s,in s,in s,in s,in
|X(f)|
(b)
−2f −f 0 f 2f Freq
s,in s,in s,in s,in
|A(f)|
|H(f)|
1
(c)
−2f −f 0 f 2f Freq
s,in s,in s,in s,in
|B(f)|
|H(f)|
2
(d)
−2f −f 0 f 2f Freq
s,in s,in s,in s,in
Figure 10–26 Multistage interpolation using half-band filters.
The job of the h (k) filter in Figure 10–26(a) is to eliminate the spectral
1
images in |A(f)| centered at ±f (half the sample rate at node A). We show
s,in
h (k)’s magnitude response as the dashed |H (f)| lines in Figure 10–26(c). The
1 1
output of the h (k) half-band filter, node B, has the |B(f)| spectrum shown in
1
Figure 10–26(d). After the signal at node B is upsampled by two, the h (k)
2
half-band filter must have the frequency magnitude response shown as
|H (f)| in Figure 10–26(d). Because the transition region width of |H (f)| is
2 2
so much wider than the transition region width of |H (f)|, the h (k) filter will
1 2
require fewer coefficients than did the h (k) filter. For similar reasons the h (k)
1 3
filter will require fewer coefficients than the h (k) filter.
2
What we’re saying is this: Unlike multistage decimation by powers of
two, in our relentless pursuit of computational efficiency, multistage interpo-
lation by powers of two should not use the same half-band filter in each
stage. In multistage interpolation each follow-on half-band filter requires
fewer taps than the preceding filter. Because we like to minimize the number
of necessary multiplications per second in real-time applications, we take
comfort in the fact that the half-band interpolation filter requiring the most
computations per output sample, h (k), operates at the lowest sample rate.
1

546 Sample Rate Conversion
From a practical standpoint, we remind the reader that if we use an FIR
filter design software package to design half-band filters, unavoidable nu-
merical computation errors will yield alternating filter coefficients that are in-
deed very small but not exactly zero-valued. So in our filter modeling efforts,
we must force those very small coefficient values to zero before we proceed to
analyze half-band filter frequency responses.
10.12.2 Half-band Filter Implementations
Here we discuss several important aspects of implementing half-band FIR fil-
ters for sample rate conversion and show why these filters are computation-
ally efficient. We illustrate half-band filter implementations in sample rate
x(n)
z–1 z–1 z–1 z–1 z–1 z–1 z–1 z–1 z–1 z–1
h(2) h(4) h(6) h(8) h(10)
h(0)
z–1
(a)
z–1 z–1 z–1 z–1 z–1 z–1 z–1 z–1
h(1) h(3) h(5) h(7) h(9)
y(m)
2
x(n)
2 z–1 z–1 z–1 z–1 z–1
h(2) h(4) h(6) h(8) h(10)
h(0)
z–1
(b)
2 z–1 z–1 z–1 z–1
h(1) h(3) h(5) h(7) h(9)
y(m)
Figure 10–27 An 11-tap polyphase half-band decimation filter: (a) polyphase
form; (b) polyphase with downsampling prior to filtering.

10.12 Sample Rate Conversion with Half-Band Filters 547
conversion applications with a decimation-by-two example showing the de-
tails of a polyphase decomposition process.
Suppose we need an N= 11-tap half-band FIR filter in a decimation-by-two
application. We could use a standard 11-tap tapped-delay line half-band filter, as
discussed in Chapter 5, followed by a downsample-by-two operation. Instead
we choose to use polyphase decomposition as shown in Figure 10–27(a).
Recall that a prototype FIR filter, which we want to decompose into Q
polyphase subfilters for a resample by Q application, must have an integer
multiple of Q taps. So we can think of our 11-tap FIR filter as being a 12-tap
filter with the h(11) twelfth coefficient being zero-valued.
Read no further until you convince yourself that the two subfilters in
Figure 10–27(a), whose outputs are summed, is equivalent to a standard 11-
tap tapped-delay line half-band filter, where both implementations have a
z-domain transfer function of
H(z) = h(0) + h(1)z–1+h(2)z–2+h(3)z–3+ ... + h(10)z–10. (10–21)
Next, we place the downsample-by-two operation in Figure 10–27(a)
ahead of the tapped-delay lines as shown in Figure 10–27(b). That modifica-
tion, because of our noble identities, reduces each dual delay element in Fig-
ure 10–27(a) to a single delay element as shown in Figure 10–27(b).
z–1 z–1 z–1 z–1 z–1
h(2) h(4) h(6) h(8) h(10)
h(0)
x(n)
(c)
(f )
s,in y(m)
h(5)
z–1 z–1 (f /2)
s,in
x(n)
z–1 z–1 z–1 z–1 z–1
(f ) h(2) h(4) h(6) h(8) h(10)
s,in
h(0)
(d)
y(m)
h(5)
z–1 z–1
(2f )
s,in
Figure 10–27(continued) Half-band filter implementations: (c) decimation by
two; (d) interpolation by two.

548 Sample Rate Conversion
Applying the input commutating switch implementation introduced in
Figure 10–24, our Figure 10–27(b) decimation-by-two polyphase half-band fil-
ter becomes what is shown in Figure 10–27(c). Because only one of the odd-
indexed filter coefficients is nonzero, namely h(5)≠0, we have only one multi-
ply operation in the bottom path of our final polyphase half-band filter. Again,
by using this polyphase implementation, we compute no filter output samples
destined to be discarded by the downsample-by-two operation, and happily
all filter computations take place at the decimated (lower) sample rate.
Figure 10–27(d) presents the structure of a polyphase version of a half-
band filter that eliminates any multiply by zero computations in an
interpolation-by-two application.
If the number of taps in a half-band filter is N, where N+1 is an integer
multiple of four, then the number of unit-delay elements in the filters’ bottom
paths in Figures 10–27(c) and 10–27(d) is (N–3)/4.
Because the half-band filter coefficients in the top path are symmetrical,
thankfully, we can use the folded FIR filter scheme described in Section 13.7 to
reduce the number of multipliers in the top path by a factor of two. This means
we can achieve the filtering performance of an N-tap half-band FIR filter while
performing only, roughly, N/4 multiplies per filter output sample. Neat!
If Figures 10–27(c) and 10–27(d)’s half-band filters’ coefficients are de-
signed such that h(5) = 0.5, which is often the case with commercial filter de-
sign software, the bottom path’s multiplication by h(5) can be replaced with a
binary right-shift-by-one-bit operation. On the other hand, to compensate for
the amplitude loss by a factor of two inherent in interpolation by two, the co-
efficients in Figure 10–27(d) are multiplied by two to make the filter’s gain
equal to two. In that case coefficient h(5) becomes one, eliminating the bottom
path multiplication altogether.
10.13 SAMPLE RATE CONVERSION WITH IFIR FILTERS
The interpolated FIR (IFIR) filters that we introduced in Chapter 7 are partic-
ularly useful in sample rate conversion applications because they’re compu-
tationally efficient, and their signal data storage requirements can be reduced
in such applications.
To see why this is so, we refer to Figure 10–28(a) showing a standard
IFIR filter with its cascaded shaping and image-reject subfilters followed by
downsampling by integer M (discard all but every Mth sample). The high-
order H (z) shaping filter is an upsampled (zero-stuffed) by M version of an
sh
H (z) prototype lowpass filter as discussed in Chapter 7. Because the H (z)
p sh
shaping subfilter and the H (z) image-reject subfilter are linear and time in-
ir
variant, we can swap their order as depicted in Figure 10–28(b). Now comes
the good part.

10.13 Sample Rate Conversion with IFIR Filters 549
x(n) H (z) y(m)
sh
(a) H(z) M
=H(zM) ir
p
x(n) H (z) y(m)
(b) H(z) sh M
ir =H(zM)
p
x(n) H (z1/M) y(m)
(c) H(z) M sh
ir =H(z)
p
Figure 10–28 IFIR filter structures used for decimation.
Due to the noble identities we can swap the order of the H (z) subfilter
sh
with the downsampler and arrive at the structure shown in Figure 10–28(c).
EveryM-unit delay in the H (z) filter’s tapped-delay line is now replaced by
sh
a single unit delay, which takes us back to using our original low-order H (z)
p
prototype filter. This fortunate scenario reduces the signal data storage re-
quirements of our traditional IFIR filter. In addition, the H (z) and M down-
ir
sampler combination can be implemented using polyphase filtering to further
reduce their computational complexity.
In a similar manner, IFIR filters can be used for interpolation as shown
in Figure 10–29(a). There we show an upsampling process followed by a stan-
dard IFIR filter structure. Again, we can swap the order of subfilter H (z)
sh
with the upsampler and arrive at the structure shown in Figure 10–29(b).
Every L-unit delay in H (z) is now replaced by a single unit delay, which,
sh
again, takes us back to using our original low-order prototype filter H (z)
p
with its reduced data storage requirements. The Lupsampler and H (z) com-
ir
bination can be implemented using polyphase filtering to reduce their com-
putational workload.
x(n) H (z) y(m)
sh
(a) L H(z)
=H(zL) ir
p
x(n) H (z1/L) y(m)
(b) sh L H(z)
=H(z) ir
p
Figure 10–29 IFIR filter structures used for interpolation.

550 Sample Rate Conversion
Before concluding this chapter on sample rate conversion, we introduce
one final topic, cascaded integrator-comb filters. These important filters have
become popular for sample rate conversion in the hardware design of mod-
ern digital communications systems.
10.14 CASCADED INTEGRATOR-COMB FILTERS
Cascaded integrator-comb (CIC) filters are computationally efficient imple-
mentations of narrowband lowpass filters and, as such, are used in hardware
implementations of decimation and interpolation.
CIC filters are well suited to improve the efficiency of anti-aliasing filter-
ing prior to decimation, as shown in Figure 10–30(a), and for anti-imaging fil-
tering for interpolating signals as in Figure 10–30(b). Both applications are
associated with very high-data-rate filtering such as hardware quadrature
modulation and demodulation in modern wireless systems, and delta-sigma
A/D and D/Aconverters.
Because their frequency magnitude response envelopes are sin(x)/x-like,
CIC filters are typically followed, or preceded, by higher-performance linear-
phase lowpass tapped-delay line FIR filters whose task is to compensate for
the CIC filter’s non-flat passband as shown in Figure 10–30. That cascaded-
filter architecture has valuable benefits. For example, with decimation, nar-
rowband lowpass filtering can be attained at a greatly reduced computational
complexity from that of a single lowpass FIR filter due to the initial CIC filter-
ing. In addition, the follow-on FIR filter operates at reduced clock rates, mini-
mizing power consumption in high-speed hardware applications. A crucial
bonus in using CIC filters, the property that makes them popular in hardware
devices, is that they require no multiplications. Their arithmetic is additions
and subtractions only.
While CIC filters were introduced to the signal processing community
over two decades ago, their application possibilities have grown in recent
years[10]. That’s because improvements in VLSI integrated circuit technology,
Decimation,f < f
s,out s,in
x(n) CIC Compensation & y(m)
(a) decimation
f s,in filter decimation FIR filter f s,out
Interpolation,f > f
s,out s,in
x(n) Preconditioning & CIC y(m)
(b) interpolation
f interpolation FIR filter f
s,in filter s,out
Figure 10–30 CIC filter applications: (a) decimation; (b) interpolation.

10.14 Cascaded Integrator-Comb Filters 551
increased use of polyphase filtering techniques, advances in delta-sigma con-
verter implementations, and the significant growth in wireless communica-
tions systems have spurred much interest in, and improvements upon,
traditional CIC filters. Here we’ll introduce the structure and behavior of tra-
ditional CIC filters, present their frequency-domain performance, and discuss
several important implementation issues.
10.14.1 Recursive Running Sum Filter
CIC filters originate from the notion of a recursive running sum filter, which is
itself an efficient version of the standard nonrecursive moving averager. Re-
viewing a D-point nonrecursive moving average process in Figure 10–31(a),
we see that D–1 summations (plus one multiply by 1/D) are necessary to
compute each y(n) output sample.
Nonrecursive moving averager Recursive moving averager
x(n) y(n) x(n) y(n)
−
z−1
1/D z−1 1/D
z−1
y(n−1)
z−1 z−1
RequiresD−1
. . .
. . . summations.
z−1
x(n−D+1)
z−1
x(n−D)
(a) (b)
Recursive running sum filter CIC filter
(recursive running sum)
x(n) y(n)
− Comb Integrator
z−1
z−1
x(n) y(n)
z−1 −
Previous
sum,y(n−1) z−D z−1
. . .
x(n−D)
z−1
x(n−D)
(c) (d)
Figure 10–31 D-point averaging filters: (a) nonrecursive moving averager; (b) re-
cursive moving averager; (c) recursive running sum filter; (d) CIC
version of a recursive running sum filter.

552 Sample Rate Conversion
TheD-point nonrecursive moving average filter’s y(n) time-domain out-
put is expressed as
1
y(n)= [x(n)+x(n−1)+x(n−2)+x(n−3)+...+x(n−D++1)].
D (10–22)
Thez-domain expression for this nonrecursive moving averager is
1
Y(z)= [X(z)+X(z)z −1+ X(z)z −2+...+X(z)z −D+1]
D (10–23)
while its z-domainH (z) transfer function is
ma
H (z)= Y(z) = 1 [1+z −1+z −2 +...+z −D+1]= 1 D∑ −1 z −nn
ma X(z) D D
n=0 (10–24)
where the subscript “ma” means “moving average.”
An equivalent, but more computationally efficient, form of a moving av-
erager is the recursive moving averager depicted in Figure 10–31(b). The re-
cursive moving averager has the sweet advantage that only two additions are
required per output sample, regardless of the delay length D!
Notice that the delay line of the recursive moving averager has D delay
elements, while the nonrecursive moving averager has D–1 delay elements.
The recursive moving averager’s difference equation is
1
y(n) = [x(n) −x(n −D)]+y(n −1),
, D (10–25)
having a z-domainH (z) transfer function of
rma
1 1
−
z
−D
H (z) = .
rma D 1− z − 1 (10–26)
where the subscript “rma” means “recursive moving average.” What is inter-
esting is that the nonrecursive moving averager and the recursive moving av-
erager have identical behavior and, as such, H (z) = H (z). The transfer
ma rma
functions of the two averagers are equal to each other! (Actually, we saw the
equivalence of nonrecursive FIR filters and special recursive structures once
before—it was in regard to frequency sampling filters in Section 7.1.)
If we ignore the 1/D gain factor, we have a structure known as a recur-
sive running sum filtershown in Figure 10–31(c). Next we’ll see how a CIC fil-
ter is itself a recursive running sum filter.

10.14 Cascaded Integrator-Comb Filters 553
10.14.2 CIC Filter Structures
If we condense the delay line representation in Figure 10–31(c), we obtain the
classic representation of a single-stage (1st-order) CIC filter, whose cascade
structure (block diagram) is shown in Figure 10–31(d). The feedforward por-
tion of the CIC filter is called the comb section, whose differential delay is D,
and the feedback section is called an integrator. The comb stage subtracts a de-
layed input sample from the current input sample, and the integrator is sim-
ply an accumulator (performing summations). The CIC filter’s difference
equation is
y(n) = x(n) – x(n–D) + y(n–1) (10–27)
and its z-domain transfer function is
1 −z −D
H (z) = .
cic 1− z − 1 (10–28)
Looking at Eq. (10–28), we see that the numerator is the transfer function of
the comb filter and the denominator is the transfer function of the integrator.
To see why the CIC filter is of interest, first we examine its time-domain
behavior, for D = 5, shown in Figure 10–32. If a unit impulse sequence, a
unity-valued sample followed by many zero-valued samples, was applied to
the comb stage, that stage’s output is as shown in Figure 10–32(a). Think,
now, what would be the output of the integrator if its input was the comb
stage’s impulse response? The initial positive impulse from the comb filter
starts the integrator’s all-ones output. Then, Dsamples later, the negative im-
pulse from the comb stage arrives at the integrator to make all further CIC fil-
ter output samples equal to zero.
The key issue is the combined unit impulse response of the CIC filter
being a rectangular sequence, identical to the unit impulse response of the
Comb impulse Integrator impulse CIC impulse
response response response
2 1.5 1.5
1 1 1
0 0.5 0.5
-1 0 0
-2 -0.5 -0.5
0 5 9 0 5 9 0 5 9
Time Time Time
Figure 10–32 Single-stage CIC filter time-domain responses when D= 5.

554 Sample Rate Conversion
Magnitude response Phase response z-plane
0 4
1
d B – – 2 1 0 0 R a di a n s –2
2
0 m a gi n ar y p
art
0 k k = = 4 0
I
–1
–30 –4
–0.5 –0.25 0 0.25 0.5 –0.5 –0.25 0 0.25 0.5 –1 0 1
Frequencyf (f/2) Frequencyf (f/2) Real part
s s
(a) (b) (c)
Figure 10–33 Characteristics of a single-stage CIC filter when D= 5: (a) magni-
tude response; (b) phase response; (c) pole/zero locations.
recursive running sum filter. (Moving averagers, recursive running sum fil-
ters, and CIC filters are close kin. They have the same z-domain pole/zero lo-
cations, their frequency magnitude responses have identical shapes, their
phase responses are identical, and their transfer functions differ only by a
constant scale factor.) The frequency magnitude (in dB) and linear-phase re-
sponse of a D= 5 CIC filter are shown in Figure 10–33(a).
We can obtain an expression for the CIC filter’s frequency response by
evaluating Eq. (10–28)’s H (z) transfer function on the z-plane’s unit circle,
cic
by setting
z=ejω =ej2πf,
yielding
1− e −j2πfD e −j2πfD/2(ej2πfD/2− e −j2πfD/2)
H (f) = = .
cic 1− e −j2πf e −j2πf/2(ej2πf/2− e −j2πf/2) (10–29)
In Eq. 10–29 the frequency variable fis in the range of –0.5 to 0.5, correspond-
ing to a continuous-time frequency range of –f/2 to f/2 Hz. Using Euler’s
s s
identity 2jsin(α) = ejα –e–jα , we can write
H (f) =
e −j2πfD / 22j sin(2πfD / 2)
= e
−jπf(D−1) sin(πfD)
.
cic e −j2πf / 2 2j sin(2πf / 2) sin(πf) (10–30)
The first positive-frequency magnitude null in Figure 10–33(a), when D = 5
for example, is located at a frequency of f/D = f/5 = 0.2f Hz (f = 0.2). Equa-
s s s
tion (10–30) is in the form of Eq. (3–46). This means, ignoring the linear-phase
factor, a 1st-order CIC filter’s frequency magnitude response is roughly equal
to a sin(x)/x function centered at zero Hz as we see in Figure 10–33(a). (This
is why CIC filters are sometimes called sincfilters.)
Let’s stop here for a moment and mention a subtle characteristic of the
phase of H (f). The phase angle, the –πf(D–1) in Eq. (10–30), is a linear func-
cic
tion of frequency. Plotting that phase, over the frequency range of –0.5 ≤ f ≤
0.5, would yield a straight line (with negative slope). However, the
sin(πfD)/sin(πf) amplitude portion of Eq. (10–30) changes sign (polarity) be-

10.14 Cascaded Integrator-Comb Filters 555
tween its amplitude nulls (zero amplitude). So those sign changes show up as
phase discontinuities of π radians (180 degrees) in phase plots. For example,
notice the phase discontinuity in Figure 10–33(b) at frequency f = 0.2. That
discontinuity is π radians, because the sin(πfD)/sin(πf) amplitude term
changed sign from positive to negative at f= 0.2.
The z-plane pole/zero characteristics of a D = 5 CIC filter are provided
in Figure 10–33(c), where the comb filter produces D zeros, equally spaced
around the unit circle, and the integrator produces a single pole canceling the
zero at z= 1. Each of the comb’s zeros, being a Dth root of 1, are located at z(k)
=ej2πk/D,
where k= 0, 1, 2, ..., D–1.
The normally risky situation of having a filter pole directly on the unit
circle need not trouble us here because there is no coefficient quantization
error in our H (z) transfer function. CIC filter coefficients are ones and can be
cic
implemented with perfect precision using binary numbers. Although recur-
sive, CIC filters are guaranteed stable, linear phase as shown in Figure
10–33(b) and have finite-length impulse responses.
If we examine just the magnitude of H (f) from Eq. (10–30), we can de-
cic
termine the DC (zero Hz) gain of our single-stage Figure 10–31(d) CIC filter.
However, setting f= 0 in Eq. (10–30), we have
sin(0) 0
CIC filter gain=|H (f)| = =
cic f=0 sin(0) 0
(10–31)
which is indeterminate. But don’t worry, we can apply the Marquis de L’Hopi-
tal’s rule to the magnitude-only portion of Eq. (10–30), then set f= 0, to yield
d[sin(πfD)] / d[f]
CIC filter gain =
d[sin( πf)] / d[f]
cos(πfD)(πD) cos(0)(πD)
= cos (πf)(π) | f=0 = co s(0)(π) =D.
(10–32)
So, the DC gain of a 1st-order CIC filter is equal to the comb filter delay D.
This fact will be very important to us when we actually implement a CIC fil-
ter in hardware.
CIC filters are primarily used for anti-aliasing filtering prior to decima-
tion and for anti-imaging filtering for interpolated signals. With those notions
in mind, we swap the order of Figure 10–31(c)’s comb and integrator—we’re
permitted to do so because those are linear time-invariant operations—and
include downsampling by a sample rate conversion factor R in Figure
10–34(a). (Readers should prove to themselves that the unit impulse response
of the integrator/comb combination, prior to the sample rate conversion, in
Figure 10–34(a) is equal to that in Figure 10–32(c).) In most CIC filter

556 Sample Rate Conversion
Decimation Interpolation
x(n) y(m) x(n) y(m)
R R
– –
z–1 z–D z–D z–1
Integrator Comb Comb Integrator
(a) (b)
Figure 10–34 Single-stage CIC filters, used in: (a) decimation; (b) interpolation.
applications the sample rate change factor Ris equal to the comb’s differential
delayD, but we’ll keep them as separate design parameters for now.
The downsampling operation in Figure 10–34(a) results in an output
sample rate of f =f /R. To investigate a CIC filter’s frequency-domain be-
s,out s,in
havior in more detail, Figure 10–35(a) shows the frequency magnitude re-
sponse of a D = 8 CIC filter prior to downsampling. The spectral band, of
widthB, centered at zero Hz, is the desired passband of the filter. Akey aspect
of CIC filters is the spectral aliasing that takes place due to downsampling.
Those B-width shaded spectral bands centered at multiples of f /R in
s,in
Figure 10–35(a) will alias directly into our desired passband after downsam-
pling by R= 8 as shown in Figure 10–35(b). Notice how the largest aliasedspec-
0
B B
±10
B B B
d
±16 dB
±20
(a)
±30
±f /8 0 f /8 f /4 3f /8 f /2
s,in s,in s,in s,in s,in
Frequency (before decimation)
0
±10
B
B ±16 dB
d
±20
(b)
±30
±f /4 0 f /4 f /2
s,out s,out s,out
Frequency (after decimation) (f /16)
s,in
Figure 10–35 Frequency magnitude response of a 1st-order, D= 8, decimating
CIC filter: (a) response before decimation; (b) response and alias-
ing after R= 8 downsampling.

10.14 Cascaded Integrator-Comb Filters 557
0
±10
(a) B B
d
±20
f /8
s,out
±30
±f 0 f 2f 3f 4f
s,in s,in s,in s,in s,in
Frequency (before interpolation)
0
B Images
±10
(b) B B
d
±20
±30
±f /8 0 f /8 f /4 3f /8 f /2
s,out s,out s,out s,out s,out
Frequency (after interpolation)
Figure 10–36 Spectra of a 1st-order, D= R= 8, interpolating CIC filter: (a) input
spectrum before interpolation; (b) output spectral images.
tral component, in this example, is approximately 16 dB below the peak of the
band of interest. Of course, the aliased power levels depend on the bandwidth
B—the smaller Bis, the lower the aliased energy after downsampling.
Figure 10–34(b) shows a CIC filter used for interpolation where upsam-
pling by R yields a y(m) output sample rate of f = Rf . (In this CIC filter
s,out s,in
discussion, interpolation is defined as zeros-insertion upsampling followed
by filtering.) Figure 10–36(a) shows an arbitrary baseband spectrum, with its
spectral replications, of a signal applied to the D=R= 8 interpolating CIC fil-
ter of Figure 10–34(b). The filter’s output spectrum in Figure 10–36(b) shows
how imperfect filtering gives rise to the undesired spectral images.
After interpolation, unwanted images of the B-width baseband spec-
trum reside at the null centers, located at integer multiples of f /D. If we
s,out
follow the CIC filter with a traditional lowpass tapped-delay line FIR filter,
whose stopband includes the first image band, fairly high image rejection can
be achieved.
10.14.3 Improving CIC Attenuation
The most common method to improve CIC filter anti-aliasing and image at-
tenuation is by increasing the order Q of the CIC filter using multiple stages.
Figure 10–37 shows the structure and frequency magnitude response of a 3rd-
order (Q= 3) CIC decimation filter.

558 Sample Rate Conversion
x(n) y(m)
R
– – –
(a)
z–1 z–1 z–1 z–D z–D z–D
0
–48 dB
–30
(b) B
B
d
–60
–90
–f /8 0 f /8 f /4 3f /8 f /2
s,in s,in s,in s,in s,in
Frequency(before decimation)
Figure 10–37 A 3rd-order (Q= 3), D= R= 8 CIC decimation filter: (a) structure;
(b) frequency magnitude response before decimation.
Notice the increased attenuation at multiples of f /D in Figure
s,in
10–37(b) compared to the 1st-order CIC filter in Figure 10–35(a). Because the
Q = 3 CIC stages are in cascade, the overall before-decimation transfer func-
tion will be the product of their individual single-stage transfer functions, or
⎡ 1−z −D⎤Q
H (z)= ⎢ ⎥ .
cic,Qth-order ⎣⎢1−z −1⎦⎥
(10–33)
The overall frequency magnitude response of the Q = 3 cascaded stages, be-
fore decimation, will be
sin(πfD) Q
|H (f)|= .
cic,Qth-order sin(πf)
(10–34)
where, again, the frequency variable fis in the range of –0.5 to 0.5 corresponding
to a continuous-time frequency range of –f/2 to f/2 Hz. The price we pay for
s s
improved anti-alias attenuation is additional hardware adders and increased
CIC filter passband droop. An additional penalty of increased orders comes
from the DC (zero Hz) gain of the decimation filter, which is DQ. That potentially
large gain causes significant binary data word-width growth for higher-order
filters. Even so, this multistage decimation implementation is common in com-
mercial integrated circuits, where a Qth-order CIC filter is called a sincQfilter.
10.14.4 CIC Filter Implementation Issues
With CIC filters, the comb section can precede, or follow, the integrator sec-
tion. However, it’s sensible to put the comb section on the side of the filter op-

10.14 Cascaded Integrator-Comb Filters 559
erating at the lower sample rate to reduce the length of the delay line. Using
the noble identities discussed earlier in this chapter, swapping the Figure
10–34 comb filters with the rate conversion operations results in the most
common implementation of CIC filters as shown in Figure 10–38. Notice that
the decimation filter’s comb section now has a delay length (differential
delay) of N=D/R. That’s because an N-sample delay after downsampling by
Ris equivalent to a D-sample delay before downsampling by R. Likewise for
the interpolation filter; an N-sample delay before upsampling by Ris equiva-
lent to a D-sample delay after upsampling by R.
Those Figure 10–38 configurations yield two major benefits: First, the
comb section’s new differential delay is decreased to N=D/R, reducing data
storage requirements; second, the comb section now operates at a reduced
clock rate. Both of these effects reduce hardware power consumption.
The comb section’s differential delay design parameter N=D/Ris typi-
cally 1 or 2 for high-sample-rate conversion ratios as is often done in commer-
cial up/down-converter chips. Value Neffectively sets the number of nulls in
the frequency response of a decimation filter, as shown in Figure 10–39(a).
An important characteristic of a CIC decimator is that the shape of the
filter response, relative to its f output sample rate, changes very little as a
s,out
function of the downsampling factor R, as shown in Figure 10–39(b). For R
larger than roughly 16, the change in the filter shape is negligible. Fortu-
nately, this allows the same compensation FIR filter to be used for variable-
decimation ratio systems.
The gain of a Qth-order CIC decimation filter is DQ, and individual inte-
grators within the filter can experience overflow. (An integrator’s gain is infi-
nite at DC!) As such, the use of two’s complement (non-saturating) arithmetic
resolves this overflow situation just so long as the integrator word width ac-
commodates the maximum value expected at the CIC filter output. Happily,
using the two’s complement binary number format, with its modular wrap-
around property, the follow-on comb filter will properly compute the correct
difference between two successive integrator output samples.
To show this behavior, assume we’re using a four-bit two’s complement
number format, and a CIC decimation filter’s integrator must sum the values
Decimation Interpolation
x(n) y(m) x(n) y(m)
R R
– –
z–1 z–N D z–N z–1
N =
R
(a) (b)
Figure 10–38 Single-stage CIC filter implementations: (a) for decimation; (b) for
interpolation.

560 Sample Rate Conversion
0 N = 1 0 N = 2
N = 2 B
–10 d
N = 3
B –10
d
–20 R = 2
R = 16
–30 –20
–f /2 –f /4 0 f /4 f /2 –f /2 –f /4 0 f /4 f /2
s,out s,out s,out s,out s,out s,out s,out s,out
Frequency Frequency
(a) (b)
Figure 10–39 CIC decimation filter frequency responses: (a) for various values of
differential delay N,whenR= 8; (b) for two Rdownsampling factors
whenN= 2.
Two’s
Decimal
complement
+7 0111 x (0) = 6 = 0110
int dec binary
+6 0110
+5 0101 x (D) = 11 = 1011 –5
int dec binary dec
+4 0100
+3 0011
+2 0010 after overflow (numerical
+1 0001 wraparound)
(a) 0 0000
–1 1111
–2 1110
–3 1101 1011 binary –5 dec
–4 1100 +1010 –6
binary dec
–5 1011
x (D) –x (0) = 0101 +5
–6 1010 int int binary dec
–7 1001
–8 1000 correct
(11 – 6 = 5 )
dec dec dec
Decimation
x(n) w(n) v(n) y(m)
R
–
(b)
z–1 z–5
Adder/
accumulator w(n–5)
Integrator Comb
Figure 10–40 Two’s complement overflow (numerical wraparound): (a) differ-
ence example; (b) D= 5 decimation example.

10.14 Cascaded Integrator-Comb Filters 561
7 + 4 and the comb filter must subtract 6 from that sum. Figure 10–40(a)
shows how a previous integrator output x (0) sample of decimal 6 can be
int
subtracted by the comb filter from a later x (D) integrator output sample of
int
decimal 11 (11 = 7 + 4, a temporary overflow condition), resulting in a correct
difference of decimal plus 5 (+5 ).
dec
This two’s complement wraparound issue is so important that it deserves
a second example. Think of the D = 5 decimation filter in Figure 10–40(b). If
we applied a unit step input (an all-ones x(n) sequence) at time n = 1, we ex-
pect the v(n) sequence to ramp up to a decimal value of 5 and remain at that
value. Now if the integrator’s adder/accumulator register was only three bits
wide, it will not accommodate the v(n) output of 5 because the most positive
value of a three-bit word in two’s complement format is +3. That scenario is
shown on the left side of Table 10–2, where all the values are shown in decimal
format. There we see that the v(n) sequence goes to an incorrect value of –3.
If we increase the integrator’s accumulator width to four bits, the inte-
grator accumulator experiences overflow but the comb filter compensates for
that situation and provides the correct v(n) sequence as shown on the right
side of Table 10–2.
Table 10–2 Accumulator Example for D= 5 Decimation
Three-bit integrator Four-bit integrator
accumulator accumulator
n v(n) w(n–5) v(n) w(n) w(n–5) v(n)
0 0 0 0 0 0 0
1 1 0 1 1 0 1
2 2 0 2 2 0 2
3 3 0 3 3 0 3
4 –4 0 –4 4 0 4
5 –3 0 –3 5 0 5
6 –2 1 –3 6 1 5
7 –1 2 –3 7 2 5
8 0 3 –3 –8 3 –11=5
9 1 –4 –3 –7 4 –11=5
10 2 –3 –3 –6 5 –11=5
11 3 –2 –3 –5 6 –11=5
12 –4 –1 –3 –4 7 –11=5
13 –3 0 –3 –3 –8 5
14 –2 1 –3 –2 –7 5
15 –1 2 –3 –1 –6 5

562 Sample Rate Conversion
So here’s the bottom line: When two’s complement fixed-point arith-
metic is used, the number of bits in a Qth-order CIC decimation filter’s inte-
grator and comb registers must accommodate the filter’s input signal times
the filter’s total gain of DQ. To be specific, overflow errors are avoided if the
number of integrator and comb register bit widths is at least
register bit widths = number of bits in x(n) + 3Qlog (D)4, (10–35)
2
where x(n) is the input to the CIC filter, and 3k4means that if kis not an inte-
ger, round it up to the next larger integer. For example, if a Q = 3-stage CIC
decimation filter accepts one-bit binary input words from a sigma-delta A/D
converter and the decimation factor is R = D = 64, binary overflow errors are
avoided if the three integrator and three comb registers’ bit widths are no less
than
register bit widths = 1 + 33 · log (D)4= 1 + 3 · 6 = 19 bits. (10–36)
2
Regarding a CIC decimation filter’s gain of DQ, we often see a multi-
stage CIC decimation filter implemented as shown in Figure 10–41 where R=
D, and a gain reduction (by 1/DQ) stage is included as a final operation. If D
is an integer power of two, the multiply operation can be performed with a
binary right shift. That’s one of the computational benefits of decimating by
an integer power of two. In the Figure 10–41 scenario, the data words out of
the final comb filter are shifted to the right by Qlog (D) bits to achieve an
2
overall decimation filter gain of unity.
Interpolating CIC filters have zero-valued samples inserted after each
original input sample reducing its gain by a factor of 1/R, so the net gain of a
CIC interpolation filter is DQ/R. For multistage interpolation CIC filters, the
integrators’ register bit widths grow in size in successive integrator stages.
This means that not all integrator accumulator registers need to have the
same bit width, so there is some flexibility in discarding some of the least sig-
nificant bits (lsbs) within the stages of a multistage CIC interpolation filter.
The specific effects of this lsb removal are, however, a complicated issue, so
we refer the reader to references [9,10] for more details.
While the preceding discussion focused on hardwired CIC filters, these
filters can also be implemented with programmable fixed-point DSP chips.
Although those chips have inflexible data paths and fixed word widths, their
x(n) y(m)
R
– – –
z–1 z–1 z–1 z–1 z–1 z–1 1/DQ
(Q=3)
Figure 10–41 Unity gain, Q= 3, D=R,CIC decimation filter.

10.14 Cascaded Integrator-Comb Filters 563
x(n) w(n)
(a) z–1 z–1 z–1
x(n) w(n)
(b) z–1 z–1 z–1
Figure 10–42 Cascaded integrator implementations: (a) traditional method;
(b) reduced pipelined critical-path delay method.
use of CIC filtering can be advantageous for high-sample-rate conversion.
Large word widths can be accommodated with multiword additions at the
expense of extra instructions. Even so, for large R the computational work-
load per output sample may be small compared to computations required
using a more conventional tapped-delay line FIR filter approach in fixed-
point DSPchips.
One further CIC filter implementation issue deserves mention. When we
need to implement cascaded integrators, we showed those integrators as in
Figure 10–42(a). As it turns out, depending on the architecture of your hard-
ware implementation, it may be advantageous to implement those cascaded
integrators as shown in Figure 10–42(b), where placing the unit-delay ele-
ments in the forward path reduces the pipelined critical-path delay from
three adder delays to a single adder delay[11]. While the Figure 10–42(b) cas-
caded network adds additional time delay, the frequency magnitude re-
sponses are identical for the two networks in Figure 10–42.
10.14.5 Compensation/Preconditioning FIR Filters
In typical decimation/interpolation filtering applications we desire a reason-
ably flat passband and narrow transition region filter response. These desir-
able properties are not provided by CIC filters alone, with their drooping
passband gains and wide transition regions. We alleviate this problem, in dec-
imation for example, by following the CIC filter with a compensation nonre-
cursive FIR filter (often called an inverse sinc filter), as in Figure 10–30(a), to
narrow the output bandwidth and flatten the passband gain.
The compensation FIR filter’s frequency magnitude response is ideally
an inverted version of the CIC filter passband response similar to that shown
by the dashed curve in Figure 10–43(a) for a simple 3-tap FIR filter whose co-
efficients are [–1/16, 9/8, –1/16]. With the dotted curve representing the un-
compensated passband droop of a 1st-order R = 8 CIC filter, the solid curve

564 Sample Rate Conversion
10
Compensation FIR
8 filter response
2 6
Compensation FIR
filter response 4
1 B
Compensated d2
Compensated
0 0
–2
–1
Uncompensated
B –4 Uncompensated
d CIC filter CIC filter
–2 –6
–8
–3
–10
–4
0 f /4 f /2 0 f /4 f /2
s,out s,out s,out s,out
Frequency Frequency
(a) (b)
Figure 10–43 Compensation FIR filter magnitude responses, dashed curves:
(a) with a 1st-order decimation CIC filter; (b) with a 3rd-order deci-
mation CIC filter.
represents the compensated response of the cascaded filters. If either the CIC
filter’s order or passband width increases, the correction becomes more de-
manding, requiring more compensation FIR filter taps. An example of this sit-
uation is shown in Figure 10–43(b) where the dotted curve represents the
passband droop of a 3rd-order R = 8 CIC filter and the dashed curve, taking
the form of [x/sin(x)]3, is the response of a 15-tap compensation FIR filter
having the coefficients [–1, 4, –16, 32, –64, 136, –352, 1312, –352, 136, –64, 32,
–16, 4, –1].
Wideband compensation also means that signals near f /2 are attenu-
s,out
ated with the CIC filter and then must be amplified in the correction filter,
which adds noise. As such, practitioners often limit the passband width of the
compensation FIR filter to roughly one-fourth the frequency of the first null
in the CIC filter response.†
Those dashed curves in Figure 10–43 represent the frequency magni-
tude responses of compensating FIR filters within which no sample rate
change takes place. (The FIR filters’ input and output sample rates are equal
to the f output rate of the decimating CIC filter.) If a compensating FIR fil-
s,out
ter were designed to provide an additional decimation by two, its frequency
†I thank my DSPpal Ray Andraka, of Andraka Consulting Group Inc., for his guidance on these
implementation issues.

10.14 Cascaded Integrator-Comb Filters 565
2
0
–2
B
d
–4
–6
–8
0 f /4 f /2
s,in s,in
Frequency
Figure 10–44 Frequency magnitude response of a decimate-by-two compensa-
tion FIR filter.
magnitude response would look similar to that in Figure 10–44, where f is
s,in
the compensation filter’s input sample rate.
After all of this discussion, just keep in mind that a decimating CIC filter
is merely a very efficient recursive implementation of a moving average filter,
having D = NR taps, whose output is decimated by R. Likewise, the interpo-
lating CIC filter is insertion of R–1 zero-valued samples after each original
input sample followed by a D = NR-tap moving average filter running at the
output sample rate f . The cascade implementations in Figure 10–30 result
s,out
in total computational workloads far less than those when using a single
tapped-delay line FIR filter alone for high-sample-rate conversion by decima-
tion or interpolation. CIC filter structures are designed to maximize the
amount of low-sample-rate processing to minimize power consumption in
high-speed hardware applications. Again, CIC filters require no multiplica-
tions; their arithmetic is strictly additions and subtractions. Their perfor-
mance allows us to state that, technically speaking, CIC filters are lean, mean,
fat-free filtering machines.
Section 13.24 provides a few advanced tricks allowing us to implement
nonrecursive CIC filters, and this eases the word-width growth problem of
the above traditional recursive CIC filters.
This chapter’s discussion of sample rate conversion has, by necessity,
only touched the surface of this important signal processing technique. Fortu-
nately for us, the excellent work of early signal processing engineers and
mathematicians is well documented in the literature of DSP. Several standard
DSP textbooks briefly discuss multirate filter design concepts[12–14], and
other texts are devoted exclusively to polyphase filters and multirate process-
ing[6–9]. The inquisitive reader can probe further to learn how to choose the
number of stages in a multistage process[1,3], the interrelated considerations
of designing optimum FIR filters[1,15], the benefits of half-band FIR fil-
ters[5,16], when IIR filter structures may be advantageous[15], what special

566 Sample Rate Conversion
considerations are applicable to sample rate conversion in image process-
ing[17–19], guidance in developing the control logic necessary for hardware
implementations of rate conversion algorithms[15], how rate conversion im-
proves the usefulness of commercial test equipment[20,21], and software de-
velopment tools for designing multirate filters[22].
REFERENCES
[1] Crochiere, R., and Rabiner, L. “Optimum FIR Digital Implementations for Decimation, In-
terpolation, and Narrow-band Filtering,” IEEE Trans. on Acoust. Speech, and Signal Proc.,
Vol. ASSP-23, No. 5, October 1975.
[2] Ballanger, M. “Computation Rate and Storage Estimation in Multirate Digital Filtering
with Half-Band Filters,” IEEE Trans. on Acoust. Speech, and Signal Proc.,Vol. ASSP-25, No. 4,
August 1977.
[3] Crochiere, R., and Rabiner, L. “Interpolation and Decimation of Digital Signals—ATutor-
ial Review,” Proceedings of the IEEE,Vol. 69, No. 3, March 1981.
[4] Neugebauer, O. Astronomical Cuneiform Texts: Babylonian Ephemerides of the Seleucid Period
for the Motion of the Sun, the Moon and the Planets, Lund Humphries, London, 1955.
[5] Schafer, R., and Rabiner, L. “ADigital Signal Processing Approach to Interpolation,” Pro-
ceedings of the IEEE,Vol. 61, No. 6, June 1973.
[6] Fliege, N. Multirate Digital Signal Processing: Multirate Systems, Filter Banks, Wavelets, John
Wiley and Sons, New York, 1995.
[7] Crochiere, R., and Rabiner, L. Multirate Digital Signal Processing, Prentice Hall, Upper Sad-
dle River, New Jersey, 1983.
[8] Vaidyanathan, P. Multirate Systems and Filter Banks, Prentice Hall, Upper Saddle River,
New Jersey, 1992.
[9] Harris, F. Multirate Signal Processing for Communication Systems, Prentice Hall, Upper Sad-
dle River, New Jersey, 2004, Chapter 11.
[10] Hogenauer, E. “An Economical Class of Digital Filters for Decimation and Interpolation,”
IEEETrans. on Acoust. Speech, and Signal Proc.,Vol. ASSP-29, April 1981, pp. 155–162.
[11] Brandt, F. “Oversampled Analog-to-Digital Conversion, Stanford Electronics Laboratories,
Technical Report No. ICL91-009, April 1991, p. 108.
[12] Proakis, J., and Manolakis, D. Digital Signal Processing: Principles, Algorithms and Applica-
tions, Prentice Hall, Upper Saddle River, New Jersey, 1996.
[13] Oppenheim, A., and Schafer, R. Discrete-Time Signal Processing, Prentice Hall, Englewood
Cliffs, New Jersey, 1st ed. 1989, 2nd ed. 1999.
[14] Rorabaugh, C. DSP Primer, McGraw-Hill, New York, 1999.

References 567
[15] Crochiere, R., and Rabiner, L. “Further Considerations in the Design of Decimators and In-
terpolators,” IEEE Trans. on Acoust. Speech, and Signal Proc., Vol. ASSP-24, No. 4, August
1976.
[16] Ballanger, M., et al. “Interpolation, Extrapolation, and Reduction of Computational Speed
in Digital Filters,” IEEE Trans. on Acoust. Speech, and Signal Proc.,Vol. ASSP-22, No. 4, Au-
gust 1974.
[17] Hou, H., and Andrews, H. “Cubic Splines for Image Interpolation and Digital Filtering,”
IEEE Trans. on Acoust. Speech, and Signal Proc.,Vol. ASSP-26, No. 6, August 1978.
[18] Keys, R. “Cubic Convolution Interpolation for Digital Image Processing,” IEEE Trans. on
Acoust. Speech, and Signal Proc.,Vol. ASSP-29, No. 6, August 1981.
[19] Parker, J., et al. “Comparison of Interpolating Methods for Image Resampling,” IEEE
Trans. on Medical Imaging,Vol. MI-2, No. 1, August 1983.
[20] Blue, K., et al. “Vector Signal Analyzers for Difficult Measurements on Time-Varying and
Complex Modulated Signals,” Hewlett-Packard Journal, December 1993.
[21] Bartz, M., et al. “Baseband Vector Signal Analyzer Hardware Design,” Hewlett-Packard
Journal, December 1993.
[22] Mitchell, J. “Multirate Filters Alter Sampling Rates Even After You’ve Captured the Data,”
EDN, August 20, 1992.

568 Sample Rate Conversion
CHAPTER 10 PROBLEMS
10.1 Assume we want to decimate an x(n) time-domain sequence by four.
(a) Should the x(n) sequence be lowpass filtered before or after we discard
every fourth sample?
(b) Draw the frequency magnitude response of an ideal lowpass filter used in
this decimation-by-four process. Label the frequency axis of your draw-
ing in both Hz (in terms of the filter’s input data sampling rate f Hz) and
s
our “discrete-system” frequency notation of radians/sample.
(c) What should be the lowpass filter’s zero-Hz (DC) magnitude so that there
is no time-domain amplitude gain or loss in our decimation process?
10.2 Assume we have a 72-sample sinusoidal x(n) time-domain sequence, the first
36 samples of which are shown in Figure P10–2(a). Next we decimate x(n) by
two to generate 36 samples of the y(m) sequence shown in Figure P10–2(b).
Sequence y(m) is also sinusoidal, as we should expect, but its frequency
appears to be double the frequency of x(n). Explain that apparent frequency
difference.
1
x(n)
0.5
(a) 0
–0.5
–1
0 5 10 15 20 25 30 35
n
1
y(m)
0.5
(b) 0
–0.5
–1
0 5 10 15 20 25 30 35
m
Figure P10–2
10.3 Assume we collected 2048 samples of a sinewave whose frequency is 128 Hz
using an f sample rate of 1024 Hz, and we call those samples w(n). The first
s
20 samples of w(n) are shown in Figure P10–3. Next we perform a 2048-point
FFT on w(n) to produce a W(m) sequence.

Chapter 10 Problems 569
1
0
w(n)
–1
0 5 n 10 15
Figure P10–3
(a) What is the m frequency index value, m , of the FFT sample having the
max
largest magnitude over the positive-frequency range of |W(m)|? Show
how you arrived at your answer.
(b) Next, suppose we decimate w(n) by a factor of two to generate the 1024-
point sequence x(n) defined by
x(n) = w(2n).
If we perform a 1024-point FFT of x(n), what is the m frequency index
value,m , of the FFT sample having the largest magnitude over the
max,dec=2
positive-frequency range of |X(m)|? Show how you arrived at your an-
swer.
(c) Finally, assume we decimate x(n) by a factor of two to generate the 512-
point sequence y(n) defined by
y(n) = x(2n).
If we perform a 512-point FFT of y(n), what is the m frequency index
value,m , of the FFT sample having the largest magnitude over the
max,dec=2
positive-frequency range of |Y(m)|? Show how you arrived at your an-
swer.
10.4 In this chapter we’ve portrayed decimation by an integer factor M with the
block diagram shown in Figure P10–4, that is, a lowpass decimation filter fol-
lowed by a downsampler (the “↓M” symbol) that discards all but every Mth
filter output sample. In this problem we explore the changes in signal time-
domain amplitude and frequency-domain magnitude caused by decimation.
x(n) Lowpass w(n) y(m)
nonrecursive M
FIR filter
f = 1000 Hz f
s1 s2
Figure P10–4

570 Sample Rate Conversion
For this problem, our assumptions are:
• The lowpass filter in Figure P10–4 has a passband gain of unity and pass-
band width of 0 to 250 Hz.
• The x(n) sequence contains a 100 Hz sinusoidal component whose time-
domain peak amplitude is P.
• In the frequency domain, the 100 Hz x(n) sinusoid is located exactly on a
4N-point discrete Fourier transform (DFT) bin center and its 4N-point
DFT spectral magnitude is K.
• Finally, we apply exactly 4Nsamples of w(n) to the M= 4 downsampler.
(a) What is the f sample rate (in Hz) of the y(m) time-domain sequence?
s2
(b) What is the peak time-domain amplitude of the 100 Hz sinusoid in
thew(n) sequence?
(c) What is the peak time-domain amplitude of the 100 Hz sinusoid in the
y(m) sequence? Justify your answer.
(d) What is the magnitude of the 100 Hz spectral component in an N-
point DFT of y(m)? Justify your answer.
(e) What is the equation that defines Figure P10–4’s downsampled y(m)
sequence in terms of the w(n) sequence?
Hint:Your solution to this part of the problem will take the form
y(m) = w(?).
10.5 Given the x(n) input signal in Figure P10–5(a), whose |X(f)| magnitude spec-
r r
trum is shown in Figure P10–5(b), draw a rough sketch of the |X (f)| spectrum
c
of the system’s complex x (m) = x(m) + jx (m) output sequence. The frequency
c I Q
magnitude responses of the complex bandpass h (k) filter and the real-valued
BP
highpassh (k) filters are provided in Figures P10–5(c) and P10-5(d).
HP
10.6 Assume we want to design the decimation by M= 30 system shown in Figure
P10–6(a). The desired LPF lowpass filter’s frequency magnitude response is
0
the solid lines shown in Figure P10–6(b). The filter’s stopband attenuation is
50 dB. (The dashed lines are the spectral replication of the lowpass filter’s
frequency response.) The one-sided passband width of the lowpass filter is
B’ = 1.7 kHz.
(a) Using the text’s Eq. (10–3), estimate the number of taps in the LPF low-
0
pass filter.
(b) Assuming we decide to implement our decimation by M = 30 system
using two-stage decimation as shown in Figure P10–6(c), what are the op-
timumM andM decimation factors?
1 2

Chapter 10 Problems 571
x(n)
r h (k) h (k) 3 x(m)
cos HP I
x(m)
(a) c
h (k) h (k) 3 x (m)
sin HP Q
h (k) x(m) =x(m) +jx (m)
BP c I Q
|X(f)|
r
(b)
–12 –8 –4 0 4 8 12 kHz
(–f/2) (f/2)
s s
|H (f)|
BP
(c)
–12 –8 –4 0 4 8 12 kHz
(–f/2) (f/2)
s s
|H (f)|
HP
(d)
–18 –12 –8 –4 0 4 8 12 18 kHz
(–f/2) (f/2)
s s
Figure P10–5
x (n) x (m)
old new
LPF M = 30
(a) 0
f = 120 kHz f = 4 kHz
s,old s,new
LPF filter response
o
0
(b) d B B'
–50
0 1 1.7 2.3 3 4 Freq
(kHz)
Decimation by 30
x (n) x (m)
old LPF M LPF M new
(c) 1 1 2 2
f = 120 kHz f = 4 kHz
s,old s,new
Figure P10–6

572 Sample Rate Conversion
(c) Using the text’s Eq. (10–3), estimate the number of taps in the LPF and
1
LPF lowpass filters in Figure P10–6(c).
1
(d) What is the reduction in number of filter taps using the system in Figure
P10–6(c) compared to the number of filter taps needed by the system in
Figure P10–6(a)?
10.7 Here is a interesting problem. In Chapter 5 we discussed the transient response
of tapped-delay line FIR filters and stated that an FIR filter’s output samples
are not valid until the filter’s delay line is filled with input data samples. As-
suming that the 23rd output sample of LPF is the first sample applied to
1
LPF , how many x (n) input samples must be applied to the two-stage deci-
2 old
mation filter shown in Figure P10–7 to fill the LPF and LPF lowpass filters
1 2
with input data?
Contains 22 unit-delay Contains 9 unit-delay
elements & 23 taps elements & 10 taps
x (n) x (m)
old LPF M = 7 LPF M = 3 new
1 1 2 2
Figure P10–7
10.8 Assume we want to interpolate an x(n) time-domain sequence by three.
(a) Should we perform upsampling (insertion of zero-valued samples) on the
x(n) sequence before or after implementing lowpass filtering?
(b) Draw the frequency magnitude response of an ideal lowpass filter used in
this interpolation-by-three process. Label the frequency axis of your
drawing in both Hz (in terms of the filter’s input data sampling rate f Hz)
s
and our “discrete-system” frequency notation of radians/sample.
(c) What should be the lowpass filter’s zero-Hz (DC) magnitude so that there
is no time-domain amplitude gain or loss in our interpolation process?
10.9 Let’s make sure we fully understand the spectral effects of interpolation by
considering the 8-sample, single-cycle, x(n) sinewave sequence in Figure
P10–9(a). That sequence’s X(m) DFT spectral magnitude samples are shown
in Figure P10–9(b). If we upsample x(n) by a factor of three, by inserting two
zero-valued samples between each x(n) sample, we produce the 24-sample
y(p) time sequence shown in Figure P10–9(c).
(a) What is the time-domain equation that defines the upsampled y(p) se-
quence in terms of the x(n) sequence?

Chapter 10 Problems 573
x(n)
1
0
(a) 0 1 2 3 4 5 6 7 n
|X(m)|
4
(b)
0
0 1 2 3 4 5 6 7 m
y(p)
1
(c) 15 18 21
0
0 3 6 9 12 p
Figure P10–9
Hint: Your solution to this part of the problem will have two parts and
look like
⎧x(?), for n=?
y(p)=⎨
⎩?, otherwise.
(b) Draw the spectral magnitude samples of the 24-point Y(m) DFT of y(p).
10.10 Assume we have a time-domain sequence of real-valued samples, x (n),
old
whose spectral magnitude is shown in Figure P10–10. (We represent spectral
replications by the dashed lines.) There we see that the frequency points of
spectral symmetry of |X (f)|, represented by the bold down arrows, can be
old
described by
⎧x(?), for n=?
y(p)=⎨
⎩?, otherwise
where k is an integer. If we upsample x (n) by two, by inserting a zero-
old
valued sample between each x (n) sample, to generate a new time sequence
old
x (m), what is the expression for the frequency points of spectral symmetry
new
of |X (f)|?
new

574 Sample Rate Conversion
Points of spectral symmetry
. . . . . .
|X (f)|
old
. . . . . .
–f –f/2 0 f/2 f Freq
s s s s
Figure P10–10
10.11 Texas Instruments Inc. produces a digital filter chip, Part #GC2011A, used in
cell phones for frequency up-conversion. The process, described in their
AN9804 application note document, is depicted in Figure P10–11(a). The low-
pass filter’s 1 MHz-wide passband covers the frequency range shown in Fig-
ure P10–11(b). (The lowpass filter block comprises two separate real-valued
1 MHz-wide filters, filtering the real and imaginary parts of the complex sig-
nal at node B.) If the spectral magnitude of the x(n) input is that shown by the
solid curves in Figure P10–11(c), where we represent spectral replications by
the dashed curves, draw the spectral magnitudes of the complex sequences at
nodes A, B, C, and the real part of the y(m) output sequence.
Hint:In Chapter 8 we learned that multiplying a time sequence by e–j2n/4= 1,
–j, –1, j, ..., translates the signal’s spectrum down in frequency.
x(n) A B Lowpass C y(m)
2
(a) filterh(k) (Real
part only)
f = 2 MHz
s
1, –j, –1, j, ... 1,j, –1, –j, ...
|H(f)|
1
(b)
–2 –1 0 1 2 Freq
(MHz)
|X(f)|
(c)
–4 –2 –f/2 0 f/2 2 4 Freq
s s
(MHz)
Figure P10–11

Chapter 10 Problems 575
10.12 Here is a fun interpolation problem. Figure P10–12(a) shows a simple digital
filtering system. Assume that the analog x(t) signal applied to the analog-
digital (A/D) converter contains a 9 kHz sinusoid and an 11 kHz sinusoid.
The spectral magnitude of the sampled x(n) sequence is given in Figure
P10–12(b). The system’s function is to filter out the 11 kHz tone and provide a
y(m) output sequence that is a 9 kHz sinusoid at a sample rate of f = 32 kHz.
s
The dashed curve in Figure P10–12(b) indicates the unity-gain bandpass fil-
ter’s frequency magnitude response, while the spectrum of our desired filter
output, whose magnitude is K, is given in Figure P10–12(c).
x(t) x(n) Bandpass y(m)
A/D
(a) filter,h(k)
f = 32 kHz 9 kHz sinusoid at f = 32 kHz
s s
|X(f)| |H(f)|
. . . K . . .
(b)
–12 –8 –4 0 4 8 12 kHz
16
(f/2)
s
|Y(f)|
. . . K . . .
(c)
–12 –8 –4 0 4 8 12 kHz
16
(f/2)
s
x(t) u(n) Processing y(m)
A/D
(d) system
f = 8 kHz 9 kHz sinusoid at f = 32 kHz
s s
Figure P10–12
Now, assume that the system is constrained to use an A/D converter whose
clock rate is 8 kHz (instead of 32 kHz), as shown in Figure P10–12(d).
(a) Draw the block diagram of the processing systemthat provides the desired
y(m) output sequence at a sample rate of f = 32 kHz which is four times
s
theu(n) sample rate.
(b) Draw spectral diagrams that justify your solution.
10.13 In this chapter we discussed various forms of interpolation. There is a well-
known interpolation process called linear interpolation. It’s an interpolation-
by-two method for estimating sample values of a continuous function

576 Sample Rate Conversion
between some given x(n) sample values of that function. For the x(n) time
samples in Figure P10–13(a), linear interpolation is the process of computing
the intermediate y(n) samples shown as the black squares in Figure
P10–13(b). That is, the interpolated sample y(1) is the value lying at the center
of the straight line connecting x(0) and x(1), the interpolated sample y(2) is the
value lying at the center of the straight line connecting x(1) and x(2), and so
on. Given this process of linear interpolation:
(a) What is the z-domain expression for the H(z) = Y(z)/X(z) transfer function
of the linear interpolation process?
(b) Draw a rough sketch of the frequency magnitude response of a linear in-
terpolation filter over the frequency range of ω = ±π radians/sample
(±f/2 Hz).
s
(c) Comment on the advantage of, and the disadvantage of, using linear in-
terpolation to perform interpolation by a factor of two.
x(n) A continuous function
(a) . . .
0
0 1 2 n
y(n) Sequencey(n) is the black squares
x(1) x(2)
x(0)
(b)
. . .
0
1 2 n
Figure P10–13
10.14 Assume we must convert a compact disc (CD) audio signal, whose sample
rate is f = 44.1 kHz, to a digital audio tape (DAT) signal whose sample rate
s,CD
is f = 48 kHz. If we interpolate that CD signal by a factor of L = 160, by
s,DAT
what factor Mmust we decimate the interpolated signal to obtain a final sam-
ple rate of 48 kHz?
10.15 Consider the x (n) time sequence in Figure P10–15(a), whose sample rate is f
o s
= 1 kHz. If we decimate x (n) by two, we obtain the x (m ) sequence shown
o D D
in Figure P10–15(b), where the odd-n samples of x (n) have been discarded.
o
Next, if we interpolate x (n) by two, we obtain the x(m) sequence shown in
o I I

Chapter 10 Problems 577
2
(a) 0
x(n)
o
–2
0 5 n 10 15
2
(b) 0
x (m )
D D
–2
0 2 4 m 6 8
D
2
(c) 0
x(m)
I I
–2
0 5 10 15 20 25 30 35
m
I
Figure P10–15
Figure P10–15(c), where the interpolated samples are shown as white dots.
Comment on how decimation and interpolation affect the time duration of
the decimated and interpolated sequences relative to the time duration of the
originalx (n) sequence.
o
10.16 Fill in the following table. When complete and correct, the table shows the
time-domain and frequency-domain gain of the two processes: decimation by
M, and interpolation by L.
Here, decimation means lowpass filtering (by a unity-gain filter) NM
time samples followed by the discarding of every Mth filter output sample to
obtainNtime samples. By “interpolation” we mean upsampling by inserting
L–1 zero-valued samples between adjacent samples of an N-length time-
domain sequence followed by lowpass filtering using a unity-gain lowpass
filter to obtain NL time samples. Assume the sample rate change factors
MandLare integers.
Sample Rate Conversion Gain
Time domain Frequency domain
Decimation by M Gain = Gain =
Interpolation by L Gain = Gain =

578 Sample Rate Conversion
10.17 Here is an interesting, and educational, problem because it shows the spectral
effects of upsampling a downsampled sequence. Think about the sample rate
change process in Figure P10–17(a). The upsampling operation “↑4” means
insert three zero-valued samples between each q(m) sample. Assume the spec-
tral magnitude of the x(n) sequence is the |X(f)| shown in Figure P10–17(b).
Decimation Interpolation
x(n) y(n) x(n) y(n)
R R
– –
z–1 z–D z–D z–1
Integrator Comb Comb Integrator
(a) (b)
Figure P10–17
(a) Draw the |Q(f)| spectrum of sequence q(m) including the peak spectral
magnitude levels in terms of K. Show spectral replications (located at
multiples of the q(m) sample rate) as dashed curves as was done in Figure
P10–17(b).
(b) Draw the |W(f)| spectrum of sequence w(p) including the peak spectral
magnitude levels in terms of K. Show spectral replications as dashed
curves.
(c) Draw the frequency magnitude response of the lowpass filter, including
its passband gain value, that would produce a y(p) output sequence
whoseY(f) spectral magnitude is equal to |X(f)|.
(d) When first learning the principles of sample rate change (multirate sys-
tems), it is easy to believe that following a “↓4” decimation process with an
“↑4” upsampling process would mean the two processes cancel each other
such that the overall cascaded effect would be no change. Is this correct?
10.18 One way to implement a secure telephone communications channel is shown
in Figure P10–18(a). Anyone monitoring the telephone line will not be able to
understand the audio speech signal on that line. The scrambling network is
shown in Figure P10–18(b), where the two identical L(f) digital lowpass filters
have passbands that extend from –2 kHz to +2 kHz. The two identical H(f)
digital highpass filters have passbands that extend from –6 kHz to –2 kHz,
and 2 kHz to 6 kHz.
(a) If the x(n) input to the first scrambling network has the spectrum shown in
Figure P10–18(c), draw the spectrum, over the frequency range of ±f, of the
s
output sequence from the first scrambling network in Figure P10–18(a).

Chapter 10 Problems 579
f = 8 kHz
s
Ideal
Analog-
Scrambling digital-to-
to-digital
network analog
converter
Microphone converter
(a)
Public telephone line
Ideal
Analog-
Scrambling digital-to-
to-digital
network analog
converter
converter
f = 8 kHz Speaker
s
Scrambling network
x(n)
L(f) 2 2 H(f)
y(n)
(b) +
H(f) 2 2 L(f)
|X(f)|
(c)
kHz
–4 –2 0 2 4
(–f/2) (f/2)
s s
Figure P10–18
(b) Draw the spectrum, over the frequency range of ±f, of the output se-
s
quence from the second scrambling network in Figure P10–18(a).
10.19 In Section 10.7 we depicted a polyphase filter, used in an interpolation-by-
four process, with the structure shown in Figure P10–19–I. The H (z) blocks
k
represent tapped-delay line FIR polyphase subfilters containing unit-delay el-
ements, multipliers, and adders.
(a) Why are the polyphase subfilters useful when used in an interpolation
process?
(b) Determine how to replace the commutating (rotating) switch in Figure
P10–19–I using only the delay and upsampler elements shown in Figure
P10–19–II(a). That is, determine what’s inside the mysterious block in
Figure P10–19–II(b) to make that figure equivalent to Figure P10–19–I.
The correct solution to this problem will show a polyphase structure with
which you should become familiar. That structure is often used in the DSP
literature of multirate systems to depict polyphase interpolation filters.

580 Sample Rate Conversion
x(n) at f rate
s H(z)
0
H(z)
1 y(m) at 4f rate
s
H(z)
2
H(z)
3
Figure P10–19–I
x(n) at
f rate
s
H(z)
0
4
y(m) at
H(z)
1 4f rate
? s
H(z)
z–1 2
H(z)
3
(a) (b)
Figure P10–19–II
Hint: Given some x(n) sequence, write the sample sequences on the four
output lines of the H (z) subfilters, and y(n) in Figure P10–19–I. Then de-
k
termine how to obtain that same y(m) output sequence in Figure
P10–19–II(b). The coefficients of polynomial H (z) are not important to
k
this problem. Assume the subfilters have no delay elements, a single mul-
tiplier, and a coefficient of one, if you wish.
10.20 Occasionally in the literature of DSP you’ll encounter documentation that
uses a drawing like that in Figure P10–20 to illustrate some concept, or princi-
ple, regarding multirate systems. Notice that the cascaded elements are not
our standard “z–1” delay-by-one-sample elements but, instead, are advance-by-
one-sampleelements indicated by a “z” (z+1).
Show how you would implement the system in Figure P10–20, in our uni-
verse where we cannot look forward in time, to provide the appropriate four
time-domain sequences to the “Some useful processing” subsystem’s input
ports?

Chapter 10 Problems 581
x(n)
A
z
x(n+1)
B Some
useful y(n)
processing
z
x(n+2)
C
z
x(n+3)
D
Figure P10–20
10.21 In the text we discussed decimation by M= 3 and showed two equivalent re-
alizations of such a decimation process as those in Figures P10–21(a) and
P10–21(b). Assume that all six subfilters in Figure P10–21 are tapped-delay
lines containing four multipliers, and that f = 30 samples/second.
s
(a) How many multiplications per second are performed in Figure
P10–21(a)?
(b) How many multiplications per second are performed in Figure
P10–21(b)?
x(n)
H(z3)
0
(f s sample y(m)
(a) rate) z–1 H 1 (z3) + 3
z–1 z–1 H(z3) (f s /3 sample
2 rate)
x(n)
3 H(z)
0
(f sample
s y(m)
(b) rate) z–1 3 H 1 (z) +
z–1 z–1 3 H 2 (z) (f s /3 r a s t a e m ) ple
Figure P10–21
10.22 The decimation-by-four (lowpass filtering followed by downsampling)
process shown in Figure P10–22(a) is inefficient because three out of every
four computational results are discarded. Amore efficient decimation process
is shown in Figure P10–22(b), where the switches driving the multipliers

582 Sample Rate Conversion
x old (n) x(n–1) x(n–2) ... x(n–11)
z–1 z–1 z–1 z –1
h(0) h(1) h(2) h(11)
(a)
x (m)
4 new
x old (n) x(n–1) x(n–2) ... x(n–11)
z–1 z–1 z–1 z –1
h(0) h(1) h(2) h(11)
(b)
x (m)
new
Figure P10–22
close once, for one sample time period only, upon the arrival of every fourth
x (n) sample. This way, no unnecessary computations are performed. Like-
old
wise, in polyphase decimation filtering no unnecessary computations are per-
formed. In real-time hardware implementations, explain the fundamental
difference between the computations performed, from a time-domain stand-
point, in the Figure P10–22(b) decimation filter and a polyphase decimation-
by-four filter having 12 multipliers?
10.23 In Section 10.7 we depicted a polyphase filter, used in a decimation-by-four
process, with the structure shown in Figure P10–23–I. The H (z) blocks repre-
k
sent tapped-delay line FIR polyphase subfilters containing unit-delay ele-
ments, multipliers, and adders.
H(z)
0
x(n) at y(m) at
H(z)
f rate 1 f/4 rate
s + s
H(z)
2
H(z)
3
Figure P10–23–I

Chapter 10 Problems 583
(a) Why are polyphase subfilters useful when used in a decimation process?
(b) Determine how to replace the commutating (rotating) input switch in Fig-
ure P10–23–I using only the delay and downsampler elements shown in
Figure P10–23–II(a). That is, determine what interconnection of delay and
downsampler elements must be inside the mysterious block in Figure
P10–23–II(b) to make that figure equivalent to Figure P10–23–I.
H(z)
0
4
x(n) at H(z) y(m) at
f rate 1 f/4 rate
s ? + s
H(z)
2
z–1
H(z)
3
(a) (b)
Figure P10–23–II
The correct solution to this problem will show a polyphase structure with
which you should become familiar. That structure is often used in the
DSPliterature of multirate systems to depict polyphase decimation filters.
Hint:Given some x(n) sequence, write the x(n),x(n–1),x(n–2), etc., sample
sequences on the four lines driving the H (z) subfilters in Figure P10–23–I.
k
Then determine how to obtain those same sample sequences for routing
to the subfilters in Figure P10–23–II(b).
10.24 This problem is related to the material in the text’s Section 10.10. Assume we
are resampling a time sequence by the rational factor 5/4 using a five-
position commutating filter output switch as shown in Figure P10–24.
(a) Determine the commutating switch’s port position value (index) k, and
the index n of the most recent input x(n) sample applied to the subfilters,
used to compute the resampler’s y(m) sample when output index m = 7.
Show your work.
(b) For the resampler in Figure P10–24 to have a DC (zero Hz) gain of unity,
what must be the DC gain of the original prototype lowpass FIR filter
from which the five H (z) subfilters were obtained?
k

584 Sample Rate Conversion
H(z)
0
Switch position
isk,where
H 1 (z) 0 < k < 4
0
1
x(n) H(z) 2 y(m)
2
3
4 k
H(z)
3 Commutator
switch port
H(z) counter
4
Figure P10–24
10.25 Think about the multirate decimation system, employing lowpass half-band
filters, in Figure P10–25(a). If the spectrum of the wideband x(n) noise se-
quence is that shown in Figure P10–25(b), the spectrum of the a(n) noise se-
quence is as shown in Figure P10–25(c). Draw the spectra, with appropriate
frequency-axis labeling in Hz, of the b(n),c(m), and y(p) sequences.
x(n) Half-band a(n) b(m) Half-band c(m) y(p)
2 2
filter,h(k) filter,h(k)
(a)
f = 1600 Hz
s
|X(f)|
. . . . . .
(b)
–1600 –800 0 800 1600 Freq
(f)
s
|A(f)|
. . . . . .
(c)
–1600 –800 0 800 1600 Freq
(f)
s
Figure P10–25
10.26 Thez-domain transfer function of a CIC filter’s comb subfilter having a delay
line length of N= 8, shown in Figure P10–26(a), is
H (z) = 1 –z–8,
comb
and its frequency magnitude response is shown on a linear scale in Figure
P10–26(b).

Chapter 10 Problems 585
|H (f)|
P comb
x(n) y(n)
–
z –8
x(n-8)
0
–f/2 0 f/2
s Frequency s
(a) (b)
Figure P10–26
(a) Each of those multiple frequency magnitude passband curves in Figure
P10–26(b) looks parabolic in shape. In terms of the frequency variable f, a
single ideal downward-opening parabola is described by the expression
|H (f)| = –Kf2
comb
where Kis some constant. Are the shapes of those passband curves in Figure
P10–26(b) indeed a function of f2, making them parabolic? Show your work.
(b) What is the peak value, P, of the |H(f)| frequency magnitude curve in
P10–26(b)? Show your work. (The Pvalue is important. It tells us what is
the maximum gain of a comb subfilter.)
Hint: Deriving an equation for the |H (f)| frequency magnitude re-
comb
sponse will provide the solutions to Part (a) and Part (b) of this problem.
10.27 In the text we stated that the interpolation CIC filter in Figure P10–27(a) has
an impulse response, when its differential delay D= 5, equal to that shown in
Interpolation CIC filter Decimation CIC filter
Comb Integrator Integrator Comb
x(n) y(n) x(n) y(n)
– –
z –5 z –1 z –1 z –5
(a) (b)
Interpolation CIC filter impulse response
1
(c)
0.5
. . .
0
0 5 n 10
Figure P10–27

586 Sample Rate Conversion
Figure P10–27(c). We also stated that swapping Figure P10–27(a)’s comb and
integrator resulted in a decimation CIC filter as shown in Figure P10–27(b).
Prove that the decimation CIC filter in Figure P10–27(b) also has an impulse
response equal to that shown in Figure P10–27(c).
10.28 Here is an important problem with regard to implementing two theoretically
equivalent digital filters. We illustrate our point using the CIC filters shown
in Figures P10–28(a) and P10–28(b). Because they are linear, we can swap the
comb and integrator stages of the CIC filter used for interpolation to obtain a
CIC filter used for decimation. The two CIC filters have identical time-
domain impulse responses.
Interpolation CIC filter Decimation CIC filter
Comb Integrator Integrator Comb
x(n) u(n) y(n) x(n) u(n) y(n)
– –
z –D z –1 z –1 z –D
x(n–D) y(n–1) u(n–1) u(n–D)
(a) (b)
x(n) input step sequence
1 . . .
(c)
0.5
0
0 5 n 10
Figure P10–28
(a) However, to understand an important fundamental difference in the
hardware implementation of the two filters, draw the u(n) and y(n) se-
quences for both filters when the x(n) input to the filters is the step se-
quence shown in Figure P10–28(c). Assume a comb delay of D = 4 for
both CIC filters. Your solution should comprise four separate drawings.
(Also, assume that the u(n) and y(n) values are zero, for both CIC filters,
at time index n< 0.)
(b) To appreciate the implementation difference between interpolation and
decimation CIC filters, we need to determine the growth of the binary
word width of the memory location, or hardware register, containing the
u(n) samples. To do so, fill in the following table, indicating how many bi-
nary bits are needed to accommodate the u(n) and y(n) samples for each
CIC filter up to time index n= 500.

Chapter 10 Problems 587
Hint: The number of binary bits needed to store u(n) is the next integer
greater than log [u(n)].
2
Memory, or Hardware Register, Bit-Width Requirements
Interpolation Decimation
Sequence CIC filter CIC filter
u(n)
y(n)
(c) This question has great practical importance. What does your solution to
Part (b) tell us about the binary-word-width requirements of the memory
locations, or hardware registers, containing the integrators’ u(n) samples
in CIC decimation and CIC interpolation filters?
10.29 Here is a typical problem faced by engineers who use CIC filters. As of this
writing, Intersil Corp. makes a decimating digital filter chip (Part #HSP43220)
that contains a 5th-order CIC filter. When used for decimation by a factor of
R= 6, and the internal comb filters have a differential delay of D= 6, the CIC
filter’s frequency magnitude response is shown in Figure P10–29(a).
0 |H (f)|
CIC
B
d B B
(a)
Atten
–0.1f 0 0.1f 0.2f 0.3f 0.4f f /2
s,in s,in s,in s,in s,in s,in
Frequency (before decimation)
f
o
0
|H (f)|
B CIC Gain loss
d
(b)
B
–0.1f 0 0.1f 0.2f 0.3f 0.4f f /2
s,in s,in s,in s,in s,in s,in
Frequency(before decimation)
Figure P10–29

588 Sample Rate Conversion
(a) After the decimation by 6, any spectral energy in the shaded area of the
filter’s response will alias into the B-width signal-of-interest passband
centered at 0 Hz as was described in the text. For this commercial 5th-
order CIC filter, what is the maximum level of the aliased spectral energy
after the decimation by 6? (Stated in different words, what is the value of
Attenmeasured in dB for the HSP43220 CIC filter?) Assume B= 0.04f .
s,in
(b) Zooming in on the top portion of the CIC filter’s passband, we show the
droop in the passband gain in Figure P10–29(b). Measured in dB, what is
the HSP43220’s maximum passband gain loss at B/2 Hz?
10.30 There are digital filtering schemes that use the process conceptually shown in
Figure P10–30(a). In that network the input is lowpass filtered to generate the
sequence w(n). The network’s y(n) output is the x(n) input sequence minus
the lowpass-filtered w(n) sequence. The actual implementation of such a
process is shown in Figure P10–30(b) where the multi-element delay line in
the upper path of Figure P10–30(b) is needed for time alignment to compen-
sate for the time (group) delay of the CIC filter. If we had to implement this
parallel-path filter with a CIC filter whose differential delay is D = 9, how
many unit-delay elements would we use in the upper path of Figure
P10–30(b)? Show how you obtained your solution.
x(n) y(n)
(a)
1 – z–D –
1 – z–1 w(n)
Single-stage CIC filter
Delay line
. . .
z–1 z–1 z–1
x(n) y(n)
(b) –
– w(n)
z–1 z–D
Figure P10–30

CHAPTER ELEVEN
Signal
Averaging
How do we determine the typical amount, a valid estimate, or the true value
of some measured parameter? In the physical world, it’s not so easy to do be-
cause unwanted random disturbances contaminate our measurements. These
disturbances are due to both the nature of the variable being measured and
the fallibility of our measuring devices. Each time we try to accurately mea-
sure some physical quantity, we’ll get a slightly different value. Those un-
wanted fluctuations in a measured value are called noise, and digital signal
processing practitioners have learned to minimize noise through the process
of averaging. In the literature, we can see not only how averaging is used to
improve measurement accuracy, but that averaging also shows up in signal
detection algorithms as well as in lowpass filter schemes. This chapter intro-
duces the mathematics of averaging and describes how and when this impor-
tant process is used. Accordingly, as we proceed to quantify the benefits of
averaging, we’re compelled to make use of the statistical measures known as
the mean, variance, and standard deviation.
In digital signal processing, averaging often takes the form of summing
a series of time-domain signal samples and then dividing that sum by the
number of individual samples. Mathematically, the average of N samples of
sequence x(n), denoted x , is expressed as
ave
1 ∑N x(1)+x(2)+x(3)+ +x(N)
x = x(n)= . (11–1)
ave N N
n=1
(What we call the average, statisticians call the mean.) In studying averaging,
a key definition that we must keep in mind is the variance of the sequence, σ2, de-
fined as
589

590 Signal Averaging
1
∑N
σ2 = [x(n)–x ]2, (11–2)
N ave
n=1
[x(1)–x ]2+[x(2)–x ]2+[x(3)–x ]2+ +[xx(N)–x ]2
= ave ave ave ave . (11–2’)
N
As explained in Appendix D, the σ2 variance in Eqs. (11–2) and (11–2’)
gives us a well-defined quantitative measure of how much the values in a se-
quence fluctuate about the sequence’s average. That’s because the x(1) – x
ave
value in the bracket, for example, is the difference between the x(1) value and
the sequence average x . The other important quantity that we’ll use is the
ave
standard deviation, defined as the positive square root of the variance, or
1
∑N
σ= [x(n)–x ]2 . (11–3)
N ave
n=1
To reiterate our thoughts, the average value x is the constant level
ave
about which the individual sequence values may vary. The variance σ2 indi-
cates the sum of the magnitudes squared of the noise fluctuations of the indi-
vidual sequence values about the x average value. If the sequence x(n)
ave
represents a time series of signal samples, we can say that x specifies the
ave
constant, or DC, value of the signal, the standard deviation σ reflects the
amount of the fluctuating, or AC, component of the signal, and the variance
σ2 is an indication of the power in the fluctuating component. (Appendix D
explains and demonstrates the nature of these statistical concepts for those
readers who don’t use them on a daily basis.)
We’re now ready to investigate two kinds of averaging, coherent and in-
coherent, to learn how they’re different from each other and to see under what
conditions they should be used.
11.1 COHERENT AVERAGING
In the coherent averaging process (also known as time-synchronous averaging),
the key feature is the timing used in sampling the original signal; that is, we
collect multiple sets of signal-plus-noise samples, and we need the time phase
of the signal in each set to be identical. For example, when averaging a
sinewave embedded in noise, coherent averaging requires that the phase of the
sinewave be the same at the beginning of each measured sample set. When this
requirement is met, the sinewave will average to its true sinewave amplitude
value. The noise, however, is different in each sample set and will average to-

11.1 Coherent Averaging 591
ward zero.† The point is that coherent averaging reduces the variance of the
noise, while preserving the amplitude of signals that are synchronous, or coher-
ent, with the beginning of the sampling interval. With coherent averaging, we
can actually improve the signal-to-noise ratio of a noisy signal. By way of ex-
ample, consider the sequence of 128 data points plotted in Figure 11–1(a). Those
data points represent the time-domain sampling of a single pulse contaminated
with random noise. (For illustrative purposes the pulse, whose peak amplitude
is 2.5, is shown in the background of Figure 11–1.) It’s very difficult to see a
pulse in the bold pulse-plus-noise waveform in the foreground of Figure
11–1(a). Let’s say we collect 32 sets of 128 pulse-plus-noise samples of the form
Sample set = x (1), x (2), x (3) , ..., x (128) ,
1 1 1 1 1
Sample set = x (1), x (2), x (3) , ..., x (128) ,
2 2 2 2 2
Sample set = x (1), x (2), x (3) , ..., x (128) ,
3 3 3 3 3 (11–4)
...
...
Sample set = x (1), x (2), x (3) , ..., x (128) .
32 32 32 32 31
Here’s where the coherent part comes in: the signal measurement times
must be synchronized, in some manner, with the beginning of the pulse, so
that the pulse is in a constant time relationship with the first sample of each
sample set. Coherent averaging of the 32 sets of samples, adding up the
columns of Eq. (11–4), takes the form of
1
∑32
x (k)= x (k)=[x (k)+x (k)+x (k)+...+x (k)]/32 ,
ave 32 n 1 2 3 32
n=1
or
x (1)=[x (1)+x (1)+x (1)+...+x (1)]/32
ave 1 2 3 32
x (2)=[x (2)+x (2)+x (2)+...+x (22)]/32 (11–5)
ave 1 2 3 32
x (3)=[x (3)+x (3)+x (3)+...+x (3)]/32
ave 1 2 3 32
...
...
x (128)=[x (128)+x (128)+x (128)+...+x (128)]/32 .
ave 1 2 3 32
If we perform 32 averages indicated by Eq. (11–5) on a noisy pulse like that in
Figure 11–1(a), we’d get the 128-point x (k) sequence plotted in Figure 11–1(b).
ave
Here, we’ve reduced the noise fluctuations riding on the pulse, and the pulse
shape is beginning to become apparent. The coherent average of 256 sets of
†Noise samples are assumed to be uncorrelated with each other and uncorrelated with the sam-
ple rate. If some component of the noise is correlated with the sample rate, that noise compo-
nent willbe preserved after averaging.

592 Signal Averaging
Amplitude
4
3
(a) 2
1
N = 1
0
1 20 40 60 80 100 120 Time
Amplitude
4
3
(b) 2
N = 32
1
1 20 40 60 80 100 120 Time
Amplitude
4
Pulse rising Pulse falling
edge edge
3
(c) 2
N = 256
1
1 20 40 60 80 100 120 Time
Figure 11–1 Signal pulse plus noise: (a) one sample set; (b) average of 32 sample
sets; (c) average of 256 sample sets.
pulse measurement sequences results in the plot shown in Figure 11–1(c),
where the pulse shape is clearly visible now. We’ve reduced the noise fluctua-
tions while preserving the pulse amplitude. (An important concept to keep in
mind is that summation and averaging both reduce noise variance. Summation
is merely implementing Eq. (11–5) without dividing the sum by N = 32. If we
perform summations and don’t divide by N, we merely change the vertical
scales for the graphs in Figures 11–1(b) and 11–1(c). However, the noise fluctua-
tions will remain unchanged relative to true pulse amplitude on the new scale.)
The mathematics of this averaging process in Eq. (11–5) is both straight-
forward and important. What we’d like to know is the signal-to-noise im-
provement gained by coherent averaging as a function of N, the number of
sample sets averaged. Let’s say that we want to measure some constant time

11.1 Coherent Averaging 593
signal with amplitude A,and each time we actually make a measurement we
get a slightly different value for A. We realize that our measurements are con-
taminated with noise such that the nth measurement result r(n) is
r(n) = A+ noise(n) (11–6)
where noise(n) is the noise contribution. Our goal is to determine Awhen the
r(n) sequence of noisy measurements is all we have to work with. For a more
accurate estimate of A, we average Nseparate r(n) measurement samples and
calculate a single average value r . To get a feeling for the accuracy of r ,
ave ave
we decide to take a series of averages, r (k), to see how that series fluctuates
ave
with each new average; that is,
r (1) = [r(1) + r(2) + r(3) + ... + r(N)]/N, ← 1stN-point average
ave
r (2) = [r(N+1) + r(N+2) + r(N+3) + ... + r(2N)]/N, ← 2ndN-point average
ave
r (3) = [r(2N+1) + r(2N+2) + r(2N+3) + ... + r(3N)]/N, ← 3rdN-point average
ave
...
...
⋅ ⋅ ⋅ ⋅
r (k) = [r([k–1] N+1) + r([k–1] N+2) + r([k–1] N+3) + ...+ r(k N)]/N, (11–7)
ave
or, more concisely,
r (k)= 1
∑N
r([k–1] ⋅ N+n). (11–8)
ave N
n=1
To see how averaging reduces our measurement uncertainty, we need to com-
pare the standard deviation of our r (k) sequence of averages with the stan-
ave
dard deviation of the original r(n) sequence.
If the standard deviation of our original series of measurements r(n) is
σ , it has been shown[1–5] that the standard deviation of our r (k) sequence
in ave
of N-point averages, σ , isgiven by
ave
σ
σ = in . (11–9)
ave
N
Likewise, we can relate the variance of our r (k) sequence of N-point aver-
ave
ages to the variance of the original series of r(n) measurements as
σ2 (11–9’)
σ2 = in.
ave N
Equation (11–9) is significant because it tells us that the r (k) series of
ave
averages will not fluctuate as much around A as the original r(n) measure-
ment values did; that is, the r (k) sequence will be less noisy than any r(n)
ave
sequence, and the more we average by increasing N, the more closely an indi-
vidual r (k) estimate will approach the true value of A.†
ave
†Equation (11–9) is based on the assumptions that the average of the original noise is zero and
that neither Anor σ changes during the time we’re performing our averages.
in

594 Signal Averaging
In a different way, we can quantify the noise reduction afforded by aver-
aging. If the quantity A represents the amplitude of a signal and σ repre-
in
sents the standard deviation of the noise riding on that signal amplitude, we
can state that the original signal-amplitude-to-noise ratio is
A
SNR = . (11–10)
in σ
in
Likewise, the signal-amplitude-to-noise ratio at the output of an averaging
process,SNR , is defined as
ave
r A
SNR = ave = . (11–11)
ave σ σ
ave ave
Continuing, the signal-to-noise ratio gain, SNR gain, that we’ve realized
coh
through coherent averaging is the ratio of SNR over SNR , or
ave in
SNR A/σ σ
SNR gain= ave = ave = in . (11–12)
coh SNR A/σ σ
in in ave
Substituting σ from Eq. (11–9) in Eq. (11–12), the SNR gain becomes
ave
σ
SNR coh gain= σ / in N = N. (11–13)
in
Through averaging, we can realize a signal-to-noise ratio improvement
proportional to the square root of the number of signal samples averaged. In
terms of signal-to-noise ratio measured in dB, we have a coherent averaging,
or integration, gain of
SNR gain(dB)=20 ⋅ log (SNR )=20 ⋅ log ( N)=10 ⋅ log (N). (11–14)
coh 10 coh 10 10
Again, Eqs. (11–13) and (11–14) are valid if Arepresents the amplitudeof a sig-
nal and σ represents the original noise standard deviation.
in
Another way to view the integration gain afforded by coherent averaging
is to consider the standard deviation of the input noise, σ , and the probability
in
of measuring a particular value for the Figure 11–1 pulse amplitude. Assume
that we made many individual measurements of the pulse amplitude and cre-
ated a fine-grained histogram of those measured values to get the dashed
curve in Figure 11–2. The vertical axis of Figure 11–2 represents the probability
of measuring a pulse-amplitude value corresponding to the values on the hori-
zontal axis. If the noise fluctuations follow the well-known normal, or Gauss-
ian, distribution, that dashed probability distribution curve is described by
p(x)= 1 e–(x−μ)2/2σ2 =Ke–(x–μ)2/2σ2 , (11–15)
σ 2π

11.1 Coherent Averaging 595
p(x)
K
No averaging
0.8K
0.6K
σ
ave
0.4K
σ
in N = 32
0.2K
0
0 1.0 2.0 3.0 4.0 Measured
amplitude (x)
Figure 11–2 Probability density curves of measured pulse amplitudes with no av-
eraging (N=1) and with N=32 averaging.
where σ = σ and the true pulse amplitude is represented by μ = 2.5. We see
in
from that dashed curve that any given measured value will most likely (with
highest probability) be near the actual pulse-amplitude value of 2.5. Notice,
however, that there’s a nonzero probability that the measured value could be
as low as 1.0 or as high as 4.0. Let’s say that the dashed curve represents the
probability curve of the pulse-plus-noise signal in Figure 11–1(a). If we aver-
aged a series of 32 pulse-amplitude values and plotted a probability curve of
our averaged pulse-amplitude measurements, we’d get the solid curve in Fig-
ure 11–2. This curve characterizes the pulse-plus-noise values in Figure
11–1(b). From this solid curve, we see that there’s a very low likelihood (prob-
ability) that a measured value, after 32-point averaging, will be less than 2.0
or greater than 3.0.
From Eq. (11–9), we know that the standard deviation of the result of av-
eraging 32 signal sample sets is
σ σ
σ = in = in .
ave 32 5.65 (11–16)
In Figure 11–2, we can see a statistical view of how an averager’s output
standard deviation is reduced from the averager’s input standard deviation.
Taking larger averages by increasing N beyond 32 would squeeze the solid
curve in Figure 11–2 even more toward its center value of 2.5, the true pulse
amplitude.†
†The curves in Figure 11–2 are normalized for convenient illustration. From Eq. (11–15) and as-
suming that σ=1 when N=1, the⋅n K=0.3989. When N=32, the new standard deviation is
σ’=σ/ N=1/ 32and K =0.3989 32 =2.23.

596 Signal Averaging
Returning to the noisy pulse signal in Figure 11–1, and performing co-
herent averaging for various numbers of sample sets N, we see in Figure
11–3(a) that as Nincreases, the averaged pulse amplitude approaches the true
amplitude of 2.5. Figure 11–3(b) shows how rapidly the variance of the noise
riding on the pulse falls off as Nis increased. An alternate way to see how the
noise variance decreases with increasing N is the noise power plotted on a
Averaged pulse amplitude
2.60
2.55
(a) 2.50
2.45
2.40
0 100 200 300 400 500 N
σ2
Averaged pulse-amplitude variance, ave
0.9
0.8
0.7
0.6
0.5
(b) 0.4
0.3
0.2
0.1
0
–0.1
0 100 200 300 400 500 N
Averaged noise power (dB)
0
–5
–10
(c) –15
–20
–25
–30
1 10 100 500 N
Figure 11–3 Results of averaging signal pulses plus noise: (a) measured pulse ampli-
tude versus N; (b) measured variance of pulse amplitude versus N;
(c) measured pulse-amplitude noise power versus Non a logarithmic scale.

11.2 Incoherent Averaging 597
logarithmic scale as in Figure 11–3(c). In this plot, the noise variance is nor-
malized to that noise variance when no averaging is performed, i.e., when
N = 1. Notice that the slope of the curve in Figure 11–3(c) closely approxi-
mates that predicted by Eqs. (11–13) and (11–14); that is, as N increases by a
factor of ten, we reduce the average noise power by 10 dB. Although the test
signal in this discussion was a pulse signal, had the signal been sinusoidal,
Eqs. (11–13) and (11–14) would still apply.
11.2 INCOHERENT AVERAGING
The process of incoherent averaging (also known as rms, postdetection, scalar,
or video averaging) is the averaging of signal samples where no sample timing
constraints are used; that is, signal measurement time intervals are not syn-
chronized in any way with the phase of the signal being measured. Think for
a moment what the average would be of the noisy pulse signal in Figure
11–1(a) if we didn’t in some way synchronize the beginning of the collection
of the individual signal sample sets with the beginning of the pulse. The re-
sult would be pulses that begin at a different time index in each sample set.
The averaging of multiple sample sets would then smear the pulse across the
sample set, or just “average the pulse signal away.” (For those readers famil-
iar with using oscilloscopes, incoherent averaging would be like trying to
view the pulse when the beginning of the scope sweep was not triggered by
the signal.) As such, incoherent averaging is not so useful in the time
domain.† In the frequency domain, however, it’s a different story because in-
coherent averaging can provide increased accuracy in measuring relative sig-
nal powers. Indeed, incoherent averaging is used in many test instruments,
such as spectrum, network, and signal analyzers.
In some analog test equipment, time-domain signals are represented in
the frequency domain using a narrowband sweeping filter followed by a
power detector. These devices measure signal power as a function of fre-
quency. The power detector is necessary because the sweeping measurement
is not synchronized, in time, with the signal being measured. Thus the
frequency-domain data represents power only and contains no signal phase
information. Although it’s too late to improve the input’s signal-amplitude-
to-noise ratio, incoherent averaging can improve the accuracy of signal power
measurements in the presence of noise; that is, if the signal-power spectrum is
very noisy, we can reduce the power estimation fluctuations and improve the
accuracy of signal-power and noise-power measurements. Figure 11–4(a)
†The term incoherent averagingis a bit of a misnomer. Averaging a set of data is just that, averag-
ing—we add up a set of data values and divide by the number of samples in the set. Incoherent
averaging should probably be called averaging data that’s obtained incoherently.

598 Signal Averaging
Power (dB)
0
N = 1
–5
–10
(a) –15
True average
–20 noise power
–25
–30
0 10 20 30 40 50 60 Freq
Power (dB)
0
N = 10
–5
–10
(b)
–15
–20
–25
0 10 20 30 40 50 60 Freq
Power (dB)
0
N = 100
–5
–10
(c)
–15
–20
–25
0 10 20 30 40 50 60 Freq
Figure 11–4 Results of averaging signal tones plus noise-power spectra: (a) no
averaging,N=1; (b) N=10; (c) N=100.
illustrates this idea where we see the power (magnitude squared) output of
an FFT of a fundamental tone and several tone harmonics buried in back-
ground noise. Notice that the noise-power levels in Figure 11–4(a) fluctuate
by almost 20 dB about the true average noise power indicated by the dashed
line at –19 dB.

11.2 Incoherent Averaging 599
Processing gain in dB
60
SNRc o h gain (dB):
50 Time-domain amplitude
SNR processing gain
40
30
20
SNRi n c o h gain (dB):
10 Frequency-domain power
SNR processing gain
0
10 1 10 2 10 3 10 4 10 5 10 6 N
Figure 11–5 Time-domain amplitude SNR processing gain from Eq. (11–14), and
the frequency-domain power SNR processing gain from Eq. (11–17),
as functions of N.
If we take 10 FFTs, average the square of their output magnitudes, and
normalize those squared values, we get the power spectrum shown in Figure
11–4(b). Here, we’ve reduced the variance of the noise in the power spectrum
but have not improved the tones’ signal-power-to-noise-power ratios; that is,
the average noise-power level remains unchanged. Averaging the output
magnitudes squared of 100 FFTs results in the spectrum in Figure 11–4(c),
which provides a more accurate measure of the relative power levels of the
fundamental tone’s harmonics.
Just as we arrived at a coherent integration SNR gain expression in Eq.
(11–14), we can express an incoherent integration gain, SNR gain, in terms
incoh
of SNR measured in dB as
⋅
SNR
incoh
gain(dB)=10 log
10
( N). (11–17)
Equation (11–17) applies when the quantity being averaged represents the
power of a signal. That’s why we used the factor of 10 in Eq. (11–17) as op-
posed to the factor of 20 used in Eq. (11–14).† We can relate the processing
gain effects of Eqs. (11–14) and (11–17) by plotting those expressions in Fig-
ure11–5.
†Section E.1 of Appendix E explains why the multiplying factor is 10 for signal-power measure-
ments and 20 when dealing with signal-amplitude values.

600 Signal Averaging
11.3 AVERAGING MULTIPLE FAST FOURIER TRANSFORMS
We discussed the processing gain associated with a single DFT in Section 3.12
and stated that we can realize further processing gain by increasing the point
size of any given N-point DFT. Let’s discuss this issue when the DFT is imple-
mented using the FFT algorithm. The problem is that large FFTs require a lot
of number crunching. Because addition is easier and faster to perform than
multiplication, we can average the outputs of multiple FFTs to obtain further
FFT signal detection sensitivity; that is, it’s easier and typically faster to aver-
age the outputs of four 128-point FFTs than it is to calculate one 512-point
FFT. The increased FFT sensitivity, or noise variance reduction, due to multi-
ple FFT averaging is also called integration gain.So the random noise fluctua-
tions in an FFT’s output bins will decrease, while the magnitude of the FFT’s
signal bin output remains constant when multiple FFT outputs are averaged.
(Inherent in this argument is the assumption that the signal is present
throughout the observation intervals for all of the FFTs that are being aver-
aged and that the noise sample values are independent of the original sample
rate.) There are two types of FFT averaging integration gain: incoherent and
coherent.
Incoherent integration, relative to FFTs, is averaging the corresponding
bin magnitudes of multiple FFTs; that is, to incoherently average k FFTs, the
zeroth bin of the incoherent FFT average F (0) is given by
incoh
F(0) + F (0) + F (0) +...+ F (0)
F (0)= 1 2 3 k (11–18)
incoh k
where |F (0)| is the magnitude of the zeroth bin from the nth FFT. Likewise,
n
the first bin of the incoherent FFT average, F (1), is given by
incoh
F(1) + F (1) + F (1) +...+ F (1)
F (1)= 1 2 3 k , (11–18’)
incoh k
and so on, out to the last bin of the FFT average, F (N–1), which is
incoh
F(N–1) + F (N–1) + F (N–1) +...+ F (N–1)
F (N–1)= 1 2 3 k .(11–18’’)
incoh k
Incoherent integration provides additional reduction in background
noise variation to augment a single FFT’s inherent processing gain. We can
demonstrate this in Figure 11–6(a), where the shaded curve is a single FFT
output of random noise added to a tone centered in the 16th bin of a 64-point
FFT. The solid curve in Figure 11–6(a) is the incoherent integration of ten indi-
vidual 64-point FFT magnitudes. Both curves are normalized to their peak
values, so that the vertical scales are referenced to 0 dB. Notice how the varia-

11.3 Averaging Multiple Fast Fourier Transforms 601
Power (dB)
0
–5 Old
SNR New
SNR
–10
(a) –15 True
average
noise power
–20
–25
Tone at FFT
bin center
–30
0 10 20 30 40 50 60 m
Power (dB)
0
Old SNR
–5 New SNR
–10 True
average
(b) –15 noise power
–20
–25
Tone is between
FFT bin centers
–30
0 10 20 30 40 50 60 m
Figure 11–6 Single FFT output magnitudes (shaded) and the average of ten FFT
output magnitudes (solid): (a) tone at bin center; (b) tone between
bin centers.
tions in the noise power in the solid curve have been reduced by the averag-
ing of the ten FFTs. The noise-power values in the solid curve don’t fluctuate
as much as the shaded noise-power values. By averaging, we haven’t raised
the power of the tone in the 16th bin, but we have reduced the peaks of the
noise-power values. The larger the number of FFTs averaged, the closer the
individual noise-power bin values will approach the true average noise
power indicated by the dashed horizontal line in Figure 11–6(a).
When the signal tone is not at a bin center, incoherent integration still re-
duces fluctuations in the FFT’s noise-power bins. The shaded curve in Figure
11–6(b) is a single FFT output of random noise added to a tone whose fre-
quency is halfway between the 16th and 17th bins of the 64-point FFT. Like-
wise, the solid curve in Figure 11–6(b) is the magnitude average of ten FFTs.
The variations in the noise power in the solid curve have again been reduced
by the integration of the ten FFTs. So incoherent integration gain reduces

602 Signal Averaging
noise-power fluctuations regardless of the frequency location of any signals
of interest. As we would expect, the signal peaks are wider, and the true aver-
age noise power is larger in Figure 11–6(b) relative to Figure 11–6(a) because
leakage raises the average noise-power level and scalloping loss reduces the
FFT bin’s output power level in Figure 11–6(b). The thing to remember is that
incoherent averaging of FFT output magnitudes reduces the variations in the
background noise power but does not reduce the average background noise
power. Equivalent to the incoherent averaging results in Section 11.2, the re-
duction in the output noise variance[6] of the incoherent average of kFFTs rel-
ative to the output noise variance of a single FFT is expressed as
σ2
1
k FFTs = . (11–19)
σ2 k
single FFT
Accordingly, if we average the magnitudes of k separate FFTs, we reduce the
noise variance by a factor of k.
In practice, when multiple FFTs are averaged and the FFT inputs are
windowed, an overlap in the time-domain sampling process is commonly
used. Figure 11–7 illustrates this concept with 5.5Nt seconds, worth of time
s
series data samples, and we wish to average ten separate N-point FFTs where
t is the sample period (1/f). Because the FFTs have a 50 percent overlap in
s s
the time domain, some of the input noise in the N time samples for the first
FFT will also be contained in the second FFT. The question is “What’s the
5.5Nt seconds
Time s
series
1st FFT Time
2nd FFT
3rd FFT
4th FFT .
.
N samples .
.
.
10th FFT
Figure 11–7 Time relationship of multiple FFTs with 50 percent overlap.

11.4 Averaging Phase Angles 603
noise variance reduction when some of the noise is common to two FFTs in
this averaging scheme?” Well, the answer depends on the window function
used on the data before the FFTs are performed. It has been shown that for the
most common window functions using an overlap of 50 percent or less, Eq.
(11–19) still applies as the level of noise variance reduction[7].
Coherent FFT integration gain is possible when we average the real
parts of multiple FFT bin outputs separately from computing the average of
the imaginary parts. We can then combine the single real average and the sin-
gle imaginary average into a single complex bin output average value. While
this process may be useful for people who use analog sinewave signals to test
the performance of A/D converters using the FFT, it only works for periodic
time-domain signal sequences that have been obtained through careful syn-
chronous sampling. Coherent integration of multiple FFT results is of no
value in reducing spectral measurement noise for nonperiodic, real-world,
information-carrying signals.
11.4 AVERAGING PHASE ANGLES
So far we’ve discussed averaging time-domain signal amplitude samples and
averaging frequency-domain magnitude samples. It’s prudent now to briefly
discuss the tricky aspect of averaging phase-angle samples. We say tricky be-
cause, as Peter Kootsookos points out, the circular (wraparound) nature of
angles can lead us into trouble when computing phase averages[8].
Consider computing the average of two phase angles, α = 7π/8 radians
and β = –7π/8 radians. Due to the directional nature of phase angles, we
know the average of α and β is an angle exactly halfway between 7π/8 radi-
ans and –7π/8 radians, or ±π radians (±180 degrees). However, standard nu-
merical averaging of the two scalar radian values 7π/8 and –7π/8 results in
zero radians (0 degrees), which is obviously incorrect.
The solution to this dilemma is to treat the two phase angles as the argu-
ments of two complex numbers, add the two complex numbers, and determine
the sum’s argument (angle) to obtain the desired average phase angle. That is,
Average phase of αand β= arg[eja + ejb ], (11–20)
where the notation “arg[ejq]” means the phase angle of complex number ejq. Of
course, the complex addition in Eq. (11–20) is performed in rectangular form.
As an example, the average of phase angles α = 7π/8 radians and
β=–7π/8 radians is found by first computing the sum:

604 Signal Averaging
Average phase of αand β= arg[ej7p/8+ e–j7p/8]
= arg[(–0.9239 + j0.3827) + (–0.9239 + j0.3827)]
= arg[–1.8478] =
arg[1.8478e±jp
]. (11–20’)
So, from Eq. (11–20’), our average phase angle is ±πradians (±180 degrees).
11.5 FILTERING ASPECTS OF TIME-DOMAIN AVERAGING
To reinforce our concept of signal averaging, let’s reiterate that we want to im-
prove the accuracy (the correctness) of our measurement of some physical quan-
tity, but our repeated measurements (signal level samples) are contaminated by
random noise as shown in Figure 11–8. That random noise can be inherent in
the physical quantity that we’re measuring, or it could be caused by an imper-
fect measurement device (transducer). Sadly, both of these sources of random
noise are usually present in our real-world signal measurement activities.
Of course, we can improve the accuracy of our estimation of the true sig-
nal level in Figure 11–8 by summing a block of 100 measurement values and
dividing that sum by 100, which gives us a single 100-point average estimate
of the true signal level. However, in a real-time scenario we’d have to wait an-
other 100-sample time interval (100/f) before we could compute a new esti-
s
mated true signal level. To compute real-time signal averages at a sample rate
of f Hz (computing a new average value upon the arrival of each new mea-
s
surement value), we use digital filters.
These measurement-sample 3
values may represent: Repeated
- the return time of radar measurement values
pulses,
- samples from an audio 2
power meter,
- light intensity signal
from a photodiode,
- an FFT bin spectral 1
magnitude,
- temperature samples
from a thermocouple, 0 20 40 60 80
- etc. n (samples)
True signal level
Figure 11–8 A constant-level signal contaminated by random noise.

11.5 Filtering Aspects of Time-Domain Averaging 605
In Section 5.2 we introduced nonrecursive FIR filters with a moving av-
erage example, and there we learned that time-domain averaging performs
lowpass filtering. Figure 11–9(a) shows an N-point nonrecursive moving av-
erager implemented with an N-tap FIR filter structure. The N-point nonrecur-
sive moving averager’s output in time is expressed as
1
y(n)= [x(n)+x(n−1)+x(n−2)+x(n−3)+...+x(n−N++1)]
(11–21)
N
while its z-domain transfer function is
H (z)= Y(z) = 1 N∑ −1 z −n = 1 [1+z −1+z −2 +....+z −N+1], (11–22)
ma X(z) N N
n=0
where the “ma” subscript means moving average.
Figure 11–9(b) illustrates an N-point recursive moving averager. The re-
cursive moving averager has the sweet advantage that only two additions are
Nonrecursive moving averager Recursive moving averager
x(n) y(n)
x(n) y(n)
–
z–1
z–1 z–1 1/N
1/N
y(n–1)
z–1
z–1 Requires Requires
N–1 delay N delay
. . .
. . . elements elements
z–1
z–1 x(n–N+1) x(n–N)
(a) (b)
0.25
N = 4
art 1 N = 4
p
0.125 y
ar 0
n
0 gi
a
m
I –1
0 5 10 –1 0 1
Time Real part
(c) (d)
Figure 11–9 N-point moving averagers: (a) nonrecursive; (b) recursive; (c) N = 4
impulse response; (d) N = 4 z-plane zeros locations.

606 Signal Averaging
required per output sample, regardless of the number of delay elements. (So a
100-point moving averager, for example, can be built that requires only two
adds per output sample.) (Some people refer to both of our moving averagers
as “boxcar averagers.”)
An N-point recursive moving averager’s difference equation is
1
y(n)= [x(n)−x(n−N)]+y(n−1)
(11–23)
N
while its z-domaintransfer function is
1 1−z −N
H (z)= ⋅ , (11–24)
rma N 1−z −1
where the “rma” subscript means recursive moving average.
The nonrecursive and recursive moving averagers have identical time-
domain impulse responses and identical linear-phase frequency responses.
As such, H (z) =H (z). The nonrecursive and recursive moving averagers
ma rma
are merely two different implementations of the process known as an “N-
point moving average.” The unit impulse response and z-plane pole/zero
plot of N = 4 moving averagers are provided in Figures 11–9(c) and 11–9(d).
Please be aware of two issues regarding the nonrecursive and recursive
moving averagers. First, the delay line of the nonrecursive moving averager
will have N–1 delay elements, while the recursive moving averager will have
N delay elements. Second, the feedback in the recursive moving averager
means that, given certain x(n) signals, the y(n) output sequence can grow
large in magnitude. This means that when implementing a recursive moving
averager in fixed-point binary hardware we must test the process against our
expected input signals to determine if binary overflow errors occur at the out-
put of the second adder.
An agreeable feature of the moving averagers is that when N is an inte-
ger power of two, the multiplications by 1/N in Figure 11–9 can be imple-
mented with binary arithmetic right shifts, thus eliminating the multipliers
altogether.
Both moving averagers have identical frequency magnitude responses,
given by
1 sin(
πfN)
H (f) = (11–25)
ma N πfN
where the normalized frequency variable fis in the range of –0.5 to 0.5 corre-
sponding to a continuous-time frequency range of –f/2 to f/2 Hz. (We de-
s s
rived Eq. (11–25) in Section 7.5.1 when k = 0, and in Section 10.14.2.) That

11.5 Filtering Aspects of Time-Domain Averaging 607
response shows us why the averagers’ outputs have reduced noise fluctua-
tions. Figure 11–10 depicts a moving averaging filter’s frequency magnitude
responses for various values of N. Those curves are approximated by the
sin(x)/x-like curves we encountered so often in Chapter 3 because they are
the discrete Fourier transform (DFT) of an averager’s rectangular time-
domain impulse responses.
In Figure 11–10 we see the moving average filter has a passband cen-
tered at zero Hz, and as N increases, the filter becomes more and more nar-
rowband, attenuating more and more of the noise spectrum of an input
signal. The frequencies of the response nulls in Figure 11–10 for N = 4 (±f/4
s
and f/2) correspond to the locations of the z-plane zeros on the unit circle in
s
Figure 11–9(d). In the general case, the z-plane zeros on the unit circle for an
N-point moving averager will be located at angles
2π
θ zeros =k radians (11–26)
N
corresponding to magnitude response nulls at frequencies
f
f =k s Hz (11–26’)
nulls N
where k= 1, 2, 3, ..., N–1.
The output variance (noise power) properties of both moving averagers
abide by the important relationship of
σ2
σ2 = in . (11–27)
out N
Moving averager frequency magnitude response
1
N = 2
0.8
N = 4
N = 8
N = 16
0.2
0
0 f/8 f/4 f/2
s s Freq s
Figure 11–10 N-point moving averager frequency magnitude response as a
function of N.

608 Signal Averaging
While used in many applications seeking noise reduction through real-
time averaging, the above moving averagers have two shortcomings. First,
the number of points in the average, N, must be an integer, so if we desired a
noise-reducing frequency response somewhere between, say, N= 4 and N= 5
in Figure 11–10, we’re out of luck. Second, in real-time applications, these av-
eragers are sluggish (slow) in their time response to abrupt amplitude
changes in an input signal. One popular solution to these shortcomings is the
computationally efficient exponential averager. Please read on.
11.6 EXPONENTIAL AVERAGING
There is a kind of time-domain averaging that’s used in many applications—
it’s called exponential averaging[9–12]. This noise-reduction process, occasion-
ally called exponential smoothing, uses a simple recursive lowpass filter
described by the difference equation
y(n)= αx(n) + (1 – α)y(n–1) (11–28)
where y(n) is the current averager output sample, y(n–1) is the previous aver-
ager output sample, and αis a constant weighting factorin the range 0<α< 1.
The process described by Eq. (11–28) is implemented as shown in Figure 11–11.
With regard to noise-reduction filtering, the exponential averager has
three very appealing properties. First, unlike the nonrecursive and recursive
moving averagers described in the last section, the exponential averager per-
mits meaningful control over its frequency response, i.e., its noise-reduction
behavior. Second, the exponential averager requires fewer computations per
output sample than standard nonrecursive moving averagers; and third, the
exponential averager has greatly reduced memory requirements. Only one
delay element, i.e., one memory location, is needed by the exponential aver-
ager to store the y(n–1) sample.
The multiply by αoperation could be placed after rather than before the
feedback network, if we chose to do so.
x(n) y(n)
z 1
y( 1)
Figure 11–11 Exponential averager.

11.6 Exponential Averaging 609
11.6.1 Time-Domain Filter Behavior
The exponential averager’s name stems from its time-domain impulse re-
sponse. Let’s assume that the input to the averager is a long string of zeros,
and we apply a single sample of value 1 at time n=0. Then the input returns
again to a string of zero-valued samples. Now if the weighting factor is
α = 0.4, the averager’s output is the impulse response sequence in Figure
11–12. When n = 0, the input sample is multiplied by α, so the output is 0.4.
On the next clock cycle, the input is zero, and the old value of 0.4 is multi-
plied by (1 – 0.4), or 0.6 multiplied by (1 – 0.4), or 0.6 to provide an output
of 0.24. On the following clock cycle the input is zero and the previous output
of 0.24 is multiplied by 0.6 to provide a new output of 0.144. This continues
with the averager’s impulse response output falling off exponentially because
of the successive multiplications by 0.6.†
Auseful feature of the exponential averager is its capability to vary the
amount of noise reduction by changing the value of the αweighting factor. If
α equals one, input samples are not attenuated, past averager outputs are ig-
nored, and no averaging takes place. In this case the averager output re-
sponds immediately to changes at the input. As αis decreased in value, input
samples are attenuated and past averager outputs begin to affect the present
output. These past values represent an exponentially weighted sum of recent
inputs, and that summation tends to smooth out noisy signals. The smaller α
gets, the more noise reduction is realized. However, with smaller values for α,
the slower the averager is in responding to changes in the input. We can
demonstrate this behavior by looking at the exponential averager’s time-
domain step response as a function of αas shown in Figure 11–13.
Exp. averager impulse response
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0
0 1 2 3 4 5 6 7 8 9 Time
n
Figure 11–12 Exponential averager impulse response with α=0.4.
†We often see exponential decay in nature—everywhere from a capacitor discharging through a resis-
tor, the flow of heat, to the shrinkage of bubbles in a glass of beer. (See reference [13].)

610 Signal Averaging
Exponential averager output
1.0
0.8
0.9
0.8
0.6
0.7 0.4
= 0.2
0.6
= 0.1
0.5
0.4
0.3
0.2
0.1
0 2 4 6 8 10 12 14 16 18 Time
n
Figure 11–13 Exponential averager output versus α when a step input is applied
at time n=0.
As so often happens in signal processing, we have a trade-off. The more
the noise reduction, the more sluggish the averager will be in responding to
abrupt changes at the input. We can see in Figure 11–13 that as αgets smaller,
affording better noise reduction, the averager’s output takes longer to re-
spond and stabilize. Some test instrumentation manufacturers use a clever
scheme to resolve this noise reduction versus response time trade-off. They
use a large value for α at the beginning of a measurement so the averager’s
output responds immediately with a nonzero value. Then as the measure-
ment proceeds, the value of αis decreased in order to reduce the noise fluctu-
ations at the input.
The exponential averager’s noise variance reduction as a function of the
weighting factor αhas been shown to be[10,11]
outputnoisevariance α
= . (11–29)
inputnoisevariance 2−α
Equation (11–29) is useful because it allows us to determine αgiven some de-
sired averager noise variance (power) reduction. That is, if our desired noise
variance reduction factor is R, where R= (2 – α)/α, we can write
2
α= . (11–30)
R+1

11.6 Exponential Averaging 611
For example, if we want the output noise variance reduced by a factor of
R = 10, then α = 2/(10+1) = 0.182. The behavior of exponential averaging is
such that to achieve noise reduction roughly equivalent to an N-point moving
averager, we define αas
2
α= , forN >3. (11–31)
N+1
Considering the exponential averager’s noise power reduction in Eq.
(11–29) as an output signal-to-noise (SNR) increase, we can say the averager’s
output SNR increase (in dB) is
⎛ α ⎞
SNR =10⋅log ⎜ . (11–32)
exp(dB) 10⎝ 2−α⎠
Equation (11–32) is plotted in Figure 11–14 to illustrate the trade-off between
output noise reduction and averager response times.
To demonstrate the exponential averager’s output noise power reduc-
tion capabilities, Figure 11–15 shows the averager’s output with a low-
frequency (relative to the sample rate) cosine wave plus high-level noise as an
input. The weighting factor α starts out with a value of 1 and decreases lin-
early to a final value of 0.1 at the 180th data input sample. Notice that the
noise is reduced as αdecreases.
Output SNR increase in dB
16
14
12
10
8
Faster time response
6
4
Slower time response
2
0
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
Figure 11–14 Exponential averager output SNR increase as a function of the
weighting factor α.

612 Signal Averaging
= 1 = 0.45 = 0.1
2
0
–2
0 45 90 135 180 225
n
Figure 11–15 Exponential averager output noise reduction as αdecreases.
11.6.2 Frequency-Domain Filter Behavior
The reader may recognize the exponential averager as a 1st-order infinite im-
pulse response (IIR) digital filter. It has a z-domain transfer function of
α (11–33)
H (z)= .
exp 1−(1−α)z −1
Being a 1st-order IIR filter, the exponential averager has a single pole on the
z-plane located at z = 1–α as shown in Figure 11–16. When α is reduced in
value, the pole resides closer to the z-plane’s unit circle, giving us a narrower
lowpass passband width.
Setting zin Eq. (11–33) equal to
ejω
, we can write the frequency response
of the exponential averager as
α α (11–34)
H (ω)= = .
exp 1−(1−α)e −jω 1−(1−α)cos(ω)+ j(1−α)siin(ω)
Imag.
z-plane pole at
z
Real
–1 1
Figure 11–16 Exponential averager z-plane pole location.

11.6 Exponential Averaging 613
If we’re interested in the magnitude response of our averager, we can express
it as
α (11–35)
H (ω) = .
exp 1−2(1−α)cos(ω)+(1−α)2
Evaluating Eq. (11–35) over the normalized angular range of 0 ≤ω≤π(cor-
responding to a continuous-time frequency range of 0 to f/2 Hz), the frequency
s
magnitude responses of our exponential averager for various values of α are
shown in Figure 11–17(a). There we see that the averager’s DC gain, its gain at
zero Hz, is unity, which is just what we want for our noise-reduction applica-
tions. It’s worth noting that if we can tolerate a DC gain of 1/α, the multiply by
α in Figure 11–11 can be eliminated to reduce the averager’s computational
workload.
Magnitude response (linear)
1
= 0.9
ar 0.8
e
Li n 0.6 = 0.7
(a) 0.4 = 0.5
0.2 = 0.3
= 0.1
0
0 0.1f 0.2f 0.3f 0.4f 0.5f
s s s s s
Magnitude response (dB)
0
= 0.9
–5 = 0.7
B
d = 0.5
–10
= 0.3
(b) –15
–20
= 0.1
–25
0 0.1f 0.2f 0.3f 0.4f 0.5f
s s s s s
Phase response
0
= 0.9
s
e = 0.7
e–20
gr
e = 0.5
D
(c) –40 = 0.3
= 0.1
–60
0 0.1f 0.2f 0.3f 0.4f 0.5f
s s s s s
Figure 11–17 Exponential averager frequency response versus α: (a) normalized
magnitude response (linear); (b) normalized magnitude response
in dB; (c) phase response in degrees.

614 Signal Averaging
The exponential averager’s magnitude responses plotted on a logarith-
mic scale (dB) are provided in Figure 11–17(b). Notice as α decreases, the
exponential averager behaves more and more like a lowpass filter. Again, it is
from this behavior that the exponential averager’s noise-reduction properties
stem.
For those readers who prefer to think of a lowpass filter in terms of its
3 dB bandwidth, we can compute the appropriate value of the weighting fac-
tor αto achieve a desired exponential averaging filter 3 dB bandwidth. If f is
c
the desired positive cutoff frequency in Hz, where the exponential averager’s
frequency magnitude response is 3 dB below the averager’s zero-Hz re-
sponse, the value of αneeded to achieve such an f cutoff frequency is
c
α=cos(2f c /f s )−1+ cos2(2f c /f s )−4cos(2f c /f s ))+3 (11–36)
where f is the averager’s input sample rate in Hz.
s
To comment on the exponential averager’s nonlinear phase response:
We’re primarily concerned with the averager’s frequency response at zero
Hz. We want the averager to pass a zero-Hz (constant-amplitude) signal and
attenuate noise fluctuations riding on the constant-amplitude signal of inter-
est. As such, the exponential averager’s phase nonlinearity is usually of little
consequence.
11.6.3 Exponential Averager Application
I first encountered the exponential averager as lowpass filter in a specialized
real-time hardware spectrum analyzer application. That analyzer, using the
fast Fourier transform (FFT) shown in Figure 11–18, was similar in operation
to a graphics equalizer in home stereo systems. As Pspectral power estimates
were displayed on a computer monitor in real time, the common weighting
factor α (used by each exponential averager) could be increased to speed the
display’s response to abrupt changes in the spectral content in x(n). Then
again, αcould be reduced to minimize abrupt fluctuations (reduced variance)
in the P power samples, yielding a slowly changing (sluggish) spectral dis-
play. The notation in Figure 11–18 is such that X (n) represents the pth FFT
p
bin’s complex sample value at the nth instant in time.
In this application the exponential averagers were referred to as “leaky
integrators” and, again, their nonlinear phase was unimportant. Their only
purpose in life was to reduce the fluctuations in the real-time |X (n)|2power
p
samples by means of lowpass filtering.
As an example of their utility, exponential averagers are used when we
swipe our charge cards through a magnetic stripe reader (MSR). The analog
signal from the magnetic read head is digitized with an A/D converter, and

References 615
X(n) |X(n)|2
0 Power 0 Exp.
estimation averager
X(n) |X(n)|2
1 Power 1 Exp.
N- estimation averager
To
x(n) point X 2 (n) Power |X 2 (n)|2 Exp. spectral
FFT estimation averager display
. . .
. . .
X (n) |X (n)|2
P–1 Power P–1 Exp.
estimation averager
a
Figure 11–18 An application of exponential averaging.
the discrete samples are exponentially averaged before the binary data (ones
and zeros) detection process is performed[14].
To conclude this section, we inform the reader that Section 13.33 pre-
sents computationally efficient implementations of exponential averagers.
REFERENCES
[1] Miller, I., and Freund, J. Probability and Statistics for Engineers, 2nd ed., Prentice Hall, Engle-
wood Cliffs, New Jersey, 1977, p. 118.
[2] Beller, J., and Pless, W. “AModular All-Haul Optical Time-Domain Reflectometer for Char-
acterizing Fiber Links,” Hewlett-Packard Journal, February 1993.
[3] Spiegel, M. R. Theory and Problems of Statistics, Shaum’s Outline Series, McGraw-Hill, New
York, 1961, p. 142.
[4] Papoulis, A. Probability, Random Variables, and Stochastic Processes, McGraw-Hill, New York,
1984, p. 245.
[5] Davenport, W. B., Jr., and Root, W. L. Random Signals and Noise, McGraw-Hill, New York,
1958, pp. 81–84.
[6] Welch, P. D. “The Use of Fast Fourier Transform for the Estimation of Power Spectra: A
Method Based on Time Averaging over Short, Modified Periodograms,” IEEE Transactions
on Audio and Electroacoust., Vol. AU-15, No. 2, June 1967.
[7] Harris, F. J. “On the Use of Windows for Harmonic Analysis with the Discrete Fourier
Transform,” Proceedings of the IEEE, Vol. 66, No. 1, January 1978.
[8] Kootsookos, P. “The Nature of Circles,” DSPrelated blog, http://www.dsprelated.com/
showarticle/57.php.

616 Signal Averaging
[9]Booster, D. H., et al. “Design of a Precision Optical Low-Coherence Reflectometer,”
Hewlett-Packard Journal, February 1993.
[10] Witte, R. A. “Averaging Techniques Reduce Test Noise, Improve Accuracy,” Microwaves &
RF, February 1988.
[11] Oxaal, J. “Temporal Averaging Techniques Reduce Image Noise,” EDN, March 17, 1983.
[12] Lymer, A. “Digital-Modulation Scheme Processes RF Broadcast Signals,” Microwaves & RF,
April 1994.
[13] Leike, A. “Demonstration of the Exponential Decay Law Using Beer Froth,” European Jour-
nal of Physics, Vol. 23, January 2002, pp. 21–26.
[14] Silicon Laboratories Inc. “Magnetic Stripe Reader,” Application Note: AN148.

Chapter 11 Problems 617
CHAPTER 11 PROBLEMS
11.1 Assume we have a four-sample x(n) sequence, where index n is 1 ≤ n ≤ 4,
whose samples are
x(1) = 1, x(2) = 2, x(3) = 3, x(4) = 4.
(a) What is the average of x(n)?
(b) What is the variance of x(n)?
(c) What is the standard deviation of x(n)?
11.2 This problem illustrates an important characteristic of the quantity known as
the average (mean value) of a sequence of numbers. Suppose we have a six-
sample x(n) sequence, where index nis 1≤n≤6, defined by
x(1) = 1, x(2) = –2, x(3) = 3, x(4) = –4, x(5) = 6, x(6) = unspecified,
and the average of x(n) is x = 4. (Note that the sixth sample in x(n) is not
ave
explicitly defined.) The difference between x(n) and x is the sequence d (n)
ave iff
= x(n) –x , given as
ave
d (1) = –3, d (2) = –6, d (3) = –1, d (4) = –8, d (5) = 2, d (6) = unspecified.
iff iff iff iff iff iff
(a) What is the value of d (6)? Justify your answer.
iff
Hint: The discussion of sequence averages in Appendix D’s Section D.1 will
be helpful here.
(b) What is the value of x(6)?
11.3 Let’s look at an important topic regarding averaging. Assume we have two
N-point discrete sequences, x(n) and y(n), where index nis 1≤n≤N, and the
N-point averages of the two sequences are
1
∑N
1
∑N
x = x(n), and y = y(n).
ave N ave N
n=1 n=1
Next, let’s add the two sequences, element for element, to obtain a new
N-point sequence z(n) =x(n) +y(n). Is it correct to say that the average of
z(n), defined as
1
∑N
z = z(n),
ave N
n=1

618 Signal Averaging
is equal to the sum of x and y ? (In different words, we’re asking, “Is the
ave ave
average of sums equal to the sum of averages?”) Explain how you arrived at
your answer.
Note: This problem is not “busy work.” If the above statement z = x +
ave ave
y is true, it tells us that the average of a noisy signal is equal to the average
ave
of the noise-free signal plus the average of the noise.
11.4 Suppose we had three unity-magnitude complex numbers whose phase an-
gles are π/4 radians, –3π/4 radians, and –π/4 radians. What is the average
phase angle, measured in degrees, of the three phase angles? Show your
work.
11.5 Assume we’re averaging magnitude samples from multiple FFTs (fast Fourier
transforms) and we want the variance of the averaged FFT magnitudesto be re-
duced below the variance of single-FFT magnitudes by a factor of 20. That is,
we want
σ2 = ⎛ ⎜ 1 ⎞ ⎟⋅(σ2 ).
kFFTs ⎝ 20 ⎠ singleFFT
How many FFTs, k, must we compute and then average their magnitude
samples?
11.6 Concerning the moving averager filters in the text’s Figure 11–9, we stated
that their transfer functions are equal. Prove that H (z) =H (z).
ma rma
Hint: H (z) is a geometric series that we’d like to represent as a closed-form
ma
equation. To obtain a closed-form equation for a geometric series, start by
looking up geometric seriesin the Index.
11.7 If we remove the 1/N multiplier from the recursive moving averager in the
text’s Figure 11–9(b), the remaining structure is called a recursive running sum.
To exercise your digital network analysis skills, plot the frequency magnitude
responses of a recursive running sum system for N = 4, 8, and 16 as we did
in Figure 11–10.
Hint: The frequency response of a recursive running sum network is, of
course, the discrete Fourier transform (DFT) of the network’s rectangular im-
pulse response. Note that the recursive running sum network’s magnitude re-
sponse curves will be similar, but not equal, to the curves in Figure 11–10.
11.8 In the text we said that the phase responses of both nonrecursive and re-
cursive N-point moving averagers are linear. Why is it valid to make that
statement?

Chapter 11 Problems 619
11.9 Draw a rough sketch of the frequency magnitude response, over the positive-
frequency range, of a three-point moving averager. Clearly show the fre-
quency magnitude response at f/2 Hz.
s
Note: The locations of the frequency response nulls are defined by the loca-
tions of the averager’s transfer function zeros on its z-plane unit circle.
11.10 Think about building a two-stage filter comprising a four-point moving aver-
ager in cascade (series) with a two-point moving averager.
(a) Draw a rough sketch of the frequency magnitude response of the two-
stage filter.
(b) Does the cascaded filter have a linear phase response? Justify your
answer.
11.11 Let’s assume we’re measuring a constant-level, but very noisy, temperature
signal from a thermocouple and we wish to reduce the noise variance (power)
of our measurements by 13 dB.
(a) What is the number of delay elements needed in a nonrecursive moving
average filter to achieve the desired measurement-noise reduction?
(b) What is the number of delay elements needed in a recursive moving aver-
age filter to achieve the desired measurement-noise reduction?
(c) What is the value of the α weighting factor in a standard exponential av-
erager to achieve the desired measurement-noise reduction?
(d) Fill in the following table describing the implementation requirements to
achieve measurement-noise variance reduction of 13 dB.
Implementation Requirements for 13 dB Noise Reduction
Computations per Nonrecursive Recursive
output sample and moving moving Exponential
memory requirements averager averager averager
Multiplies
Additions
Data memory locations
11.12 Regarding the exponential averaging filter, when α=0, the filter’s single pole
lies right on the z-plane unit circle. In Chapter 6 and Chapter 7 we discussed
that having digital filter poles on the unit circle can lead to filter stability
problems because quantizing a filter’s coefficients to a fixed-width binary

620 Signal Averaging
word representation can sometimes cause the poles to reside just outside the
unit circle. Why does using α=0 cause no stability problems for us when we
use exponential averagers?
11.13 In the text we stated that an alternate version of an exponential averager,
shown in Figure P11–13, has a DC (zero Hz) gain of 1/α. Prove that this DC
gain factor of 1/αis correct.
w(n) + y(n)
z
Figure P11–13
11.14 Show how to derive the equation for the frequency magnitude response of an
exponential averager whose weighting factor is α.
11.15 Explain why it’s valid to call the exponential averager in Figure P11–15(a),
where for example α = 0.4, by the name leaky integrator compared to a stan-
dard (rectangular rule) integrator shown in Figure P11–15(b)?
Hint:Compare the impulse responses of the two networks.
Exponential averager, = 0.4 Rectangular rule integrator
x(n) y(n) x(n) y(n)
z–1 z–1
= 0.4 = 0.4
y(n–1) y(n–1)
= 0.6
(a) (b)
Figure P11–15

Chapter 11 Problems 621
11.16 Here are (somewhat) challenging problems regarding the exponential aver-
ager in Figure P11–16:
x(n) y(n)
z–1
Figure P11–16
(a) Derive an algebraic expression for the exponential averager’s time-
domain response to a unity-valued input sample applied at time n= 0 fol-
lowed by all zero-valued input samples. Use the term h(n) to represent
this impulse response, where nis the time-domain index. (Assume the out-
put of the z–1delay element is zero at time n= 0.)
(b) Use your h(n) expression from Part (a) to determine the exponential aver-
ager’s gain at zero Hz (DC gain).
Hint: Recall the relationship between a filter’s impulse response and its gain
at zero Hz.
(c) Comment on how the value of the α weighting factor affects the aver-
ager’s gain at zero Hz (DC gain).

This page intentionally left blank

CHAPTER TWELVE
Digital Data
•
••
Formats and
Their Effects
•••
In digital signal processing, there are many ways to represent numerical data
in computing hardware. These representations, known as data formats, have a
profound effect on the accuracy and ease of implementation of any given sig-
nal processing algorithm. The simpler data formats enable uncomplicated
hardware designs to be used at the expense of a restricted range of number
representation and susceptibility to arithmetic errors. The more elaborate data
formats are somewhat difficult to implement in hardware, but they allow us to
manipulate very large and very small numbers while providing immunity to
many problems associated with digital arithmetic. The data format chosen for
any given application can mean the difference between processing success and
failure—it’s where our algorithmic rubber meets the road.
In this chapter, we’ll introduce the most common types of fixed-point
digital data formats and show why and when they’re used. Next, we’ll use
analog-to-digital (A/D) converter operations to establish the precision and
dynamic range afforded by these fixed-point formats along with the inherent
errors encountered with their use. Finally, we’ll cover the interesting subject
of floating-pointbinary formats.
12.1 FIXED-POINT BINARY FORMATS
Within digital hardware, numbers are represented by binary digits known as
bits—in fact, the term bit originated from the words Binary digIT. Asingle bit
can be in only one of two possible states: either a one or a zero.† A six-bit
†Binary numbers are used because early electronic computer pioneers quickly realized that it
was much more practical and reliable to use electrical devices (relays, vacuum tubes, transistors,
etc.) that had only two states, onor off. Thus, the on/off state of a device could represent a single
binary digit.
623

624 Digital Data Formats and Their Effects
binary number could, for example, take the form 101101, with the leftmost bit
known as the most significant bit (msb); the rightmost bit is called the least sig-
nificant bit (lsb). The number of bits in a binary number is known as the word
length—hence 101101 has a word length of six. Like the decimal number sys-
tem so familiar to us, the binary number system assumes a weight associated
with each digit in the number. That weight is the base of the system (two for
binary numbers and ten for decimal numbers) raised to an integral power. To
illustrate this with a simple example, the decimal number 4631 is
⋅ ⋅ ⋅ ⋅
(4 103) + (6 102) + (3 101) + (1 100)
= 4000 + 600 + 30 + 1 = 4631. (12–1)
The factors 103, 102, 101, and 100 are the digit weights in Eq. (12–1). Similarly,
the six-bit binary number 101101 is equal to decimal 45 as shown by
⋅ ⋅ ⋅ ⋅ ⋅ ⋅
(1 25) + (0 24) + (1 23) + (1 22) + (0 21) + (1 20)
= 32 + 8 + 4 + 1 = 45. (12–2)
Using subscripts to signify the base of a number, we can write Eq. (12–2) as
101101 = 45 . Equation (12–2) shows us that, like decimal numbers, binary
2 10
numbers use the place value system where the position of a digit signifies its
weight. If we use B to denote a number system’s base, the place value repre-
sentation of the four-digit number a a a a is
3 2 1 0
⋅ ⋅ ⋅ ⋅
(a B3) + (a B2) + (a B1) + (a B0). (12–3)
3 2 1 0
In Eq. (12–3), Bn is the weight multiplier for the digit a , where 0 ≤ a ≤ B–1.
n n
(This place value system of representing numbers is very old—so old, in fact,
that its origin is obscure. However, with its inherent positioning of the deci-
mal or binary point, this number system is so convenient and powerful that
its importance has been compared to that of the alphabet[1].)
12.1.1 Octal Numbers
As the use of minicomputers and microprocessors rapidly expanded in the
1960s, people grew tired of manipulating long strings of ones and zeros on
paper and began to use more convenient ways to represent binary numbers.
One way to express a binary number is an octal format, with its base of eight.
(Of course, the only valid digits in the octal format are 0 to 7—the digits 8 and
9 have no meaning in octal representation.)
Converting from binary to octal is as simple as separating the binary
number into three-bit groups starting from the right. For example, the binary
number 10101001 can be converted to octal format as
2
10101001 2 → 10 | 101 | 001 = 251 8 .

12.1 Fixed-Point Binary Formats 625
Thus the octal format enables us to represent an eight-digit binary value
witha simplerthree-digitoctalvalue.However,therelentlessmarchoftechnol-
ogyispushingoctalnumbers,likewoodentennisrackets,intoextinction.
12.1.2 Hexadecimal Numbers
Today the predominant binary number representation formatis the hexadec-
imal number format using 16 as its base. Converting from binary to hexa-
decimal is done, this time, by separating the binary number into four-bit
groups starting from the right. The binary number 10101001 is converted
2
to hexadecimal format as
10101001 2 → 1010 | 1001 = A9 16 .
If you haven’t seen the hexadecimal format used before, don’t let the A9 dig-
its confuse you. In this format, the characters A, B, C, D, E, and F represent
the digits whose decimal values are 10, 11, 12, 13, 14, and 15 respectively. We
convert the two groups of bits above to two hexadecimal digits by starting
with the left group of bits, 1010 =10 =A , and 1001 =9 =9 . Hexadeci-
2 10 16 2 10 16
mal format numbers also use the place value system, meaning that A9 =
⋅ ⋅ 16
(A 161 + 9 160). For convenience, then, we can represent the eight-digit
10101001 with the two-digit number A9 . Table 12–1 lists the permissible
2 16
digit representations in the number systems discussed thus far.
In the above example we used a subscripted 16 to signify a hexadecimal
number. Note that it’s common, in the literature of binary number formats, to
havehexadecimalnumbersprecededbyspecialcharacterstosignifythatindeed
they are hexadecimal. You may see, for example, numbers like $A9 or 0xA9
wherethe“$”and“0x”charactersspecifythefollow-ondigitstobehexadecimal.
12.1.3 Sign-Magnitude Binary Format
For binary numbers to be at all useful in practice, they must be able to repre-
sent negative values. Binary numbers do this by dedicating one of the bits in
a binary word to indicate the sign of a number. Let’s consider a popular bi-
nary format known as sign magnitude. Here, we assume that a binary word’s
leftmost bit is a sign bit and the remaining bits represent the magnitude of a
number that is always positive. For example, we can say that the four-bit
number 0011 is +3 and the binary number 1011 is equal to –3 , or
2 10 2 10
magnitude bits magnitude bits
↓↓↓ ↓↓↓
0011
2
=3
10
, and 1011
2
=−3
10
.
↑ ↑
sign bit of zero sign bit of one
signifies positive signifies negative

626 Digital Data Formats and Their Effects
Table 12–1Allowable Digit Representations versus Number System Base
Decimal
Binary Octal Decimal Hexadecimal equivalent
0 0 0 0 0
1 1 1 1 1
2 2 2 2
3 3 3 3
4 4 4 4
5 5 5 5
6 6 6 6
7 7 7 7
8 8 8
9 9 9
A 10
B 11
C 12
D 13
E 14
F 15
Of course, using one of the bits as a sign bit reduces the magnitude of the
numbers we can represent. If an unsigned binary number’s word length is b
bits, the number of different values that can be represented is 2b. An eight-bit
word, for example, can represent 28 = 256 different integral values. With zero
being one of the values we have to express, a b-bit unsigned binary word can
represent integers from 0 to 2b–1. The largest value represented by an un-
signed eight-bit word is 28–1 = 255 = 11111111 . In the sign-magnitude bi-
10 2
nary format a b-bit word can represent only a magnitude of ±2b–1–1, so the
largest positive or negative value we can represent by an eight-bit sign-
magnitude word is ±28–1–1=±127.
12.1.4 Two’s Complement Format
Another common binary number scheme, known as the two’s complementfor-
mat, also uses the leftmost bit as a sign bit. The two’s complement format is
the most convenient numbering scheme from a hardware design standpoint
and has been used for decades. It enables computers to perform both addi-
tion and subtraction using the same hardware adder logic. To obtain the neg-

12.1 Fixed-Point Binary Formats 627
ative version of a positive two’s complement number, we merely comple-
ment (change a one to a zero, and change a zero to a one) each bit, add a bi-
nary one to the complemented word, and discard any bits carried beyond
the original word length. For example, with 0011 representing a decimal 3 in
2
two’s complement format, we obtain a negative decimal 3 through the fol-
lowing steps:
+3 in two’s complement → 0 0 1 1
complement of +3 → 1 1 0 0
add one → +0 0 0 1
–3 in two’s complement → 1 1 0 1.
In the two’s complement format, a b-bit word can represent positive ampli-
tudes as great as 2b–1–1, and negative amplitudes as large as –2b–1. Table 12–2
shows four-bit word examples of sign-magnitude and two’s complement bi-
nary formats.
While using two’s complement numbers, we have to be careful when
adding two numbers of different word lengths. Consider the case where a
four-bit number is added to an eight-bit number:
+15 in two’s complement → 0 0 0 0 1 1 1 1
add +3 in two’s complement → +0 0 1 1
+18 in two’s complement → 0 0 0 1 0 0 1 0.
No problem so far. The trouble occurs when our four-bit number is negative.
Instead of adding a +3 to the +15, let’s try to add a –3 to the +15:
+15 in two’s complement → 0 0 0 0 1 1 1 1
add a –3 in two’s complement → +1 1 0 1
+28 in two’s complement → 0 0 0 1 1 1 0 0. ←Wrong answer
The above arithmetic error can be avoided by performing what’s called a sign-
extendoperation on the four-bit number. This process, typically performed au-
tomatically in hardware, extends the sign bit of the four-bit negative number
to the left, making it an eight-bit negative number. If we sign-extend the –3
and then perform the addition, we’ll get the correct answer:
+15 in two’s complement → 0 0 0 0 1 1 1 1
add a sign-extended –3 in two’s complement → +1 1 1 1 1 1 0 1
+12 in two’s complement → 1 0 0 0 0 1 1 0 0. ←That’s better
↑
overflow bit is ignored
12.1.5 Offset Binary Format
Another useful binary number scheme is known as the offset binary format.
While this format is not as common as two’s complement, it still shows up in

628 Digital Data Formats and Their Effects
some hardware devices. Table 12–2 shows offset binary format examples for
four-bit words. Offset binary represents numbers by subtracting 2b–1 from an
unsigned binary value. For example, in the second row of Table 12–2, the off-
set binary number is 1110 . When this number is treated as an unsigned
2
binary number, it’s equivalent to 14 . For four-bit words b=4 and 2b–1=8, so
10
14 – 8 = 6 , which is the decimal equivalent of 1110 in offset binary. The
10 10 10 2
difference between the unsigned binary equivalent and the actual decimal
equivalent of the offset binary numbers in Table 12–2 is always –8. This kind
of offset is sometimes referred to as a bias when the offset binary format is
used. (It may interest the reader that we can convert back and forth between
the two’s complement and offset binary formats merely by complementing a
word’s most significant bit.)
The history, arithmetic, and utility of the many available number for-
mats is a very broad field of study. Athorough and very readable discussion
of the subject is given by Knuth in reference [2].
Table 12–2 Integer Binary Number Formats
Decimal equivalent Sign-magnitude Two’s complement Offset binary
7 0111 0111 1111
6 0110 0110 1110
5 0101 0101 1101
4 0100 0100 1100
3 0011 0011 1011
2 0010 0010 1010
1 0001 0001 1001
+0 0000 0000 1000
–0 1000 — —
–1 1001 1111 0111
–2 1010 1110 0110
–3 1011 1101 0101
–4 1100 1100 0100
–5 1101 1011 0011
–6 1110 1010 0010
–7 1111 1001 0001
–8 — 1000 0000

12.1 Fixed-Point Binary Formats 629
12.1.6 Fractional Binary Numbers
All of the binary numbers we’ve considered so far had integer decimal val-
ues. Noninteger decimal numbers, numbers with nonzero digits to the right
of the decimal point, can also be represented with binary numbers if we use a
binary point, also called a radix point, identical in function to our familiar deci-
mal point. (As such, in the binary numbers we’ve discussed so far, the binary
point is assumed to be fixed just to the right of the rightmost, lsb, bit.) For ex-
ample, using the symbol (cid:2) to denote a binary point, the six-bit unsigned bi-
nary number 11 0101 is equal to decimal 3.3125 as shown by
(cid:2) 2
(1⋅21)+(1⋅20)+(0⋅2 −1)+(1⋅2 −2)+(0⋅2 −3)+(1⋅2 −44)
( ) ( ) ⎛ 1⎞ ⎛ 1⎞ ⎛ 1⎞⎞ ⎛ 1 ⎞
= 1⋅2 + 1⋅1 +⎜0⋅ ⎟ +⎜1⋅ ⎟ +⎜0⋅ ⎟ +⎜1⋅ ⎟
⎝ ⎠ ⎝ ⎠ ⎝ ⎠ ⎝ ⎠
2 4 8 16
= 2 + 1 + 0 + 0.25 + 0 + 0.0625 = 3.3125 . (12–4)
10
For our 11 0101 example in Eq. (12–4) the binary point is set between the sec-
(cid:2) 2
ond and third most significant bits and we call that binary number a fractional
number. Having a stationary position for the binary point is why this binary
number format is called fixed-pointbinary. The unsigned number 11 0101 has
(cid:2) 2
two integer bits and four fractional bits, so, in the parlance of binary num-
bers, such a number is said to have a 2.4, “two dot four,” format (two integer
bits and four fractional bits).
Two’s complement binary numbers can also have this integer plus frac-
tion format, and Table 12–3 shows, for example, the decimal value ranges for
all possible eight-bit two’s complement fractional binary numbers. Notice
how the 8.0-format row in Table 12–3 shows the decimal values associated
with an eight-bit two’s complement binary number whose binary point is to
the right of the lsb, signifying an all-integer binary number. On the other
hand, the 1.7-format row in Table 12–3 shows the decimal values associated
with an eight-bit two’s complement binary number whose binary point is just
to the right of the msb (the sign bit), signifying an all-fraction binary number.
The decimal value range of a general fractional two’s complement bi-
nary number is
–2(# of integer bits – 1)≤decimal value ≤2(# of integer bits – 1)
– 2–(# of fraction bits), (12–5)
where the “# of integer bits” notation means the number of bits to the left of
the binary point and “# of fraction bits” means the number of bits to the right
of the binary point.
Table 12–3 teaches us two important lessons. First, we can place the
implied binary point anywhere we wish in the eight-bit word, just so long as

630 Digital Data Formats and Their Effects
Table 12–3 Eight-Bit, Two’s Complement, Fractional Format Values
Number of Maximum Maximum
Fract. integer bits Number of positive negative Lsb
binary (including fractional decimal decimal decimal
format sign bit) bits value value value
8.0 8 0 127.0 –128.0 1.0
7.1 7 1 63.5 –64.0 0.5
6.2 6 2 31.75 –32.0 0.25
5.3 5 3 15.875 –16.0 0.125
4.4 4 4 7.9375 –8.0 0.0625
3.5 3 5 3.96875 –4.0 0.03125
2.6 2 6 1.984375 –2.0 0.015625
1.7 1 7 0.9921875 –1.0 0.0078125
everyone accessing the data agrees on that binary point placement and the
designer keeps track of that placement throughout all of the system’s arith-
metic computations. Binary arithmetic hardware behavior does not depend
on the “agreed upon” binary point placement. Stated in different words, bi-
nary point placement does not affect two’s complement binary arithmetic op-
erations. That is, adding or multiplying two binary numbers will yield the
same binary result regardless of the implied binary point location within the
data words. We leave an example of this behavior as a homework problem.
Second, for a fixed number of bits, fractional two’s complement binary
numbers allow us to represent decimal numbers with poor precision over a
wide range of values, or we can represent decimal numbers with fine preci-
sion but only over a narrow range of values. In practice you must “pick your
poison” by choosing the position of the binary point based on what’s more
important to you, number range or number precision.
Due to their 16-bit internal data paths, it’s very common for program-
mable 16-bit DSPchips to use a 1.15 format (one integer bit to represent sign,
and 15 fractional bits) to represent two’s complement numbers. These 16-bit
signed all-fractionbinary numbers are particularly useful because multiplying
two such numbers results in an all-fraction product, avoiding any unpleasant
binary overflowproblems, to be discussed shortly. (Be aware that this 1.15 for-
mat is also called Q15 format.) Because the 1.15-format is so commonly used
in programmable hardware, we give examples of it and other 16-bit formats
in Table 12–4. In that table, the “resolution” is the decimal value of the for-
mat’s lsb.
Multiplication of two 1.15 binary words results in a 2.30-format (also
called a Q30-format) fractional number. That 32-bit product word contains two

12.1 Fixed-Point Binary Formats 631
sign bits and 30 fractional bits, with the msb being called an extended sign
bit. We have two ways to convert (truncate) such a 32-bit product to the 1.15
format so that it can be stored as a 16-bit word. They are
• shifting the 32-bit word left by one bit and storing the upper 16 bits, and
• shifting the 32-bit word right by 15 bits and storing the lower 16 bits.
Table 12–4 16-Bit Format Values
16-bit binary format Decimal value Binary Hex
Two’s complement 0.9999694824... 0 111 1111 1111 1111 7FFF
(cid:2)
1.15 (Q15) format 0.5 0 100 0000 0000 0000 4000
(cid:2)
0.25 0 010 0000 0000 0000 2000
(cid:2)
resolution → 0.0000305175... 0 000 0000 0000 0001 0001
(cid:2)
0 0 000 0000 0000 0000 0000
(cid:2)
–0.0000305175... 1 111 1111 1111 1111 FFFF
(cid:2)
–0.25 1 110 0000 0000 0000 E000
(cid:2)
–0.5 1 100 0000 0000 0000 C000
(cid:2)
–1.0 1 000 0000 0000 0000 8000
(cid:2)
Standard two’s 32767 0111 1111 1111 1111 7FFF
(cid:2)
complement integer 16384 0100 0000 0000 0000 4000
(cid:2)
(16.0 format) 8192 0010 0000 0000 0000 2000
(cid:2)
resolution → 1 0000 0000 0000 0001 0001
(cid:2)
0 0000 0000 0000 0000 0000
(cid:2)
–1 1111 1111 1111 1111 FFFF
(cid:2)
–8192 1110 0000 0000 0000 E000
(cid:2)
–16384 1100 0000 0000 0000 C000
(cid:2)
–32767 1000 0000 0000 0001 8001
(cid:2)
–32768 1000 0000 0000 0000 8000
(cid:2)
Unsigned integer 65535 1111 1111 1111 1111 FFFF
32768 1000 0000 0000 0000 8000
32767 0111 1111 1111 1111 7FFF
resolution → 1 0000 0000 0000 0001 0001
0 0000 0000 0000 0000 0000

632 Digital Data Formats and Their Effects
To conclude this fractional binary discussion, we provide the steps to
convert a decimal number whose magnitude is less than one, such as an FIR
digital filter coefficient, to the 1.15 binary format. As an example, to convert
the decimal value 0.452 to the two’s complement 1.15 binary format:
1. Multiply the absolute value of the original decimal number 0.452 by
32768 (215), yielding a scaled decimal 14811.136.
2. Round the value 14811.136 to an integer, using your preferred rounding
method, producing a scaled decimal value of 14811.
3. Convert the decimal 14811 to a binary integer and place the binary point
to the right of the msb, yielding 0 011 1001 1101 1011 (39DB ).
(cid:2) 16
4. If the original decimal value was positive, stop now. If the original deci-
mal value was negative, implement a two’s complement conversion by
inverting Step 3’s binary bits and add one.
If you, unfortunately, do not have software to perform the above positive dec-
imal integer to 1.15 binary conversion in Step 3, here’s how the conversion
can be done (painfully) by hand:
3.1. Divide 14811 by 2, obtaining integer 7405 plus a remainder of 0.5. Be-
cause the remainder is not zero, place a one as the lsb of the desired bi-
nary number. Our binary number is 1.
3.2. Divide 7405 by 2, obtaining integer 3702 plus a remainder of 0.5. Because
the remainder is not zero, place a one as the bit to the left of the lsb bit
established in Step 3.1 above. Our binary number is now 11.
3.3. Divide 3702 by 2, obtaining integer 1851 plus a remainder of zero. Be-
cause the remainder is zero, place a zero as the bit to the left of the bit es-
tablished in Step 3.2 above. Our binary number is now 011.
3.4. Continue this process until the integer portion of the divide-by-two quo-
tient is zero. Append zeros to the left of the binary word to extend its
length to 16 bits.
Using the above steps to convert decimal 14811 to binary 1.15 format
10
proceeds as shown in Table 12–5, producing our desired binary number of
0 011 1001 1101 1011 (39DB ).
(cid:2) 16
12.2 BINARY NUMBER PRECISION AND DYNAMIC RANGE
As we implied earlier, for any binary number format, the number of bits in a
data word is a key consideration. The more bits used in the word, the better
the resolution of the number, and the larger the maximum value that can be

12.2 Binary Number Precision and Dynamic Range 633
Table 12–5 Decimal 14811 to Binary 1.15 Conversion Example
Operation Binary word
14811 ÷2 = 7405 + remainder of 0.5 → 1
7405 ÷2 = 3702 + remainder of 0.5 → 11
3702 ÷2 = 1851 + remainder of 0 → 011
1851 ÷2 = 925 + remainder of 0.5 → 1011
925 ÷2 = 462 + remainder of 0.5 → 1 1011
462 ÷2 = 231 + remainder of 0 → 01 1011
231 ÷2 = 115 + remainder of 0.5 → 101 1011
115 ÷2 = 57 + remainder of 0.5 → 1101 1011
57 ÷2 = 28 + remainder of 0.5 → 1 1101 1011
28 ÷2 = 14 + remainder of 0 → 01 1101 1011
14 ÷2 = 7 + remainder of 0 → 001 1101 1011
7 ÷2 = 3 + remainder of 0.5 → 1001 1101 1011
3 ÷2 = 1 + remainder of 0.5 → 1 1001 1101 1011
1 ÷2 = 0 + remainder of 0.5 → 11 1001 1101 1011
Append two msb zeros (We’re done!) 0 011 1001 1101 1011
(cid:2)
represented.†Assuming that a binary word represents the amplitude of a sig-
nal, digital signal processing practitioners find it useful to quantify the dy-
namic range of various binary number schemes. For a signed integer binary
word length of b+1 bits (one sign bit and b magnitude bits), the dynamic
range is defined by
largestpositivewordvalue
dynamicrange =
linear smallestpositivewordvalue
(12–6)
2b −1
= = 2b −1.
1
⋅
†Some computers use 64-bit words. Now, 264is approximately equal to 1.8 1019—that’s a pretty
large number. So large, in fact, that if we started incrementing a 64-bit counter once per second
at the beginning of the universe (≈20 billion years ago), the most significant four bits of this
counter would stillbe all zeros today.

634 Digital Data Formats and Their Effects
The dynamic range measured in dB is
dynamic range = 20 · log (dynamic range )
dB 10 linear
= 20 · log (2b–1). (12–6’)
10
When 2bis much larger than 1, we can ignore the –1 in Eq. (12–6’) and state that
dynamic range = 20 · log (2b)
dB 10
= 20 · log (2) ·b= 6.02 · bdB. (12–6’’)
10
Equation (12–6’’), dimensioned in dB, tells us that the dynamic range of our
number system is directly proportional to the word length. Thus, an eight-bit
two’s complement word, with seven bits available to represent signal magni-
⋅
tude, has a dynamic range of 6.02 7 = 42.14 dB. Most people simplify
Eq. (12–6’’) by using the rule of thumb that the dynamic range is equal to “6
dB per bit.”
12.3 EFFECTS OF FINITE FIXED-POINT BINARY WORD LENGTH
The effects of finite binary word lengths touch all aspects of digital signal pro-
cessing. Using finite word lengths prevents us from representing values with
infinite precision, increases the background noise in our spectral estimation
techniques, creates nonideal digital filter responses, induces noise in analog-
to-digital (A/D) converter outputs, and can (if we’re not careful) lead to
wildly inaccurate arithmetic results. The smaller the word lengths, the greater
these problems will be. Fortunately, these finite, word-length effects are rather
well understood. We can predict their consequences and take steps to mini-
mize any unpleasant surprises. The first finite, word-length effect we’ll cover
is the errors that occur during the A/D conversion process.
12.3.1 A/D Converter Quantization Errors
Practical A/D converters are constrained to have binary output words of fi-
nite length. Commercial A/D converters are categorized by their output
word lengths, which are normally in the range from 8 to 16 bits. A typical
A/D converter input analog voltage range is from –1 to +1 volt. If we used
such an A/D converter having 8-bit output words, the least significant bit
would represent
full voltage range 2 volts
lsb value= = =7.81 millivolts. (12–7)
2word length 28
What this means is that we can represent continuous (analog) voltages per-
fectly as long as they’re integral multiples of 7.81 millivolts—any intermediate

12.3 Effects of Finite Fixed-Point Binary Word Length 635
input voltage will cause the A/D converter to output a best estimate digital
data value. The inaccuracies in this process are called quantization errors be-
cause an A/D output least significant bit is an indivisible quantity. We illus-
trate this situation in Figure 12–1(a), where the continuous waveform is being
digitized by an 8-bit A/D converter whose output is in the sign-magnitudefor-
mat. When we start sampling at time t=0, the continuous waveform happens
to have a value of 31.25 millivolts (mv), and our A/D output data word will be
exactly correct for sample x(0). At time Twhen we get the second A/D output
word for sample x(1), the continuous voltage is between 0 and –7.81 mv. In this
case, the A/D converter outputs a sample value of 10000001, representing
–7.81 mv, even though the continuous input was not quite as negative as –7.81
mv. The 10000001 A/D output word contains some quantization error. Each
successive sample contains quantization error because the A/D’s digitized
A/D output values
Signal amplitude in mv
x(0)
31.25 00000100
23.43 00000011
x(5)
15.62 00000010
x(4)
7.81 00000001
(a) 0 00000000
x(3)
x(1)
–7.81 10000001
x(2) x(6)
–15.62 10000010
–23.43 10000011
–31.25 10000100
0 T 2T T3T 4T 5T 6T Time
A/D quantization error in mv
7.81 00000001
}
(b) 0 Quantization
error range
–7.81 10000001
0 T 2T T3T 4T 5T 6T Time
Figure 12–1 Quantization errors: (a) digitized x(n)values of a continuous signal;
(b)quantization error between the actual analog signal values and
the digitized signal values.

636 Digital Data Formats and Their Effects
output values must lie on the horizontal line in Figure 12–1(a). The difference
between the actual continuous input voltage and the A/D converter’s repre-
sentation of the input is shown as the quantization error in Figure 12–1(b). For
an ideal A/D converter, the quantization error, a kind of roundoff noise, can
never be greater than ±1/2 an lsb, or ±3.905 mv.
While Figure 12–1(b) shows A/D quantization noise in the time domain,
we can also illustrate this noise in the frequency domain. Figure 12–2(a) de-
picts a continuous sinewave of one cycle over the sample interval shown as
the dashed line and a quantized version of the time-domain samples of that
Sinewave amplitude
8
Continuous sinewave
6
4
2
33 36 39 42 45 48 51 54 57 60 63
(a) 0
n
0 3 6 9 12 15 18 21 24 27 30
–2
–4
–6
–8
DFT of unquantized sinewave (dB)
0
–20
(b)
–40
–60
–80
m
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
DFT of quantized sinewave (dB)
0
–20
(c)
–40
–60
–80
m
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Figure 12–2 Quantization noise effects: (a) input sinewave applied to a 64-point
DFT; (b) theoretical DFT magnitude of high-precision sinewave
samples; (c) DFT magnitude of a sinewave quantized to four bits.

12.3 Effects of Finite Fixed-Point Binary Word Length 637
wave as the dots. Notice how the quantized version of the wave is con-
strained to have only integral values, giving it a stair-step effect oscillating
above and below the true unquantized sinewave. The quantization here is
four bits, meaning that we have a sign bit and three bits to represent the mag-
nitude of the wave. With three bits, the maximum peak values for the wave
are ±7. Figure 12–2(b) shows the discrete Fourier transform (DFT) of a dis-
crete version of the sinewave whose time-domain sample values are not
forced to be integers but have high precision. Notice in this case that the DFT
has a nonzero value only at m = 1. On the other hand, Figure 12–2(c) shows
the spectrum of the four-bit quantized samples in Figure 12–2(a), where
quantization effects have induced noise components across the entire spectral
band. If the quantization noise depictions in Figures 12–1(b) and 12–2(c) look
random, that’s because they are. As it turns out, even though A/D quantiza-
tion noise is random, we can still quantify its effects in a useful way.
In the field of communications, people often use the notion of output
signal-to-noise ratio, or SNR = (signal power)/(noise power), to judge the
usefulness of a process or device. We can do likewise and obtain an important
expression for the output SNR of an ideal A/D converter, SNR , accounting
A/D
for finite word-length quantization effects. Because quantization noise is ran-
dom, we can’t explicitly represent its power level, but we can use its statistical
equivalent of variance to define SNR measured in dB as
A/D
⎛ ⎞
SNR =10 ⋅ log ⎜ input signal variance ⎟
A/D 10⎝A/D quantization noise variance⎠
(12–8)
=10 ⋅ log
⎛
⎜
σ2
signal
⎞
⎟ .
10⎝σ2 ⎠
A/D noise
Next, we’ll determine an A/D converter’s quantization noise variance
relative to the converter’s maximum input peak voltage V . If the full-scale
p
(–V to +V volts) continuous input range of a b-bit A/D converter is 2Vp, a
p p
single quantization level q is that voltage range divided by the number of
possible A/D output binary values, or q = 2V /2b. (In Figure 12–1, for exam-
p
ple, the quantization level q is the lsb value of 7.81 mv.) A depiction of the
likelihood of encountering any given quantization error value, called the
probability density function p(e) of the quantization error, is shown in Fig-
ure12–3.
This simple rectangular function has much to tell us. It indicates that
there’s an equal chance that any error value between –q/2 and +q/2 can
occur. By definition, because probability density functions have an area of
unity (i.e., the probability is 100 percent that the error will be somewhere
under the curve), the amplitude of the p(e) density function must be the area

638 Digital Data Formats and Their Effects
A/D quantization error probability
density function p(e)
1
q
–q/2 0 q/2 e (Error value)
Figure 12–3 Probability density function of A/D conversion roundoff error (noise).
divided by the width, or p(e) = 1/q. From Figure D–7 and Eq. (D–29) in Ap-
pendix D, the variance of our uniform p(e)is
q/2 q/2
σ2 A/D noise =
∫
e2p(e)de=
1⋅ ∫
e2de=
q2
. (12–9)
q 12
−q/2 −q/2
We can now express the A/D noise error variance in terms of A/D parame-
ters by replacing qin Eq. (12–9) with q=2V /2bto get
p
(2V )2 V2
σ2 A/D noise = 12 ⋅ (2 p b)2 = 3 ⋅ 2 p 2b . (12–10)
OK, we’re halfway to our goal—with Eq. (12–10) giving us the denominator
of Eq. (12–8), we need the numerator. To arrive at a general result, let’s ex-
press the input signal in terms of its root mean square (rms), the A/D con-
verter’s peak voltage, and a loading factor LFdefined as
σ
rms of the input signal
LF= = signal . † (12–11)
V V
p p
With the loading factor defined as the input rms voltage over the A/D con-
verter’s peak input voltage, we square and rearrange Eq. (12–11) to show the
signal variance σ2 as
signal
σ2 =(LF) 2 V 2 . (12–12)
signal p
†As covered in Appendix D, Section D.2, although the variance σ2is associated with the power
of a signal, the standard deviation is associated with the rms value of a signal.

12.3 Effects of Finite Fixed-Point Binary Word Length 639
Substituting Eqs. (12–10) and (12–12) in Eq. (12–8),
SNR A/D =10 ⋅ log 10 ⎛ ⎝ ⎜ V ( 2 L / F ( ) 3 2 ⋅ V 2 p 2 2b) ⎞ ⎠ ⎟ =10 ⋅ log 10 [(LF)2 (3 ⋅ 22b)]
p
(12–13)
⋅
=6.02 b+4.77+20 log (LF).
10
Eq. (12–13) gives us the SNR of an ideal b-bit A/D converter in terms
A/D
of the loading factor and the number of bits b. Figure 12–4 plots Eq. (12–13) for
various A/D word lengths as a function of the loading factor. Notice that the
loading factor in Figure 12–4 is never greater than –3 dB, because the maximum
continuous A/D input peak value must not be greater than V volts. Thus, for a
p
sinusoid input, its rms value must not be greater than V / 2 volts (3 dB below
p
V ).
p
When the input sinewave’s peak amplitude is equal to the A/D con-
verter’s full-scale voltage V , the full-scale LFis
p
LF full scale = V p V / 2 = 1 2 . (12–14)
p
Under this condition, the maximum A/D output SNR from Eq. (12–13) is
SNR =6.02 ⋅ b+4.77+20 ⋅ log (1/ 2)
A/D-max 10
(12–15)
=6.02 ⋅ b+4.77−3.01=6.02 ⋅ b+1.76 dB.
SNR (ideal)
A/D-max
90
80
70 14-bit
60 12-bit RapidSNR
degradation
50 10-bit due to
clipping
40
8-bit
30
6-bit
20
10
-21 -18 -15 -12 -9 -6 -3 Loading
factor (dB)
Figure 12–4 SNR of ideal A/D converters as a function of loading factor in dB.
A/D

640 Digital Data Formats and Their Effects
This discussion of SNR relative to A/D converters means three important
things to us:
1. An ideal A/D converter will have an SNR defined by Eq. (12–13), so
A/D
any discrete x(n) signal produced by a b-bit A/D converter can never
have an SNR greater than Eq. (12–13). (Appendix D dicusses methods
for computing the SNR of discrete signals.) For example, let’s say we
want to digitize a continuous signal whose SNR is 55 dB. Using an ideal
⋅
eight-bit A/D converter with its full-scale SNR of 6.02 8 + 1.76 =
A/D
49.9 dB from Eq. (12–15), the quantization noise will contaminate the
digitized values, and the resultant digital signal’s SNR can be no better
than 49.9 dB. We’ll have lost signal SNR through the A/D conversion
process. (Aten-bit A/D, with its ideal SNR ≈62 dB, could be used to
A/D
digitize a 55 dB SNR continuous signal to reduce the SNR degradation
caused by quantization noise.) Equations (12–13) and (12–15) apply to
ideal A/D converters and don’t take into account such additional A/D
noise sources as aperture jitter error, missing output bit patterns, and
other nonlinearities. So actual A/D converters are likely to have SNRs
that are lower than that indicated by theoretical Eq. (12–13). To be safe in
practice, it’s sensible to assume that SNR is 3 to 6 dB lower than
A/D-max
indicated by Eq. (12–15).
2. Equation (12–15) is often expressed in the literature, but it can be a little
misleading because it’s imprudent to force an A/D converter’s input to
full scale. It’s wise to drive an A/D converter to some level below full
scale because inadvertent overdriving will lead to signal clipping and
will induce distortion in the A/D’s output. So Eq. (12–15) is overly opti-
mistic, and, in practice, A/D converter SNRs will be less than indicated
by Eq. (12–15). The best approximation for an A/D’s SNR is to deter-
mine the input signal’s rms value that will never (or rarely) overdrive
the converter input, and plug that value into Eq. (12–11) to get the load-
ing factor value for use in Eq. (12–13).† Again, using an A/D converter
with a wider word length will alleviate this problem by increasing the
available SNR .
A/D
3. Remember, now, real-world continuous signals always have their own
inherent continuous SNR, so using an A/D converter whose SNR is
A/D
a great deal larger than the continuous signal’s SNR serves no purpose.
In this case, we would be wasting A/D converter bits by digitizing the
analog signal’s noise to a high degree of accuracy, which does not im-
†By the way, some folks use the term crest factorto describe how hard an A/D converter’s input
is being driven. The crest factor is the reciprocal of the loading factor, or CF=V/(rms of the
p
input signal).

12.3 Effects of Finite Fixed-Point Binary Word Length 641
prove our digital signal’s overall SNR. In general, we want the con-
verter’s SNR value to be approximately 6 dB greater than an analog
A/D
signal’s SNR.
A word of caution is appropriate here concerning our analysis of A/D
converter quantization errors. The derivations of Eqs. (12–13) and (12–15) are
based upon three assumptions:
1. The cause of A/D quantization errors is a stationary random process;
that is, the performance of the A/D converter does not change over
time. Given the same continuous input voltage, we always expect an
A/D converter to provide exactly the same output binary code.
2. The probability density function of the A/D quantization error is uni-
form. We’re assuming that the A/D converter is ideal in its operation
and all possible errors between –q/2 and +q/2 are equally likely. An
A/D converter having stuck bits or missing output codes would violate
this assumption. High-quality A/D converters being driven by continu-
ous signals that cross many quantization levels will result in our desired
uniform quantization noise probability density function.
3. The A/D quantization errors are uncorrelated with the continuous input
signal. If we were to digitize a single continuous sinewave whose fre-
quency was harmonically related to the A/D sample rate, we’d end up
sampling the same input voltage repeatedly and the quantization error
sequence would not be random. The quantization error would be pre-
dictable and repetitive, and our quantization noise variance derivation
would be invalid. In practice, complicated continuous signals such as
music or speech, with their rich spectral content, avoid this problem.
To conclude our discussion of A/D converters, let’s consider one last
topic. In the literature the reader is likely to encounter the expression
SNR−1.76
b = . (12–16)
eff
6.02
Equation (12–16) is used by test equipment manufacturers to specify the sensi-
tivity of test instruments using a b parameter known as the number of effec-
eff
tive bits, or effective number of bits (ENOB)[3–8]. Equation (12–16) is merely
Eq. (12–15) solved for b and is based on the assumption that the A/D con-
verter’s analog input peak-peak voltage spans roughly 90 percent of the con-
verter’s full-scale voltage range. Test equipment manufacturers measure the
actual SNR of their product, indicating its ability to capture continuous input
signals relative to the instrument’s inherent noise characteristics. Given this
true SNR, they use Eq. (12–16) to determine the b value for advertisement in
eff
their product literature. The larger the b , the greater the continuous voltage
eff

642 Digital Data Formats and Their Effects
that can be accurately digitized relative to the equipment’s intrinsic quantiza-
tion noise.
12.3.2 Data Overflow
The next finite, word-length effect we’ll consider is called overflow. Overflow
is what happens when the result of an arithmetic operation has too many bits,
or digits, to be represented in the hardware registers designed to contain that
result. We can demonstrate this situation to ourselves rather easily using a
simple four-function, eight-digit pocket calculator. The sum of a decimal
9.9999999 plus 1.0 is 10.9999999, but on an eight-digit calculator the sum is
10.999999 as
9 . 9 9 9 9 9 9 9
+1 . 0 0 0 0 0 0 0
10 . 9 9 9 9 9 9 9.
↑
this digit gets discarded
The hardware registers, which contain the arithmetic result and drive the cal-
culator’s display, can hold only eight decimal digits; so the least significant
digit is discarded (of course). Although the above error is less than one part in
ten million, overflow effects can be striking when we work with large num-
bers. If we use our calculator to add 99,999,999 plus 1, instead of getting the
correct result of 100 million, we’ll get a result of 1. Now that’s an authentic
overflow error!
Let’s illustrate overflow effects with examples more closely related to
our discussion of binary number formats. First, adding two unsigned binary
numbers is as straightforward as adding two decimal numbers. The sum of
42 plus 39 is 81, or
1 1 1 ←carry bits
+42 in unsigned binary → 1 0 1 0 1 0
+39 in unsigned binary → +1 0 0 1 1 1
+81 in unsigned binary → 1 0 1 0 0 0 1.
In this case, two 6-bit binary numbers required 7 bits to represent the results.
The general rule is the sum of m individual b-bit binary numbers can require as
many as [b+log (m)]bits to represent the results. So, for example, a 24-bit result
2
register (accumulator) is needed to accumulate the sum of sixteen 20-bit bi-
nary numbers, or 20 + log (16) = 24. The sum of 256 eight-bit words requires
2
an accumulator whose word length is [8+log (256)], or 16 bits, to ensure that
2
no overflow errors occur.

12.3 Effects of Finite Fixed-Point Binary Word Length 643
In the preceding example, if our accumulator word length was six bits,
an overflow error occurs as
1 1 1 ←carry bits
+42 in unsigned binary → 1 0 1 0 1 0
+39 in unsigned binary → +1 0 0 1 1 1
+17 in unsigned binary → 1 0 1 0 0 0 1. ←overflow error
↑
an overflow out of the sign bit is ignored, causing an overflow error
Here, the most significant bit of the result overflowed the six-bit accumulator,
and an error occurred.
With regard to overflow errors, the two’s complement binary format
has two interesting characteristics. First, under certain conditions, over-
flow during the summation of two numbers causes no error. Second, with
multiple summations, intermediate overflow errors cause no problems if
the final magnitude of the sum of the b-bit two’s complement numbers is
less than 2b–1. Let’s illustrate these properties by considering the four-bit
two’s complement format in Figure 12–5, whose binary values are taken
from Table 12–2.
The first property of two’s complement overflow, which sometimes
causes no errors, can be shown by the following examples:
Decimal
equivalent
0111 0111
+7
0110 0110
+6
0101 0101
+5
0100
+4
0011
+3
0010
+2
0001
+1
0000
0
1111
–1
1110 –2
1101 –3
1100 –4
1011 –5
1010 1010 –6
1001 1001 –7
–8
1000 1000
Figure 12–5 Four-bit two’s complement binary numbers.

644 Digital Data Formats and Their Effects
0 1 0 ←carry bits
–5 in two’s complement → 1 0 1 1
+2 in two’s complement → +0 0 1 0
–3 in two’s complement → 0 1 1 0 1 ←valid negative result
↑
zero overflow out of the sign bit
1 1 0 ←carry bits
–2 in two’s complement → 1 1 1 0
+6 in two’s complement → +0 1 1 0
+4 in two’s complement → 1 0 1 0 0 ←valid positive result
↑
overflow out of the sign bit ignored, no harm done
Then again, the following examples show how two’s complement overflow
sometimes does cause errors:
0 0 0 ←carry bits
–7 in two’s complement → 1 0 0 1
–6 in two’s complement → +1 0 1 0
+3 in two’s complement → 1 0 0 1 1 ←invalid positive result
↑
overflow out of the sign bit ignored, causing overflow error
1 1 1 ←carry bits
+7 in two’s complement → 0 1 1 1
+7 in two’s complement → 0 1 1 1
–2 in two’s complement → 0 1 1 1 0 ←invalid negative result
↑
zero overflow out of the sign bit
The rule with two’s complement addition is if the carry bit into the sign bit is the
same as the overflow bit out of the sign bit, the overflow bit can be ignored, causing
no errors; if the carry bit into the sign bit is different from the overflow bit out of the
sign bit, the result is invalid. An even more interesting property of two’s com-
plement numbers is that a series of b-bit word summations can be performed
where intermediate sums are invalid, but the final sum will be correct if its
magnitude is less than 2b–1. We show this by the following example. If we add
a +6 to a +7, and then add a –7, we’ll encounter an intermediate overflow
error but our final sum will be correct, as
+7 in two’s complement → 0 1 1 1
+6 in two’s complement → +0 1 1 0
–3 in two’s complement → 1 1 0 1 ←overflow error here
–7 in two’s complement → +1 0 0 1
+6 in two’s complement → 1 0 1 1 0 ←valid positive result
↑
overflow ignored, with no harm done

12.3 Effects of Finite Fixed-Point Binary Word Length 645
The magnitude of the sum of the three four-bit numbers was less than 24–1
(<8), so our result was valid. If we add a +6 to a +7, and next add a –5, we’ll
encounter an intermediate overflow error, and our final sum will also be in
error because its magnitude is not less than 8.
+7 in two’s complement → 0 1 1 1
+6 in two’s complement → +0 1 1 0
–3 in two’s complement → 1 1 0 1 ←overflow error here
–5 in two’s complement → +1 0 1 1
–8 in two’s complement → 1 1 0 0 0 ←invalid negative result
Another situation where overflow problems are conspicuous is during
the calculation of the fast Fourier transform (FFT). It’s difficult at first to
imagine that multiplying complex numbers by sines and cosines can lead to
excessive data word growth—particularly because sines and cosines are
never greater than unity. Well, we can show how FFT data word growth oc-
curs by considering a decimation-in-time FFT butterfly from Figure 4–14(c),
repeated here as Figure 12–6(a), and grinding through a little algebra. The ex-
pression for the x’ output of this FFT butterfly, from Eq. (4-26), is
x’=x+Wk ⋅ y. (12–17)
N
Breaking up the butterfly’s x and y inputs into their real and imaginary parts
and remembering that
Wk=e–j2πk/N,
we can express Eq. (12–17) as
N
⋅
x’ = x + jx +
(e–j2πk/N)
(y + jy ) . (12–18)
real imag real imag
If we let α be the twiddle factor angle of 2πk/N, and recall that
e–jα =cos(α) –jsin(α), we can simplify Eq. (12–18) as
x’=x + jx +[cos(α)− jsin(α)] ⋅ (y + jy )
real imag real immag
(12–19)
=x +cos(α)y +sin(α)y + j(x ++cos(α)y −sin(α)y ).
real real imag imag imag real
x(n) w(n)
x x'
z−1
a(1) z−1
y W k −1 y'
N
a(2)
(a) (b)
Figure 12–6 Data overflow scenarios: (a) single decimation-in-time FFT butterfly;
(b) 2nd-order IIR filter.

646 Digital Data Formats and Their Effects
If we look, for example, at just the real part of the x’ output, x’ , it comprises
real
the three terms
x’ = x + cos(α)y + sin(α)y . (12–20)
real real real imag
If x , y , and y are of unity value when they enter the butterfly and the
real real imag
twiddle factor angle α = 2πk/N happens to be π/4 = 45o, then, x’ can be
real
greater than 2 as
⋅ ⋅
x’ =1+cos(45° ) 1+sin(45° ) 1
real
(12–21)
=1+0.707+0.7077 =2.414.
So we see that the real part of a complex number can more than double in
magnitude in a single stage of an FFT. The imaginary part of a complex num-
ber is equally likely to more than double in magnitude in a single FFT stage.
Without mitigating this word growth problem, overflow errors could render
an FFT algorithm useless.
Overflow problems can also be troublesome for fixed-point systems con-
taining feedback as shown in Figure 12–6(b). Examples of such networks are
infinite impulse response (IIR) filters, cascaded integrator-comb (CIC) filters,
and exponential averagers. The hardware register (accumulator) containing
w(n) must have a binary word width that will hold data values as large as the
network’s DC (zero Hz) gain G times the input signal, or G · x(n). To avoid
data overflow, the number of bits in the w(n)-results register must be at least
accumulator bits = number of bits in x(n) + Llog (G)M, (12–22)
2
where Llog (G)M means that if log (G) is not an integer, round it up to the next
2 2
larger integer. (As a quick reminder, we can determine the DC gain of a digi-
tal network by substituting z=1 in the network’s z-domain transfer function.)
OK, overflow problems are handled in one of two ways—by truncation
or rounding—each inducing its own individual kind of quantization errors,
as we shall see.
12.3.3 Truncation
Truncation is the process where some number of least significant bits are dis-
carded from a binary number. Apractical example of truncation is the situa-
tion where the results of a processing system are 16-bit signal samples that
must be passed on to a 12-bit digital-to-analog converter. To avoid overflow-
ing the converter’s 12-bit input register, the least significant 4 bits of the 16-bit
signal samples must be discarded. Thinking about decimal numbers, if we’re
quantizing to decimal integer values, for example, the real value 1.2 would be
quantized to 1.

12.3 Effects of Finite Fixed-Point Binary Word Length 647
An example of truncation to integer values is shown in Figure 12–7(a),
where all values of xin the range of 0≤x<1 are set equal to 0, values of xin
the range of 1 ≤ x < 2 are set equal to 1, and so on. The quantization level
(value), in that figure, is q = 1. The quantization error induced by this trunca-
tion is the vertical distance between the horizontal bold lines and the dashed
diagonal line in Figure 12–7(a).
As we did with A/D converter quantization errors, we can call upon
the concept of probability density functions to characterize the quantization
errors induced by truncation. The probability density function of truncation
errors, in terms of the quantization level q, is shown in Figure 12–7(b). In
·
·
·
Quantizedx 3
2
1 Truncation
–3 –2 –1
0
(a)
1 2 3 4 x
–1
–2
–3
·
·
·
Truncation error
1 probability density
q function
(b)
–q 0 Error value
Retain
Original
binary R5 R4 R3 R2 R1 R0 T3 T2 T1 T0
wordW
(c) Truncate
WordW
after 0 0 0 0 R5 R4 R3 R2 R1 R0
truncation
Figure 12–7 Truncation: (a) quantization nonlinearities; (b) error probability den-
sity function; (c) binary truncation.

648 Digital Data Formats and Their Effects
Figure 12–7(a) the quantization level qis 1, so in this case we can have trunca-
tion errors as great as –1. Drawing upon our results from Eqs. (D–11) and
(D–12) in Appendix D, the mean and variance of our uniform truncation error
probability density function are expressed as
−q
μ =
(12–23)
trunc 2
and
q2
σ2 = . (12–24)
trunc 12
The notion of binary number truncation is shown in Figure 12–7(c),
where the ten-bit binary word Wis to be truncated to six bits by discarding the
four Truncate bits. So in this binary truncation situation, qin Figure 12–7(b) is
equal to the least significant bit (lsb) value (bit R0) of the retained binary word.
In a sense, truncation error is the price we pay for the privilege of using
integer binary arithmetic. One aspect of this is the error introduced when we
use truncation to implement division by some integer power of two. Aquick
way of dividing a binary value by 2Kis to shift a binary word K bits to the
right; that is, we’re truncating the data value (not the binary word width) by
discarding the rightmost Kbits after the right shift.
For example, let’s say we have the value 31 represented by the six-bit bi-
nary number 011111, and we want to divide it by 16 through shifting the bits
2
K= 4 places to the right and discarding those shifted bits. After the right shift
we have a binary quotient of 000001. Well, we see the significance of the
2
problem because this type of division gave us a result of one instead of the
correct quotient 31/16 = 1.9375. Our division-by-truncation error here is
roughly 50 percent of the correct quotient. Had our original dividend been 63
represented by the six-bit binary number 111111, dividing it by 16 through a
2
four-bit shift would give us an answer of binary 000011, or decimal three.
2
The correct answer, of course, is 63/16 = 3.9375. In this case the percentage
error is 0.9375/3.9375, or about 23.8 percent. So, the larger the dividend, the
lower the truncation error.
If we study these kinds of errors, we’ll find that truncation error de-
pends on three things: the number of value bits shifted and discarded, the
values of the discarded bits (were those dropped bits ones or zeros?), and the
magnitude of the binary number left over after shifting. Although a complete
analysis of these truncation errors is beyond the scope of this book, a practical
example of how division by truncation can cause serious numerical errors is
given in reference [9].
Unfortunately, truncation induces a DC bias (an error whose average is a
nonzero negative number) on the truncated signal samples, as predicted by

12.3 Effects of Finite Fixed-Point Binary Word Length 649
Eq. (12–23). We see this behavior in Figure 12–7(b) where the truncation error
is always negative. Inducing a constant (DC) error to a signal sequence can be
troublesome in many applications because the always-negative truncation
error can grow to an unacceptable level in subsequent computations. So, in an
effort to avoid overflow errors, rounding (discussed in the next section) is
often preferred over truncation.
12.3.4 Data Rounding
Rounding is where a binary number requiring truncation is slightly modified
before the truncation operation is performed. Let’s review the behavior of
rounding by first defining rounding as the process wherein a number is mod-
ified such that it is subsequently represented by, or rounded off to, its nearest
quantization level. For example, if we’re quantizing to integer values, the dec-
imal number 1.2 would be quantized to 1, and the number 1.6 would be
quantized to 2. This is shown in Figure 12–8(a), where all values of x in the
range of –0.5 ≤ x < 0.5 are set equal to 0, values of x in the range of
0.5 ≤ x<1.5 are set equal to 1, and so on.
·
·
·
Quantizedx 3
2
1 Rounding
–7/2 –5/2 –3/2 –1/2
(a)
0 1/2 3/2 5/2 7/2 x
–1
–2
–3
·
·
·
Rounding error probability
density function
1
q
(b)
–q/2 0 q/2 Error value
Figure 12–8 Rounding: (a) quantization nonlinearities; (b) error probability density
function.

650 Digital Data Formats and Their Effects
The quantization error induced by such a rounding operation is the ver-
tical distance between the bold horizontal lines and the dashed diagonal line
in Figure 12–8(a). The probability density function of the error induced by
rounding, in terms of the quantization level q, is shown in Figure 12–8(b). In
Figure 12–8(a) the quantization level is q = 1, so in this case we can have
quantization error magnitudes no greater than q/2, or 1/2. Using our Eqs.
(D–11) and (D–12) results from Appendix D, the mean and variance of our
uniform rounding probability density function are expressed as
μ =0
round (12–25)
and
q2
σ2 = . (12–26)
round 12
The notion of binary number rounding can be described using Figure
12–7(c), where the binary word W is to be truncated by discarding the four
Truncate bits. With rounding, the binary word Wis modified before the Trun-
cate bits are discarded. So with binary rounding, q in Figure 12–8(b) is equal
to the lsb value of the preserved binary word R0.
Let’s not forget: the purpose of rounding, its goal in life, is to avoid data
overflow errors while reducing the DC bias error (an error whose average is
not zero) induced by simple truncation. Rounding achieves this goal because,
in theory, its average error is zero as shown by Eq. (12–25). Next we discuss
two popular methods of data rounding.
A common form of binary data rounding is straightforward to imple-
ment. Called round-to-nearest,it comprises the two-step process of adding one
to the most significant (leftmost) of the lsb bits to be discarded, bit T3 of word
Win Figure 12–7(c), and then discarding the appropriate Truncate bits. For an
example of this rounding method, let’s say we have 16-bit signal samples des-
tined to be routed to a 12-bit digital-to-analog converter. To avoid overflowing
the converter’s 12-bit input register, we add a binary value of 1000 (decimal
2
8 = 23) to the original 16-bit sample value and then truncate (discard) the
10
sum’s least significant 4 bits. As another example of round-to-nearest round-
ing, if a 32-bit “long” word is rounded to 16 bits, a value of 215is added to the
long word before discarding the sum’s 16 least significant bits.
Stated in different words, this round-to-nearest rounding method
means: If the T3 bit is a one, increment the R bits by one. Then shift the R bits
to the right, discarding the Truncate bits.
The round-to-nearest method does reduce the average (DC bias) of the
quantization error induced by simple truncation; however the round-to-near-
est method’s average error bias is close to but not exactly equal to zero.

12.3 Effects of Finite Fixed-Point Binary Word Length 651
(That’s because the R bits, in Figure 12–7(c), are always incremented when the
value of the Truncate bits is equal to the value R0/2. This means that over
time the R bits are rounded up slightly more often than they are rounded
down.) With additional bit checking we can force the average rounding error
to be exactly zero using a scheme called convergent rounding.
Convergent rounding, also called round to even, is a slightly more com-
plicated method of rounding, but one that yields zero-average rounding error
on the rounded binary signal samples. Similar to the round-to-nearest
method, convergent rounding does not always increment Figure 12–7(c)’s R
bits (the value Retain) when the value of the Truncate bits is equal to R0/2. In
the convergent rounding scheme, when Truncate = R0/2, the value Retain is
only incremented if its original value was an odd number. This clever process
is shown in Figure 12–9.
OK, here’s what we’ve learned about rounding: Relative to simple trun-
cation, rounding requires more computations, but rounding both minimizes
the constant-level (DC bias) quantization error induced by truncation alone,
and rounding has a lower maximum quantization error. So rounding is often
the preferred method used to avoid binary data overflow errors. The above
two rounding methods can, by the way, be used in two’s complement number
format systems.
As a practical rule, to retain maximum numerical precision, all necessary
full-width binary arithmetic should be performed first and then rounding (or
truncation) should be performed as the very last operation. For example, if
we must add twenty 16-bit binary numbers followed by rounding the sum to
12 bits, we should perform the additions at full 16-bit precision and, as a final
step, round the summation result to 12 bits.
W
Truncate = R0/2?
[Is Truncate = 2 T 3 ?] No
Yes
No
Is bit R0 equal to 0?
Yes T3
Add 2 to W
TruncateW to R bits
Figure 12–9 Convergent rounding.

652 Digital Data Formats and Their Effects
In digital signal processing, statistical analysis of quantization error ef-
fects is complicated because quantization is a nonlinear process. Analytical
results depend on the types of quantization errors, the magnitude of the data
being represented, the numerical format used, and which of the many FFT or
digital filter structures we are implementing. Be that as it may, digital signal
processing experts have developed simplified error models whose analysis
has proved useful. Although discussion of these analysis techniques and their
results is beyond the scope of this introductory text, many references are
available for the energetic reader[10–18]. (Reference [11] has an extensive ref-
erence list of its own on the topic of quantization error analysis.)
Again, the overflow problems using fixed-point binary formats—which
we try to alleviate with truncation or rounding—arise because so many digi-
tal signal processing algorithms comprise large numbers of additions or mul-
tiplications. This obstacle, particularly in hardware implementations of
digital filters and the FFT, is avoided by hardware designers through the use
of floating-point binary number formats.
12.4 FLOATING-POINT BINARY FORMATS
Floating-point binary formats allow us to overcome most of the limitations
of precision and dynamic range mandated by fixed-point binary formats,
particularly in reducing the ill effects of overflow[19]. Floating-point for-
mats segment a data word into two parts: a mantissa m and an exponent e.
Using these parts, the value of a binary floating-point number n is evalu-
ated as
⋅
n= m 2e, (12–27)
that is, the number’s value is the product of the mantissa and 2 raised to
the power of the exponent. (Mantissa is a somewhat unfortunate choice of
terms because it has a meaning here very different from that in the math-
ematics of logarithms. Mantissa originally meant the decimal fraction of a
logarithm.†However, due to its abundance in the literature we’ll continue
using the term mantissa here.) Of course, both the mantissa and the expo-
nent in Eq. (12–27) can be either positive or negative numbers.
Let’s assume that a b-bit floating-point number will use b bits for the
e
fixed-point signed exponent and b bits for the fixed-point signed mantissa.
m
†For example, the common logarithm (log to the base 10) of 256 is 2.4082. The 2 to the left of the
decimal point is called the characteristic of the logarithm and the 4082 digits are called the man-
tissa. The 2 in 2.4082 does not mean that we multiply .4082 by 102. The 2 means that we take the
antilog of .4082 to get 2.56 and multiply that by 102to get 256.

12.4 Floating-Point Binary Formats 653
The greater the number of b bits used, the larger the dynamic range of the
e
number. The more bits used for b , the better the resolution, or precision, of
m
the number. Early computer simulations conducted by the developers of b-bit
floating-point formats indicated that the best trade-off occurred with b ≈b/4
e
and b ≈ 3b/4. We’ll see that for typical 32-bit floating-point formats used
m
today, b ≈8 bits and b ≈24 bits.
e m
To take advantage of a mantissa’s full dynamic range, most implementa-
tions of floating-point numbers treat the mantissa as a fractional fixed-point
binary number, shift the mantissa bits to the right or left, so that the most sig-
nificant bit is a one, and adjust the exponent accordingly. The process of shift-
ing a binary bit pattern so that the most significant bit is a one is called bit
normalization.When normalized, the mantissa bits are typically called the frac-
tion of the floating-point number, instead of the mantissa. For example, the
decimal value 3.6875 can be represented by the fractional binary number
10
11.1011 . If we use a two-bit exponent with a six-bit fraction floating-point
2
word, we can just as well represent 11.1011 by shifting it to the right two
2
places and setting the exponent to two as
exponent fraction
↓ ↓
11.1011 = 1 0 1 1 1 0 1 1 . (12–28)
2 ⋅
↑
binary point
The floating-point word above can be evaluated to retrieve our decimal num-
ber again as
[◊(1⋅2–1)+(1⋅2–2)+(1⋅2–3)+(0⋅2–4)+(1⋅2–5)+(1⋅2–6)]⋅22
1 1 1 1 1 1
=[◊(1⋅ )+(1⋅ )+(1⋅ )+(0⋅ )+(1⋅ )+(1⋅ )]⋅22
2 4 8 16 32 64
=[0.5+0.25+0.125+0.0625+0.03125+0.015625]⋅22
(12–29)
=0.921875⋅4=3.6875.
After some experience using floating-point normalization, users soon realized
that always having a one in the most significant bit of the fraction was waste-
ful. That redundant one was taking up a single bit position in all data words
and serving no purpose. So practical implementations of floating-point for-
mats discard that one, assume its existence, and increase the useful number of
fraction bits by one. This is why the term hidden bit is used to describe some
floating-point formats. While increasing the fraction’s precision, this scheme
uses less memory because the hidden bit is merely accounted for in the

654 Digital Data Formats and Their Effects
hardware arithmetic logic. Using a hidden bit, the fraction in Eq. (12–28)’s
floating-point number is shifted to the left one place and would now be
exponent fraction
↓ ↓
11.1011 2 = 1 0 ⋅ 1 1 0 1 1 0 . (12–30)
↑
binary point
Recall that the exponent and mantissa bits were fixed-point signed bi-
nary numbers, and we’ve discussed several formats for representing signed
binary numbers, i.e., sign magnitude, two’s complement, and offset binary.
As it turns out, all three signed binary formats are used in industry-standard
floating-point formats. The most common floating-point formats, all using
32-bit words, are listed in Table 12–6.
Table 12–6 Floating–Point Number Formats
IEEE Standard P754 Format
Bit 31 30 29 28 27 26 25 24 23 22 21 20 ⋅⋅⋅ 2 1 0
S 27 26 25 24 23 22 21 20 2–1 2–2 2–3 ⋅⋅⋅ 2–21 2–22 2–23
Sign (s) ←Exponent (e)→ ←Fraction (f) →
IBM Format
Bit 31 30 29 28 27 26 25 24 23 22 21 20 ⋅⋅⋅ 2 1 0
S 26 25 24 23 22 21 20 2–1 2–2 2–3 2–4 ⋅⋅⋅ 2–22 2–23 2–24
Sign (s) ←Exponent (e)→ ←Fraction (f) →
DEC (Digital Equipment Corp.) Format
Bit 31 30 29 28 27 26 25 24 23 22 21 20 ⋅⋅⋅ 2 1 0
S 27 26 25 24 23 22 21 20 2–2 2–3 2–4 ⋅⋅⋅ 2–22 2–23 2–24
Sign (s) ←Exponent (e)→ ←Fraction (f) →
MIL–STD 1750AFormat
Bit 31 30 29 ⋅⋅⋅ 11 10 9 8 7 6 5 4 3 2 1 0
20 2–1 2–2 ⋅⋅⋅ 2–20 2–21 2–22 2–23 27 26 25 24 23 22 21 20
←Fraction (f) → ←Exponent (e)→

12.4 Floating-Point Binary Formats 655
The IEEE P754 floating-point format is the most popular because so
many manufacturers of floating-point integrated circuits comply with this
standard[8,20–22]. Its exponent e is offset binary (biased exponent), and its
fraction is a sign-magnitude binary number with a hidden bit that’s assumed
to be 20. The decimal value of a normalized IEEE P754 floating-point number
is evaluated as
⋅ ⋅
value = (–1)s 1 f 2e– 127 (12–31)
IEEE ◊
↑
hidden bit
where f is the decimal-formatted value of the fractional bits divided by 223.
Value eis the decimal value of the floating-point number’s exponent bits.
The IBM floating-point format differs somewhat from the other float-
ing-point formats because it uses a base of 16 rather than 2. Its exponent
is offset binary, and its fraction is sign magnitude with no hidden bit. The
decimal value of a normalized IBM floating-point number is evaluated as
⋅ ⋅
value = (–1)s 0 f 16e– 64. (12–32)
IBM ◊
The DEC floating-point format uses an offset binary exponent, and its
fraction is sign magnitude with a hidden bit that’s assumed to be 2–1. The dec-
imal value of a normalized DEC floating-point number is evaluated as
⋅ ⋅
value = (–1)s 0 1f 2e– 128. (12–33)
DEC ◊
↑
hidden bit
MIL-STD 1750Ais a United States Military Airborne floating-point stan-
dard. Its exponent e is a two’s complement binary number residing in the
least significant eight bits. MIL-STD 1750A’s fraction is also a two’s comple-
ment number (with no hidden bit), and that’s why no sign bit is specifically
indicated in Table 12–6. The decimal value of a MIL-STD 1750Afloating-point
number is evaluated as
⋅
value = f 2e. (12–34)
1750A
Notice how the floating-point formats in Table 12–6 all have word
lengths of 32 bits. This was not accidental. Using 32-bit words makes these
formats easier to handle using 8-, 16-, and 32-bit hardware processors. That
fact not withstanding and given the advantages afforded by floating-point
number formats, these formats do require a significant amount of logical
comparisons and branching to correctly perform arithmetic operations. Refer-
ence [23] provides useful flow charts showing what procedural steps must be
taken when floating-point numbers are added and multiplied.

656 Digital Data Formats and Their Effects
12.4.1 Floating-Point Dynamic Range
Attempting to determine the dynamic range of an arbitrary floating-point
number format is a challenging exercise. We start by repeating the expression
for a number system’s dynamic range from Eq. (12–6) as
⎛ ⎞
⋅ largest possible word value
dynamic range =20 log ⎜ ⎟ . (12–35)
dB 10⎝smallest possible word value⎠
When we attempt to determine the largest and smallest possible values for a
floating-point number format, we quickly see that they depend on such fac-
tors as
• the position of the binary point
• whether a hidden bit is used or not (If used, its position relative to the
binary point is important.)
• the base value of the floating-point number format
• the signed binary format used for the exponent and the fraction (For ex-
ample, recall from Table 12–2 that the binary two’s complement format
can represent larger negative numbers than the sign-magnitude format.)
• how unnormalized fractions are handled, if at all (Unnormalized, also
called gradual underflow, means a nonzero number that’s less than the
minimum normalized format but can still be represented when the ex-
ponent and hidden bit are both zero.)
• how exponents are handled when they’re either all ones or all zeros.
(For example, the IEEE P754 format treats a number having an all-ones
exponent and a nonzero fraction as an invalid number, whereas the DEC
format handles a number having a sign = 1 and a zero exponent as a
special instruction instead of a valid number.)
Trying to develop a dynamic range expression that accounts for all the possi-
ble combinations of the above factors is impractical. What we can do is derive
a rule-of-thumb expression for dynamic range that’s often used in prac-
tice[8,22,24].
Let’s assume the following for our derivation: the exponent is a b-bit off-
e
set binary number, the fraction is a normalized sign-magnitude number hav-
ing a sign bit and b magnitude bits, and a hidden bit is used just left of the
m
binary point. Our hypothetical floating-point word takes the following form:
Bit b +b–1 b +b–2 ⋅⋅⋅ b +2 b b –1 b –2 ⋅⋅⋅ 1 0
m e m e m m m m
S 2be–1 2be–2 ⋅⋅⋅ 21 20 2–1 2–2 ⋅⋅⋅ 2–bm+1 2–bm
Sign (s) ←Exponent (e)→ ←Fraction (f) →

12.4 Floating-Point Binary Formats 657
First we’ll determine what the largest value can be for our floating-point
word. The largest fraction is a one in the hidden bit, and the remaining b
m
fraction bits are all ones. This would make fraction f=[1+(1–2–bm)]. The first
1 in this expression is the hidden bit to the left of the binary point, and the value
in parentheses is all b bits equal to ones to the right of the binary point. The
m
greatest positive value we can have for the b-bit offset binary exponent is
e
2(2be–1–1).
So the largest value that can be represented with the floating-point
number is the largest fraction raised to the largest positive exponent, or
largest possible word value=[1+(1–2 −b m)] ⋅ 2(2be−1−1). (12–36)
The smallest value we can represent with our floating-point word is a one in the
hidden bit times two raised to the exponent’s most negative value, 2–(2be–1), or
smallest possible word value=1 ⋅ 2 −(2be−1). (12–37)
Plugging Eqs. (12–36) and (12–37) into Eq. (12–35),
dynamic range =20
⋅
log
⎛
⎜
[1+(1−2 −b m)] ⋅ 2(2be−1−1) ⎞
⎟. (12–38)
dB 10
⎝ 1
⋅
2
−(2be−1)
⎠
Now here’s where the thumb comes in—when b is large, say over seven, the
m
2–bm value approaches zero; that is, as b increases, the all-ones fraction
m
(1 – 2–bm) value in the numerator approaches 1. Assuming this, Eq. (12–38)
becomes
⋅ ⎛ [1+1] ⋅ 2(2be−1−1) ⎞
dynamic range ≈20 log ⎜ ⎟
dB 10
⎝ 1
⋅
2
−(2be−1)
⎠
⋅
⎛
2
⋅ 2(2be−1−1) ⎞
⋅
⎛ 2(2be−1) ⎞
=20 log ⎜ ⎟ =20 log ⎜ ⎟ (12–39)
10
⎝ 2
−(2be−1)
⎠
10
⎝2
−(2be−1)⎠
=20 ⋅ log (2 ⋅ 2(2be−1)))=20 ⋅ log (2(2be))=6.02 ⋅ 2b e.
10 10
Using Eq. (12–39), we can estimate, for example, the dynamic range of the
single-precision IEEE P754 standard floating-point format with its eight-bit
exponent:
⋅
dynamic range = 6.02 28= 1529 dB. (12–40)
IEEE P754

658 Digital Data Formats and Their Effects
Although we’ve introduced the major features of the most common
floating-point formats, there are still more details to learn about floating-
point numbers. For the interested reader, the references given in this section
provide a good place to start.
12.5 BLOCK FLOATING-POINT BINARY FORMAT
Amarriage of fixed-point and floating-point binary formats is known as block
floating point. This scheme is used, particularly in dedicated FFT integrated
circuits, when large arrays, or blocks, of associated data are to be manipulated
mathematically. Block floating-point schemes begin by examining all the
words in a block of data, normalizing the largest-valued word’s fraction, and
establishing the correct exponent. This normalization takes advantage of the
fraction’s full dynamic range. Next, the fractions of the remaining data words
are shifted appropriately, so that they can use the exponent of the largest
word. In this way, all of the data words use the same exponent value to con-
serve hardware memory.
In FFT implementations, the arithmetic is performed treating the block
normalized data values as fixed-point binary. However, when an addition
causes an overflow condition, all of the data words are shifted one bit to the
right (division by two), and the exponent is incremented by one. As the reader
may have guessed, block floating-point formats have increased dynamic range
and avoid the overflow problems inherent in fixed-point formats but do not
reach the performance level of true floating-point formats[8,25,26].
REFERENCES
[1] Neugebauer, O. “The History of Ancient Astronomy,” Journal of Near Eastern Studies,Vol.
4, 1945, p. 12.
[2] Knuth, D. E. The Art of Computer Programming: Seminumerical Methods, Vol. 2,
Addison-Wesley, Reading, Massachusetts, 1981, Section 4.1, p. 179.
[3] Kester, W. “Peripheral Circuits Can Make or Break Sampling-ADC Systems,” EDN Maga-
zine, October 1, 1992.
[4] Grove, M. “Measuring Frequency Response and Effective Bits Using Digital Signal Pro-
cessing Techniques,” Hewlett-Packard Journal,February 1992.
[5] Tektronix. “Effective Bits Testing Evaluates Dynamic Range Performance of Digitizing In-
struments,” Tektronix Application Note,No. 45W-7527, December 1989.
[6] Ushani, R. “Subranging ADCs Operate at High Speed with High Resolution,” EDN Maga-
zine, April 11, 1991.

References 659
[7] Demler, M. “Time-Domain Techniques Enhance Testing of High-Speed ADCs,” EDN
Magazine, March 30, 1992.
[8] Hilton, H. “A 10-MHz Analog-to-Digital Converter with 110-dB Linearity,”
Hewlett-Packard Journal, October 1993.
[9] Lyons, R. G. “Providing Software Flexibility for Optical Processor Noise Analysis,” Com-
puter Design, July 1978, p. 95.
[10] Knuth, D. E. The Art of Computer Programming: Seminumerical Methods, Vol. 2,
Addison-Wesley, Reading, Massachusetts, 1981, Section 4.2, p. 198.
[11] Rabiner, L. R., and Gold, B. Theory and Application of Digital Signal Processing, Chapter 5,
Prentice Hall, Englewood Cliffs, New Jersey, 1975, p. 353.
[12] Jackson, L. B. “An Analysis of Limit Cycles Due to Multiplicative Rounding in Recursive
Digital Filters,” Proc. 7th Allerton Conf. Circuit System Theory, 1969, pp. 69–78.
[13] Kan, E. P. F., and Aggarwal, J. K. “Error Analysis of Digital Filters Employing Floating
Point Arithmetic,” IEEE Trans. Circuit Theory, Vol. CT-18, November 1971, pp. 678–686.
[14] Crochiere, R. E. “Digital Ladder Structures and Coefficient Sensitivity,” IEEE Trans. Audio
Electroacoustics, Vol. AU-20, October 1972, pp. 240–246.
[15] Jackson, L. B. “On the Interaction of Roundoff Noise and Dynamic Range in Digital Fil-
ters,” Bell System Technical Journal, Vol. 49, February 1970, pp. 159–184.
[16] Roberts, R. A., and Mullis, C. T. Digital Signal Processing, Addison-Wesley, Reading, Mass-
achusetts, 1987, p. 277.
[17] Jackson, L. B. “Roundoff Noise Analysis for Fixed-Point Digital Filters Realized in Cas-
cade or Parallel Form,” IEEE Trans. Audio Electroacoustics, Vol. AU-18, June 1970,
pp.107–122.
[18] Oppenheim, A. V., and Schafer, R. W. Discrete-Time Signal Processing, Prentice Hall, Engle-
wood Cliffs, New Jersey, 1989, Sections 9.7 and 9.8.
[19] Larimer, J., and Chen, D. “Fixed or Floating? APointed Question in DSPs,” EDN Maga-
zine, August 3, 1995.
[20] Ashton, C. “Floating Point Math Handles Iterative and Recursive Algorithms,” EDN
Magazine, January 9, 1986.
[21] Windsor, B., and Wilson, J. “Arithmetic Duo Excels in Computing Floating Point Prod-
ucts,” Electronic Design, May 17, 1984.
[22] Windsor, W. A. “IEEE Floating Point Chips Implement DSPArchitectures,” Computer De-
sign, January 1985.
[23] Texas Instruments Inc. Digital Signal Processing Applications with the TMS320 Family: The-
ory, Algorithms, and Implementations, SPRA012A, Texas Instruments, Dallas, Texas, 1986.
[24] Strauss, W. I. “Integer or Floating Point? Making the Choice,” Computer Design Magazine,
April 1, 1990, p. 85.

660 Digital Data Formats and Their Effects
[25] Oppenheim, A. V., and Weinstein, C. J. “Effects of Finite Register Length in Digital Filter-
ing and the Fast Fourier Transform,” Proceedings of the IEEE, August 1972, pp. 957–976.
[26] Woods, R. E. “Transform-Based Processing: How Much Precision Is Needed?” ESD: The
Electronic System Design Magazine, February 1987.

Chapter 12 Problems 661
CHAPTER 12 PROBLEMS
12.1 Given their specified format, convert the following integer binary numbers to
decimal format:
(a) 1100 0111, unsigned,
(b) 1100 0111, sign magnitude,
(c) 1100 0111, two’s complement,
(d) 1100 0111, offset binary.
12.2 Convert the following unsigned integer binary numbers, given here in hexa-
decimal format, to decimal:
(a) $A231,
(b) 0x71F.
12.3 Given the hexadecimal integer numbers $07 and $E2 in two’s complement
format, what is the decimal value of $07 minus $E2? Show your work.
12.4 Sign-extend the following two’s complement integer numbers, given in hexa-
decimal format, to 16 bits and express the results in hexadecimal format:
(a) $45,
(b) $B3.
12.5 Show that the binary addition operation
0000 1111
+0000 1101
———————-
0001 1100
gives the correct decimal results when the two binary addends and the sum
are in the following two’s complement fractional formats:
(a) 7.1 (7 integer bits and 1 fractional bit),
(b) 6.2 (6 integer bits and 2 fractional bits),
(c) 4.4 (4 integer bits and 4 fractional bits).
12.6 Microchip Technology Inc. produces a microcontroller chip (Part #PIC24F)
that accommodates 16-bit data words. When using a two’s complement inte-
ger number format, what are the most positive and most negative decimal
numbers that can be represented by the microcontroller’s data word?

662 Digital Data Formats and Their Effects
12.7 Consider four-bit unsigned binary words using a 2.2 (“two dot two”) “integer
plus fraction” format. List all 16 possible binary words in this format and give
their decimal equivalents.
12.8 The annual residential property tax in California is 0.0165 times the assessed
dollar value of the property. What is this 0.0165 tax rate factor in a two’s com-
plement 1.15 format? Give the answer in both binary and hexadecimal repre-
sentations. Show how you arrived at your solution.
12.9 The decimal number 1/3 cannot be represented exactly with a finite number
of decimal digits, nor with a finite number of binary bits. What would be the
base of a number system that would allow decimal 1/3 to be exactly repre-
sented with a finite number of digits?
12.10 If the number 4273 is in a base 6 numbering system, what would be its deci-
6
mal value?
12.11 Think about a 32-bit two’s complement fixed-point binary number having 31
fractional bits (a “1.31” two’s complement number). This number format is
very common in today’s high-performance programmable DSPchips.
(a) What is the most positive decimal value that can be represented by such a
binary number? Show how you arrived at your solution.
(b) What is the most negative decimal value?
12.12 As of this writing, Analog Devices Inc. produces an integrated circuit (Part
#AD9958), called a direct digital synthesizer, that generates high-precision ana-
log sinewaves. The AD9958 uses a 31-bit binary word to control the device’s
output frequency. When the control word is at its minimum value, the de-
vice’s output frequency is zero Hz. When the control word is at its maximum
value, the output frequency is 250 MHz. What is the frequency resolution (the
frequency step size) of this sinusoidal signal generator in Hz?
12.13 The first commercial audio compact disc (CD) players used 16-bit samples to
represent an analog audio signal. Their sample rate was f= 44.1 kHz. Those
s
16-bit samples were applied to a digital-to-analog (D/A) converter whose
analog output was routed to a speaker. What is the combined data output rate
of the digital portion, measured in bytes (8-bit binary words) per second, of a
stereo CD player?
12.14 When implementing a digital filter using a fixed-point binary number format,
care must be taken to avoid arithmetic overflow errors. With that notion in
mind, if the x(n) input samples in Figure P12–14 are eight-bit binary words,

Chapter 12 Problems 663
how many bits are needed to represent the y(n) output sequence to avoid any
data overflow errors? Show how you arrived at your answer.
Hint:Review the last portion of the text’s Section 12.3.2.
x(n) y(n)
z –1
z –1
0.85
–0.12
Figure P12–14
12.15 Review the brief description of allpass filters in Appendix F. One form of an
allpass filter is shown in Figure P12–15(a). For the filter to have the desired
constant magnitude response over its full operating frequency, coefficient A
must be equal to
1
A= .
B
If the filter is designed such that B=2.5, show why we cannot achieve the de-
sired constant frequency magnitude response when coefficients A and B are
quantized using four-bit unsigned binary words in a 2.2 (“two dot two”) “in-
teger plus fraction” format, where A and B are the quantized coefficients as
Q Q
shown in Figure P12–15(b).
x(n) y(n) x(n) y(n)
– –
z–1 z–1
A = 1/B B A B
Q Q
(a) (b)
Figure P12–15

664 Digital Data Formats and Their Effects
12.16 National Semiconductors Inc. produces a digital tuner chip (Part #CLC5903),
used for building digital receivers, that has the capability to amplify its out-
put signal by shifting its binary signal sample values to the left by as few as
one bit to as many as seven bits. What is the maximum gain, measured in dB
(decibels), of this tuner’s bit-shifting amplification capability?
12.17 Figure P12–17 shows an algorithm that approximates the operation of divid-
ing a sign-magnitude binary number x(n) by an integer value K. (Ablock con-
taining the “—> 2” symbol means truncation by way of a binary right shift by
two bits.) What is the value of integer K? Show your work.
x(n) 2
2
2
2
2
2
2
x(n)/K
Figure P12–17
12.18 When using programmable DSPchips, multiplication is a simple straightfor-
ward operation. However, when using field-programmable gate arrays
(FPGAs), multiplier hardware is typically difficult to implement and should
be avoided whenever possible. Figure P12–18 shows how we can multiply
a binary x(n) input sequence by 54, without the need for multiplier hard-
ware. What are the values for A and B in Figure P12–18 so that y(n) equals
54 times x(n)?
x(n)
Left shift by
A bits .
y(n) = 54 x(n)
Left shift by
3 bits
–
Left shift by
B bits
Figure P12–18

Chapter 12 Problems 665
12.19 Consider the network shown in Figure P12–19 which approximates a 2nd-
order differentiation operation. In many DSP implementations (using field-
programmable gate arrays, for example) it is advantageous to minimize the
number of multiplications. Assuming that all the sequences in Figure P12–19
use a binary two’s complement integer number format, what data bit manip-
ulations must be implemented to eliminate the two multipliers?
x(n) x(n–2) x(n–4)
z–2 z–2
–2
y(n)
0.25
Figure P12–19
12.20 Agilent Inc. produces an A/D converter (Model #DP1400) whose sample rate
is 2x109samples/second (f =2 GHz). This digitizer provides super-fine time
s
resolution samples of analog signals whose durations are T= 5x10–6 seconds
(5 microseconds) as shown in Figure P12–20. If each converter output sample
is stored in one memory location of a computer, how many memory locations
are required to store the converter’s x(n) output sequence representing the
5-microsecond-duration x(t) signal?
x(t)
T = 5
microseconds
t
Figure P12–20
12.21 Here is a problem often encountered by DSP engineers. Assume we sample
exactly three cycles of a continuous x(t) sinewave resulting in a block of 1024

666 Digital Data Formats and Their Effects
x(n) time samples and compute a 1024-point fast Fourier transform (FFT) to
obtain the FFT magnitude samples. Also assume that we repeat the sampling
and FFT magnitude computations many times and average the FFT magni-
tude sequences to produce the average magnitude samples, |X (m)|,
ave
shown in Figure P12–21. (We averaged multiple FFT magnitude sequences to
increase the accuracy, by reducing the variance, of our final |X (m)| se-
ave
quence.) If the A/D converter produces ten-bit binary words in sign-magni-
tude format and has an input full-scale bipolar voltage range of ±5 volts,
what is the peak value of the continuous x(t) sinewave? Justify your answer.
|X (m)|
ave
65536
0
0 1 2 3 4 5 6 1023 m
1021
Figure P12–21
12.22 Suppose we have a 12-bit A/D converter that operates over an input voltage
range of ±5 volts (10 volts peak-peak). Assume the A/D converter is ideal in
its operation and its transfer function is that shown in Figure P12–22 where
the tick mark spacing of the x(t) and x(n) axes is the converter’s quantization-
level q.
x(n)
. . .
3q
2q
q
–2.5 1.5 3.5
–3.5 –1.5 0.5 2.5 x(t)
–q
–2q
. . . –3q
Figure P12–22

Chapter 12 Problems 667
(a) What is the A/D converter’s quantization-level q (least significant bit)
voltage?
(b) What are the A/D converter’s maximum positive and maximum negative
quantization error voltages?
(c) If we apply a 7-volt peak-peak sinusoidal voltage to the converter’s ana-
log input, what A/D output signal-to-quantization noise value, SNR
A/D
in dB, should we expect? Show how you arrived at your answer.
12.23 Suppose an A/D converter manufacturer applies a 10-volt peak-peak sinu-
soidal voltage to their 12-bit converter’s analog input, conducts careful test-
ing, and measures the converter’s overall signal-to-noise level to be 67 dB.
What is the effective number of bitsvalue, b , for their A/D converter?
eff
12.24 Let’s reinforce our understanding of the quantization errors induced by typi-
cal A/D converters.
(a) Figure P12–24 shows the quantized x(n) output integer values of truncat-
ing and rounding A/D converters as a function of their continuous x(t)
input voltage. It’s sensible to call those bold stair-stepcurves the “transfer
functions” of the A/D converters. The curves are normalized to the A/D
converter’s quantization-level voltage q, such that an x(t) value of 2 repre-
sents a voltage of 2q volts. Draw the curves of the quantization error as a
function of the continuous x(t) input for both truncating and rounding
A/D converters.
x(n),quantized x(n),quantized
by truncation by rounding
3q 3q
2q 2q
q q
–2.5 1.5 3.5
–3 –2 –1 1 2 3 4 x(t) –3.5 –1.5 0.5 2.5 x(t)
–q –q
–2q –2q
–3q –3q
Figure P12–24
(b) Fill in the following table of important A/D converter quantization error
properties in terms of the A/D converters’ quantization-level voltage q.

668 Digital Data Formats and Their Effects
Max peak-peak
Most negative Most positive quantization
A/D Type quantization error quantization error error
Truncating
Rounding
12.25 Assume we want to digitize the output voltage of a temperature measure-
ment system, monitoring the internal temperature of an automobile radiator,
as shown in Figure P12–25. The system’s manufacturer states that its output
voltage v(t) will represent the thermocouple’s junction temperature with an
accuracy of 2 degrees Fahrenheit (1.1 degrees Celsius), and its operating
range covers temperatures as low as just-freezing water to twice the tempera-
ture of boiling water. To accommodate the precision and operating range of
the temperature measurement system, how many bits, b, do we need for our
A/D converter? Show your work.
Signal v(t)
A/D
conditioning
converter
hardware
b bits
Thermocouple is
Computer
inside radiator
Figure P12–25
12.26 One useful way to test the performance of A/D converters is to apply a spe-
cific analog signal to the A/D converter’s analog input and perform a his-
togram of the converter’s output samples. For example, if an analog
squarewave-like signal is applied to an A/D converter, the converter’s output
sequence might be that shown in the left panel of Figure P12–26(a), and the
histogram of the converter’s output samples is shown in the right panel of
Figure P12–26(a). That histogram shows that there are many converter out-
put samples whose values are –0.2, and many converter output samples
whose values are 0.5, and no sample values other than –0.2 and 0.5. The
shape of the histogram curve will indicate any severe defects in the con-
verter’s performance.
If a triangular analog signal is applied to an A/D converter, the con-
verter’s output sequence would be that shown in the left panel of Figure

Chapter 12 Problems 669
P12–26(b) and the histogram of the converter’s output samples is shown in
the right panel of Figure P12–26(b). This histogram shows that there are (ide-
ally) an equal number of samples at all amplitudes between –1 and +1, which
happens to indicate correct converter behavior.
In the testing of high-frequency A/D converters, high-frequency analog
square and triangular waves are difficult to generate, so A/D converter engi-
neers use high-frequency analog sinewaves to test their converters. Assuming
that an analog sinewave is used as an input for A/D converter histogram test-
ing and the converter output samples are those shown in the left panel of Fig-
ure P12–26(c), draw a rough sketch of the histogram of converter output
samples.
s Histogram of
0.4 c e sequence #1
n
Sequence #1 e
0.2 urr
(a) c c
0 o
of
–0.2 #
0
0 Time –0.2 0 0.2 0.4 0.6
Sample value
1 Histogram of
Sequence #2 e s sequence #2
0.5 n c
e
(b)
0
urr
c
c
–0.5
of
o
–1 # 0
0 Time –1 –0.5 0 0.5 1
Sample value
1 s Histogram of
0.5 n c e sequence #3
e
(c) 0
urr
?
c
Sequence #3 c
o
–0.5
of
#
–1 0
0 Time –1 –0.5 0 0.5 1
Sample value
Figure P12–26
12.27 In the text we discussed how to use the concept of a uniform probability den-
sity function (PDF), described in Section D.3 of Appendix D, to help us deter-
mine the variance (a measure of power) of random A/D-converter

670 Digital Data Formats and Their Effects
quantization noise. Sometimes we want to generate random noise samples,
for testing purposes, that have a uniform PDF such as that shown in Figure
P12–27. What is the value of A for a uniform PDF random sequence whose
variance is equal to 2?
pdf
1
2A
A 0 A Noise
amplitude
Figure P12–27
12.28 Assume we have a single numerical data sample value in floating-point bi-
nary format. What two bit manipulation methods exist to multiply that sam-
ple by 4 without using any multiplier hardware circuitry?
12.29 Convert the following IEEE P754 floating-point number, given here in hexa-
decimal format, to a decimal number:
$C2ED0000
Show your work.
Hint:Don’t forget to account for the hidden one in the IEEE P754 format.

CHAPTER THIRTEEN
Digital Signal
4
2
Processing
0
Tricks
As we study the literature of digital signal processing, we’ll encounter some
creative techniques that professionals use to make their algorithms more effi-
cient. These practical techniques are straightforward examples of the philoso-
phy “Don’t work hard, work smart,” and studying them will give us a deeper
understanding of the underlying mathematical subtleties of DSP. In this chap-
ter, we present a collection of these tricks of the trade, in no particular order,
and explore several of them in detail because doing so reinforces the lessons
we’ve learned in previous chapters.
13.1 FREQUENCY TRANSLATION WITHOUT MULTIPLICATION
Frequency translation is often called for in digital signal processing algo-
rithms. There are simple schemes for inducing frequency translation by 1/2
and 1/4 of the signal sequence sample rate. Let’s take a look at these mixing
schemes.
13.1.1 Frequency Translation by f/2
s
First we’ll consider a technique for frequency translating an input sequence
by f/2 by merely multiplying a sequence by (–1)n=1,–1,1,–1, ..., etc., where f
s s
is the signal sample rate in Hz. This process may seem a bit mysterious at
first, but it can be explained in a straightforward way if we review Fig-
ure 13–1(a). There we see that multiplying a time-domain signal sequence by
the (–1)nmixing sequence is equivalent to multiplying the signal sequence by
a sampled cosinusoid where the mixing sequence samples are shown as the
671

672 Digital Signal Processing Tricks
(-1)n
1
. . .
1 3 5
(a) 0
0 2 4 6 n
-1
Magnitude of (-1)n sequence
N N = 32
(b) 0
0 4 8 12 16 20 24 28 30 m
Phase of (-1)n sequence
100
50
0o
(c) 0
4 8 12 16 20 24 28 30 m
-50
-100
Figure 13–1 Mixing sequence comprising (–1)n= 1,–1,1,–1, etc.: (a) time-domain
sequence; (b) frequency-domain magnitudes for 32 samples;
(c) frequency-domain phase.
dots in Figure 13–1(a). Because the mixing sequence’s cosine repeats every
two sample values, its frequency is f/2. Figures 13–1(b) and 13–1(c) show the
s
discrete Fourier transform (DFT) magnitude and phase of a 32-sample (–1)n
sequence. As such, the right half of those figures represents the negative fre-
quency range.
Let’s demonstrate this (–1)n mixing with an example. Consider a real
x(n) signal sequence having 32 samples of the sum of three sinusoids whose
|X(m)| frequency magnitude and φ(m) phase spectra are as shown in Figures
13–2(a) and 13–2(b). If we multiply that time signal sequence by (–1)n, the re-
sulting x (n) time sequence will have the magnitude and phase spectra that
1,–1
are shown in Figures 13–2(c) and 13–2(d). Multiplying a time signal by our
(–1)n cosine shifts half its spectral energy up by f/2 and half its spectral en-
s
ergy down by –f/2. Notice in these non-circular frequency depictions that as
s
we count up, or down, in frequency, we wrap around the end points.
Here’s a terrific opportunity for the DSP novice to convolve the (–1)n
spectrum in Figure 13–1 with the X(m) spectrum to obtain the frequency-
translated X (m) signal spectrum. Please do so; that exercise will help you
1,–1
comprehend the nature of discrete sequences and their time- and frequency-
domain relationships by way of the convolution theorem.

13.1 Frequency Translation without Multiplication 673
|X(m)|
16 Negative-frequency
components
(a)
8
4
0
0 4 8 12 16 20 24 28 30 m
¯(m)
67.5o
100
45o
50 0o
(b) 0
4 8 16 20 24 28 30 m
-50
-100
|X (m)|
16 1,-1
(c)
8
4
0
0 4 8 12 16 20 24 28 30 m
¯ 1,-1 (m) 67.5o
100 45o
50 0o
28
(d) 0
4 8 12 16 20 24 30 m
-50
-100
Figure 13–2 A signal and its frequency translation by f/2: (a) original signal mag-
s
nitude spectrum; (b) original phase; (c) the magnitude spectrum of
the translated signal; (d) translated phase.
Remember, now, we didn’t really perform any explicit multiplications—
the whole idea here is to avoid multiplications; we merely changed the sign of
alternating x(n) samples to get x (n). One way to look at the X (m) magni-
1,–1 1,–1
tudes in Figure 13–2(c) is to see that multiplication by the (–1)n mixing se-
quence flips the positive-frequency band of X(m) (X(0) to X(16)) about the f/4
s
Hz point and flips the negative-frequency band of X(m) (X(17) to X(31)) about
the –f/4 Hz sample. This process can be used to invert the spectra of real sig-
s
nals when bandpass sampling is used as described in Section 2.4. By the way,
in the DSP literature be aware that some clever authors may represent the
(–1)nsequence with its equivalent expressions of
(–1)n= cos(πn) = ejπn. (13–1)

674 Digital Signal Processing Tricks
13.1.2 Frequency Translation by –f/4
s
Two other simple mixing sequences form the real and imaginary parts of a
complex –f/4 oscillator used for frequency down-conversion to obtain a
s
quadrature version (complex and centered at 0 Hz) of a real bandpass signal
originally centered at f/4. The real (in-phase) mixing sequence is
s
cos(πn/2) = 1,0,–1,0, etc., shown in Figure 13–3(a). That mixing sequence’s
quadrature companion is –sin(πn/2) = 0,–1,0,1, etc., as shown in Figure
13–3(b). The spectral magnitudes of those two sequences are identical as
shown in Figure 13–3(c), but their phase spectrum has a 90-degree shift rela-
tionship (what we call quadrature).
1,0,-1,0, ...
1
. . .
2 6 10
(a) 0
0 4 8 12 n
-1
0,-1,0,1, ...
1
. . .
1 5 9
(b) 0
0 3 7 11 n
-1
Mag of 1,0,-1,0, ..., and 0,-1,0,1, ...
N/2
Negative-frequency
component
(c) 0
0 4 8 12 16 20 24 28 30 m
Phase of 1,0,-1,0, ...
90
0o 0o
(d) 0
4 8 12 16 20 24 28 30 m
-90
Phase of 0,-1,0,1, ...
90 90o
12 24
(e) 0
4 8 16 20 28 30 m
-90 -90o
Figure 13–3 Quadrature mixing sequences for down-conversion by f/4: (a) in-
s
phase mixing sequence; (b) quadrature-phase mixing sequence;
(c)the frequency magnitudes of both sequences for N=32 samples;
(d) the phase of the cosine sequence; (e) phase of the sine sequence.

13.1 Frequency Translation without Multiplication 675
If we multiply the x(n) sequence whose spectrum is that shown in Fig-
ures 13–2(a) and 13–2(b) by the in-phase (cosine) mixing sequence, the prod-
uct will have the I(m) spectrum shown in Figures 13–4(a) and 13–4(b). Again,
X(m)’s spectral energy is translated up and down in frequency, only this time
the translation is by ±f/4. Multiplying x(n) by the quadrature-phase (sine) se-
s
quence yields the Q(m) spectrum in Figures 13–4(a) and 13–4(c).
Because their time sample values are merely 1, –1, and 0, the quadrature
mixing sequences are useful because down-conversion by f/4 can be imple-
s
mented without multiplication. That’s why these mixing sequences are of so
much interest: down-conversion of an input time sequence is accomplished
merely with data assignment, or signal routing.
To down-convert a general x(n) = x (n) + jx (n) sequence by f/4, the
real imag s
value assignments are
x (0) = x (0) + jx (0)
new real imag
x (1) = x (1) – jx (1)
new imag real
x (2) = –x (2) – jx (2)
new real imag
x (3) = –x (3) + jx (3)
new imag real
repeat for down-conversion ... (13–2)
|I(m)| and |Q(m)|
8
(a)
4
2
0
0 4 8 12 16 20 24 28 30 m
¯ø I (m) 67.5o
100
45o
50 0o
4 20
(b) 0
8 12 16 24 28 30 m
-50
-100
¯ø(m) 157.5o
Q
200 90o 135o
45o 90o
100
22.5o
4
(c) 0
8 12 16 20 24 28 30 m
-100
-200
Figure 13–4 Spectra after translation down by f/4: (a) I(m) and Q(m) spectral
s
magnitudes; (b) phase of I(m) ; (c) phase of Q(m).

676 Digital Signal Processing Tricks
If your implementation is hardwired gates, the above data assignments are
performed by means of routing signals (and their negatives). Although we’ve
focused on down-conversion so far, it’s worth mentioning that up-conversion
of a general x(n) sequence by f/4 can be performed with the following data
s
assignments:
x (0) = x (0) + jx (0)
new real imag
x (1) = –x (1) + jx (1)
new imag real
x (2) = –x (2) – jx (2)
new real imag
x (3) = x (3) – jx (3)
new imag real
repeat for up-conversion ... (13–3)
We notify the reader, at this point, that Section 13.29 presents an interest-
ing trick for performing frequency translation using decimation rather than
multiplication.
13.1.3 Filtering and Decimation after f/4 Down-Conversion
s
There’s an efficient way to perform the complex down-conversion, by f/4,
s
and filtering of a real signal process that we discussed for the quadrature
sampling scheme in Section 8.9. We can use a novel technique to greatly re-
duce the computational workload of the linear-phase lowpass filters[1–3]. In
addition, decimation of the complex down-converted sequence by a factor of
two is inherent, with no effort on our part, in this process.
Considering Figure 13–5(a), notice that if an original x(n) sequence was
real-only, and its spectrum is centered at f/4, multiplying x(n) by
s
cos(πn/2) = 1,0,–1,0, for the in-phase path and –sin(πn/2) = 0,–1,0,1, for the
quadrature-phase path to down-convert x(n)’s spectrum to 0 Hz yields the
new complex sequence x (n)=x (n) + x (n), or
new i q
x (0) = x(0) + j0
new
x (1) = 0 – jx(1)
new
x (2) = –x(2) + j0
new
x (3) = 0 + jx(3)
new
x (4) = x(4) + j0
new
x (5) = 0 – jx(5)
new
repeat ... (13–4)
Next, we want to lowpass filter (LPF) both the x(n) and x (n) sequences fol-
i q
lowed by decimation by a factor of two.
Here’s the trick. Let’s say we’re using 5-tap FIR filters and at the n = 4
time index the data residing in the two lowpass filters would be that shown
in Figures 13–5(b) and 13–5(c). Due to the alternating zero-valued samples in
the x(n) and x (n) sequences, we see that only five nonzero multiplies are
i q
being performed at this time instant. Those computations, at time index n=4,
are shown in the third row of the rightmost column in Table 13–1. Because

13.1 Frequency Translation without Multiplication 677
x(n)
i In-phase
2 i(n')
LPF
x(n)
(a) x(n') = i(n') + jq(n')
cos(πn/2) c
Quadrature
2 q(n')
x (n) LPF
q
-sin(πn/2)
In-phase filter at time n = 4
x(4) 0 -x(2) 0 x(0)
x(n) z -1 z -1 z -1 z -1
i
(b)
h h h h h
0 1 2 3 4
i(4)
Quadrature-phase filter at time n= 4
0 x(3) 0 -x(1) 0
x (n) z -1 z -1 z -1 z -1
q
(c)
h h h h h
0 1 2 3 4
q(4)
Figure 13-5 Complex down-conversion by f/4 and filtering by a 5-tap LPF: (a) the
s
process; (b) in-phase filter data; (c) quadrature-phase filter data.
we’re decimating by two, we ignore the time index n = 5 computations. The
necessary computations during the next time index (n = 6) are given in the
fourth row of Table 13–1, where again only five nonzero multiplies are com-
puted.
Areview of Table 13–1 tells us we can multiplex the real-valued x(n) se-
quence, multiply the multiplexed sequences by the repeating mixing se-
quence 1,–1, ..., etc., and apply the resulting x(n) and x (n) sequences to two
i q
filters, as shown in Figure 13–6(a). Those two filters have decimated coeffi-
cients in the sense that their coefficients are the alternating h(k) coefficients
from the original lowpass filter in Figure 13–5. The two new filters are de-
picted in Figure 13–6(b), showing the necessary computations at time index
n=4. Using this new process, we’ve reduced our multiplication workload by
a factor of two. The original data multiplexing in Figure 13–6(a) is what im-
plemented our desired decimation by two.

678 Digital Signal Processing Tricks
Table 13–1 Filter Data and Necessary Computations after Decimation by Two
Time Data in the filters Necessary computations
n= 0 x(0) – – – – i(0) = x(0)h
0
h h h h h
0 1 2 3 4
0 – – – – q(0) = 0
n= 2 –x(2) 0 x(0) – – i(2) = x(0)h –x(2)h
2 0
h h h h h
0 1 2 3 4
0 –x(1) 0 – – q(2) = –x(1)h
1
n= 4 x(4) 0 –x(2) 0 x(0) i(4) = x(0)h –x(2)h +x(4)h
4 2 0
h h h h h
0 1 2 3 4
0 x(3) 0 –x(1) 0 q(4) = –x(1)h +x(3)h
3 1
n= 6 –x(6) 0 x(4) 0 –x(2) i(6) = –x(2)h +x(4)h –x(6)h
4 2 0
h h h h h
0 1 2 3 4
0 –x(5) 0 x(3) 0 q(6) = x(3)h –x(5)h
3 1
n= 8 x(8) 0 –x(6) 0 x(4) i(8) = x(4)h –x(6)h +x(8)h
4 2 0
h h h h h
0 1 2 3 4
0 x(7) 0 –x(5) 0 q(8) = –x(5)h +x(7)h
3 1
Here’s another feature of this efficient down-conversion structure. If
half-band filters are used in Figure 13–5(a), then only one of the coefficients in
the modified quadrature lowpass filter is nonzero. This means we can imple-
ment the quadrature-path filtering as K unit delays, a single multiply by the
original half-band filter’s center coefficient, followed by another K delay as
depicted in Figure 13–6(c). For an original N-tap half-band filter, Kis the inte-
ger part of N/4. If the original half-band filter’s h(N–1)/2 center coefficient is
0.5, as is often the case, we can implement its multiply by an arithmetic right
shift of the delayed x (n).
q
This down-conversion process is indeed slick. Here’s another attribute.
If the original lowpass filter in Figure 13–5(a) has an odd number of taps, the
coefficients of the modified filters in Figure 13–6(b) will be symmetrical, and
we can use the foldedFIR filter scheme (Section 13.7) to reduce the number of
multipliers by almost another factor of two!
Finally, if we need to invert the output x (n’) spectrum, there are two
c
ways to do so. We can negate the 1,–1, sequence driving the mixer in the
quadrature path, or we can swap the order of the single unit delay and the
mixer in the quadrature path.

13.2 High-Speed Vector Magnitude Approximation 679
x(n) Modified
i in-phase i(n')
–x(6),x(4),–x(2),x(0)
LPF
x(n)
1,-1, ... x(n')
(a) c
x (n) Modified
z–1 q quadrature q(n')
x(7),–x(5),x(3),–x(1),0
LPF
Modified in-phase filter Modified quadrature
at time n = 4 filter at time n= 4
x(4) –x(2) x(0) x(3) –x(1)
x(n) z –1 z –1 x (n) z –1
i q
(b)
h h h h h
0 2 4 1 3
i(4) q(4)
x(n) Modified
i in-phase i(n')
LPF
x(n)
(c) 1,–1,1,–1 ... h (N-1)/2 x c (n')
x (n)
z–1 q z–K z–K q(n')
Figure 13–6 Efficient down-conversion, filtering by a 5-tap LPF, and decimation:
(a) process block diagram; (b) the modified filters and data at time
n= 4; (c)process when a half-band filter is used.
13.2 HIGH-SPEED VECTOR MAGNITUDE APPROXIMATION
The quadrature processing techniques employed in spectrum analysis, com-
puter graphics, and digital communications routinely require high-speed de-
termination of the magnitude of a complex number (vector V) given its real
and imaginary parts, i.e., the in-phase part Iand the quadrature-phase part Q.
This magnitude calculation requires a square root operation because the mag-
nitude of Vis
|V|= I2 +Q2 . (13–5)
Assuming that the sum I2 + Q2 is available, the problem is to efficiently per-
form the square root computation.

680 Digital Signal Processing Tricks
There are several ways to obtain square roots, but the optimum tech-
nique depends on the capabilities of the available hardware and software. For
example, when performing a square root using a high-level software lan-
guage, we employ whatever software square root function is available. Accu-
rate software square root routines, however, require many floating-point
arithmetic computations. In contrast, if a system must accomplish a square
root operation in just a few system clock cycles, high-speed magnitude ap-
proximations are required[4,5]. Let’s look at a neat magnitude approximation
scheme that avoids the dreaded square root operation.
There is a technique called the αMax+βMin (read as “alpha max plus
beta min”) algorithm for estimating the magnitude of a complex vector.†It’s a
linear approximation to the vector magnitude problem that requires the de-
termination of which orthogonal vector, Ior Q, has the greater absolute value.
If the maximum absolute value of Ior Qis designated by Max, and the mini-
mum absolute value of either I or Q is Min, an approximation of |V| using
the αMax+βMin algorithm is expressed as
|V| ≈αMax + βMin. (13–6)
There are several pairs for the α and β constants that provide varying
degrees of vector magnitude approximation accuracy to within 0.1 dB[4,7].
The αMax+βMin algorithms in reference [8] determine a vector magnitude at
whatever speed it takes a system to perform a magnitude comparison, two
multiplications, and one addition. But those algorithms require, as a mini-
mum, a 16-bit multiplier to achieve reasonably accurate results. If, however,
hardware multipliers are not available, all is not lost. By restricting the αand
βconstants to reciprocals of integer powers of two, Eq. (13–6) lends itself well
to implementation in binary integer arithmetic. A prevailing application of
the αMax+βMin algorithm uses α=1.0 and β=0.5. The 0.5 multiplication op-
eration is performed by shifting the value Min to the right by one bit. We can
gauge the accuracy of any vector magnitude estimation algorithm by plotting
its |V| as a function of vector phase angle. Let’s do that. The Max + 0.5Min
estimate for a complex vector of unity magnitude, over the vector angular
range of 0 to 90 degrees, is shown as the solid curve in Figure 13–7. (The
curves in Figure 13–7 repeat every 90 degrees.)
An ideal estimation curve for a unity magnitude vector would have a
value of one, and we’ll use this ideal curve as a yardstick to measure the merit of
various αMax+βMin algorithms. Let’s make sure we know what the solid curve
in Figure 13–7 is telling us. That curve indicates that a unity magnitude vector
oriented at an angle of approximately 26 degrees will be estimated by Eq. (13–6)
to have a magnitude of 1.118 instead of the correct magnitude of one. The error
†A“Max+βMin” algorithm had been in use, but in 1988 this author suggested expanding it to the
αMax+βMin form where αcould be a value other than unity[6].

13.2 High-Speed Vector Magnitude Approximation 681
Vector magnitude estimate
1.15
Max + Min/2
1.10
Max + 3Min/8
1.05
1.00
Ideal estimate curve
0.95
Max + Min/4
0.90
0.85
0 10 20 30 40 50 60 70 80 90
Vectorphase angle(degrees)
Figure 13–7 αMax+βMin estimation performance.
then, at 26 degrees, is 11.8 percent. For comparison, two other magnitude ap-
proximation curves for various values of αand βare shown in Figure 13–7.
Although the values for αand βin Figure 13–7 yield somewhat accurate
vector magnitude estimates, there are other values for α and β that deserve
our attention because they result in smaller magnitude estimation errors. The
α = 15/16 and β = 15/32 solid curve in Figure 13–8 is an example of a
reduced-error algorithm. Multiplications by those values of α and β can be
performed by multiplying by 15 and using binary right shifts to implement
the divisions by 16 and 32. A mathematically simple, single-multiply, α = 1
and β=0.4 algorithm is also shown as the dashed curve[9]. For the interested
reader, the performance of the optimum values for α and β is shown as the
Vector magnitude estimate
1.15
1.10 Max + 0.4Min
1.05
1.00
0.95
15(Max + Min/2)/16 0.9486Max + 0.39293Min
0.90
0.85
0 10 20 30 40 50 60 70 80 90
Vectorphase angle(degrees)
Figure 13–8 Alternate αMax+βMin algorithm performance.

682 Digital Signal Processing Tricks
dotted curve in Figure 13–8. (The word optimum, as used here, means mini-
mizing the magnitude estimation error fluctuations both above and below the
ideal unity line.)
To add to our catalog of magnitude estimation algorithms, at the ex-
pense of an additional multiply/shift and a compare operation, an accurate
magnitude estimation scheme is that defined by Eq. (13–7)[10]:
⎧ Max+Min/8, ifMin<3Max/8
|V|=⎨ (13–7)
⎩27Max/32+19Minn/16, if Min≥3Max/8.
Again, the divisions in Eq. (13–7) are implemented as binary right shifts.
In a similar vein we mention an algorithm that exhibits a maximum error of a
mere 1 percent, when floating-point arithmetic is used, as defined by Eq.
(13–7’)[11]:
⎧0.99Max+0.197Min, ifMin<0.4142135Max
|V|=⎨ (13–7’)
⎩0.884Max+0.561Min, if Min≥0.4142135Max.
The performance curves of the last two magnitude estimation algo-
rithms are shown in Figure 13–9.
To summarize the behavior of the magnitude estimation algorithms we
just covered so far, the relative performances of the various algorithms are
shown in Table 13–2. The table lists the magnitude of the algorithms’ maximum
error in both percent and decibels. The rightmost column of Table 13–2 is the
mean squared error (MSE) of the algorithms. That MSE value indicates how
much the algorithms’ results fluctuate about the ideal result of one, and we’d
like to have that MSE value be as close to zero (a flat line) as possible.
So, the αMax+βMin algorithms enable high-speed vector magnitude
computation without the need for performing square root operations. Of
Vector magnitude estimate
1.02
Max + Min/8
27Max/32 + 9Min/16
1.01
1.00
0.99
0.99Max + 0.197Min
0.84Max + 0.561Min
0.98
0 10 20 30 40 50 60 70 80 90
Vector phase angle (degrees)
Figure 13–9 AdditionalαMax+βMin algorithm performance.

13.3 Frequency-Domain Windowing 683
Table 13–2 αMax+βMin Algorithm Performance Comparisons
Maximum Maximum Mean
|error| |error| squared
Algorithm (%) (dB) error (x10–3)
Max + Min/2 11.8% 0.97 dB 8.4851
Max + Min/4 11.6% 0.95 dB 1.7302
Max + 3Min/8 6.8% 0.57 dB 2.2682
15(Max + Min/2)/16 6.3% 0.53 dB 1.2412
0.9486Max + 0.39293Min 5.1% 0.43 dB 0.5773
Max + 0.4Min 7.7% 0.64 dB 3.0573
Max + Min/8,
27Max/32 + 19Min/16 1.8% 0.16 dB 0.0744
0.99Max + 0.197Min,
0.84Max + 0.561Min 1.0% 0.09 dB 0.0456
course, with the availability of floating-point multiplier integrated circuits—
with their ability to multiply in one or two clock cycles—the α and β coeffi-
cients need not always be restricted to multiples of reciprocals of integer
powers of two.
13.3 FREQUENCY-DOMAIN WINDOWING
There’s an interesting technique for minimizing the calculations necessary to
implement windowing of FFT input data to reduce spectral leakage. There
are times when we need the FFT of unwindowed time-domain data, while at
the same time we also want the FFT of that same time-domain data with a
window function applied. In this situation, we don’t have to perform two
separate FFTs. We can perform the FFT of the unwindowed data, and then we
can perform frequency-domain windowing on that FFT result to reduce leak-
age. Let’s see how.
Recall from Section 3.9 that the expressions for the Hanning and the
Hamming windows were w (n) = 0.5 –0.5cos(2πn/N) and w (n) =
Han Ham
0.54 –0.46cos(2πn/N), respectively, where N is a window sequence length.
They both have the general cosine function form of
w(n) = α– βcos(2πn/N) (13–8)
for n=0, 1, 2, ..., N–1. Looking at the frequency response of the general cosine
window function, using the definition of the DFT, the transform of Eq. (13–8) is

684 Digital Signal Processing Tricks
N−1
W(m)= ∑ [α−βcos(2πn/N)]e −j2πnm/N. (13–9)
n=0
j2πn/N −j2πn/N
Because cos(2πn/N) = e + e , Eq. (13–9) can be written as
2 2
N∑ −1 β NN∑ −1 β N∑ −1
W(m)= αe −j2πnm/N – ⋅ ej2πn/Ne −j2πnm/N – ⋅ e −j2πn/Ne −j2πnm/N
2 2
n=0 n=0 n=0
N∑ −1 β N∑ −1 β N∑ −1
= α e −j2πnm/N – ⋅ ej2πn(m−1)/N – ⋅ e −j2πn(m+1)/N. (13–10)
2 2
n=0 n=0 n=0
Equation (13–10) looks pretty complicated, but using the derivation
from Section 3.13 for expressions like those summations, we find that
Eq. (13–10) merely results in the superposition of three sin(x)/x functions in
the frequency domain. Their amplitudes are shown in Figure 13–10.
Notice that the two translated sin(x)/xfunctions have sidelobes with op-
posite phase from that of the center sin(x)/x function. This means that Nα
times the mth bin output, minus Nβ/2 times the (m–1)th bin output, minus
β/2 times the (m+1)th bin output will minimize the sidelobes of the mth bin.
This frequency-domain convolution process is equivalent to multiplying the
input time data sequence by the N-valued window function w(n) in Eq.
(13–8)[12–14].
For example, let’s say the output of the mth FFT bin is X(m) =a + jb ,
m m
and the outputs of its two neighboring bins are X(m–1) = a + jb and
–1 –1
X(m+1)=a +jb . Then frequency-domain windowing for the mth bin of the
+1 +1
unwindowed X(m) is as follows:
β β
X (m)=αX(m) – X(m–1) – X(m+1)
three-term 2 2
β β
= α(a +jb ) – (a +jb ) – (a +jb ) (13–11)
m m 2 –1 –1 2 +1 +1
β β
= αa – (a + a )+j[αb – (b + b )].
m 2 –1 +1 m 2 –1 +1
To compute a windowed N-point FFT, X (m), we can apply Eq.
three-term
(13–11), requiring 4N additions and 3N multiplications, to the unwindowed
N-point FFT result X(m) and avoid having to perform the Nmultiplications of
time-domain windowing and a second FFT with its Nlog (N) additions and
2

13.3 Frequency-Domain Windowing 685
N
N /2
Freq
m-1 m m+1
Figure 13–10 General cosine window frequency response amplitude.
2Nlog (N) multiplications. (In this case, we called our windowed results
2
X (m) because we’re performing a convolution of a three-term W(m) se-
three-term
quence with the X(m) sequence.)
To accommodate the m = 0 beginning and the m = N–1 end of our
N-point FFT, we effectively wrap the FFT samples back on themselves. That
is, due to the circular nature of FFT samples based on real-valued time
sequences, we use
β β
X (0)=αX(0)− X(N−1)− X(1) (13–11’)
three-term 2 2
and
β β
X (N−1)=αX(N−1)− X(N−2)− X(0). (13–11’’)
three-term 2 2
Now if the FFT’s x(n) input sequence is real-only, then X(0) = a , and Eq.
0
(13–11’) simplifies to a real-only X (0)=αa – βa .
three-term 0 1
The neat situation here is the frequency-domain coefficients, values, α
and β, for the Hanning window. They’re both 0.5, and the multiplications in
Eq. (13–11) can be performed in hardware with two binary right shifts by a
single bit for α=0.5 and two shifts for each of the two β/2=0.25 factors, for a
total of six binary shifts. If a gain of four is acceptable, we can get away with
only two left shifts (one for the real and one for the imaginary parts of X(m))
using
X (m)= 2X(m) – X(m–1) – X(m+1). (13–12)
Hanning, gain=4
In application-specific integrated circuit (ASIC) and field-programmable gate
array(FPGA) hardware implementations, where multiplies are to be avoided,
the binary shifts can be eliminated through hardwired data routing. Thus
only additions are necessary to implement frequency-domain Hanning win-
dowing. The issues we need to consider are which window function is best
for the application, and the efficiency of available hardware in performing the

686 Digital Signal Processing Tricks
frequency-domain multiplications. Frequency-domain Hamming windowing
can be implemented but, unfortunately, not with simple binary shifts.
Along with the Hanning and Hamming windows, reference [14] de-
scribes a family of windows known as Blackman windows that provide fur-
ther FFT spectral leakage reduction when performing frequency-domain
windowing. (Note: Reference [14] reportedly has two typographical errors in
the 4-Term (–74 dB) window coefficients column on its page 65. Reference [15]
specifies those coefficients to be 0.40217, 0.49703, 0.09892, and 0.00188.) Black-
man windows have five nonzero frequency-domain coefficients, and their use
requires the following five-term convolution:
γ β β γ
X (m)=αX(m) + X(m–2) – X(m–1) – X(m+1) + X(m+2).(13–13)
five-term
2 2 2 2
Table 13–3 provides the frequency-domain coefficients for several com-
mon window functions.
Let’s end our discussion of the frequency-domain windowing trick by
saying this scheme can be efficient because we don’t have to window the en-
tire set of FFT data; windowing need only be performed on those FFT bin out-
puts of interest to us. An application of frequency-domain windowing is
presented in Section 13.18.
13.4 FAST MULTIPLICATION OF COMPLEX NUMBERS
The multiplication of two complex numbers is one of the most common func-
tions performed in digital signal processing. It’s mandatory in all discrete and
fast Fourier transformation algorithms, necessary for graphics transforma-
tions, and used in processing digital communications signals. Be it in hard-
ware or software, it’s always to our benefit to streamline the processing
Table 13–3 Frequency-Domain Windowing Coefficients
Window function (cid:3) (cid:4) (cid:5)
Rectangular 1.0 — —
Hanning 0.5 0.5 —
Hamming 0.54 0.46 —
Blackman 0.42 0.5 0.08
Exact Blackman 7938 9240 1430
18608 18608 18608
3-term Blackman-Harris 0.42323 0.49755 0.07922

13.5 Efficiently Performing the FFTof Real Sequences 687
necessary to perform a complex multiply whenever we can. If the available
hardware can perform three additions faster than a single multiplication,
there’s a way to speed up a complex multiply operation[16].
The multiplication of two complex numbers, a + jb and c + jd, results in
the complex product
R+ jI=(a+ jb)(c+ jd)= (ac– bd) + j(bc+ ad). (13–14)
We can see that Eq. (13–14) requires four multiplications and two addi-
tions. (From a computational standpoint we’ll assume a subtraction is equiva-
lent to an addition.) Instead of using Eq. (13–14), we can calculate the
following intermediate values:
k =a(c+ d) ,
1
k =d(a+ b) , and (13–15)
2
k =c(b– a).
3
We then perform the following operations to get the final Rand I:
R=k – k , and
1 2 (13–16)
I=k + k .
1 3
The reader is invited to plug the k values from Eq. (13–15) into
Eq. (13–16) to verify that the expressions in Eq. (13–16) are equivalent to
Eq. (13–14). The intermediate values in Eq. (13–15) required three additions
and three multiplications, while the results in Eq. (13–16) required two more
additions. So we traded one of the multiplications required in Eq. (13–14) for
three addition operations needed by Eqs. (13–15) and (13–16). If our hardware
uses fewer clock cycles to perform three additions than a single multiplica-
tion, we may well gain overall processing speed by using Eqs. (13–15) and
(13–16) instead of Eq. (13–14) for complex multiplication.
13.5 EFFICIENTLY PERFORMING THE FFT OF REAL SEQUENCES
Upon recognizing its linearity property and understanding the odd and even
symmetries of the transform’s output, the early investigators of the fast
Fourier transform (FFT) realized that two separate, real N-point input data se-
quences could be transformed using a single N-point complex FFT. They also
developed a technique using a single N-point complex FFT to transform a
2N-point real input sequence. Let’s see how these two techniques work.
13.5.1 Performing Two N-Point Real FFTs
The standard FFT algorithms were developed to accept complex inputs; that
is, the FFT’s normal input x(n) sequence is assumed to comprise real and
imaginary parts, such as

688 Digital Signal Processing Tricks
x(0) = x(0) + jx(0),
r i
x(1) = x(1) + jx(1),
r i
x(2) = x(2) + jx(2),
r i
. . .
. . .
x(N–1) = x(N–1) + jx(N–1). (13–17)
r i
In typical signal processing schemes, FFT input data sequences are usu-
ally real. The most common example of this is the FFT input samples coming
from an A/D converter that provides real integer values of some continuous
(analog) signal. In this case the FFT’s imaginary x(n)’s inputs are all zero. So
i
initial FFT computations performed on the x(n) inputs represent wasted opera-
i
tions. Early FFT pioneers recognized this inefficiency, studied the problem, and
developed a technique where two independent N-point, real input data se-
quences could be transformed by a single N-point complex FFT. We call this
scheme the Two N-Point Real FFTs algorithm. The derivation of this technique
is straightforward and described in the literature[17–19]. If two N-point, real
input sequences are a(n) and b(n), they’ll have discrete Fourier transforms rep-
resented by X(m) and X (m). If we treat the a(n) sequence as the real part of an
a b
FFT input and the b(n) sequence as the imaginary part of the FFT input, then
x(0) = a(0) + jb(0),
x(1) = a(1) + jb(1),
x(2) = a(2) + jb(2),
. . .
. . .
x(N–1) = a(N–1) + jb(N–1). (13–18)
Applying the x(n) values from Eq. (13–18) to the standard DFT,
N∑ −1
X(m)= x(n)e −j2πnm/N , (13–19)
n=0
we’ll get a DFT output X(m) where mgoes from 0 to N–1. (We’re assuming, of
course, that the DFT is implemented by way of an FFT algorithm.) Using the
superscript “*” symbol to represent the complex conjugate, we can extract the
two desired FFT outputs X (m) and X (m) from X(m) by using the following:
a b
X*(N−m)+X(m)
X (m)= (13–20)
a
2
and
j[X*(N−m)−X(m)]
X (m)= . (13–21)
b 2

13.5 Efficiently Performing the FFTof Real Sequences 689
Let’s break Eqs. (13–20) and (13–21) into their real and imaginary parts
to get expressions for X (m) and X (m) that are easier to understand and im-
a b
plement. Using the notation showing X(m)’s real and imaginary parts, where
X(m) = X(m) + jX(m), we can rewrite Eq. (13–20) as
r i
X (N−m)+X (m)+ j[X (m)−X (N−m)]
X (m)= r r i i (13–22)
a
2
where m= 1, 2, 3, . . ., N–1. What about the first X (m), when m= 0? Well, this
a
is where we run into a bind if we actually try to implement Eq. (13–20) di-
rectly. Letting m= 0 in Eq. (13–20), we quickly realize that the first term in the
numerator, X*(N–0) = X*(N), isn’t available because the X(N) sample does not
exist in the output of an N-point FFT! We resolve this problem by remember-
ing that X(m) is periodic with a period N, so X(N) = X(0).† When m = 0,
Eq.(13–20) becomes
X (0)− jX (0)+X (0)+ jX (0)
X (0)= r i r i =X (0).
a r
2 (13–23)
Next, simplifying Eq. (13–21),
j[X (N−m)− jX (N−m)−X (m)− jX (m)]
X (m)= r i r i
b 2
(13–24)
X (N−m)+X (m)+ j[X (N−m)−X (m)]
= i i r r
2
where, again, m= 1, 2, 3, . . ., N–1. By the same argument used for Eq. (13–23),
when m= 0, X (0) in Eq. (13–24) becomes
b
X (0)+X (0)+ j[X (0)−X (0)]
X (0)= i i r r =X (0). (13–25)
b 2 i
This discussion brings up a good point for beginners to keep in mind. In
the literature Eqs. (13–20) and (13–21) are often presented without any discus-
sion of the m= 0 problem. So, whenever you’re grinding through an algebraic
derivation or have some equations tossed out at you, be a little skeptical. Try
the equations out on an example—see if they’re true. (After all, both authors
and book typesetters are human and sometimes make mistakes. We had an
old saying in Ohio for this situation: “Trust everybody, but cut the cards.”)
Following this advice, let’s prove that this Two N-Point Real FFTs algorithm
really does work by applying the 8-point data sequences from Chapter 3’s
† This fact is illustrated in Section 3.8 during the discussion of spectral leakage in DFTs.

690 Digital Signal Processing Tricks
DFT examples to Eqs. (13–22) through (13–25). Taking the 8-point input data
sequence from Section 3.1’s DFT Example 1 and denoting it a(n),
a(0)=0.3535, a(1)=0.3535,
a(2)=0.6464, a(3)=1.0607,
a(4)=0.3535, a(5)=–1.0607,
a(6)=–1.3535, a(7)=–0.3535. (13–26)
Taking the 8-point input data sequence from Section 3.6’s DFT Example 2 and
calling it b(n),
b(0)=1.0607, b(1)=0.3535,
b(2)=–1.0607, b(3)=–1.3535,
b(4)=–0.3535, b(5)=0.3535,
b(6)=0.3535, b(7)=0.6464. (13–27)
Combining the sequences in Eqs. (13–26) and (13–27) into a single complex
sequence x(n),
a(n) b(n)
↓ ↓
x(n) = 0.3535 + j1.0607
+ 0.3535 + j0.3535
+ 0.6464 – j1.0607
+ 1.0607 – j1.3535
+ 0.3535 – j0.3535
– 1.0607 + j0.3535
– 1.3535 + j0.3535
– 0.3535 + j0.6464. (13–28)
Now, taking the 8-point FFT of the complex sequence in Eq. (13–28), we get
X(m) X(m)
r i
↓ ↓
X(m) = 0.0000 + j0.0000 ←m=0 term
– 2.8283 – j1.1717 ←m=1 term
+ 2.8282 + j2.8282 ←m=2 term
+ 0.0000 + j0.0000 ←m=3 term
+ 0.0000 + j0.0000 ←m=4 term
+ 0.0000 + j0.0000 ←m=5 term
+ 0.0000 + j0.0000 ←m=6 term
+ 2.8283 + j6.8282 ←m=7 term. (13–29)
So from Eq. (13–23),
X (0) =X(0) =0.
a r

13.5 Efficiently Performing the FFTof Real Sequences 691
To get the rest of X (m), we have to plug the FFT output’s X(m) and X(N–m)
a
values into Eq. (13–22).†Doing so,
X (7)+X (1)+ j[X (1)−X (7)] 2.8283−2.8283+ j[−1.1717−6.8282]
X (1)= r r i i =
a
2 2
0− j7.9999
= =0− j4.0=4– −90o,
2
X (6)+X (2)+ j[X (2)−X (6)] 0.0+2.8282+ j[2.8282−0.0]
X (2)= r r i i =
a
2 2
2.8282+ j2.8282
= =1.414+ j1.414=2– 45°,
2
X (5)+X (3)+ j[X (3)−X (5)] 0.0+0.0+ j[0.0−0.0]
X (3)= r r i i = =0–0° ,
a
2 2
X (4)+X (4)+ j[X (4)−X (4)] 0.0+0.0+ j[0.0−0.0]
X (4)= r r i i = =0–0° ,
a
2 2
X (3)+X (5)+ j[X (5)−X (3)] 0.0+0.0+ j[0.0−0.0]
X (5)= r r i i = =0–0° ,
a
2 2
X (2)+X (6)+ j[X (6)−X (2)] 2.8282+0.0+ j[0.0−2.8282]
X (6)= r r i i =
a
2 2
2.8282− j2.8282
= =1.414− j1.414=2– −45°, and
2
X (1)+X (7)+ j[X (7)−X (1)] −2.8282+2.8282+ j[6.8282+1.1717]
X (7)= r r i i =
a
2 2
0.0+ j79.999
= =0+ j4.0=4–90°.
2
So Eq. (13–22) really does extract X (m) from the X(m) sequence in Eq. (13–29).
a
We can see that we need not solve Eq. (13–22) when m is greater than 4
(or N/2) because X (m) will always be conjugate symmetric. Because
a
†Remember, when the FFT’s input is complex, the FFT outputs may not be conjugate symmet-
ric; that is, we can’t assume that F(m) is equal to F*(N–m) when the FFT input sequence’s real
and imaginary parts are both nonzero.

692 Digital Signal Processing Tricks
X (7) = X (1), X (6) = X (2), etc., only the first N/2 elements in X (m) are inde-
a a a a a
pendent and need be calculated.
OK, let’s keep going and use Eqs. (13–24) and (13–25) to extract X (m)
b
from the FFT output. From Eq. (13–25),
X (0) =X(0) =0.
b i
Plugging the FFT’s output values into Eq. (13–24) to get the next four X (m)s,
b
we have
X (7)+X (1)+ j[X (7)−X (1)] 6.8282−1.1717+ j[2.8283+2.8283]
X (1)= i i r r =
b 2 2
5.656+ j5.656
= =2.828+ j2.828=4– 45° ,
2
X (6)+X (2)+ j[X (6)−X (2)] 0.0+2.8282+ j[0.0−2.8282]
X (2)= i i r r =
b 2 2
2.8282− j2.8282
= =1.414− j1.414=2– −45° , and
2
X (5)+X (3)+ j[X (5)−X (3)] 0.0+0.0+ j[0.0−0.0]
X (3)= i i r r = =0–0° , and
b 2 2
X (4)+X (4)+ j[X (4)−X (4)] 0.0+0.0+ j[0.0−0.0]
X (4)= i i r r = =0–0°.
b 2 2
The question arises “With the additional processing required by Eqs.
(13–22) and (13–24) after the initial FFT, how much computational saving (or
loss) is to be had by this Two N-Point Real FFTs algorithm?” We can estimate
the efficiency of this algorithm by considering the number of arithmetic opera-
tions required relative to two separate N-point radix-2 FFTs. First, we estimate
the number of arithmetic operations in two separate N-point complex FFTs.
From Section 4.4, we know that a standard radix-2 N-point complex FFT
comprises (N/2) · log N butterfly operations. If we use the optimized butter-
2
fly structure, each butterfly requires one complex multiplication and two
complex additions. Now, one complex multiplication requires two real addi-
tions and four real multiplications, and one complex addition requires two
real additions.†So a single FFT butterfly operation comprises four real multi-
†Thecomplexa⋅ddition(a+jb)+(c+jd)=(a+c)+j(b+d)requirestworealadditions.Acomplexmulti-
plication(a+jb) (c+jd)=ac–bd+j(ad+bc)requirestworealadditionsandfourrealmultiplications.

13.5 Efficiently Performing the FFTof Real Sequences 693
plications and six real additions. This means that a single N-point complex
FFT requires (4N/2) · log Nreal multiplications, and (6N/2) · log Nreal addi-
2 2
tions. Finally, we can say that two separate N-point complex radix-2 FFTs
require
twoN-point complex FFTs → 4N· log Nreal multiplications, and (13–30)
2
6N· log Nreal additions. (13–30’)
2
Next, we need to determine the computational workload of the Two N-Point
Real FFTs algorithm. If we add up the number of real multiplications and real
additions required by the algorithm’s N-point complex FFT, plus those re-
quired by Eq. (13–22) to get X (m), and those required by Eq. (13–24) to get
a
X (m), the Two N-Point Real FFTs algorithm requires
b
twoN-Point Real FFTs algorithm → 2N· log N+Nreal multiplications, and (13–31)
2
3N· log N+ 2Nreal additions. (13–31’)
2
Equations (13–31) and (13–31’) assume that we’re calculating only the first
N/2 independent elements of X (m) and X (m). The single N term in Eq.
a b
(13–31) accounts for the N/2 divide by 2 operations in Eq. (13–22) and the
N/2 divide by 2 operations in Eq. (13–24).
OK, now we can find out how efficient the Two N-Point Real FFTs algo-
rithm is compared to two separate complex N-point radix-2 FFTs. This com-
parison, however, depends on the hardware used for the calculations. If our
arithmetic hardware takes many more clock cycles to perform a multiplica-
tion than an addition, then the difference between multiplications in Eqs.
(13–30) and (13–31) is the most important comparison. In this case, the per-
centage gain in computational saving of the Two N-Point Real FFTs algorithm
relative to two separate N-point complex FFTs is the difference in their neces-
sary multiplications over the number of multiplications needed for two sepa-
rate N-point complex FFTs, or
4N ⋅ log N−(2N ⋅ log N+N)⋅ 2 ⋅ log N−1⋅
2 ⋅ 2 100%= ⋅ 2 100%. (13–32)
4N log N 4 log N
2 2
The computational (multiplications only) saving from Eq. (13–32) is plotted as
the top curve of Figure 13–11. In terms of multiplications, for N≥32, the Two
N-Point Real FFTs algorithm saves us over 45 percent in computational work-
load compared to two separate N-point complex FFTs.
For hardware using high-speed multiplier integrated circuits, multiplica-
tion and addition can take roughly equivalent clock cycles. This makes addi-
tion operations just as important and time consuming as multiplications. Thus
the difference between those combined arithmetic operations in Eqs. (13–30)

694 Digital Signal Processing Tricks
plus (13–30’) and Eqs. (13–31) plus (13–31’) is the appropriate comparison. In
this case, the percentage gain in computational saving of our algorithm over
two FFTs is their total arithmetic operational difference over the total arith-
metic operations in two separate N-point complex FFTs, or
(4N ⋅ log N+6N ⋅ log N)−(2N ⋅ log N+N+3N ⋅ log N+2N)⋅
2 2 ⋅ ⋅ 2 2 100%
4N log N+6N log N
2 2
(13–33)
5 ⋅ log N−3⋅
= ⋅ 2 100%.
10 log N
2
The full computational (multiplications and additions) saving from Eq.
(13–33) is plotted as the bottom curve of Figure 13–11. This concludes our
discussion and illustration of how a single N-point complex FFT can be
used to transform two separate N-point real input data sequences.
13.5.2 Performing a 2N-Point Real FFT
Similar to the scheme above where two separate N-point real data se-
quences are transformed using a single N-point FFT, a technique exists
where a 2N-point real sequence can be transformed with a single complex
N-point FFT. This 2N-Point Real FFT algorithm, whose derivation is also
% Computational saving of the Two N-Point Real FFTs algorithm
50
48
Multiplications only
46
44
%
42
40
38
Multiplications and additions
36
34
10 0 10 1 10 2 10 3 10 4 10 5 N
Figure 13–11 Computational saving of the Two N-Point Real FFTs algorithm over
that of two separate N-point complex FFTs. The top curve indicates
the saving when only multiplications are considered. The bottom
curve is the saving when both additions and multiplications are
used in the comparison.

13.5 Efficiently Performing the FFTof Real Sequences 695
described in the literature, requires that the 2N-sample real input se-
quence be separated into two parts[19,20]—not broken in two, but un-
zipped—separating the even and odd sequence samples. The N even-
indexed input samples are loaded into the real part of a complex N-point
input sequence x(n). Likewise, the input’s N odd-indexed samples are
loaded into x(n)’s imaginary parts. To illustrate this process, let’s say we
have a 2N-sample real input data sequence a(n) where 0 ≤ n ≤ 2N–1. We
want a(n)’s 2N-point transform X (m). Loading a(n)’s odd/even sequence
a
values appropriately into an N-point complex FFT’s input sequence, x(n),
x(0) = a(0) + ja(1),
x(1) = a(2) + ja(3),
x(2) = a(4) + ja(5),
. . .
. . .
x(N–1) = a(2N–2) +ja(2N–1). (13–34)
Applying the N complex values in Eq. (13–34) to an N-point complex FFT,
we’ll get an FFT output X(m) = X(m) + jX(m), where mgoes from 0to N–1. To
r i
extract the desired 2N-Point Real FFT algorithm output X (m) = X (m) +
a a,real
jX (m)from X(m), let’s define the following relationships:
a,imag
X + (m)= X r (m)+X r (N−m) , (13–35)
r
2
X − (m)= X r (m)−X r (N−m) , (13–36)
r
2
X + (m)= X i (m)+X i (N−m) , and (13–37)
i
2
X − (m)= X i (m)−X i (N−m) . (13–38)
i
2
For the reasons presented following Eq. (13–22) in the last section, in the
above expressions recall that X(N) =X (0), and X(N) =X (0). The values re-
r r i i
sulting from Eqs. (13–35) through (13–38) are, then, used as factors in the fol-
lowing expressions to obtain the real and imaginary parts of our final X (m):
a
X (m)=X + (m)+cos(
πm
) ⋅ X + (m)−sin(
πm
) ⋅ X − (m) (13–39)
a,real r N i N r
and
X (m)=X − (m)−sin(
πm
) ⋅ X + (m)−cos(
πm
) ⋅ X − (m). (13–40)
a,imag i N i N r

696 Digital Signal Processing Tricks
Remember, now, the original a(n) input index ngoes from 0 to 2N–1, and our
N-point FFT output index mgoes from 0 to N–1. We apply 2Nreal input time-
domain samples to this algorithm and get back Ncomplex frequency-domain
samples representing the first half of the equivalent 2N-point complex FFT,
X (0) through X (N–1). Because this algorithm’s a(n) input is constrained to be
a a
real, X (N+1) through X (2N–1) are merely the complex conjugates of their
a a
X (1) through X (N–1) counterparts and need not be calculated.
a a
The above process does not compute the X (N) sample. The X (N) sam-
a a
ple, which is real-only, is
X (N) = X(0) – X(0). (13–40’)
a r i
To help us keep all of this straight, Figure 13–12 depicts the computational
steps of the 2N-Point Real FFT algorithm.
To demonstrate this process by way of example, let’s apply the 8-point
data sequence from Eq. (13–26) to the 2N-Point Real FFT algorithm. Partition-
ing those Eq. (13–26), samples as dictated by Eq. (13–34), we have our new
FFT input sequence:
x(0) = 0.3535 + j0.3535,
x(1) = 0.6464 + j1.0607,
x(2) = 0.3535 – j1.0607,
x(3) = –1.3535 – j0.3535. (13–41)
With N= 4 in this example, taking the 4-point FFT of the complex sequence in
Eq. (13–41), we get
X(m) X(m)
r i
↓ ↓
X(m) = 0.0000 + j0.0000 ←m=0 term
+ 1.4142 – j0.5857 ←m=1 term
+ 1.4141 – j1.4141 ←m=2 term
– 1.4142 + j3.4141 ←m=3 term. (13–42)
Using these values, we now get the intermediate factors from Eqs. (13–35)
through (13–38). Calculating our first X+(0) value, again we’re reminded that
r
X(m) is periodic with a period N, so X(4) = X(0), and X+(0) = [X (0) + X (0)]/2
r r r
= 0. Continuing to use Eqs. (13–35) through (13–38),
Unzip the Calculate the four Calculate the final
a(n) 2N-point real a(n) x(n) Calculate the X(m) N-point sequences N-point complex X (m)
s e e s q t u a e b n lis c h e t a h n e d com N pl - e p x o F in F t T of X r + (m),X r − (m), sequ X e n c e X (m a ( ) m) = a
N x - ( p n o ) i s n e t q c u o e m n p c l e e . x x(n) to get X(m). X + i (m), and X i − (m). + jX a, a r e , i a m l a g (m).
Figure 13–12 Computational flow of the 2N-Point Real FFT algorithm.

13.5 Efficiently Performing the FFTof Real Sequences 697
X + (0)=0, X − (0)=0, X + (0)=0, X − (0)=0,
r r i i
X + (1)=0, X − (1)=1.4142, X + (1)=1.4142, X − (1)=−1.9999,
r r i i
X + (2)=1.4141, X − (2)=0, X + (2)=−1.4144, X − (2))=0,
r r i i
X + (3)=0, X − (3)=−1.4142, X + (3)=1.4142, X − (3)=1.9999. (13–43)
r r i i
Using the intermediate values from Eq. (13–43) in Eqs. (13–39) and (13–40),
⋅ ⋅
⎛π 0⎞⋅ ⎛π 0⎞⋅
X (0)=(0)+cos⎜ (0)−sin⎜ ⎟ (0)
a,real ⎝ ⎠ ⎝ ⎠
4 4
⋅ ⋅
⎛π 0⎞⋅ ⎛π 0⎞⋅
X (0)=(0)−sin⎜ ⎟ (0)−cos⎜ ⎟ (0)
a,imag ⎝ ⎠ ⎝ ⎠
4 4
⎛π ⋅ 1⎞⋅ ⎛π ⋅ 1⎞⋅
X (1)=(0)+cos⎜ ⎟ (1.4142)−ssiinn⎜ ⎟ (1.4142)
a,real ⎝
4
⎠ ⎝
4
⎠
⋅ ⋅
⎛π 1⎞⋅ ⎛π 1⎞⋅
X (1)=(−1.9999)−sin⎜ ⎟ (1.4142)−cos⎜ ⎟ (1.4142)
a,imag ⎝ ⎠ ⎝ ⎠
4 4
⋅ ⋅
⎛π 2⎞⋅ ⎛π 2⎞⋅
X (2)=(1.4141)+cos⎜ ⎟ (−1.4144)−sin⎜ ⎟ (0))
a,real ⎝ ⎠ ⎝ ⎠
4 4
⋅ ⋅
⎛π 2⎞⋅ ⎛π 2⎞⋅
X (2)=(0)−sin⎜ (−1.4144)−cos⎜ ⎟ (0)
a,imag ⎝ ⎠ ⎝ ⎠
4 4
⋅ ⋅
⎛π 3⎞⋅ ⎛π 3⎞⋅
X (3)=(0)+cos⎜ (1.4142)−sin⎜ (−1.4142)
a,real ⎝ ⎠ ⎝ ⎠
4 4
X a,imag (3)=(1.9999)−sin ⎛ ⎝ ⎜ π
⋅
3⎞ ⎠ ⋅ (1.4142)−cos ⎛ ⎝ ⎜
π⋅
3⎞ ⎠ ⋅ (−1.4142). (13–44)
4 4
Evaluating the sine and cosine terms in Eq. (13–44),
X (0)=(0) + (1) · (0) – (0) · (0)=0,
a,real
X (0)=(0) – (0) · (0) – (1) · (0)=0,
a,imag
X (1)=(0) + (0.7071) · (1.4142) – (0.7071) · (1.4142)=0,
a,real
X (1)=(–1.9999) – (0.7071) · (1.4142) – (0.7071) · (1.4142)=–3.9999,
a,imag
X (2)=(1.4141) + (0) · (–1.4144) – (1) · (0)=1.4141,
a,real
X (2)=(0) – (1) · (–1.4144) – (0) · (0)=1.4144,
a,imag
X (3)=(0) + (–0.7071) · (1.4142) – (0.7071) · (–1.4142)=0, and
a,real
X (3)=(1.9999) – (0.7071) · (1.4142) – (–0.7071) · (–1.4142)=0. (13–45)
a,imag
Combining the results of the terms in Eq. (13–45), we have our final correct
answer of
X (0)=X (0) +jX (0)=0 + j0=0 ∠0°,
a a,real a,imag
X (1)=X (1) +jX (1)=0 – j3.999=4 ∠–90°,
a a,real a,imag
X (2)=X (2) +jX (2)=1.4141 + j1.4144=2 ∠45°, and
a a,real a,imag
X (3)=X (3) +jX (3)=0 + j0=0 ∠0°. (13–46)
a a,real a,imag

698 Digital Signal Processing Tricks
After going through all the steps required by Eqs. (13–35) through
(13–40), the reader might question the efficiency of this 2N-Point Real
FFT algorithm. Using the same process as the above Two N-Point Real
FFTs algorithm analysis, let’s show that the 2N-Point Real FFT algorithm
does provide some modest computational saving. First, we know that a
single 2N-point radix-2 FFT has (2N/2) · log 2N = N· (log N+1) butterflies
2 2
and requires
2N-point complex FFT → 4N· (log N+1) real multiplications (13–47)
2
and
6N· (log N+1) real additions. (13–47’)
2
If we add up the number of real multiplications and real additions required
by the algorithm’s N-point complex FFT, plus those required by Eqs. (13–35)
through (13–38) and those required by Eqs. (13–39) and (13–40), the complete
2N-Point Real FFT algorithm requires
2N-Point Real FFT algorithm → 2N· log N+ 8Nreal multiplications (13–48)
2
and
3N· log N+ 8Nreal additions. (13–48’)
2
OK, using the same hardware considerations (multiplications only) we
used to arrive at Eq. (13–32), the percentage gain in multiplication saving of
the 2N-Point Real FFT algorithm relative to a 2N-point complex FFT is
4N ⋅ (log N+1)−(2N ⋅ log N+8N)⋅
2 ⋅ 2 100%
4N (log N+1)
2
2N ⋅ log N+2N−N ⋅ log N−4N⋅
= 2 ⋅ 2 100%
2N log N+2N
2
log N−2 ⋅
= ⋅ 2 100%.
2 log N+2
2 (13–49)
The computational (multiplications only) saving from Eq. (13–49) is plotted
as the bottom curve of Figure 13–13. In terms of multiplications, the 2N-Point
Real FFT algorithm provides a saving of >30 percent when N ≥ 128 or when-
ever we transform input data sequences whose lengths are ≥256.
Again, for hardware using high-speed multipliers, we consider both
multiplication and addition operations. The difference between those com-
bined arithmetic operations in Eqs. (13–47) plus (13–47’) and Eqs. (13–48)

13.6 Computing the Inverse FFT Using the Forward FFT 699
% Computational saving of the 2N-Point Real FFT algorithm
50
Multiplications and additions
40
30
%
20
10
0
Multiplications only
–10
10 0 10 1 10 2 10 3 10 4 10 5 N
Figure 13–13 Computational saving of the 2N-Point Real FFT algorithm over that
of a single 2N-point complex FFT. The top curve is the saving when
both additions and multiplications are used in the comparison. The
bottom curve indicates the saving when only multiplications are
considered.
plus (13–48’) is the appropriate comparison. In this case, the percentage gain
in computational saving of our algorithm is
4N ⋅ (log N+1)+6N ⋅ (log N+1)−(2N ⋅ log N+8N+3N ⋅ log N+8N)⋅
2 ⋅ 2 ⋅ 2 2 100%
4N (log N+1)+6N (log N+1)
2 2
⋅ ⋅
10 (log N+1)−5 log N−16⋅
= 2 ⋅ 2 100%
10 (log N+1)
2
⋅
5 log N−6 ⋅
= ⋅ 2 100%.
10 (log N+1) (13–50)
2
The full computational (multiplications and additions) saving from Eq.
(13–50) is plotted as a function of Nin the top curve of Figure 13–13.
13.6 COMPUTING THE INVERSE FFT USING THE FORWARD FFT
There are many signal processing applications where the capability to per-
form the inverse FFT is necessary. This can be a problem if available hard-
ware, or software routines, have only the capability to perform the forward
FFT. Fortunately, there are two slick ways to perform the inverse FFT using
the forward FFT algorithm.

700 Digital Signal Processing Tricks
13.6.1 Inverse FFT Method 1
The first inverse FFT calculation scheme is implemented following the
processes shown in Figure 13–14.
To see how this works, consider the expressions for the forward and in-
verse DFTs. They are
N−1
∑ −j2πnm/N
X(m)= x(n)e (13–51)
Forward DFT →
n=0
N−1
1 ∑ j2πmn/N
x(n)= X(m)e . (13–52)
Inverse DFT → N
m=0
To reiterate our goal, we want to use the process in Eq. (13–51) to imple-
ment Eq. (13–52). The first step of our approach is to use complex conjuga-
tion. Remember, conjugation (represented by the superscript “*” symbol) is
the reversal of the sign of a complex number’s imaginary exponent—if x=ejø,
then x*=e–jø. So, as a first step we take the complex conjugate of both sides of
Eq. (13–52) to give us
⎡N−1 ⎤
x*(n)=
1 ⎢ ∑
X(m)e
j2πmn/N⎥*.
(13–53)
N ⎣ ⎢ m=0 ⎦ ⎥
One of the properties of complex numbers, discussed in Appendix A, is
that the conjugate of a product is equal to the product of the conjugates. That
is, if c=ab,then c*=(ab)*=a*b*. Using this, we can show the conjugate of the
right side of Eq. (13–53) to be
N−1 N−1
1 ∑ j2πmn/N 1 ∑ −j2πmn/N
x*(n)= X(m)*(e )* = X(m)*e . (13–54)
N N
m=0 m=0
Hold on; we’re almost there. Notice the similarity of Eq. (13–54) to our
original forward DFT expression, Eq. (13–51). If we perform a forward DFT
X real (m) ÷N x real (n)
Forward
FFT
X (m) ÷N x (n)
imag imag
−1 −1
Figure 13–14 Processing for first inverse FFT calculation method.

13.6 Computing the Inverse FFT Using the Forward FFT 701
on the conjugate of the X(m) in Eq. (13–54), and divide the results by N, we
get the conjugate of our desired time samples x(n). Taking the conjugate of
both sides of Eq. (13–54), we get a more straightforward expression for x(n):
⎡N−1 ⎤
1 ⎢ ∑ −j2πmn/N⎥ *
x(n)= X(m)*e . (13–55)
N ⎣ ⎢ m=0 ⎦ ⎥
13.6.2 Inverse FFT Method 2
The second inverse FFT calculation technique is implemented following the
interesting data flow shown in Figure 13–15.
In this clever inverse FFT scheme we don’t bother with conjugation. In-
stead, we merely swap the real and imaginary parts of sequences of complex
data[21]. To see why this process works, let’s look at the inverse DFT equation
again while separating the input X(m) term into its real and imaginary parts
and remembering that ejø=cos(ø)+jsin(ø).
N−1
x(n)=
1 ∑
X(m)e
j2πmn/N
Inverse DFT →
N
m=0
N−1
1 ∑
= [X real (m)+ jX imag (m)][cos(2πmn/N)+jsin(2πmn/N)]. (13–56)
N
m=0
Multiplying the complex terms in Eq. (13–56) gives us
1
N∑ −1
x(n)= [X (m)cos(2πmn/N)−X (m)sin(2πmnn/N)]
N real imag
m=0 (13–57)
+ j[X (m)sin(2πmn/N) + X (m)cos(2πmn/N)].
real imag
Equation (13–57) is the general expression for the inverse DFT, and we’ll
now quickly show that the process in Figure 13–15 implements this equation.
With X(m)=X (m) + jX (m), then swapping these terms gives us
real imag
X (m) = X (m) + jX (m). (13–58)
swap imag real
The forward DFT of our X (m) is
swap
N−1
∑
[X (m)+ jX (m)][cos(2πmn/N)− jsin(2πmn/N)]. (13–59)
Forward DFT→ imag real
n=0

702 Digital Signal Processing Tricks
X real (m) Real Real ÷N x real (n)
Forward
FFT
X imag (m) Imag Imag ÷N x imag (n)
Figure 13–15 Processing for second inverse FFT calculation method.
Multiplying the complex terms in Eq. (13–59) gives us
N−1
∑
[X (m)cos(2πmn/N)+X (m)sin(2πmn/N)]
Forward DFT→ imag real
n=0
+ j[X (m)cos(2πmn/N) – X (m)sin(2πmn/N)]. (13–60)
real imag
Swapping the real and imaginary parts of the results of this forward DFT
gives us what we’re after:
N−1
∑
[X (m)cos(2πmn/N)−X (m)sin(2πmn/N)]
Forward DFT → real imag
swap
n=0
+ j[X (m)cos(2πmn/N) + X (m)sin(2πmn/N)]. (13–61)
imag real
If we divided Eq. (13–61) by N, it would be exactly equal to the inverse
DFT expression in Eq. (13–57), and that’s what we set out to show.
13.7 SIMPLIFIED FIR FILTER STRUCTURE
If we implement a linear-phase FIR digital filter using the standard structure
in Figure 13–16(a), there’s a way to reduce the number of multipliers when the
filter has an odd number of taps. Let’s look at the top of Figure 13–16(a) where
the 5-tap FIR filter coefficients are h(0) through h(4) and the y(n) output is
y(n)=h(4)x(n–4) + h(3)x(n–3) + h(2)x(n–2) + h(1)x(n–1) + h(0)x(n). (13–62)
If the FIR filter’s coefficients are symmetrical, we can reduce the number
of necessary multipliers. That is, if h(4) = h(0), and h(3) = h(1), we can imple-
ment Eq. (13–62) by
y(n)=h(4)[x(n–4)+x(n)] + h(3)[x(n–3)+x(n–1)] + h(2)x(n–2) (13–63)
where only three multiplications are necessary as shown at the bottom of Fig-
ure 13–16(a). In our 5-tap filter case, we’ve eliminated two multipliers. This
minimum-multiplier structure is called a foldedFIR filter.

13.7 Simplified FIR Filter Structure 703
x(n) x(n-1) x(n-2) x(n-3) x(n-4)
z -1 z -1 z -1 z -1
h(0) h(1) h(2) h(3) h(4)
y(n)
(a)
x(n-4) x(n-3)
z -1 z -1
x(n-2)
x(n) x(n-1)
z -1 z -1
h(0) = h(4) h(1) = h(3) h(2)
y(n)
x(n) x(n-1) x(n-2) x(n-3) x(n-4) x(n-5)
z -1 z -1 z -1 z -1 z -1
h(0) h(1) h(2) h(3) h(4) h(5)
y(n)
(b)
x(n-5) x(n-4) x(n-3)
z -1 z -1
x(n) x(n-1) x(n-2)
z -1 z -1 z -1
h(0) = h(5) h(1) = h(4) h(2) = h(3)
y(n)
Figure 13–16 Conventional and simplified structures of an FIR filter: (a) with an
odd number of taps; (b) with an even number of taps.
So in the case of an odd number of taps, we need only perform
(S–1)/2+1 multiplications for each filter output sample. For an even number
of symmetrical taps as shown in Figure 13–16(b), the saving afforded by this
technique reduces the necessary number of multiplications to S/2. Some
commercial programmable DSPchips have specialized instructions, and dual

704 Digital Signal Processing Tricks
multiply-and-accumulate (MAC) units, that take advantage of the folded FIR
filter implementation.
13.8 REDUCING A/D CONVERTER QUANTIZATION NOISE
In Section 12.3 we discussed the mathematical details, and ill effects, of quan-
tization noise in analog-to-digital (A/D) converters. DSP practitioners com-
monly use two tricks to reduce converter quantization noise. Those schemes
are called oversamplingand dithering.
13.8.1 Oversampling
The process of oversampling to reduce A/D converter quantization noise is
straightforward. We merely sample an analog signal at an f sample rate
s
higher than the minimum rate needed to satisfy the Nyquist criterion (twice
the analog signal’s bandwidth), and then lowpass filter. What could be sim-
pler? The theory behind oversampling is based on the assumption that an
A/D converter’s total quantization noise power (variance) is the converter’s
least significant bit (lsb) value squared over 12, or
2
total quantization noise power = σ2= (lsb value) . (13–64)
12
We derived that expression in Section 12.3. The next assumptions are:
The quantization noise values are truly random, and in the frequency domain
thequantization noise has a flat spectrum. (These assumptions are valid if the
A/D converter is being driven by an analog signal that covers most of the
converter’s analog input voltage range and is not highly periodic.) Next we
consider the notion of quantization noise power spectral density (PSD), a
frequency-domain characterization of quantization noise measured in noise
power per hertz as shown in Figure 13–17. Thus we can consider the idea that
quantization noise can be represented as a certain amount of power (watts, if
we wish) per unit bandwidth.
In our world of discrete systems, the flat noise spectrum assumption re-
sults in the total quantization noise (a fixed value based on the converter’s lsb
voltage) being distributed equally in the frequency domain, from –f/2 to
s
+f/2 as indicated in Figure 13–17. The amplitude of this quantization noise
s
PSD is the rectangle area (total quantization noise power) divided by the rec-
tangle width (f), or
s
2 2
(lsb value) 1 (lsb value)
PSD = = (13–65)
noise
12 f 12f
s s
measured in watts/Hz.

13.8 Reducing A/D Converter Quantization Noise 705
PSD
PSD
noise
Total
quantization
noise power
–f/2 0 f/2 Freq
s s
Figure 13–17 Frequency-domain power spectral density of an ideal A/D
converter.
The next question is: “How can we reduce the PSD level defined by
noise
Eq. (13–65)?” We could reduce the lsb value (volts) in the numerator by using
an A/D converter with additional bits. That would make the lsb value
smaller and certainly reduce PSD , but that’s an expensive solution. Extra
noise
converter bits cost money. Better yet, let’s increase the denominator of Eq.
(13–65) by increasing the sample rate f.
s
Consider a low-level discrete signal of interest whose spectrum is de-
picted in Figure 13–18(a). By increasing the f sample rate to some larger
s,old
value f (oversampling), we spread the total noise power (a fixed value)
s,new
over a wider frequency range as shown in Figure 13–18(b). The areas under
the shaded curves in Figures 13–18(a) and 13–18(b) are equal. Next we low-
pass filter the converter’s output samples. At the output of the filter, the
quantization noise level contaminating our signal will be reduced from that at
the input of the filter.
The improvement in signal-to-quantization-noise ratio, measured in dB,
achieved by oversampling is
SNR = 10log (f /f ). (13–66)
A/D-gain 10 s,new s,old
For example, if f = 100 kHz, and f = 400 kHz, the SNR =
s,old s,new A/D-gain
10log (4) = 6.02 dB. Thus oversampling by a factor of four (and filtering), we
10
gain a single bit’s worth of quantization noise reduction. Consequently we
can achieve N+1-bit performance from an N-bit A/D converter, because
wegain signal amplitude resolution at the expense of higher sampling speed.
After digital filtering, we can decimate to the lower f without degrading
s,old
the improved SNR. Of course, the number of bits used for the lowpass filter’s
coefficients and registers must exceed the original number of A/D converter
bits, or this oversampling scheme doesn’t work.
With the use of a digital lowpass filter, depending on the interfering ana-
log noise in x(t), it’s possible to use a lower-performance (simpler) analog
anti-aliasing filter relative to the analog filter necessary at the lower sampling
rate.

706 Digital Signal Processing Tricks
PSD Signal of
interest
(a) Quant.
noise
-f /2 0 f /2 Freq
s,old s,old
PSD
Shaded
Lowpass filter regions have
response the same area
(b) Quant. noise
-f /2 0 f /2 Freq
s,new s,new
x(t) Analog A/D Digital x(n)
(c) anti-aliasing lowpass Decimation
converter
lowpass filter filter
f
s,new
Figure 13–18 Oversampling example: (a) noise PSD at an f samples rate;
s,old
(b)noise PSD at the higher f samples rate; (c) processing steps.
s,new
13.8.2 Dithering
Dithering, another technique used to minimize the effects of A/D quantiza-
tion noise, is the process of adding noise to our analog signal prior to A/D
conversion. This scheme, which doesn’t seem at all like a good idea, can in-
deed be useful and is easily illustrated with an example. Consider digitizing
the low-level analog sinusoid shown in Figure 13–19(a), whose peak voltage
just exceeds a single A/D converter least significant bit (lsb) voltage level,
yielding the converter output x (n) samples in Figure 13–19(b). The x (n) out-
1 1
put sequence is clipped.This generates all sorts of spectral harmonics. Another
way to explain the spectral harmonics is to recognize the periodicity of the
quantization noise in Figure 13–19(c).
We show the spectrum of x (n) in Figure 13–20(a) where the spurious
1
quantization noise harmonics are apparent. It’s worthwhile to note that av-
eraging multiple spectra will not enable us to pull some spectral component
of interest up above those spurious harmonics in Figure 13–20(a). Because
the quantization noise is highly correlated with our input sinewave—the
quantization noise has the same time period as the input sinewave—spec-
tral averaging will also raise the noise harmonic levels. Dithering to the
rescue.

13.8 Reducing A/D Converter Quantization Noise 707
1 lsb Analog input
to converter
0.5 lsb
(a) 0
Time
-0.5 lsb
-1 lsb
x(n)
1
1 lsb Converter
output code
0.5 lsb
(b) 0
Time
-0.5 lsb
-1 lsb
1 lsb Converter
quantization error
0.5 lsb
(c) 0
Time
-0.5 lsb
-1 lsb
Figure 13–19 Dithering: (a) a low-level analog signal; (b) the A/D converter out-
put sequence; (c) the quantization error in the converter’s output.
Dithering is the technique where random analog noise is added to the
analog input sinusoid before it is digitized. This technique results in a
noisy analog signal that crosses additional converter lsb boundaries and
yields a quantization noise that’s much more random, with a reduced level
of undesirable spectral harmonics as shown in Figure 13–20(b). Dithering
raises the average spectral noise floor but increases our signal-to-noise
ratio SNR . Dithering forces the quantization noise to lose its coherence
2
with the original input signal, and we could then perform signal averaging
if desired.
Dithering is indeed useful when we’re digitizing
• low-amplitude analog signals,
• highly periodic analog signals (like a sinewave with an even number of
cycles in the sample time interval), and
• slowly varying (very low frequency, including DC) analog signals.

708 Digital Signal Processing Tricks
|X(m)| |X(m)|
1 2
0 0
Signal of
SNR
-10 interest 1 -10 SNR
2
-20 -20
-30 -30
-40 N i o n i c s r e e a fl s o e or -40
-50 -50
0 Freq 0 Freq
(a) (b)
Figure 13–20 Spectra of a low-level discrete sinusoid: (a) with no dithering;
(b)with dithering.
The standard implementation of dithering is shown in Figure 13–21(a).
The typical amount of random wideband analog noise used in this process,
provided by a noise diode or noise generator ICs, has an rms (root mean
squared) level equivalent to 1/3 to 1 lsb voltage level. The system-level effect
of adding the analog dithering signal is to linearize the undithered stair-step
transfer function of an A/D converter as shown in Figure 13–21(c).
For high-performance audio applications, engineers have found that
adding dither noise from two separate noise generators improves back-
ground audio low-level noise suppression. The probability density function
(PDF) of the sum of two noise sources (having rectangular PDFs) is the con-
volution of their individual PDFs. Because the convolution of two rectangular
functions is triangular, this dual-noise-source dithering scheme is called trian-
gular dither. Typical triangular dither noise has rms levels equivalent to,
roughly, 2 lsb voltage levels.
In the situation where our signal of interest occupies some well-defined
portion of the full frequency band, injecting narrowband dither noise having
an rms level equivalent to 4 to 6 lsb voltage levels, whose spectral energy is
outside that signal band, would be advantageous. (Remember, though: the
dither signal can’t be too narrowband, like a sinewave. Quantization noise
from a sinewave signal would generate more spurious harmonics!) That nar-
rowband dither noise can then be removed by follow-on digital filtering.
One last note about dithering: To improve our ability to detect low-level
signals, we could add the analog dither noise and then subtract that noise
from the digitized data, as shown in Figure 13–21(b). This way, we randomize
the quantization noise but reduce the amount of total noise power injected in
the analog signal. This scheme is used in commercial analog test equip-
ment[22,23].

13.9 A/D Converter Testing Techniques 709
x(t) A/D + x(n)
+ +
x(t) x(n) converter
+ A/D –
converter
Digital
D/A pseudo-random
Random analog noisegenerator
noise
(a) (b)
1
No dithering
x
d
e
z 0
nti
a
u
Q
With dithering
–1
–1 –0.5 0 0.5 1
Analogx
(c)
Figure 13-21 Dithering implementations: (a) standard dithering process; (b) ad-
vanced dithering with noise subtraction; (c) improved transfer func-
tion due to dithering.
13.9 A/D CONVERTER TESTING TECHNIQUES
We can take advantage of digital signal processing techniques to facilitate the
testing of A/D converters. In this section we present two schemes for measur-
ing converter performance: first, a technique using the FFT to estimate overall
converter noise, and second, a histogram analysis scheme to detect missing
converter output codes.
13.9.1 Estimating A/D Quantization Noise with the FFT
The combination of A/D converter quantization noise, missing bits, harmonic
distortion, and other nonlinearities can be characterized by analyzing the
spectral content of the converter’s output. Converter performance degrada-
tion caused by these nonlinearities is not difficult to recognize because they
show up as spurious spectral components and increased background noise
levels in the A/D converter’s output samples. The traditional test method in-
volves applying a sinusoidal analog voltage to an A/D converter’s input and
examining the spectrum of the converter’s digitized time-domain output
samples. We can use the FFT to compute the spectrum of an A/D converter’s

710 Digital Signal Processing Tricks
output samples, but we have to minimize FFT spectral leakage to improve the
sensitivity of our spectral measurements. Traditional time-domain window-
ing, however, often provides insufficient FFT leakage reduction for high-
performance A/D converter testing.
The trick to circumvent this FFT leakage problem is to use a sinusoidal
analog input voltage whose frequency is a rational factor of the A/D con-
verter’s clock frequency as shown in Figure 13–22(a). That frequency is mf/N
s
where mis an integer, f is the clock frequency (sample rate), and Nis the FFT
s
size. Figure 13–22(a) shows the x(n) time-domain output of an ideal 5-bit A/D
converter when its analog input is a sinewave having exactly m = 4 cycles
over N = 64 converter output samples. In this case, the analog input fre-
quency is 4f/64 Hz. Recall from Chapter 3 that the expression mf/Ndefined
s s
the analysis frequencies, or bin centers, of the DFT, and a DFT input sinusoid
whose frequency is at a bin center causes no spectral leakage.
The magnitudes of the first half of an N = 64-point FFT of x(n) are
shown in the logarithmic plot in Figure 13–22(b) where the analog input spec-
tral component lies exactly at the m = 4 bin center. (The additional nonzero
spectral samples are not due to FFT leakage; they represent A/D converter
quantization noise.) Specifically, if the sample rate were 1 MHz, then the
A/D’s input analog sinewave’s frequency is 4(106/64) = 62.5 kHz. In order to
1
x(n)
(a) 0
–1
0 10 20 30 40 50 60
Time index n
|X(m)|
0
B
d
–20
(b)
–40
–60
0 5 10 15 20 25 30
Freq index m
m = 4
Figure 13–22 A/D converter (5-bit) output with an analog 4f/64 Hz sinewave
s
input: (a) m= 4-cycle sinusoidal time samples; (b) spectral magni-
tude in dB.

13.9 A/D Converter Testing Techniques 711
implement this A/D testing scheme we must ensure that the analog test-
signal generator is synchronized, exactly, with the A/D converter’s clock fre-
quency of f Hz. Achieving this synchronization is why this A/D converter
s
testing procedure is referred to as coherentsampling[24–26]. That is, the analog
signal generator and the A/D clock generator providing f must not drift in
s
frequency relative to each other—they must remain coherent. (Here we must
take care from a semantic viewpoint because the quadrature sampling
schemes described in Chapter 8 are also sometimes called coherent sampling,
and they are unrelated to this A/D converter testing procedure.)
As it turns out, some values of m are more advantageous than others.
Notice in Figure 13–22(a), that when m = 4, only ten different binary output
values, output codes, are output by the A/D converter. Those values are re-
peated over and over, and the quantization noise is far from being random.
As shown in Figure 13–23(a), when m= 5, we exercise more than ten different
A/D output codes, and the quantization noise in Figure 13–23(b) is much
more random than when m= 4.
Because it’s best to test as many A/D output codes as possible, while
keeping the quantization noise sufficiently random, users of this A/D testing
scheme have discovered another trick; they found making m an odd prime
number (3, 5, 7, 11, etc.) minimizes the number of redundant A/D output
code values and makes the quantization noise more random, which is what
we want. The larger m is, the more codes that are exercised. (We can use his-
togram testing, discussed in the next section, to determine how many of a b-bit
A/D converter’s 2bpossible output codes have been exercised.)
While examining the quantization noise level in Figure 13–23(b), we
might be tempted to say the A/D converter has a signal-to-quantization-noise
ratio of 40 to 50 dB. As it turns out, the true A/D converter noise levels will be
higher than those indicated by Figure 13–23(b). That’s because the inherent
processing gain of the FFT (discussed in Section 3.12.1) will pullthe high-level
m = 5 signal spectral component up out of the background converter noise,
making that m = 5 spectral magnitude sample appear higher above the back-
ground noise than is correct. Consequently, when viewing Figure 13–23(b),
we must keep in mind an N= 64-point FFT’s processing gain of 10log (64/2).
10
Our interpretation of A/D performance based on the FFT magnitude results
is given in Figure 13–23(c).
There is a technique used to characterize an A/D converter’s true signal-
to-noise ratio (including quantization noise, harmonic distortion, and other
nonlinearities). That testing technique measures what is commonly called an
A/D converter’s SINAD—for signal-to-noise-and-distortion—and does not
require us to consider FFT processing gain. The SINAD value for an A/D con-
verter, based on spectral power samples, is
⎛ ⎞
Totalsignalpower
SINAD=10log ⎜ ⎟, indB. (13–66’)
10⎝ Totalnoiseppower⎠

712 Digital Signal Processing Tricks
1
x(n)
(a) 0
–1
0 10 20 30 40 50 60
Time index n
|X(m)|
0
B
d
–20
(b)
–40
–60
0 5 10 15 20 25 30
Freq index m
|X(m)|
0
B
d 6.5 + 1.7 = 31.7 dB
–20
(c) A/D rms quantization noise
–40 10log (64/2) = 15 dB
10
FFT noise floor
–60
0 5 10 15 20 25 30
Freq index m
Figure 13–23 A/D converter (5-bit) output with an analog 5f/64 Hz sinewave
s
input: (a) m= 5-cycle time samples; (b) spectral magnitude in dB;
(c) FFT results interpretation.
The SINAD value for an A/D converter is a good quantitative indicator of a
converter’s overall dynamic performance. The steps to compute SINAD are:
1. Compute an N-point FFT of an A/D converter’s output sequence. Dis-
card the negative-frequency samples of the FFT results.
2. Over the positive-frequency range of the FFT results, compute the total
signal spectral power by summing the squares of all signal-only spectral
magnitude samples. For our Figure 13–23 example that’s simply squar-
ing the FFT’s |X(5)| magnitude value. (We square the linear |X(5)|
value and not the value of |X(5)| in dB!)

13.9 A/D Converter Testing Techniques 713
3. Over the positive-frequency range of the FFT results, sum the squares of
all noise-only spectral magnitude samples, including any signal har-
monics, but excluding the zero-Hz X(0) sample. This summation result
represents total noise power, which includes harmonic distortion.
4. Perform the computation given in Eq. (13–66’).
Performing those steps on the spectrum in Figure 13–23(b) yields a SINAD
value of 31.6 dB. This result is reasonable for our simulated 5-bit A/D con-
verter because its signal-to-quantization-noise ratio would ideally be 6·5 +
1.7 = 31.7 dB.
Figure 13–24(a) illustrates an extreme example of nonlinear A/D con-
verter operation with several binary output codes (words) having dropped
bits in the time-domain x(n) sequence with m = 5. The FFT magnitudes, pro-
vided in Figure 13–24(b), indicate severe A/D converter nonlinear distortion
because we can see the increased background noise level compared to Figure
13–23(b). Performing Eq. (13–66’) for this noisy A/D gives us a measured
SINAD value of 15.2 dB, which is drastically smaller than the ideal 5-bit A/D
converter’s SINAD = 31.6 dB. The point here is that we can quickly measure
an A/D converter’s performance using FFTs and Eq. (13–66’).
Dropped bits
1
x(n)
(a) 0
–1
0 10 20 30 40 50 60
Time index n
|X(m)|
0
B
d
–20
(b)
–40
–60
0 5 10 15 20 25 30
Freq index m
Figure 13–24 Nonideal A/D converter output showing several dropped bits:
(a) time samples; (b) spectral magnitude in dB.

714 Digital Signal Processing Tricks
To fully characterize the dynamic performance of an A/D converter
we’d need to perform this SINAD testing technique at many different input
frequencies and amplitudes. (The analog sinewave applied to an A/D con-
verter must, of course, be as pure as possible. Any distortion inherent in the
analog signal will show up in the final FFT output and could be mistaken for
A/D nonlinearity.) The key issue here is that when any input frequency is
mf/N, where mis less than N/2 to satisfy the Nyquist sampling criterion, we
s
can take full advantage of the FFT’s processing capability while minimizing
spectral leakage.
For completeness, we mention that what we called SINAD in Eq.
(13–66’) is sometimes called SNDR. In addition, there is a measurement
scheme called SINAD used by RF engineers to quantify the sensitivity of
radio receivers. That receiver SINAD concept is quite different from our Eq.
(13–66’) A/D converter SINAD estimation process and will not be discussed
here.
13.9.2 Estimating A/D Dynamic Range
In this section we describe a technique of applying the sum of two analog
sinewaves to an A/D converter’s input to quantify the intermodulation dis-
tortion performance of a converter, which in turn measures the converter’s
dynamic range. That dynamic range is called the converter’s spurious free dy-
namic range(SFDR). In this testing scheme both input sinewaves must comply
with the mf/Nrestriction. Figure 13–25(a) shows the test configuration.
s
The SFDR test starts by applying the sum of two equal-amplitude ana-
log sinewaves to an A/D converter and monitoring the spectrum of the con-
verter’s output samples. Next we increase both analog sinewaves’
amplitudes until we see a spurious spectral component rising above the con-
verter’s background spectral noise as shown in Figure 13–25(b). Finally we
measure the converter’s SFDR as the dB difference between a high-level sig-
nal spectral magnitude sample and the spurious signal’s spectral magnitude.
For this SFDR testing it’s prudent to use bandpass filters (BPFs) to im-
prove the spectral purity of the sinewave generators’ outputs, and small-
valued fixed attenuators (pads) are used to keep the generators from ad-
versely interacting with each other. (I recommend 3 dB fixed attenuators for
this.) The power combiner is typically an analog power splitter driven back-
ward, and the A/D clock generator output is a squarewave. The dashed lines
in Figure 13–25(a) indicate that all three generators are synchronized to the
same reference frequency source.

13.9 A/D Converter Testing Techniques 715
Sinewave Sinewave A/D clock
BPF BPF
generator 1 generator 2 generator
Pad Pad
A/D
(a) Converter
Power Variable
combiner attenuator
Computer
(FFT)
|X(m)|
0
B
d
–20 SFDR
Spurious
(b) signal
–40
–60
0 5 10 15 20 25 30
Freq index m
Figure 13–25 A/D converter SFDR testing: (a) hardware test configuration; (b) ex-
ample test results.
13.9.3 Detecting Missing Codes
One problem that can plague A/D converters is missing codes. This defect oc-
curs when a converter is incapable of outputting a specific binary word (a
code). Think about driving an eight-bit converter with an analog sinusoid
and the effect when its output should be the binary word 00100001 (decimal
33); its output is actually the word 00100000 (decimal 32) as shown in Figure
13–26(a). The binary word representing decimal 33 is a missing code. This
subtle nonlinearity is very difficult to detect by examining time-domain sam-
ples or performing spectrum analysis. Fortunately there is a simple, reliable
way to detect the missing 33 using histogram analysis.
The histogram testing technique merely involves collecting many A/D
converter output samples and plotting the number of occurrences of each
sample value versus that sample value as shown in Figure 13–26(b). Any
missing code (like our missing 33) would show up in the histogram as a zero
value. That is, there were zero occurrences of the binary code representing a
decimal 33.
Additional useful information can be obtained from our histogram re-
sults. That is, counting the number of nonzero samples in Figure 13–26(b)
tells us how many actual different A/D converter output codes (out of a pos-
sible 2bcodes) have been exercised.

716 Digital Signal Processing Tricks
34
33
x(n) 32
128
(a) 0
–128
0 200 400 600 800 Time
Number of occurrences
4
30
2
0
20
(b)
10
0
–128 –100 –50 0 50 100 127
A/D code value 33
Figure 13-26 Eight-bit converter missing codes: (a) missing code of binary
00100001, decimal 33; (b) histogram plot.
In practice, the input analog sinewave must have an amplitude that’s
somewhat greater than the analog signal that we intend to digitize in an ac-
tual application, and a frequency that is unrelated to (incoherent with) the f
s
sampling rate. In an effort to exercise (test) all of the converter’s output codes,
we digitize as many cycles of the input sinewave as possible for our his-
togram test.
13.10 FAST FIR FILTERING USING THE FFT
In the late 1960s, while contemplating the notion of time-domain convolution,
DSPpioneer Thomas Stockham (digital audio expert and inventor of the com-
pact disc) realized that time-domain convolution could sometimes be per-
formed much more efficiently using fast Fourier transform (FFT) algorithms
rather than using the direct convolution implemented with tapped-delay line
FIR filters. The principle behind this FFT-based convolution scheme, called

13.10 Fast FIR Filtering Using the FFT 717
Convolution
N-point
x(n) X(m)
FFT H(m) · X(m) N-point y(n) = h(k)*x(n)
(a) inverse
FFT
N-point
h(k) H(m)
FFT
x107
15
Fast convolution (FFTs)
s
n Time-domain convolution, Q = 100
o
ati10 Time-domain convolution, Q = 200
pli c Time-domain convolution, Q = 500
ulti
m 5
al
e
R
0
2x104 104
Length of x(n)
105
(b)
x107
15
s
n10
o
diti
d
a
al 5
e
R
0
2x104 104 105
Length of x(n)
Figure 13–27 Fast convolution: (a) basic process; (b) computational workloads
for various FIR filter tap lengths Q.
fast convolution (also called block convolution or FFT convolution), is dia-
grammed in Figure 13–27(a). In that figure x(n) is an input signal sequence
and h(k) is the Q-length impulse response (coefficients) of a tapped-delay line
FIR filter. Figure 13–27(a) is a graphical depiction of one form of the convolu-
tion theorem: Multiplication in the frequency domain is equivalent to convo-
lution in the time domain.
The standard convolution equation, for a Q-tap FIR filter, given in Eq.
(5–6) is repeated here for reference as
Q∑ −1
y(n)= h(k)x(n−k)=h(k)∗x(n) (13–67)
k=0
where the symbol “*” means convolution. When the filter’s h(k) impulse re-
sponse has a length greater than 40 to 80 (depending on the hardware and

718 Digital Signal Processing Tricks
software being used), the process in Figure 13–27(a) requires fewer computa-
tions than directly implementing the convolution expression in Eq. (13–67).
Consequently, this fast convolution technique is a computationally efficient
signal processing tool, particularly when used for digital filtering. Fast convo-
lution’s gain in computational efficiency becomes quite significant when the
lengths of h(k) and x(n) are large.
Figure 13–27(b) indicates the reduction in the fast convolution algo-
rithm’s computational workload relative to the standard (tapped-delay line)
time-domain convolution method, Eq. (13–67), versus the length of the x(n)
sequence for various filter impulse response lengths Q. (Please do not view
Figure 13–27(b) as any sort of gospel truth. That figure is merely an indicator of
fast convolution’s computational efficiency.)
The necessary forward and inverse FFT sizes, N, in Figure 13–27(a) must
of course be equal and are dependent upon the length of the original h(k) and
x(n) sequences. Recall from Eq. (5–29) that if h(k) is of length Q and x(n) is of
length P, the length of the final y(n) sequence will be Lwhere
Length of y(n): L= Q+ P–1. (13–67’)
For this fast convolution technique to yield valid results, the forward and in-
verse FFT sizes must be equal to or greater than L. So, to implement fast con-
volution we must choose an N-point FFT size such that N ≥ L, and zero-pad
h(k) and x(n) so they have new lengths equal to N. The desired y(n) output is
the real part of the first L samples of the inverse FFT. Note that the H(m) se-
quence, the FFT of the FIR filter’s h(k) impulse response, need only be com-
puted once and stored in memory.
Now if the x(n) input sequence length P is so large that FFT processing
becomes impractical, or your hardware memory buffer can only hold small
segments of the x(n) time samples, then x(n) must be partitioned into multiple
blocks of samples and each sample block processed individually. If the
partitioned-x(n) block lengths are N, a straightforward implementation of
Figure 13–27(a) leads to time-domain aliasing errors in y(n) due to the circular
nature (spectral wraparound) of the discrete Fourier transform (and the FFT).
Two techniques are used to avoid that time-domain aliasing problem, the
overlap-and-save method and the overlap-and-add method. Of these two meth-
ods, let’s first have a look at the overlap-and-save fast convolution filtering
technique shown in Figure 13–28(a).
Given that the desired FIR filter’s h(k) impulse response length is Qand
the x(n) filter input sequence is of length P, the steps to perform overlap-and-
save fast convolution filtering are as follows:
1. Choose an FFT size of N, where N is an integer power of two equal to
roughly four times Q.

13.10 Fast FIR Filtering Using the FFT 719
Q–1
zeros M samples
Q–1 samples
x(n) FFT 1 x 1 (n)
FFT 2 x 2 (n)
FFT 3 x(n)
3
N samples
Overlap-and-
N-point FFTs
save method
(a)
H(m)
N-point inverse FFTs
Q–1
samples M samples
Discard first Q–1 y(n) = y(n),y(n),y(n), etc.
1 2 3
sa e m ac p h le I s F f F ro T m IFFT 1 y 1 (n) y(n)
IFFT 2 y(n)
( = discard) 2
IFFT 3 y(n)
3
Figure 13–28 Fast convolution block processing (continues).
2. Append (N–Q) zero-valued samples to the end of the h(k) impulse re-
sponse and perform an N-point FFT on the extended sequence, produc-
ing the complex H(m) sequence.
3. Compute integer Musing M= N–(Q–1).
4. Insert (Q–1) zero-valued samples prior to the first Msamples of x(n), cre-
ating the first N-point FFT input sequence x (n).
1
5. Perform an N-point FFT on x (n), multiply that FFT result by the H(m)
1
sequence, and perform an N-point inverse FFT on the product. Discard
the first (Q–1) samples of the inverse FFT results to generate the first
M-point output block of data y (n).
1
6. Attach the last (Q–1) samples of x (n) to the beginning of the second
1
M-length block of the original x(n) sequence, creating the second
N-point FFT input sequence x (n) as shown in Figure 13–28(a).
2
7. Perform an N-point FFT on x (n), multiply that FFT result by the H(m)
2
sequence, and perform an N-point inverse FFT on the product. Discard
the first (Q–1) samples of the inverse FFT results to generate the second
M-point output block of data y (n).
2
8. Repeat Steps 6 and 7 until we have gone through the entire original x(n)
filter input sequence. Depending on the length P of the original x(n)

720 Digital Signal Processing Tricks
Q–1
M samples zeros
x(n) = x(n),x(n),x(n), etc.
1 2 3
FFT 1 x(n)
x(n) 1
FFT 2 x(n)
2
FFT 3 x(n)
3
N samples
Overlap-and-
N-point FFTs
add method
(b) H(m)
N-point inverse FFTs
Q–1
M samples samples
y(n) = y(n),y(n),y(n), etc.
( = add to IFFT 1 y(n) 1 2 3
1
the beginning y(n)
of next IFFT) IFFT 2 y 2 (n)
IFFT 3 y(n)
3
N samples
Figure 13–28 (continued)
input sequence and the chosen value for N, we must append anywhere
from Q–1 to N–1 zero-valued samples to the end of the original x(n)
input samples in order to accommodate the final block of forward and
inverse FFT processing.
9. Concatenate the y (n), y (n), y (n), . . . sequences shown in Figure
1 2 3
13–28(a), discarding any unnecessary trailing zero-valued samples, to
generate your final linear-convolution filter output y(n) sequence.
10. Finally, experiment with different values of Nto see if there exists an opti-
mum N that minimizes the computational workload for your hardware
and software implementation. In any case, N must not be less than
(M+Q–1). (Smaller Nmeans many small-sized FFTs are needed, and large
Nmeans fewer, but larger-sized, FFTs are necessary. Pick your poison.)
The second fast convolution method, the overlap-and-add technique, is
shown in Figure 13–28(b). In this method, the x(n) input sequence is parti-
tioned (segmented) into data blocks of length M, and our data overlapping
takes place in the inverse FFT time-domain sequences. Given that the desired
FIR filter’s h(k) impulse response length is Q and the x(n) filter input se-

13.10 Fast FIR Filtering Using the FFT 721
quence is of length P, the steps to perform overlap-and-add fast convolution
filtering are as follows:
1. Choose an FFT size of N, where N is an integer power of two equal to
roughly two times Q.
2. Append (N–Q) zero-valued samples to the end of the h(k) impulse re-
sponse and perform an N-point FFT on the extended sequence, produc-
ing the complex H(m) sequence.
3. Compute integer Musing M= N–(Q–1).
4. Append (Q–1) zero-valued samples to the end of the first M samples,
x (n), of the original x(n) sequence, creating the first N-point FFT input
1
sequence.
5. Perform an N-point FFT on the first N-point FFT input sequence, multi-
ply that FFT result by the H(m) sequence, and perform an N-point in-
verse FFT on the product. Retain the first M samples of the inverse FFT
sequence, generating the first M-point output block of data y (n).
1
6. Append (Q–1) zero-valued samples to the end of the second Msamples,
x (n), of the original x(n) sequence, creating the second N-point FFT
2
input sequence.
7. Perform an N-point FFT on the second N-point FFT input sequence,
multiply that FFT result by the H(m) sequence, and perform an N-point
inverse FFT on the product. Add the last (Q–1) samples from the previ-
ous inverse FFT to the first (Q–1) samples of the current inverse FFT se-
quence. Retain the first M samples of the sequence resulting from the
(Q–1)-element addition process, generating the second M-point output
block of data y (n).
2
8. Repeat Steps 6 and 7 until we have gone through the entire original x(n)
filter input sequence. Depending on the length P of the original x(n)
input sequence and the chosen value for N, we must append anywhere
from Q–1 to N–1 zero-valued samples to the end of the original x(n)
input samples in order to accommodate the final block of forward and
inverse FFT processing.
9. Concatenate the y (n), y (n), y (n), . . . sequences shown in Figure
1 2 3
13–28(b), discarding any unnecessary trailing zero-valued samples, to
generate your final linear-convolution filter output y(n) sequence.
10. Finally, experiment with different values of Nto see if there exists an op-
timumNthat minimizes the computational workload for your hardware
and software implementation. Nmust not be less than (M+Q–1). (Again,
smaller Nmeans many small-sized FFTs are needed, and large Nmeans
fewer, but larger-sized, FFTs are necessary.)

722 Digital Signal Processing Tricks
It’s useful to realize that the computational workload of these fast con-
volution filtering schemes does not change as Q increases in length up to a
value of N. Another interesting aspect of fast convolution, from a hardware
standpoint, is that the FFT indexing bit-reversal problem discussed in Sec-
tions 4.5 and 4.6 is not an issue here. If the FFTs result in X(m) and H(m) hav-
ing bit-reversed output sample indices, the multiplication can still be
performed directly on the scrambled H(m) and X(m) sequences. Then an ap-
propriate inverse FFT structure can be used that expects bit-reversed input
data. That inverse FFT then provides an output sequence whose time-domain
indexing is in the correct order. Neat!
By the way, it’s worth knowing that there are no restrictions on the fil-
ter’s finite-length h(k) impulse response—h(k) is not limited to being real-
valued and symmetrical as is traditional with tapped-delay line FIR filters.
Sequence h(k) can be complex-valued, asymmetrical (to achieve nonlinear-
phase filtering), or whatever you choose.
One last issue to bear in mind: the complex amplitudes of the standard
radix-2 FFT’s output samples are proportional to the FFT sizes, N, so the
product of two FFT outputs will have a gain proportional to N2. The inverse
FFT has a normalizing gain reduction of only 1/N. As such, our fast convolu-
tion filtering methods will have an overall gain that is not unity. We suggest
that practitioners give this gain normalization topic some thought during the
design of their fast convolution system.
To summarize this frequency-domain filtering discussion, the two fast
convolution filtering schemes can be computationally efficient, compared to
standard tapped-delay line FIR convolution filtering, particularly when the
x(n) input sequence is large and high-performance filtering is needed (requir-
ing many filter taps, i.e., Q = 40 to 80). As for which method, overlap-and-
save or overlap-and-add, should be used in any given situation, there is no
simple answer. Choosing a fast convolution method depends on many fac-
tors: the fixed/floating-point arithmetic used, memory size and access la-
tency, computational hardware architecture, and specialized built-in filtering
instructions, etc.
13.11 GENERATING NORMALLY DISTRIBUTED RANDOM DATA
Section D.7 in Appendix D discusses the normal distribution curve as it re-
lates to random data. Aproblem we may encounter is how actually to gener-
ate random data samples whose distribution follows that normal (Gaussian)
curve. There’s a straightforward way to solve this problem using any soft-
ware package that can generate uniformly distributed random data, as most
of them do[27]. Figure 13–29 shows our situation pictorially where we require

13.11 Generating Normally Distributed Random Data 723
Normal probability Uniform probability
distribution distribution
1
-3σ' -2σ' -σ' μ' +σ' +2σ' +3σ' 0 1
(a) (b)
Figure 13–29 Probability distribution functions: (a) normal distribution with
mean = μ’ and standard deviation σ’; (b) uniform distribution be-
tween zero and one.
random data that’s distributed normally with a mean (average) of μ’ and a
standard deviation of σ’, as in Figure 13–29(a), and all we have available is a
software routine that generates random data that’s uniformly distributed be-
tween zero and one as in Figure 13–29(b).
As it turns out, there’s a principle in advanced probability theory,
known as the Central Limit Theorem, that says when random data from an ar-
bitrary distribution is summed over M samples, the probability distribution
of the sum begins to approach a normal distribution as Mincreases[28–30]. In
other words, if we generate a set of Nrandom samples that are uniformly dis-
tributed between zero and one, we can begin adding other sets of N samples
to the first set. As we continue summing additional sets, the distribution of
the N-element set of sums becomes more and more normal. We can sound im-
pressive and state that “the sum becomes asymptotically normal.” Experience
has shown that for practical purposes, if we sum M ≥ 30 times, the summed
data distribution is essentially normal. With this rule in mind, we’re halfway
to solving our problem.
After summing M sets of uniformly distributed samples, the summed
set y will have a distribution as shown in Figure 13–30.
sum
Because we’ve summed Mdata sets whose mean values were all 0.5, the
mean of y is the sum of those M means, or μ = M/2. From Section D.6 of
sum
Probability distribution of data set y
sum
0 =M/2 M
Figure 13–30 Probability distribution of the summed set of random data derived
from uniformly distributed data.

724 Digital Signal Processing Tricks
Appendix D we know the variance of a single data sample set, having the
probability distribution in Figure 13–29(b), is 1/12. Because the variance of
the sum of M data sets is equal to the sum of their individual variances, we
can say
varianceof y is: σ2 ≈ M (13–68)
sum 12
and
M
standarddeviationof y is: σ≈ . (13–69)
sum 12
So, here’s the trick: To convert the y data set to our desired data set
sum
having a mean of μ’ and a standard deviation of σ’, we
1. subtract M/2 from each element of y to shift its mean to zero;
sum
2. scale y so that its standard deviation is the desired σ’, by multiplying
sum
each sample in the shifted data set by σ’/σ; and
3. finally, center the new data set at the desired μ’ value by adding μ’ to
each sample of the scaled data set.
If we call our desired normally distributed random data set y , then
desired
the nth element of that set is described mathematically as
y (n) = 12 ⋅ σ' ⋅
⎡
⎢
⎛
⎜
∑M
x (n)
⎞
⎟ − M
⎤
⎥+μ' . (13–70)
desired M ⎢⎝ k ⎠ 2 ⎥
⎣ k=1 ⎦
Our discussion thus far has had a decidedly software algorithm flavor,
but hardware designers also occasionally need to generate normally distrib-
uted random data at high speeds in their designs. For you hardware design-
ers, reference [30] presents an efficient hardware design technique to generate
normally distributed random data using fixed-point arithmetic integrated cir-
cuits.
The above method for generating normally distributed random num-
bers works reasonably well, but its results are not perfect because the tails of
the probability distribution curve in Figure 13–30 are not perfectly Gaussian.†
An advanced, and more statistically correct (improved randomness), tech-
nique that you may want to explore is called the Ziggurat method[31–33].
†I thank my DSPpal Dr. Peter Kootsookos, of UTC Fire and Security, Farmington, Connecticut,
for his advice on this issue.

13.12 Zero-Phase Filtering 725
13.12 ZERO-PHASE FILTERING
You can cancel the nonlinear phase effects of an IIR filter by following the
process shown in Figure 13–31(a). The y(n) output will be a filtered version of
x(n) with no filter-induced phase distortion. The same IIR filter is used twice
in this scheme, and the time reversal step is a straight left-right flipping of a
time-domain sequence. Consider the following. If some spectral component
in x(n) has an arbitrary phase of αdegrees, and the first filter induces a phase
shift of –βdegrees, that spectral component’s phase at node Awill be α–βde-
grees. The first time reversal step will conjugate that phase and induce an ad-
ditional phase shift of –θ degrees. (Appendix C explains this effect.)
Consequently, the component’s phase at node B will be –α+β–θ degrees. The
second filter’s phase shift of –β degrees yields a phase of –α–θ degrees at
node C. The final time reversal step (often omitted in literary descriptions of
this zero-phase filtering process) will conjugate that phase and again induce
an additional phase shift of –θ degrees. Thankfully, the spectral component’s
phase in y(n) will be α+θ–θ=αdegrees, the same phase as in x(n). This prop-
erty yields an overall filter whose phase response is zero degrees over the en-
tire frequency range.
An equivalent zero-phase filter is presented in Figure 13–31(b). Of
course, these methods of zero-phase filtering cannot be performed in real
time because we can’t reverse the flow of time (at least not in our universe).
This filtering is a blockprocessing, or off-line,process, such as filtering an audio
file stored in a computer. We must have all the time samples available before
we start processing. The initial time reversal in Figure 13–31(b) illustrates this
restriction.
There will be filter transient effects at the beginning and end of the fil-
tered sequences. If transient effects are bothersome in a given application,
consider discarding L samples from the beginning and end of the final y(n)
time sequence, where Lis four (or five) times the order of the IIR filter.
By the way, the final peak-to-peak passband ripple (in dB) of this zero-
phase filtering process will be twice the peak-to-peak passband ripple of the
single IIR filter. The final stopband attenuation will also be double that of the
single filter.
x(n) A B C y(n)
IIR Time IIR Time
(a)
filter reversal filter reversal
x(n) y(n)
Time IIR Time IIR
(b)
reversal filter reversal filter
Figure 13–31 Two equivalent zero-phase filtering techniques.

726 Digital Signal Processing Tricks
13.13 SHARPENED FIR FILTERS
Here’s an interesting technique for improving the stopband attenuation of a
digital filter under the condition that we’re unable, for whatever reason, to
modify that filter’s coefficients. Actually, we can double a filter’s stopband at-
tenuation by cascading the filter with itself. This works, as shown in Figure
13–32(a), where the frequency magnitude response of a single filter is a
dashed curve |H(m)| and the response of the filter cascaded with itself is rep-
resented by the solid curve |H2(m)|. The problem with this simple cascade
idea is that it also doubles the passband peak-to-peak ripple as shown in Fig-
ure 13–32(b). The frequency axis in Figure 13–32 is normalized such that a
value of 0.5 represents half the signal sample rate.
Well, there’s a better scheme for improving the stopband attenuation
performance of a filter and avoiding passband ripple degradation without ac-
tually changing the filter’s coefficients. The technique is called filter sharpen-
ing[34] and is shown as H in Figure 13–33.
s
The delay element in Figure 13–33 is equal to (N–1)/2 samples where N
is the number of h(k) coefficients, the unit-impulse response length, in the
original H(m) FIR filter. Using the sharpening process results in the improved
|H (m)| filter performance shown as the solid curve in Figure 13–34, where
s
we see the increased stopband attenuation and reduced passband ripple be-
yond that afforded by the original H(m) filter. Because of the delayed time-
alignment constraint, filter sharpening is not applicable to filters having
non-constant group delay, such as minimum-phase FIR filters or IIR filters.
If need be, we can eliminate the multipliers shown in Figure 13–33. The
multiply by two operation can be implemented with an arithmetic left shift
by one binary bit. The multiply by three operation can be implemented by
adding the Delay output sample to a shifted-left-by-one-bit version of itself.
1
0
-20
0
-40
-60
-1
-80
0 0.1 0.2 0.3 0.4 0.5 0 0.05 0.1 0.15 0.2 0.25 0.3
Frequency Frequency
(a) (b)
Figure 13–32 Frequency magnitude responses of a single filter and that filter cas-
caded with itself: (a) full response; (b) passband detail.
Bd
Bd |H(m)| |H2(m)|
|H2(m)|
|H(m)|

13.13 Sharpened FIR Filters 727
+ y(n)
Delay h(k) h(k)
-
3 2
x(n)
h(k)
H
s
Figure 13–33 Filter sharpening process.
Be aware that the gain factors in Figure 13–33 are based on the assump-
tion that the original h(k) filter to be sharpened has a passband gain of one. If
the h(k) filter has a non-unity passband gain of G ≠ 1, then the network in
Figure 13–35(a) should be used, where the alternate constant gain factors pro-
vide optimum filter sharpening. On the other hand, the Figure 13–35(a) gain
factors can be modified to some extent if doing so simplifies the filter imple-
mentation. For example, if 2/G2 = 1.7, for ease of implementation, the practi-
tioner should try using a factor of 2 in place of the factor 1.7. Using a gain
factor of 2 will not be optimum but it may well be acceptable, depending on
the characteristics of the filter to be sharpened. Software modeling will re-
solve this issue.
If additional stopband attenuation is needed, then the process shown in
Figure 13-35(b) can be used, where again the Delay element is equal to
(N-1)/2 unit delays.
In real-time applications, the filter sharpening networks we presented
are straightforward and applicable to linear-phase lowpass, bandpass, and
highpass FIR filters, just so long as the original filter’s H(f) has an integer
group delay. (That restriction is necessary because the number of unit delays
1
0
|H(m)| B |H(m)|
-20 d
B |H(m)| 0
d s
-40
|H(m)|
s
-60
-1
-80
0 0.1 0.2 0.3 0.4 0.5 0 0.05 0.1 0.15 0.2 0.25 0.3
Frequency Frequency
(a) (b)
Figure 13–34 |H(m)| and |H(m)| performance: (a) full frequency response;
s
(b) passband detail.

728 Digital Signal Processing Tricks
y(n)
+
Delay h(k) h(k)
–
(a)
3/G
2/G2
x(n)
h(k)
y(n)
+
Delay h(k) h(k) h(k)
–
(b) 4/G2 3/G3
x(n)
h(k)
Figure 13-35 Non-unity gain filter sharpening: (a) low-order sharpening;
(b) higher-order sharpening for increased stopband attenuation.
of the Delay element, needed for time synchronization in real-time systems,
in the parallel path must be an integer.) This sharpening procedure is particu-
larly useful if the original filter hardware is constrained to have some fixed
number of bits to represent its coefficients. If an FIR filter’s coefficient bit
width is b bits, the filter sharpening process in Figure 13–33 can, luckily for
us, achieve the performance of filters having (b + 4)-bit coefficients. So, if our
hardware forces us to use, say, 8-bit coefficients, we can achieve roughly 12-
bit-coefficient filter performance.
Filter sharpening can be used whenever a given filter response cannot
be modified, such as an unchangeable software subroutine, and can even be
applied to cascaded integrator-comb (CIC) filters to flatten their passband re-
sponses, as well as FIR fixed-point multiplierless filters where the coefficients
are constrained to be powers of two[35,36].
As a historical aside, filter sharpening is a process refined and expanded
by the accomplished R. Hamming (of Hamming window fame) based on an
idea originally proposed by the great American mathematician John Tukey,
the inventor of the radix-2 fast Fourier transform (FFT).
13.14 INTERPOLATING A BANDPASS SIGNAL
There are many digital communications applications where a real signal is
centered at one-fourth the sample rate, or f/4. This condition makes quadra-
s
ture down-conversion particularly simple. (See Sections 8.9 and 13.1.) In the

13.14 Interpolating a Bandpass Signal 729
event that you’d like to generate an interpolated (increased sample rate) ver-
sion of the bandpass signal but maintain its f/4 center frequency, there’s an
s
efficient way to do so[37]. Suppose we want to interpolate by a factor of two
so the output sample rate is twice the input sample rate, f = 2f . In this
s-out s-in
case the process is: quadrature down-conversion by f /4, interpolation fac-
s-in
tor of two, quadrature up-conversion by f /4, and then take only the real
s-out
part of the complex upconverted sequence. The implementation of this
scheme is shown at the top of Figure 13–36.
The sequences applied to the first multiplier in the top signal path are
the real x(n) input and the repeating mixing sequence 1,0,–1,0. That mixing
sequence is the real (or in-phase) part of the complexexponential
e–j2π(fs-in/4)ts-in= e–j2π(fs-in/4)(1/fs-in)= e–j2π(1/4)
(13–71)
needed for quadrature down-conversion by f/4. Likewise, the repeating mix-
s
ing sequence 0,–1,0,1 applied to the first multiplier in the bottom path is the
x(n) A B Lowpass C D
2
filter
+ y(n)
1, 0, -1, 0, ... 1, 0, -1, 0, ...
+
-
A B Lowpass C D
2
filter
0, -1, 0, 1, ... 0, 1, 0, -1, ...
X(m)
-2f s-in -f s-in 0 f s-in 2f s-in Freq
A Nodes
-2f s-in -f s-in 0 f s-in 2f s-in Freq
B Nodes
-f s-out -f s-out /2 0 f s-out /2 f s-out Freq
C Nodes
-f s-out -f s-out /2 0 f s-out /2 f s-out Freq
D Nodes
-f s-out -f s-out /2 0 f s-out /2 f s-out Freq
Y(m)
-f s-out -f s-out /2 0 f s-out /2 f s-out Freq
Figure 13–36 Bandpass signal interpolation scheme, and spectra.

730 Digital Signal Processing Tricks
imaginary (or quadrature phase) part of the complex down-conversion expo-
nential
e–j2π(fs-in/4)ts-in.
The “↓2” symbol means insert one zero-valued sample
between each sample at the A nodes. The final subtraction to obtain y(n) is
how we extract the real part of the complex sequence at Node D. (That is,
we’re extracting the real part of the product of the complex signal at Node C
times
ej2π(1/4).)
The spectra at various nodes of this process are shown at the
bottom of Figure 13–35. The shaded spectra indicate true spectral compo-
nents, while the white spectra represent spectral replications. Of course, the
same lowpass filter must be used in both processing paths to maintain the
proper time delay and orthogonal phase relationships.
There are several additional issues worth considering regarding this in-
terpolation process[38]. If the amplitude loss, inherent in interpolation, of a
factor of two is bothersome, we can make the final mixing sequences 2,0,–2,0
and 0,2,0,–2 to compensate for that loss. Because there are so many zeros in the
sequences at Node B (three-fourths of the samples), we should consider those
efficient polyphase filters for the lowpass filtering. Finally, if it’s sensible in
your implementation, consider replacing the final adder with a multiplexer
(because alternate samples of the sequences at Node D are zeros). In this case,
the mixing sequence in the bottom path would be changed to 0,–1,0,1.
13.15 SPECTRAL PEAK LOCATION ALGORITHM
In the practical world of discrete spectrum analysis, we often want to esti-
mate the frequency of a sinusoid (or the center frequency of a very narrow-
band signal of interest). Upon applying the radix-2 fast Fourier transform
(FFT), our narrowband signals of interest rarely reside exactly on an FFT bin
center whose frequency is exactly known. As such, due to the FFT’s leakage
properties, the discrete spectrum of a sinusoid having N time-domain sam-
ples may look like the magnitude samples shown in Figure 13–37(a). There
we see the sinusoid’s spectral peak residing between the FFT’s m = 5 and
m = 6 bin centers. (Variable m is an N-point FFT’s frequency-domain index.
The FFT bin spacing is f/Nwhere, as always, f is the sample rate.) Close ex-
s s
amination of Figure 13–37(a) allows us to say the sinusoid lies in the range of
m=5 and m=5.5, because we see that the maximum spectral sample is closer
to the m = 5 bin center than the m = 6 bin center. The real-valued sinusoidal
time signal has, in this example, a frequency of 5.25f/NHz. In this situation,
s
our frequency estimation resolution is half the FFT bin spacing. We often
need better frequency estimation resolution, and there are indeed several
ways to improve that resolution.
We could collect, say, 4Ntime-domain signal samples and perform a 4N-
point FFT, yielding a reduced bin spacing of f/4N. Or we could pad (append to
s
the end of the original time samples) the original Ntime samples with 3Nzero-
valued samples and perform a 4N-point FFT on the lengthened time sequence.

13.15 Spectral Peak Location Algorithm 731
|X(m)|
1
5
1
N-point FFT
0
(a) (ofN time
5 samples)
0
0 1 2 3 4 5 6 7 8 9 1 1 1 1 m
0 1 2 3
The peak of the sinusoid's spectrum is
somewhere in this frequency range
|X'(m)|
Peak now located
15 atm = 21
4N-point FFT
(b) 10 (3N zeros
appended to N
5 time samples)
0
5 9 13 17 21 25 29 33 m
Figure 13–37 Spectral magnitudes: (a) N-point FFT; (b) 4N-point FFT.
That would also provide an improved spectral peak estimation granularity of
f/4N, as shown in Figure 13–37(b). With the spectral peak located at bin m
s peak
= 21, we estimate the signal’s center frequency, in Hz, using f = m f/4N.
peak peak s
Both schemes, collect more data and zero padding, are computationally ex-
pensive. Many other techniques for enhanced-precision tone frequency mea-
surement have been described in the scientific literature—from the
close-to-home field of geophysics to the lofty studies of astrophysics—but
most of those schemes seek precision without regard to computational com-
plexity. Here we describe several computationally simple frequency estima-
tion schemes.
Assume we have the X(m) spectral samples from an N-point FFT of a si-
nusoidal time signal, whose magnitudes are shown in Figure 13–38(a). (The
vertical magnitude axis is linear, not logarithmic.) The notation in the figure is
that m is the integer index of the largest magnitude sample |X(m )|. The
k k
value m , which in general will not be an integer, is the value we wish to
peak
estimate and use in
f
f =m ⋅ s (13–72)
peak peak N
to accurately estimate the sinusoid’s center frequency in Hz.

732 Digital Signal Processing Tricks
Next, let’s say the FFT’s input sinusoid sweeps in frequency starting at
the FFT’s m bin center frequency to the center frequency of the m bin and
k k+1
we assign m to be equal to the index value (either m or m ) of the highest
peak k k+1
spectral magnitude sample. The error in that m value will be that shown
peak
in Figure 13–38(b). The maximum error in that naive m assignment
peak
scheme is 0.5 FFT bins (half the FFT bin spacing). Happily for us, there are
more accurate methods for estimating m .
peak
As it turns out, we can estimate the signal’s index-based center fre-
quency, m , using
peak
m = m + C (13–73)
peak k i
where C is a scalar correction factor in the range of –0.5 ≤ C ≤ 0.5. There are
i i
many algorithms, based on fitting a generic parabolic curve to the |X(m)|
samples, floating around in the literature of DSP for estimating C. Those al-
i
gorithms have varying degrees of accuracy depending on the window func-
tion applied to the FFT’s input samples.
|X(m)|
|X(m k )| True |X(f)|
spectrum
(a)
|X(m )|
k+1
|X(m )|
k–1
0 m k–1 m m k+1 m
k m
peak
s) 0.5
n
bi
T Setm = m
(b) F 0 peak k
F Set m = m
or
( peak k+1
Err –0.5
m k Truem peak m k+1
s) 0.06 UsingC
n 2
bi
T 0.02
(c) F F 0
or
(
–0.02
Err
–0.06 UsingC
3
m k Truem peak m k+1
Figure 13–38 Spectral peak detection: (a) FFT magnitudes; (b) m error by
peak
naive assignment; (c) m algorithm error performance.
peak

13.15 Spectral Peak Location Algorithm 733
Anoteworthy correction factor expression is
X(m )−X(m )
C =realpartof
k−1 k+1
. (13–74)
1 2X(m )−X(m )−X(m )
k k−11 k+1
This complex-valued spectral peak location estimation algorithm is quite ac-
curate for its simplicity[3]. Its maximum frequency estimation error is
roughly 0.06, 0.04, and 0.03 bin widths for signal-to-noise ratios of 3, 6, and 9
dB respectively. Not bad at all! The nice features of the algorithm are that it
does not require the original time samples to be windowed, as do some other
spectral peak location algorithms; and it does not require computation of FFT
magnitude samples.
If a time-domain window sequence has been applied to the FFT’s input
samples, then other C correction factor expressions should be used in place of
i
Eq. (13–74). Three notable candidate expressions for C are
i
|X(m )|−|X(m )|
C =
k+1 k−1
(13–75)
2 4|X(m )|−2|X(m )|−2||X(m )|
k k−1 k+1
[ ]
P|X(m )|−|X(m )|
C =
k+1 k−1
(13–75’)
3 |X(m )|+|X(m )|+||X(m )|
k−1 k k+1
Q[X(m )−X(m )]
C =realpartof k−1 k+1 . (13–75’’)
4 2X(m )+X(mm )+X(m )
k k−1 k+1
where again we use subscripts on C merely to identify the different expres-
sions for the correction factor C. The above window-dependent P and Q fac-
i
tors, determined empirically, are
• Hamming, P= 1.22, Q= 0.60;
• Hanning, P= 1.36, Q= 0.55;
• Blackman, P= 1.75, Q= 0.55; and
• Blackman-Harris (3-term), P= 1.72, Q= 0.56.
Equation (13–75) is the best known peak location algorithm and has
been used in the DSP business for decades. The lesser-known Eq. (13–75’)
provides a more accurate windowed-FFT peak location estimate than Eq.
(13–75)[39]. Inspired by Eqs. (13–74) and (13–75’), the author has developed
Eq. (13–75’’) which can be used in case the FFT magnitude samples are un-
available for use in Eq. (13–75’). Equation (13–75’’) is also more accurate than
the better-known Eq. (13–75).
The solid curve in Figure 13–38(c) shows the m error in using Eq.
peak
(13–75’) with Blackman-windowed time-domain samples whose signal-to-

734 Digital Signal Processing Tricks
noise ratio is 9 dB. For comparison, the dashed curve is the m error when
peak
using Eq. (13–75). Equation (13–75’’)’s accuracy is very similar to that of
Eq. (13–75’).
Equations (13–74) and (13–75’’) have the advantage that FFT magnitude
calculations, with their computationally costly square root operations, are not
required as is necessary with other spectral peak location algorithms de-
scribed above. However, the question naturally arises, “How do we deter-
mine the index m of the largest-magnitude FFT sample, |X(m )|, in Figure
k k
13–38(a) without computing square roots to obtain FFT magnitudes?” The an-
swer is that we can use the complex vector-magnitude approximations, re-
quiring no square root computations, described in Section 13.2.
Be aware that the above spectral peak location methods are only applic-
able when the majority of the signal’s spectral energy lies within a single FFT
bin width (f/N), and the FFT spectral samples are not substantially contami-
s
nated by leakagefrom another spectral component.
13.16 COMPUTING FFT TWIDDLE FACTORS
Typical applications using an N-point radix-2 FFT accept N x(n) input time
samples and compute NX(m) frequency-domain samples. However, there are
non-standard FFT applications (for example, specialized harmonic analysis,
or perhaps using an FFT to implement a bank of filters) where only a subset
of the full X(m) results is required. Consider Figure 13–39 which shows the
butterfly operations for an 8-point radix-2 decimation-in-frequency FFT. No-
tice that the FFT butterflies in Figure 13–39 are the optimized butterflies intro-
duced in Figure 4–14. Assuming we are only interested in the X(3) and X(7)
output samples, rather than compute the entire FFT we perform only the
computations indicated by the bold lines in the figure.
Reduced-computation FFTs are often called pruned FFTs[40–43]. To im-
plement pruned FFTs we need to know the twiddle phase angles associated
with each necessary butterfly computation in the paths of any bold signal-
flow line in Figure 13–39. (As we did in Chapter 4 for simplicity, the butter-
flies in Figure 13–39 only show the twiddle phase-angle factors and not the
entire complex-valued twiddle factors.) Here we show how to compute those
individual twiddle phase angles.
13.16.1 Decimation-in-Frequency FFT Twiddle Factors
For the decimation-in-frequency (DIF) radix-2 FFT using the optimized
butterflies:
• The N-point DIF FFT has log (N) stages, numbered P = 1, 2, ...,
2
log (N).
2

13.16 Computing FFT Twiddle Factors 735
stage P = 1 stageP = 2 stage P = 3
x(0) X(0)
x(1) –1 0 X(4)
x(2) –1 0 X(2)
x(3) –1 2 –1 0 X(6)
x(4) –1 0 X(1)
x(5) –1 1 –1 0 X(5)
x(6) –1 2 –1 0 X(3)
x(7) –1 3 –1 2 –1 0 X(7)
N/2P = 4 N/2P = 2 N/2P = 1
k = 0,1,2,3 k = 0,1 k = 0
Figure 13–39 Eight-point decimation-in-frequency FFT signal-flow diagram.
• Each stage comprises N/2 butterflies.
• Not counting the –1 twiddle factors, the Pth stage has N/2P unique
twiddle factors, numbered k = 0, 1, 2, ... , N/2P–1 as indicated by the
upward arrows at the bottom of Figure 13–39.
Given those characteristics, the kth unique twiddle factor phase angle for the
Pth stage is computed using
kth DIF twiddle factor angle = k·2P/2 (13–76)
where 0 ≤ k ≤ N/2P–1. For example, for the second stage (P = 2) of an N =
8-point DIF FFT, the unique twiddle factor angles are
k= 0, angle = 0·2P/2 = 0·4/2 = 0
k= 1, angle = 1·2P/2 = 1·4/2 = 2.
13.16.2 Decimation-in-Time FFT Twiddle Factors
Here we present an interesting algorithm for computing the individual twid-
dle factor angles of a radix-2 decimation-in-time (DIT) FFT[44]. Consider Fig-
ure 13–40 showing the butterfly signal flow of an 8-point DIT FFT.

736 Digital Signal Processing Tricks
stageP = 1 stageP = 2 stageP = 3
x(0) X(0)
x(1) 0 –1 X(4)
x(2) 0 –1 X(2)
x(3) 0 –1 2 –1 X(6)
x(4) 0 –1 X(1)
x(5) 0 –1 1 –1 X(5)
x(6) 0 –1 2 –1 X(3)
x(7) 0 –1 2 –1 3 –1 X(7)
k = 0,1,2,3 k = 0,1,2,3 k = 0,1,2,3
Figure 13–40 Eight-point decimation-in-time FFT signal-flow diagram.
For the decimation-in-time (DIT) FFT using the optimized butterflies:
• The N-point DIT FFT has log (N) stages, numbered P = 1, 2, . . .,
2
log (N).
2
• Each stage comprises N/2 butterflies.
• Not counting the –1 twiddle factors, the Pth stage has N/2 twiddle
factors, numbered k = 0, 1, 2, ... , N/2–1 as indicated by the upward
arrows at the bottom of Figure 13–40.
Given those characteristics, the kth twiddle factor phase angle for the Pth
stage is computed using
kth DIT twiddle factor angle = [⎣k2P/N⎦] (13–76’)
bit-rev
where 0 ≤ k ≤ N/2–1. The ⎣q⎦ operation means the integer part of q. The
[z] function represents the three-step operation of: convert decimal inte-
bit-rev
ger zto a binary number represented by log (N)–1 binary bits, perform bit re-
2
versal on the binary number as discussed in Section 4.5, and convert the
bit-reversed number back to a decimal integer.
As an example of using Eq. (13–76’), for the second stage (P = 2) of an
N=8-point DIT FFT, the k=3 twiddle factor angle is
3rd twiddle factor angle = [⎣3·22/8⎦] = [⎣1.5⎦] = [1] = 2.
bit-rev bit-rev bit-rev

13.17 Single Tone Detection 737
The above [1] operation is: Take the decimal number 1 and represent it
bit-rev
with log (N)–1 = 2 bits, i.e., as 01 . Next, reverse those bits to a binary 10 and
2 2 2
convert that binary number to our desired decimal result of 2.
13.17 SINGLE TONE DETECTION
In this section we present an IIR filter structure used to perform spectrum
analysis in the detection and measurement of single sinusoidal tones. The
standard method for spectral energy is the discrete Fourier transform (DFT),
typically implemented using a fast Fourier transform (FFT) algorithm. How-
ever, there are applications that require spectrum analysis only over a subset
of the N bin-center frequencies of an N-point DFT. Apopular, as well as effi-
cient, technique for computing sparse FFT results is the Goertzel algorithm,
using an IIR filter implementation to compute a single complex DFT spectral
bin value based upon Ninput time samples. The most common application of
this process is to detect the presence of a single continuous-wave sinusoidal
tone. With that in mind, let’s look briefly at tone detection.
It’s certainly possible to use the FFT to detect the presence of a single si-
nusoidal tone in a time-domain sequence x(n). For example, if we wanted to
detect a 30 kHz tone in a time-domain sequence whose sample rate was
f = 128 kHz, we could start by performing a 64-point FFT as shown in Figure
s
13–41. Then we would examine the magnitude of the X(15) complex sample
to see if it exceeds some predefined threshold.
This FFT method is very inefficient. In our example, we’d be performing
192, (64/2)(log 64), complex multiplies to obtain the 64-point complex X(m)
2
in order to compute the one X(15) in which we’re interested. We discarded
98 percent of our computation results! We could be more efficient and calcu-
late our desired X(15) using the single-point discrete Fourier transform (DFT)
in Eq.(13–77), which requires N=64 complex multiplies using
63
∑ −j2πn15/64
X(15)= x(n)e . (13–77)
n=0
X(0)
128 kHz
X(1) 15 64 = 30 kHz
x(n) 64-point, X(2)
. . .
radix-2 This is the only FFT
FFT X(15) result we care about
. . . (m = 15)
X(63)
Figure 13–41 DFT method, using an FFT algorithm, to detect a 30 kHz tone.

738 Digital Signal Processing Tricks
That would be an improvement but, happily, there’s a better way. It’s
called the Goertzel algorithm(pronounced ‘girt-zel).
13.17.1 Goertzel Algorithm
The Goertzel algorithm is implemented in the form of a 2nd-order IIR filter,
with two real feedback coefficients and a single complex feedforward coeffi-
cient, as shown in Figure 13–42. (Although we don’t use this process as a tra-
ditional filter, common terminology refers to the structure as a filter.) This
filter computes a single-bin DFT output (the mthbin of an N-point DFT) de-
fined by
N–1
∑ −j2πnm/N
X(m)= x(n)e . (13–78)
n=0
The filter’s y(n) output is equal to the DFT output frequency coefficient,
X(m), at the time index n=N, where the first time index value is n=0. For em-
phasis, we remind the reader that the filter’s y(n) output is not equal to X(m) at
any time index when n ≠ N. To be equivalent to the DFT, the frequency-
domain index mmust an integer in the range 0≤m≤N–1. You’re welcome to
think of the Goertzel algorithm as a single-binDFT. The derivation of this filter
(this algorithm) structure is readily available in the literature[45–47].
The z-domain transfer function of the Goertzel filter is
Y(z) 1−e −j2πm/N z −1
H (z) = = , (13–79)
G X(z) 1−2cos(2πm/N)z −1+z −2
with a single z-domain zero located at z =
e–j2πm/N
and conjugate poles at
z=e±j2πm/Nas
shown in Figure 13–43(a). The pole/zero pair at
z=e–j2πm/Ncan-
cel each other. Having a filter pole on the unit circle is typically a risky thing
to do for stability reasons, but not so with the Goertzel algorithm. Because it
processes N+1-length blocks of time samples (where N is usually in the hun-
x(n) w(n) y(n)
z -1
The value m
2cos(2πm/N)
z -1
-e-j2πm/N
f
d
il
e
te
te
r'
r
s
m
re
in
s
e
o
s
n
t
a
h
n
e
t
frequency
-1
Figure 13–42 IIR filter implementation of the Goertzel algorithm.

13.17 Single Tone Detection 739
1
0.5
0
-0.5
-1
-2 -1 0 1 2
Real part
Figure 13–43 Goertzel filter: (a) z-domain pole/zero locations; (b) frequency
magnitude response.
dreds), the filter remains stable for such short time sequences because its in-
ternal data storage registers, w(n–1) and w(n–2), are reset to zero at the begin-
ning of each new block of input data. The filter’s frequency magnitude
response, provided in Figure 13–43(b), shows resonance centered at a normal-
ized frequency of 2πm/N, corresponding to a cyclic frequency of mf/N Hz
s
(where f is the signal sample rate).
s
The Goertzel algorithm is implemented with a complex resonator hav-
ing an infinite-length unit impulse response, h(n) =
ej2πnm/N,
and that’s why its
frequency magnitude response is so narrow. The time-domain difference
equations for the Goertzel filter are
w(n) = 2cos(2πm/N)w(n–1) –w(n–2) + x(n) (13–80)
y(n) = w(n)
–e–j2πm/Nw(n–1).
(13–81)
An advantage of the Goertzel filter in computing an N-point X(m) DFT
bin value is that Eq. (13–80) is implemented N times while Eq. (13–81), the
feedforward path in Figure 13–42, need only be computed once after the ar-
rival of the Nthinput sample. Thus for real x(n) inputs the filter requires N+2
real multiplies and 2N+1 real adds to compute an N-point X(m). However,
when modeling the Goertzel filter, if the time index begins at n = 0, the filter
must process N+1 time samples with x(N)=0 to compute X(m).
In typical applications, to minimize spectral leakage, we choose N so
there’s an integer number of cycles in our input sequence of the tone we’re try-
ing to detect. Ncan be any integer, and the larger Nis, the better the frequency
resolution and noise immunity. However, larger Nmeans more computations.
It’s worth noting that while the typical Goertzel algorithm description in
the literature specifies the frequency resonance variable m to be an integer
(making the Goertzel filter’s output equivalent to an N-point DFT bin
trap
yranigamI
0
z-plane
-10
2πm/N
-20
-30
m-2 m-1 m m+1 m+2
Frequency
(a) (b)
Bd

740 Digital Signal Processing Tricks
output), the m in Figure 13–42 and Eq. (13–79) can in fact be any value be-
tween 0 and N–1, giving us full flexibility in specifying our filter’s resonant
frequency.
13.17.2 Goertzel Example
Let’s use Goertzel to calculate the spectral magnitude of that f = 30 kHz
tone
tone from the Figure 13–41 example. When f =128 kHz and N=64, our reso-
s
nant frequency integer mis
f (64)(30) kHz
m= tone = = 15. (13–82)
f /N 128 kHz
s
The Goertzel filter and the necessary computations for our 30 kHz detec-
tion example are provided in Figure 13–44.
It’s useful to know that if we want to compute the power of X(15), |X(15)2|,
the final feedforward complex calculations can be avoided by computing
|X(m)|2= |y(N–1)|2
= w(N–1)2+ w(N–2)2–w(N–1)w(N–2)[2cos(2πm/N)]. (13–83)
In our example, Eq. (13–83) becomes
|X(15)|2= |y(63)|2= w(63)2+ w(62)2–w(63)w(62)[2cos(2π15/64)]. (13–84)
13.17.3 Goertzel Advantages over the FFT
Here are some implementation advantages of the Goertzel algorithm over the
standard radix-2 FFT for single tone detection:
x(n) w(n) y(n)
Coefficients
2cos(2π15/64) = 0.196,
z -1
-e-j2πm/N = -e-j2π15/64
= -e-j1.473
0.196 -1 -0.098 + j0.995 = -0.098 + j0.995
z -1
Perform these Perform these What we do to
calculations 64 times calculations once compute |X(15)|
Perform these Perform these What we do to
calculations 65 times calculations once computeX(15)
with inputx(65) = 0
Figure 13–44 Filter, coefficients, and computations to detect the 30 kHz tone.

13.18 The Sliding DFT 741
• Ndoes not need to be an integer power of two.
• The resonant frequency can be any value between zero and f Hz.
s
• The amount of filter coefficient (versus FFT twiddle factor) storage is re-
duced. If Eq. (13–83) is used, only one coefficient need be stored.
• No storing a block of input data is needed before processing can begin (as
with the FFT). Processing can begin with the first input time sample.
• No data bitreversalis needed for Goertzel.
• If you implement the Goertzel algorithm M times to detect M different
tones, Goertzel is more efficient (fewer multiplies) than the FFT when
M<log N.
2
• Computational requirements to detect a single tone (assuming real-only
x(n) input) are given in Table 13–4.
As a final note, although the Goertzel algorithm is implemented with a
complex resonating filter structure, it’s not used as a typical filter where we
retain each output sample. For the Goertzel algorithm we retain only every
Nth, or (N+1)th, output sample. As such, the frequency magnitude response
of the Goertzel algorithm when treated as a black-box process is equivalent to
the |sin(x)/x|-like magnitude response of a single bin of an N-point DFT, a
portion of which is shown in Figure 13–45.
13.18 THE SLIDING DFT
The above Goertzel algorithm computes a single complex DFT spectral bin
value for every Ninput time samples. Here we describe a slidingDFTprocess
whose spectral bin output rate is equal to the input data rate, on a sample-by-
sample basis, with the advantage that it requires fewer computations than the
Goertzel algorithm for real-time spectral analysis. In applications where a
new DFT output spectrum is desired every sample, or every few samples, the
sliding DFT is computationally simpler than the traditional radix-2 FFT.
Table 13–4 Single-Bin DFT Computational Comparisons
Method Real multiplies Real additions
Single-bin DFT 4N 2N
FFT 2Nlog N Nlog N
2 2
Goertzel N+ 2 2N+ 1

742 Digital Signal Processing Tricks
0
-10
-20
-30
Figure 13–45 Goertzel algorithm frequency magnitude response.
13.18.1 The Sliding DFT Algorithm
The sliding DFT (SDFT) algorithm computes a single bin result of an N-point
DFT on time samples within a sliding window. That is, for the mth bin of an
N-point DFT, the SDFT computes
N−1
Xm(q)=
∑
x(n)e
−j2πnm/N
. (13–85)
n=0
Let’s take care to understand the notation of Xm(q). Typically, as in Chap-
ter 3, the index of a DFT result value was the frequency-domain index m. In
Eq. (13–85) the index of the DFT result is a time-domain index q=0, 1, 2, 3, ...,
such that our first mth-bin SDFT is Xm(0), our second SDFT is Xm(1), and
soon.
An example SDFT analysis time window is shown in Figure 13–46(a)
where Xm(0) is computed for the N = 16 time samples x(0) to x(15). The time
window is then advanced one sample, as in Figure 13–46(b), and the new
Xm(1) is calculated. The value of this process is that each new DFT result is ef-
ficiently computed directly from the result of the previous DFT. The incre-
mental advance of the time window for each output computation leads to the
name slidingDFTor sliding-windowDFT.
We can develop the mathematical expression for the SDFT as follows:
the standard N-point DFT equation, of the mth DFT bin, for the qth DFT of the
time sequence x(q), x(q+1), ..., x(q+N–1) is
N−1
Xm(q) = ∑ x(n+q)e −j2πnm/N . (13–86)
n=0
(Variable m is the frequency-domain index, where m = 0, 1, 2, ..., N–1.) Like-
wise, the expression for the next DFT, the (q+1)th DFT performed on time
samples x(q+1), x(q+2), ..., x(q+N), is
Bd
m-3 m-2 m-1 m m+1 m+2 m+3
Frequency

13.18 The Sliding DFT 743
Window for Xk(0)
1
(a) 0
-1
x(0) Time x(15)
Shifted window for Xk(1)
1
(b) 0
-1
x(0) x(1) Time x(16)
Figure 13–46 Analysis window for two 16-point DFTs: (a) data samples in the first
computation; (b) second computation samples.
N−1
Xm(q+1) = ∑ x(n+q+1)e −j2πnm/N. (13–87)
n=0
Letting p=n+1 in Eq. (13–87), we can write
N
Xm(q+1) = ∑ x(p+q)e −j2π(p−1)m/N . (13–88)
p=1
Shifting the limits of summation in Eq. (13–88), and including the appropriate
terms (subtract the p = 0 term and add the p = N term) to compensate for the
shifted limits, we write
Xm(q+1)
⎡N−1
⎣
= ⎢ ∑ x(p+q)e −j2π(p−1)m/N⎢–x(q)e–j2π(–1)m/N+ x(q+N)e–j2π(N–1)m/N. (13–89)
⎣ ⎡
p=0
Factoring the common exponential term
(ej2πm/N),
we write
Xm(q+1) = ej2πm/N ⎛ ⎜ ⎡ ⎢ N ∑ −1 x(p+q)e −j2πpm/N⎢ ⎣ –x(q) + x(q+N)e–j2πNm/N⎜ ⎝ . (13–90)
⎝⎣ p=0 ⎡ ⎛
Recognizing the summation in the brackets being equal to the previous
Xm(q) in Eq. (13–86), and e–j2πm = 1, we write the desired recursive expression
for the sliding N-point DFT as
Xm(q+1) = ej2πm/N[Xm(q) + x(q+N) –x(q)], (13–91)

744 Digital Signal Processing Tricks
where Xm(q+1) is the new single-bin DFT result and Xm(q) is the previous
single-bin DFT value. The superscript m reminds us that the Xm(q) spectral
samples are those associated with the mth DFT bin.
Let’s plug some numbers into Eq. (13–91) to reveal the nature of its time
indexing. If N=20, then 20 time samples (x(0) to x(19)) are needed to compute
the first result Xm(0). The computation of Xm(1) is then
Xm(1) = ej2πm/N[Xm(0) + x(20) –x(0)]. (13–92)
Due to our derivation method’s time indexing, Eq. (13–92) appears com-
pelled to look into the future for x(20) to compute Xm(1). With no loss in gen-
erality, we can modify Eq. (13–91)’s time indexing so that the x(n) input
samples and the Xm(q) output samples use the same time index n. That modi-
fication yields our SDFT time-domain difference equation of
Xm(n) = ej2πm/N[Xm(n–1) + x(n) –x(n–N)]. (13–93)
Equation (13–93) reveals the value of this process in computing real-time
spectra. We compute Xm(n) by subtracting the x(n–N) sample and adding the
current x(n) sample to the previous Xm(n–1), and phase shifting the result.
Thus the SDFT requires only two real additions and one complex multiply
per output sample. Not bad at all! Equation (13–93) leads to the single-bin
SDFT filter implementation shown in Figure 13–47.
The single-bin SDFT algorithm is implemented as an IIR filter with a
comb filter followed by a complex resonator. (If you need to compute all N
DFT spectral components, Nresonators with m=0 to N–1 will be needed, all
driven by a single comb filter.) The comb filter delay of N samples forces the
SDFT filter’s transient response to be N samples in length, so the output will
not reach steady state until the Xm(N–1) sample. The output will not be valid,
or equivalent to Eq. (13–86)’s Xm(q), until N input samples have been
processed. The z-transform of Eq. (13–93) is
Xm(z) = ej2πm/N[Xm(z)z–1+ X(z) –X(z)z–N], (13–94)
Comb Complex resonator
x(n) Xm(n)
-1
z -N ej2πm/N z -1
Xm(n-1)
x(n-N)
Figure 13–47 Single-bin sliding DFT filter structure.

13.18 The Sliding DFT 745
where factors of Xm(z) and X(z) are collected, yielding the z-domain transfer
function for the mth bin of the SDFT filter as
X m (z) (1−z −N )e j2πm/N
H (z) = = . (13–95)
SDFT X(z) 1−e j2πm/N z −1
This complex filter has N zeros equally spaced around the z-domain’s
unit circle, due to the N-delay comb filter, as well as a single pole canceling
the zero at z =
ej2πm/N.
The SDFT filter’s complex unit impulse response h(n)
and pole/zero locations are shown in Figure 13–48 for the example where
m=2 and N=20.
Because of the comb subfilter, the SDFT filter’s complex sinusoidal unit
impulse response is finite in length—truncated in time to N samples—and
that property makes the frequency magnitude response of the SDFT filter
identical to the sin(Nx)/sin(x) response of a single DFT bin centered at a fre-
quency of 2πm/N.
One of the attributes of the SDFT is that once an Xm(n) is obtained, the
number of computations to compute Xm(n+1) is fixed and independent of N.
A computational workload comparison between the Goertzel and SDFT fil-
ters is provided later in this section. Unlike the radix-2 FFT, the SDFT’s Ncan
be any positive integer, giving us greater flexibility to tune the SDFT’s center
frequency by defining integer msuch that m=Nf/f, when f is a frequency of
i s i
interest in Hz and f is the signal sample rate in Hz. In addition, the SDFT re-
s
quires no bit-reversal processing as does the FFT. Like the Goertzel algorithm,
the SDFT is especially efficient for narrowband spectrum analysis.
For completeness, we mention that a radix-2 slidingFFTtechnique exists
for computing all N bins of Xm(q) in Eq. (13–85)[48,49]. That technique is
1
1
0
-1 0.5
0 2 4 6 8 10 12 14 16 18 20 22
Real[h(n)] 0
1 -0.5
0
-1
-1 0 2 4 6 8 10 12 14 16 18 20 22 -1 0 1
Real part
Imaginary[h(n)]
(a) (b)
Figure 13–48 Sliding DFT characteristics for m= 2 and N= 20: (a) complex im-
pulse response; (b) pole/zero locations.
trap
yranigamI 2πm/N
m = 2
N = 20

746 Digital Signal Processing Tricks
computationally attractive because it requires only N complex multiplies to
update the N-point FFT for all N bins; however, it requires 3N memory loca-
tions (2N for data and N for twiddle coefficients). Unlike the SDFT, the
radix-2 sliding FFT scheme requires address bit-reversal processing and re-
stricts Nto be an integer power of two.
13.18.2 SDFT Stability
The SDFT filter is only marginally stable because its pole resides on the
z-domain’s unit circle. If filter coefficient numerical rounding error is not se-
vere, the SDFT is bounded-input-bounded-output stable. Filter instability can
be a problem, however, if numerical coefficient rounding causes the filter’s
pole to move outside the unit circle. We can use a damping factor r to force
the pole and zeros in Figure 13–48(b) to be at a radius of rjust slightly inside
the unit circle and guarantee stability using a transfer function of
(1−r N z −N )re j2πm/N
H (z) = , (13–96)
SDFT,gs 1−re j2πm/N z −1
with the subscript “gs” meaning guaranteed-stable. (Section 7.5.3 provides the
mathematical details of moving a filter’s poles and zeros inside the unit circle.)
The stabilized feedforward and feedback coefficients become –rN and rej2πm/N,
respectively. The difference equation for the stable SDFT filter becomes
Xm(n) = rej2πm/N[Xm(n–1) + x(n) –rNx(n–N)] (13–97)
with the stabilized-filter structure shown in Figure 13–49. In this case, we per-
form five real multiplies and four real additions per output sample.
Using a damping factor as in Figure 13–49 guarantees stability, but the
Xm(q) output, defined by
N−1
∑ (N–n) −j2πnm/N
Xm (q)= x(n)r e , (13–98)
r<1
n=0
is no longer exactly equal to the mth bin of an N-point DFT in Eq. (13–85).
While the error is reduced by making r very close to (but less than) unity, a
x(n) Xm(n)
-rN
z -N rej2πm/N z -1
Xm(n-1)
Figure 13–49 Guaranteed-stable sliding DFT filter structure.

13.18 The Sliding DFT 747
scheme does exist for eliminating that error completely once every N output
samples at the expense of additional conditional logic operations[50]. Deter-
mining if the damping factor r is necessary for a particular SDFT application
requires careful empirical investigation. As is so often the case in the world of
DSP, this means you have to test your SDFT implementation very thoroughly
and carefully!
Another stabilization method worth consideration is decrementing the
largest component (either real or imaginary) of the filter’s
ej2πm/Nfeedback
co-
efficient by one least significant bit. This technique can be applied selectively
to problematic output bins and is effective in combating instability due to
rounding errors that result in finite-precision
ej2πm/N
coefficients having mag-
nitudes greater than unity. Like the DFT, the SDFT’s output is proportional to
N, so in fixed-point binary implementations the designer must allocate suffi-
ciently wide registers to hold the computed results.
13.18.3 SDFT Leakage Reduction
Being equivalent to the DFT, the SDFT also suffers from spectral leakage ef-
fects. As with the DFT, SDFT leakage can be reduced by the standard concept
of windowing the x(n) input time samples as discussed in Section 3.9. How-
ever, windowing by time-domain multiplication would ruin the real-time
computational simplicity of the SDFT. Thanks to the convolution theorem
properties of discrete systems, we can implement time-domain windowing
by means of frequency-domain convolution, as discussed in Section 13.3.
Spectral leakage reduction performed in the frequency domain is accom-
plished by convolving adjacent Xm(q) values with the DFT of a window func-
tion. For example, the DFT of a Hamming window comprises only three
nonzero values, –0.23, 0.54, and –0.23. As such, we can compute a Hamming-
windowed Xm(q) with a three-point convolution using
Hamming-windowed Xm(q) = –0.23Xm–1(q) +0.54Xm(q) –0.23Xm+1(q). (13–99)
Figure 13–50 shows this process using three resonators, each tuned to
adjacent DFT bins (m–1, m, and m+1). The comb filter stage need only be im-
plemented once.
Table 13–5 provides a computational workload comparison of various
spectrum analysis schemes in computing an initial Xm(n) value and comput-
ing a subsequent Xm(n+1) value.
To compute the initial windowed Xm(n) values in Table 13–5, the three-
term frequency-domain convolution need only be performed once, upon ar-
rival of the Nth time sample. However, the convolution needs to be
performed for all subsequent computations
We remind the reader that Section 13.3 discusses several implementation
issues regarding Hanning windowing in the frequency domain, using binary

748 Digital Signal Processing Tricks
Resonator
Xm-1(q)
m-1
-0.23 Windowed
x(n)
Resonator
Xm(q) Xm(q) output
m
z -N
0.54
Xm+1(q)
Resonator
m+1
-1 -0.23
Figure 13–50 Three-resonator structure to compute a single Hamming-windowed
Xm(q).
shifts to eliminate the multiplications in Eq. (13–99), as well as the use of
other window functions.
13.18.4 A Little-Known SDFT Property
The SDFT has a special property that’s not widely known but is very
important. If we change the SDFT’s comb filter feedforward coefficient
(in Figure 13–47) from –1 to +1, the comb’s zeros will be rotated counter-
clockwise around the unit circle by an angle of π/N radians. This situation,
for N = 8, is shown on the right side of Figure 13–51(a). The zeros are lo-
cated at angles of 2π(m + 1/2)/N radians. The m = 0 zeros are shown as
solid dots. Figure 13–51(b) shows the zeros locations for an N = 9 SDFT
Table 13–5 Single-Bin DFT Computation Comparison
Method Compute initial Xm(n) ComputeXm(n+1)
Real Real Real Real
multiplies adds multiplies adds
DFT 4N 2N 4N 2N
Goertzel algorithm N+ 2 2N+ 1 N+ 2 2N+ 1
Sliding DFT (marginally stable) 4N 4N 4 4
Sliding DFT (guaranteed stable) 5N 4N 5 4
Three-term windowed sliding DFT 12N+ 6 10N+ 4 18 14
(marginally stable)
Three-term windowed sliding DFT 13N+ 6 10N+ 4 19 14
(guaranteed stable)

13.19 The Zoom FFT 749
Comb coefficient = -1 Comb coefficient = +1
1
N even 0
(a) -1
-1 0 1
Real part
N odd
(b)
Figure 13–51 Four possible orientations of comb filter zeros on the unit circle.
under the two conditions of the comb filter’s feedforward coefficient being
–1 and +1.
This alternate situation is useful: we can now expand our set of spec-
trum analysis center frequencies to more than just Nangular frequency points
around the unit circle. The analysis frequencies can be either 2πm/N or
2π(m+1/2)/N, where integer mis in the range 0≤m≤N–1. Thus we can build
an SDFT analyzer that resonates at any one of 2N frequencies between 0 and
f Hz. Of course, if the comb filter’s feedforward coefficient is set to +1, the
s
resonator’s feedforward coefficient must be
ej2π(m+1/2)/N
to achieve pole/zero
cancellation.
13.19 THE ZOOM FFT
The Zoom FFT is a spectrum analysis method that blends complex down-
conversion, lowpass filtering, and sample rate change by way of decimation.
The Zoom FFT scheme (also called the zoom transform or spectral vernier) is
used when fine-grained spectral resolution is needed within a small portion
of a signal’s overall frequency bandwidth range. In some spectrum analysis
situations, this technique can be more efficient than the traditional FFT. The
Zoom FFT can also be useful if we’re constrained, for some reason, to use
trap
yranigamI
N = 8
2π/N
1
0
-1
-1 0 1
Real part
trap
yranigamI
N = 9 1
4π/N
0
-1
-1 0 1
Realpart
trap
yranigamI
1
0
-1
-1 0 1
Real part
N = 9
5π/N
trap
yranigamI
N = 8
3π/N

750 Digital Signal Processing Tricks
software that performs N-point FFTs for spectrum analysis of signal se-
quences whose lengths are greater than N.
Think of the spectral analysis situation where we require fine fre-
quency resolution, closely spaced FFT bins, over the frequency range occu-
pied by the signal of interest shown in Figure 13–52(a). (The other signals
are of no interest to us.) We could collect many time samples and perform a
large-size radix-2 FFT to satisfy our fine spectral resolution requirement.
This solution is inefficient because we’d be discarding most of our FFT re-
sults. The Zoom FFT can help us improve our computational efficiency
through
• frequency translation by means of complex down-conversion,
• lowpass filtering,
• decimation, and finally
• performing a smaller-size FFT.
The process begins with the continuous x(t) signal being digitized at a
sample rate of f by an analog-to-digital (A/D) converter, yielding the
s1
N-point x(n) time sequence whose spectral magnitude is |X(m)| in Figure
13–52(a). The Zoom FFT technique requires narrowband filtering and decima-
|X(m)| Signal of interest
(a)
0 f Freq
-f /2 c f /2
s1 s1
x(n) x(n) x'(n)
x(t) A/D c LPF D c FFT
(b)
-j2πfnt
f s1 e c s1
|X(m)|
c
(c)
0 Freq
-f /2 f /2
s1 s1
|X'(m)|
c
(d)
0 Freq
-f /2 f /2
s2 s2
Figure 13–52 Zoom FFT spectra: (a) input spectrum; (b) processing scheme; (c)
down-converted spectrum; (d) filtered and decimated spectrum.

13.19 The Zoom FFT 751
tion in order to reduce the number of time samples prior to the final FFT, as
shown in Figure 13–52(b). The down-converted signal’s spectrum, centered at
zero Hz, is the |X(m)| shown in Figure 13–52(c). (The lowpass filter’s fre-
c
quency response is the dashed curve.) After lowpass filtering x(n), the filter’s
c
output is decimated by an integer factor D, yielding a time sequence x’(n)
c
whose sample rate is f = f /D prior to the FFT operation. The key here is
s2 s1
that the length of x’(n) is N/D, allowing a reduced-size FFT. (N/Dmust be an
c
integer power of two to enable the use of radix-2 FFTs.) We perform the FFT
only over the decimated signal’s bandwidth. It’s of interest to note that, be-
cause its input is complex, the N/D-point FFT has a non-redundant fre-
quency analysis range from –f /2 to +f /2 (unlike the case of real inputs,
s2 s2
where the positive- and negative-frequency ranges are redundant).
The implementation of the Zoom FFT is given in Figure 13–53, where all
discrete sequences are real-valued.
Relating the discrete sequences in Figure 13–52(b) and Figure 13–53, the
complex time sequence x(n) is represented mathematically as
c
x(n) = i(n) + jq(n), (13–100)
c
while the complex decimated sequence x’(n) is
c
x’(n) = i’ (n) + jq’ (n). (13–101)
c LPF LPF
The complex mixing sequence
e–j2πfcnts1,
where t = 1/f , can be represented
s1 s1
in the two forms of
e–j2πfcnts1= cos(2πfnt ) – jsin(2πfnt ) = cos(2πnf/f ) – jsin(2πnf/f ). (13–102)
c s1 c s1 c s1 c s1
Relative to FFT computations, we see that an N/D-point Zoom FFT
yields a reduction in computations compared to a standard N-point FFT for
spectrum analysis of a narrowband portion of some X(m)spectrum—and the
computational savings improve as the decimation factor D increases. Ah, but
here’s the rub. As D increases, the lowpass filters must become narrower,
i(n) LPF i LPF (n) D i' LPF (n) Real
N
x(t) x(n) -point
A/D cos(2πfnt ) D
c s1
FFT
q(n) q LPF (n) q' LPF (n)
LPF D Imaginary
f
s1
-sin(2πfnt )
c s1
Figure 13–53 Zoom FFT processing details.

752 Digital Signal Processing Tricks
which increases their computational workload, and this is the trade-off we
face. What we must ask ourselves is “Does the Zoom FFT’s reduced FFT size
compensate for the additional quadrature mixing and dual filtering computa-
tional workload?” (It certainly would if a large-size FFT is impossible with
your available FFT hardware or software.)
To gain a rough appreciation for the computational savings gained by
using an N/D-point Zoom FFT, compared to a standard N-point FFT, let’s
look at Figure 13–54. That figure shows the percent computational savings of
a Zoom FFT versus a standard N-point FFT for various decimation factors D.
The curves were computed using the following definition for percent
computation reduction
% Computation reduction
⎛ ⎞
N/D-ptZoomFFTcomputations
=100⋅ ⎜1−
⎟ (13–103)
⎝ N-ptFFTTcomputations ⎠
under the assumptions that the time sequences applied to the FFTs were win-
dowed, and the Zoom FFT’s lowpass filters were 24th-order (25 multiplica-
tions per output sample) tapped-delay line FIR filters using folded FIR
structures. In Eq. (13–103) a single real multiply and a single real addition are
both considered as a single computation.
The range where Figure 13–54’s curves have negative values means that
the Zoom FFT is less efficient (more computations) than a standard N-point
FFT. As it turns out, the curves in Figure 13–54 quickly move downward in ef-
ficiency as the order of the lowpass filters increases. So it’s in our best interest
to make the lowpass filters as computationally efficient as possible. Some
ways to do this are:
D = 64
n 20
o D = 16
d u
cti
10 D = 8
e
o n
r 0
D = 4
ati –10
ut
m p –20 Savings in real multiplies
o plus real additions,
%
C –30 with 24th-order FIR filters
–40
0 5000 10000 15000
Pre-decimation FFT size (N)
Figure 13–54 Zoom FFT computation reduction.

13.20 A Practical Spectrum Analyzer 753
• Partition the lowpass filtering/decimation process into multiple stages
(multistage decimation) as discussed in Chapter 10.
• Incorporate cascaded integrator-comb (CIC) filters into the lowpass fil-
tering if the spectrum of interest is very narrowband relative to the f .
s1
• Use interpolated FIR filters as discussed in Chapter 7.
• Use polyphase filters as discussed in Chapter 10.
• Restrict the decimation factor Dto be an integer power of two such that
efficient half-band filters can be used.
• Use IIR filters, if spectral phase distortion can be tolerated.
13.20 A PRACTICAL SPECTRUM ANALYZER
Here’s a clever trick for implementing a practical spectrum analyzer by modi-
fying the time-domain data before applying a radix-2 FFT algorithm.
Let’s say we need to build a spectrum analyzer to display, in some man-
ner, the spectral magnitude of a time-domain sequence. We’d like our spec-
trum analyzer, a bank of bandpass filters, to have a frequency magnitude
response something like that shown in Figure 13–55(a). For spectrum analy-
sis, the radix-2 FFT algorithm comes to mind first, as it should. However, the
frequency response of individual FFT bins is that shown in Figure 13–55(b),
with their non-flat passbands, unpleasantly high sidelobes due to spectral
leakage, and overlapped main lobes. We can reduce the leakage sidelobe lev-
els by windowing the time-domain sequence, but that leads to the increased
1
(a)
0
m-2 m-1 m m+1 m+2 Freq
1
(b)
0
m-2 m-1 m m+1 m+2 Freq
1
(c)
0
m-2 m-1 m m+1 m+2 Freq
Figure 13–55 Spectrum analyzer: (a) desired frequency response; (b) frequency
response of standard FFT bins; (c) windowed-data FFT frequency
response.

754 Digital Signal Processing Tricks
main lobe overlap shown in Figure 13–55(c) and degraded frequency resolu-
tion, and we still have considerable droop in the passband response.
Here’s how we can solve our problem. Consider an x(n) sequence of
time samples of length Mwhose M-point DFT is
M−1
∑ −j2πnk/M
X(k)= x(n)e . (13–104)
n=0
Next, consider partitioning x(n) into P subsequences, each of length N. Thus
PN = M. If we add, element for element, the P subsequences, we’ll obtain a
new y(n) sequence of length Nwhose N-point DFT is
N−1
∑ −j2πnm/N
Y(m)= y(n)e . (13–105)
n=0
The good news is that
|Y(m)|=|X(Pm)|. (13–106)
That is, the DFT magnitudes of sequence y(n) are equal to a subset of the
longer DFT magnitudes of x(n). Y(m) is equal to a decimated-by-P version of
X(k). The relationship between |Y(m)| and |X(Pm)| doesn’t seem too impor-
tant, but here’s how we’ll take advantage of that equality. We’ll create an
M-point window sequence whose single-bin frequency response, of an
M-point FFT, is the bold curve in Figure 13–56(a). Instead of computing all
MFFT outputs, we’ll only compute every Pth output of the M-point FFT, im-
plementing Eq. (13–105), giving us the decimated FFT bins shown in Figure
13–56(b). In that figure P=5.
That decimation of the frequency-domain |X(k)| spectrum is accom-
plished in the time domain by a time-aliasing operation as shown in Figure
1
(a)
0
k Freq
k-5 k-3 k-1 k+1 k+3 k+5
1
(b)
0
m-1 m m+1 Freq
Figure 13–56 FFT spectrum analyzer frequency responses.

13.20 A Practical Spectrum Analyzer 755
Window
M time samples Time
y(n)
N-point FFT,
|Y(m)|
Figure 13–57 FFT spectrum analyzer process.
13–57, where again, for example, P = 5. We partition the M-sample
windowed-x(n) time sequence into P = 5 subsequences and sum the subse-
quences element for element to obtain the time-aliased N-sample y(n) sequence.
Next, the |Y(m)| spectral magnitudes are computed using the radix-2 FFT.
This process, sweet in its simplicity, is called the weighted overlap-add
structure[51,52] and is alternatively referred to as the window-presumFFT[53].
The most difficult part of building this analyzer is designing the M-point win-
dow sequence used to window the original x(n) sequence. We do that by
specifying the window’s frequency-domain characteristics, just as if it were a
digital filter frequency response, and using our favorite filter design software
to compute the filter’s time-domain impulse response. That impulse response
is the window sequence. With the signal sample rate being f, the window’s
s
passband width will be just less than f/N. This makes the filter’s one-sided
s
passband width roughly f/2N.
s
Figure 13–58 illustrates an example FFT analyzer with f = 1 MHz,
s
N = 64, with P = 5 making M = 320. The FFT bin spacing is 15.63 kHz, so the
window design was set for a passband width of 10 kHz (thus the filter’s one-
sided bandwidth was specified as 5 kHz in a Parks-McClellan design rou-
tine). Figure 13–58(a) is the 320-point window sequence, while Figure
13–58(b) shows the FFT analyzer’s response for the m = 3, 4, and 5 bins, with
the |Y(4)| response being the solid curve.
The width of the spectrum analyzer’s passbands is primarily controlled
by the window’s passband width. The center frequencies of the analyzer’s in-
dividual passbands are defined by f/N. What this means is that the amount
s
of overlap in the analyzer’s passbands depends on both the window’s pass-
band width, f, and N. The dynamic range of the analyzer can be increased by
s
increasing P, which increases M and lengthens the x(n) sequence. As M is in-
creased, the longer window sequence will yield analyzer passbands having a
more rectangular shape, lower sidelobes, and reduced passband ripple.

756 Digital Signal Processing Tricks
0.02
0.01
0
(a)
-0.01
0 50 100 150 200 250 300
Time
0
|Y(3)| |Y(5)|
dB Dynamic
range
-20
(b)
-40
0 20 40 60 80 100 kHz
Frequency
Figure 13–58 FFT analyzer example: (a) window sequence; (b) analyzer response
for 64-point FFT bins |Y(3)|, |Y(4)|, and |Y(5)|.
Again, to implement this radix-2 FFT spectrum analyzer, the length of
the time-domain sequence (M) must be an integer multiple (P) of an integer
power of two (N).
13.21 AN EFFICIENT ARCTANGENT APPROXIMATION
Fast and accurate methods for computing the arctangent of a complex num-
ber x=I+ jQhave been the subject of extensive study because estimating the
angle θof a complex value has so many applications in the field of signal pro-
cessing. The angle of xis defined as θ=tan–1(Q/I).
Practitioners interested in computing high-speed (minimum computa-
tions) arctangents typically use look-up tables where the value Q/Ispecifies a
memory address in read-only memory (ROM) containing an approximation
of angle θ. For high accuracy, though, this method may require very large
ROM tables. Those folk interested in enhanced accuracy implement compute-
intensive high-order algebraic polynomials, where Chebyshev polynomials
seem to be more popular than Taylor series, to approximate angle θ. But this
polynomial method requires many computations. Unfortunately, because it is
such a nonlinear function, the arctangent is resistant to accurate reasonable-
length polynomial approximations. There is a processing method called
“CORDIC” (an acronym for COordinate Rotation DIgital Computer) that can
compute accurate arctangents using only binary shifts and additions, but this
technique can require long processing times. So, sadly, we end up choosing
the leastundesirablemethod for computing arctangents.

13.21 An Efficient Arctangent Approximation 757
If you want to become famous in the field of signal processing, all you
have to do is produce a very accurate arctangent algorithm that requires very
few computations. (After solving that problem, you can then apply your
skills to developing a perpetual-motion machine.)
Here’s another contender in the arctangent approximation race that uses
neither look-up tables nor high-order polynomials. We can estimate the angle
θ, in radians, of x=I+jQusing the following approximation:
tan–1(Q/I) ≈θ’ = Q/I radians, (13–107)
1+0.28125(Q/I) 2
where –1 ≤ Q/I ≤ 1. That is, θ is in the range –45 to +45 degrees
(–π/4 ≤ θ ≤+π/4 radians). Equation (13–107) has surprisingly good perfor-
mance, particularly for a 90-degree (π/2 radians) angle range. Figure 13–59
shows the maximum error is 0.28 degrees using Eq. (13–107) when the true
angle θis within the angular range of –45 to +45 degrees
Anice feature of this θ’ computation is that it can be written as
IQ
θ’ = , (13–108)
I 2 +0.28125Q 2
eliminating Eq. (13–107)’s Q/Idivision operation, at the expense of two addi-
tional multiplies. Another attribute of Eq. (13–108) is that a single multiply
can be eliminated with binary right shifts. The product 0.28125Q2 is equal to
(1/4+1/32)Q2, so we can implement the product by adding Q2 shifted right
by two bits to Q2 shifted right by five bits. This arctangent scheme may be
useful in a digital receiver application where I2 and Q2 have been previously
computed in conjunction with an AM (amplitude modulation) demodulation
process or envelope detection associated with automatic gain control (AGC).
We can extend the angle range over which our approximation operates.
If we break up a circle into eight 45-degree octants, with the first octant being
0.4
error = 0.26o error = 0.28o
0.2
0
error = –0.26o
–0.2
error = –0.28o
–0.4
–40 –20 0 20 40
True angle (degrees)
Figure 13–59 Estimated angle θ’ error in degrees.
)seerged(
rorre
'θ

758 Digital Signal Processing Tricks
Table 13–6 Octant Location versus Arctangent Expressions
Octant Arctan approximation
IQ
1st, or 8th θ’ =
I 2 +0.28125Q 2
IQ
2nd, or 3rd θ’ = π/2 –
Q 2 +0.28125I 2
IQ
4th, or 5th θ’ = sign (Q)·π +
I 2 +0.28125Q 2
6th, or 7th θ’ = –π/2 – IQ
Q 2 +0.28125I 2
0 to 45degrees, we can compute the arctangent of a complex number residing
in any octant. We do this by using the rotational symmetry properties of the
arctangent:
tan–1(–Q/I)=–tan–1(Q/I) (13–109)
tan–1(Q/I)=π/2–tan–1(I/Q). (13–110)
Those properties allow us to create Table 13–6.
So we have to check the signs of Qand I, and see if |Q| > |I|, to deter-
mine the octant location, and then use the appropriate approximation in Table
13–6. Section 13.38 gives a method for determining the octant of the originalθ.
The maximum angle approximation error is 0.28degreesfor all octants.
13.22 FREQUENCY DEMODULATION ALGORITHMS
In Section 9.2 we discussed the notion of measuring the instantaneous fre-
quency of a complex sinusoidal signal by computing the derivative of the sig-
nal’s instantaneous θ(n) phase as shown in Figure 13–60. This is the
traditional discrete-signal FM demodulation method, and it works fine. The
demodulator’s instantaneous output frequency is
f [Δθ (n)]
f(n) = s rad Hz, (13–111)
2π
where f is the sample rate in Hz.
s

13.22 Frequency Demodulation Algoithms 759
i(n) (cid:6)(n) (n)
tan–1 rad Differentiator (n)
(cid:6)(n) i(n) rad
Figure 13–60 Frequency demodulator using an arctangent function.
Computing instantaneous phase θ(n) requires an arctangent operation,
which is difficult to implement accurately without considerable computa-
tional resources. Here’s a scheme for computing Δθ(n) for use in Eq. (13–111)
without the intermediate θ(n) phase computation (and its pesky arctan-
gent)[54,55]. We derive the Δθ(n) computation algorithm as follows, initially
using continuous-time variables based on the following definitions:
i(t) =in-phase signal,
q(t) =quadrature phase signal,
θ(t) =tan–1[q(t)/i(t)] = instantaneous phase,
Δθ(t)=time derivative of θ(t). (13–112)
The following algorithm is based on the assumption that the spectrum of the
i(t) +jq(t) signal is centered at zero Hz. First, we let r(t)= q(t)/i(t) be the sig-
nal for which we’re trying to compute the derivative of its arctangent. The
time derivative of tan–1[r(t)], a calculus identity, is
−1
Δθ(t) = d{tan [r(t)]} = 1 d[r(t)] . (13–113)
dt 1+r 2 (t) dt
Because d[r(t)]/dt= d[q(t)/i(t)]/dt, we use the calculus identity for the deriv-
ative of a ratio to write
d[q(t)] d[i(t)]
i(t) – q(t)
d[r(t)] d[q(t)/i(t)] dt dt
= = . (13–114)
dt dt i 2 (t)
Plugging Eq. (13–114)’s result into Eq. (13–113), we have
d[q(t)] d[i(t)]
i(t) – q(t)
Δθ(t) = 1 dt dt . (13–115)
1+r 2 (t) i 2 (t)
Replacing r(t) in Eq. (13–115) with q(t)/i(t) yields
d[q(t)] d[i(t)]
i(t) – q(t)
Δθ(t) = 1 dt dt . (13–116)
1+[q(t)/i(t)] 2 i 2 (t)

760 Digital Signal Processing Tricks
We’re getting there. Next we multiply the numerator and denominator of the
first ratio in Eq. (13–116) by i2(t) and replace t with our discrete time variable
index nto arrive at our final result of
d[q(n)] d[i(n)]
i(n) – q(n)
Δθ(n) = dn dn . (13–117)
2 2
i (n) + q (n)
The implementation of this algorithm, where the derivatives of i(n) and
q(n) are i’(n) and q’(n) respectively, is shown in Figure 13–61(a). The Δφ(n) out-
put sequence is used in Eq. (13–111) to compute instantaneous frequency.
The Differentiators are tapped-delay line FIR differentiating filters with
an odd number of taps. The z–Ddelay elements in Figure 13–61(a) are used to
time-align the input i(n) and q(n) sequences with the outputs of the differen-
i(n) Delay
q´(n)
Differentiator
+ Δθ(n)
–
Differentiator
(a) i´(n)
Delay
q(n) Delay
q2(n) 1
q2(n) + i2(n)
Inverse
i2(n)
–
i(n) z-1 z-1
q´(n)
+ + Δθ(n)
(b) Scaling
+ –
–
i´(n)
q(n) z-1 z-1
Figure 13–61 Frequency demodulator without arctangent: (a) standard process;
(b) simplified process.

13.23 DC Removal 761
tiators. The delay is D= (K–1)/2 samples when a K-tap differentiator is used.
In practice, those z–D delays can be obtained by tapping off the center tap of
the differentiating filter as shown in Figure 13–61(b), where the differentiator
is an FIR filter having 1,0,–1 as coefficients, and D = 1 in this case[55]. Such a
differentiator is the simple “central-difference differentiator” we discussed in
Chapter 7, and its optimum performance occurs when the input signal is low
frequency relative to the demodulator’s input f sample rate. Reference [55]
s
reports acceptable results using the differentiator in Figure 13–61(b), but
that’s only true if the complex input signal has a bandwidth no greater than
f/10.
s
If the i(n)+jq(n) signal is purely FM and hard limited such that
i2(n)+q2(n) = Constant, the denominator computations in Eq. (13–117) need
not be performed. In this case, using the 1,0,–1 coefficient differentiators, the
FM demodulator is simplified to that shown in Figure 13–61(b), where the
Scaling operation is multiplication by the reciprocal of Constant.
Two final things to consider: First, in practice we may want to detect the
unusual situation where both i(n) and q(n) are zero-valued, making the de-
nominator of Eq. (13–117) equal to zero. We should set Δθ(n) to zero in that
case. Second, for real-world noisy signals it may be prudent to apply the
Δθ(n) output to a lowpass filter to reduce unwanted high-frequency noise.
13.23 DC REMOVAL
When we digitize analog signals using an analog-to-digital (A/D) converter,
the converter’s output typically contains some small DC bias; that is, the av-
erage of the digitized time samples is not zero. That DC bias may have come
from the original analog signal or from imperfections within the A/D con-
verter. Another source of DC bias contamination in DSPis when we truncate
a discrete sequence from a B-bit representation to word widths less than B
bits. Whatever the source, unwanted DC bias on a signal can cause problems.
When we’re performing spectrum analysis, any DC bias on the signal shows
up in the frequency domain as energy at zero Hz, the X(0) spectral sample.
For an N-point FFT the X(0) spectral value is proportional to N and becomes
inconveniently large for large-sized FFTs. When we plot our spectral magni-
tudes, the plotting software will accommodate any large X(0) value and
squash down the remainder of the spectrum in which we are more interested.
Anonzero DC bias level in audio signals is particularly troublesome be-
cause concatenating two audio signals, or switching between two audio sig-
nals, results in unpleasant audible clicks. In modern digital communications
systems, a DC bias on quadrature signals degrades system performance and
increases bit error rates. With that said, it’s clear that methods for DC removal
are of interest to many DSPpractitioners.

762 Digital Signal Processing Tricks
13.23.1 Block-Data DC Removal
If you’re processing in non-real time, and the signal data is acquired in blocks
(fixed-length sequences) of block length N, DC removal is straightforward.
We merely compute the average of our N time samples and subtract that av-
erage value from each original sample to yield a new time sequence whose
DC bias will be extremely small.
This scheme, although very effective, is not compatible with continuous-
throughput (real-time) systems. For real-time systems we’re forced to use fil-
ters for DC removal.
13.23.2 Real-Time DC Removal
The author has encountered three proposed filters for DC removal[56–58];
their structures are shown in Figures 13–62(a), 13–62(b), and 13–62(c).
Ignoring the constant gains of those DC-removal filters, all three filters
have identical performance with the generalDC-removalfilter structure in Fig-
ure 13–62(d) having a z-domain transfer function of
Y(z) 1−z −1
H(z)= = . (13–118)
X(z) 1−αz −1
(It’s not immediately obvious that the filters in Figures 13–62(c) and 13–62(d)
are equivalent. You can verify that equivalency by writing the time-domain
difference equations relating the various nodes in the feedback path of Figure
13–62(c)’s filter. Next, convert those equations to z-transform expressions and
solve for Y(z)/X(z) to yield Eq. (13–118)).
x(n) y(n) x(n) y(n)
z -1 1 + α K z -1
2
α -(1 + α) α -1
(a) 2 (b)
x(n) y(n) x(n) y(n)
- -
1-α
z -1
z -1
(c) α (d)
Figure 13–62 Filters used for DC bias removal.

13.23 DC Removal 763
Because the DC-removal filters can be modeled with the general DC-
removal filter in Figure 13–62(d), we provide the general filter’s frequency
magnitude and phase responses in Figures 13–63(a) and 13–63(b) for α=0.95.
The filter’s pole/zero locations are given in Figure 13–63(c), where a zero re-
sides at z = 1 providing infinite attenuation at DC (zero Hz) and a pole at
z=αmaking the magnitude notch at DC very sharp. The closer αis to unity,
the narrower the frequency magnitude notch centered at zero Hz. Figure
13–63(d) shows the general filter’s unit-sample impulse response.
Figure 13–64 shows the time-domain input/output performance of the
general DC-removal filter (with α = 0.95) when its input is a sinusoid sud-
denly contaminated with a DC bias of 2 beginning at the 100th time sample
and disappearing at the 200th sample. The DC-removal filter works well.
13.23.3 Real-Time DC Removal with Quantization
Because the general DC-removal filter has feedback, the y(n) output samples
may require wider binary word widths than those used for the x(n) input
samples. This could result in overflows in fixed-point binary implementa-
tions. The scaling factors of (1+α)/2 and K, in Figures 13–62(a) and 13–62(b),
are less than one to minimize the chance of y(n) binary overflow.
In fixed-point hardware the y(n) samples are often truncated to the same
word width as the input x(n). This quantization (by means of truncation) will
1 2
s Phase
0.8 Magnitude
a
n1
0.6 di
0.4 R
a0
0.2 -1
-0.5f -0.25f 0 0.25f 0.5f -0.5f -0.25f 0 0.25f 0.5f
s s s s s s s s
(a) (b)
1.2
1
1
art
0.5 0.8
p
y
ar z = 0.95 0.6
n 0
a
gi
0.4
m
I-0.5 0.2
0
-1
-0.2
-1 -0.5 0 0.5 1 0 5 10 15 20 25 30
Real part Time samples
(c) (d)
Figure 13–63 DC-removal filter, α = 0.95: (a) magnitude response; (b) phase re-
sponse; (c) pole/zero locations; (d) impulse response.

764 Digital Signal Processing Tricks
3
2 Filter input
(a) 1
0
-1
0 50 100 150 200 250 300
2
1
0
(b)
-1
-2 Filter output
-3
0 50 100 150 200 250 300
Figure 13–64 DC-removal filter performance: (a) filter input with sudden DC bias;
(b) filter output.
induce a negative DC bias onto the quantized output samples, degrading our
desired DC removal. When we truncate a binary sample value, by discarding
some of its least significant bits, we induce a negative error in the truncated
sample. Fortunately, that error value is available for us to add to the next un-
quantized signal sample, increasing its positive DC bias. When that next sam-
ple is truncated, the positive error we’ve added minimizes the negative error
induced by truncation of the next sample.
Figure 13–65(a) shows the addition of a quantizing sigma-delta modula-
tor to the feedback path of the DC-removal filter given in Figure 13–62(c). The
x(n) y(n)
-
1-α
Q z -1
(a)
- +
error(n) z -1 error(n-1)
error(n-1) error(n)
z -1
+ -
x(n) y(n)
Q
(b) -
z -1 z -1
α
Figure 13–65 Two DC-removal filters using fixed-point quantization to avoid data
overflow.

13.24 Improving Traditional CICFilters 765
positive error induced by truncation quantization (the Q block) is delayed by
one sample time and fed back to the quantizer input. Because the modulator
has a noise shaping property where quantization error noise is shifted up in
frequency, away from zero Hz (DC), the overall DC bias at the output of the
filter is minimized[57].
An equivalent quantization noise shaping process can be applied to a
Direct Form I version of the Figure 13–62(d) general DC-removal filter as
shown in Figure 13–65(b). Again, the positive quantization error is delayed by
one sample time and added to the quantizer input[59–61]. To reiterate, the
DC-removal filters in Figure 13–65 are used to avoid binary data overflow, by
means of quantization, without the use of scaling multipliers.
Later in this chapter we discuss a DC-removal filter whose frequency re-
sponse exhibits linear phase.
13.24 IMPROVING TRADITIONAL CIC FILTERS
Amajor design goal for cascaded integrator-comb (CIC) filters, as introduced
in Chapter 10 in conjunction with sample rate conversion, is to minimize their
hardware power consumption by reducing data word width and reducing
data clock rates wherever possible. Here we introduce a clever trick that re-
duces CIC filter power consumption using nonrecursive structures, by means
of polynomial factoring, easing the word width growth problem. These nonre-
cursive structures require that the sample rate change R be an integer power
of two, enhancing computational simplicity through polyphase decomposition,
transposed structures, simplified multiplication, and substructure sharing[62–64].
(These processes are not complicated; they merely have fancy names.) Next,
we’ll review a nonrecursive scheme that enables sample rate changes other
than powers of two. The following discussion assumes that the reader is fa-
miliar with the CIC filter material in Chapter 10.
13.24.1 Nonrecursive CIC Filters
Recall that the structures of 1st-order (M=1) and 3rd-order (M=3) CIC deci-
mation filters, having a comb delay equal to the sample rate change factor R,
are those shown in Figure 13–66. As presented in Chapter 10, the transfer
x(n) y(n) x(n) y(n)
R R
- - - -
z -1 z -1 z -1 z -1 z -1 z -1 z -1 z -1
(a) (b)
Figure 13–66 Recursive decimation CIC filters: (a) 1st-order filter; (b) 3rd-order
filter.

766 Digital Signal Processing Tricks
function of an Mth-order decimating CIC filter can be expressed in either a re-
cursive form or a nonrecursive form, as indicated in Eq. (13–119). (You could,
if you wish, use the geometric series discussion in Appendix B to show the
equality of the two forms of the filter’s transfer function.)
⎡ 1−z −R⎤M
H (z) = ⎢ ⎥
cic ⎣⎢1−z −1 ⎦⎥
recursive form (13–119)
⎡R−1 ⎤M
H (z) = ⎢ ∑ z −n⎥ = (1 + z–1+ z–2+ . . . + z–R+1)M.
cic ⎢ ⎥
⎣ n=0 ⎦
nonrecursive (13–119’)
form
Now if the sample rate change factor R is an integer power of two,
R=2Kwhere Kis some positive integer, the Eq. (13–119’) Mth-order nonrecur-
sive polynomial form of H (z) can be factored as
cic
H (z) = (1 + z–1)M(1 + z–2)M(1 + z–4)M. . .
⎛
⎝ 1+z
−2K−1⎞
⎠
M
. (13–120)
cic
The reward for this factorization is that the CIC filter can then be imple-
mented with Knonrecursive stages as shown in Figure 13–67. This implemen-
tation eliminates filter feedback loops with their unpleasant binary word
width growth. The data word width does increase in this nonrecursive struc-
ture by M bits for each stage, but the sampling rate is reduced by a factor of
two for each stage. This nonrecursive structure has been shown to consume
less power than the Figure 13–66(b) recursive implementation for filter orders
greater than three and decimation/interpolation factors larger than eight[64].
Thus the power savings from sample rate reduction are greater than the
power consumption increase due to data word width growth.
Happily, further improvements are possible with each stage of this non-
recursive structure[63]. For example, assume we desire an M=5th-order deci-
mating CIC for Stage 1 in Figure 13–67. In that case, the stage’s transfer
function is
H (z) = (1 + z–1)5= 1 + 5z–1+ 10z–2+ 10z–3+ 5z–4+ z–5
1
= 1 + 10z–2+ 5z–4+ (5 + 10z–2+ z–4)z–1= F (z) + F (z)z–1. (13–121)
A B
Stage 1 Stage 2 Stage K
x(n) y(n)
. . .
(1 + z -1)M 2 (1 + z -1)M 2 (1 + z -1)M 2
Figure 13–67 MultistageMth-order nonrecursive CIC structure.

13.24 Improving Traditional CICFilters 767
(1 + z -1)5 2
2 F (z) = 1 + 10z -1 + 5z -2
A'
z -1
2 F (z) = 5 + 10z -1 + z -2
B'
Figure 13–68 Polyphase structure of a single nonrecursive 5th-order CIC stage.
The second step in Eq. (13–121), known as polyphase decomposi-
tion[65–69], enables a polyphase implementation having two parallel paths
as shown in Figure 13–68. The initial delay element and the dual decimation-
by-two operations are implemented by routing the odd-indexed input sam-
ples to F (z), and the even-indexed samples to F (z). Because we implement
A’ B’
decimation by two before the filtering, the new polyphase components are
F (z) = 1 + 10z–1 + 5z–2, and F (z) = 5 + 10z–1 + z–2 implemented at half the
A’ B’
data rate into the stage. (Reducing data rates as early as possible is a key de-
sign goal in the implementation of CIC decimation filters.)
The F (z) and F (z) polyphase components are implemented in a
A’ B’
tapped-delay line fashion and, fortunately, further simplifications are possi-
ble. Let’s consider the F (z) polyphase filter component, in a tapped-delay
A’
line configuration, shown in Figure 13–69(a). The transposed version of this
x(n) x(n)
z -1 z -1
1 10 5 5 10 1
y(n)
z -1 z -1
y(n)
(a) (b)
x(n) x(n)
22 23 21 22
10x(n)
21
10x(n) 5x(n)
5x(n) y(n) y(n)
z -1 z -1 z -1 z -1
(c) (d)
Figure 13–69 Filter component F (z): (a) delay line structure; (b) transposed
A’
structure; (c) simplified multiplication; (d) substructure sharing.

768 Digital Signal Processing Tricks
filter is presented in Figure 13–69(b) with its flipped coefficient sequence. The
adder in Figure 13–69(a) must perform two additions per input data sample,
while in the transposed structure no adder need perform more than one add
per data sample. Thus the transposed structure can operate at a higher speed.
The next improvement uses simplified multiplication, as shown in Fig-
ure 13–69(c), by means of arithmetic shifts and adds. Thus a factor of five is
implemented as 22 + 1, eliminating all multiplications. Finally, because of the
transposed structure, we can use the technique of substructure sharing in Fig-
ure 13–69(d) to reduce the hardware component count. Pretty slick! By the
way, these nonrecursive filters are still called cascaded integrator-comb filters,
even though they have no integrators. Go figure.
Table 13–7 is provided to help the reader avoid computing the polyno-
mial equivalent of several Mth-order nonrecursive stages, as was performed
in Eq. (13–121).
13.24.2 Nonrecursive Prime-Factor-RCIC Filters
The nonrecursive CIC decimation filters described above have the restriction
that the Rdecimation factor must be an integer power of two. That constraint
is loosened due to a clever scheme of factoring R into a product of prime
numbers[70]. This multiple prime-factor-R technique is based on the process of
factoring integer R into the form R = 2p3q5r7s11t ..., where 2, 3, 5, 7, 11 are the
prime numbers. (This process is called prime factorization, or prime decomposi-
tion, and has been of interest since the days of Euclid.) Then the appropriate
number of CIC subfilters are cascaded as shown in Figure 13–70(a). The fortu-
nate condition is that those Mth-order CIC filters are described by
Table 13–7 Expansions of (1 + z–1)M
M (1 + z–1)M
2 (1 + z–1)2= 1 + 2z–1+ z–2
3 (1 + z–1)3= 1 + 3z–1+ 3z–2+ z–3
4 (1 + z–1)4= 1 + 4z–1+ 6z–2+ 4z–3+ z–4
5 (1 + z–1)5= 1 + 5z–1+ 10z–2+ 10z–3+ 5z–4+ z–5
6 (1 + z–1)6= 1 + 6z–1+ 15z–2+ 20z–3+ 15z–4+ 6z–5+ z–6
7 (1 + z–1)7= 1 + 7z–1+ 21z–2+ 35z–3+ 35z–4+ 21z–5+ 7z–6+ z–7
8 (1 + z–1)8= 1 + 8z–1+ 28z–2+ 56z–3+ 70z–4+ 56z–5+ 28z–6+ 8z–7+ z–8
9 (1 + z–1)9= 1 + 9z–1+ 36z–2+ 84z–3+ 126z–4+ 126z–5+ 84z–6+ 36z–7+ 9z–8+ z–9

13.24 Improving Traditional CICFilters 769
⎡ 1−z −2⎤M
H (z) = ⎢ ⎥ = (1 + z–1)M
2 ⎣⎢1−z −1 ⎦⎥
⎡ 1−z −3⎤M
H (z) = ⎢ ⎥ = (1 + z–1+ z–2)M
3 ⎣⎢1−z −1 ⎦⎥
⎡ 1−z −5⎤M
H (z) = ⎢ ⎥ = (1 + z–1+ z–2+ z–3+ z–4)M (13–122)
5 ⎣⎢1−z −1 ⎦⎥
and so on, enabling nonrecursive implementations.
Due to space constraints, the elegant and arduous derivation of this
technique is not given here; but this process can be illustrated with an exam-
ple. Assume we desire a 2nd-order (M= 2) CIC filter with a decimation factor
of R = 90. That decimation rate is factored as 90 = (2)(3)(3)(5), so p = 1, q = 2,
and r = 1. Our composite CIC filter is implemented as H (z)H (z)H (z)H (z)
2 3 3 5
shown in Figure 13–70(b).
At first glance the many additions of the Figure 13–70(b) CIC filter ap-
pear to aggravate the power consumption of such a filter, but the reduced
p stages q stages r stages
x(n)
H(z) 2 ... H(z) 2 H(z) 3 ... H(z) 3 H(z) 5 ... H(z) 5 ...
2 2 3 3 5 5
(a)
Section 1, p = 1 Section 2, q = 2 Section 3, r = 1
x(n) y(n)
+ + 2 + + 3 + + 3 + + 5
z-1 z-1 z-1 z-1 z-1 z-1 z-1 z-1
z-1 z-1 z-1 z-1 z-1 z-1
z-1 z-1
z-1 z-1
(b)
Figure 13–70 Multiple prime-factor nonrecursive CIC example: (a) cascaded-
stage structure; (b) 2nd-order, R=90, nonrecursive CIC example.

770 Digital Signal Processing Tricks
sample rates significantly reduce power requirements[70]. If one addition in
Section 1 of Figure 13–70(b) consumes P units of power, then Section 1 con-
sumes 2P units of power, and each addition in the first portion of Section 2
consumes P/2 units of power. Each addition in the second portion of Section
2 consumes P/6 of units power, while each addition in Section 3 consumes
P/18 units of power.
We have flexibility here because the subfilters in each section of Figure
13–70(b) can be implemented recursively or nonrecursively, as indicated in
Eq. (13–122). In nonrecursive implementations the polyphase decomposition,
transposed structures, simplified multiplication, and substructure sharing
schemes can be applied. CIC filter design certainly has come a long way since
its introduction in the early 1980s.
13.25 SMOOTHING IMPULSIVE NOISE
In practice we may be required to make precise measurements in the presence
of high noise or interference. Without some sort of analog signal condition-
ing, or digital signal processing, it can be difficult to obtain stable and repeat-
able measurements. This impulsive-noise smoothing trick, originally
developed to detect microampere changes in milliampere signals, describes a
smoothing algorithm that improves the stability of precision measurements
in the presence of impulsive noise[71].
Practical noise-reduction methods often involve multiple-sample aver-
aging (block averaging) of a sequence of measured values, x(n), to compute a
sequence of N-sample arithmetic means, M(q). As such, the block-averaged
sequence M(q) is defined by
1
(q+ ∑1)N−1
M(q)= x(n) (13–123)
N
k=qN
where the time index of the averaging process is q = 0, 1, 2, 3, etc. When
N = 10, for example, for the first block of data (q = 0), time samples x(0)
through x(9) are averaged to compute M(0). For the second block of data
(q = 1), time samples x(10) through x(19) are averaged to compute M(1), and
so on[72].
The following impulsive-noise smoothing algorithm processes a block of
time-domain samples, obtained through periodic sampling, and the number
of samples, N, may be varied according to individual needs and processing
resources. The processing of a single block of Ntime samples proceeds as fol-
lows: Collect N+2 samples of x(n), discard the maximum (most positive) and
minimum (most negative) samples to obtain an N-sample block of data, and
compute the arithmetic mean, M(q), of the N samples. Each sample in the

13.25 Smoothing Impulsive Noise 771
block is then compared to the mean. The direction of each sample relative to
the mean (greater than, or less than) is accumulated, as well as the cumulative
magnitude of the deviation of the samples in one direction (which, by defini-
tion of the mean, equals that of the other direction). This data is used to com-
pute a correction term that is added to the mean according to the following
formula,
(P −N )|D |
A(q) = M(q) + os eg total , (13–124)
2
N
where A(q) is the corrected mean, M(q) is the arithmetic mean (average) from
Eq. (13–123), P is the number of samples greater than M(q), N is the num-
os eg
ber of samples less than M(q), and D is the sum of deviations from
total
the mean (absolute values and one direction only). D , then, is the sum of
total
the differences between the P samples and M(q).
os
For an example, consider a system acquiring ten measured samples of
10, 10, 11, 9, 10, 10, 13, 10, 10, and 10. The mean is M=10.3, the total number
of samples positive is P = 2, and the total number of samples negative is
os
N =8 (so P –N =–6). The total deviation in either direction from the mean
eg os eg
is 3.4 (using the eight samples less than the mean, (10.3–10) times 7 plus
(10.3–9); or using the two samples greater than the mean, (13–10.3) plus
(11–10.3)). With D = 3.4, Eq. (13–124) yields an improved result of
total
A=10.096.
The smoothing algorithm’s performance, relative to traditional block av-
eraging, can be illustrated by example. Figure 13–71(a) shows a measured
300-sample x(n) signal sequence comprising a step signal of amplitude one
2
Block
6 N = 10
average
1.5
4
1
2
0.5
0
0
-2
-4 -0.5
0 100 200 300 0 100 200 300
Time Time
(a) (b)
Figure 13–71 Noise smoothing for N= 10: (a) input x(n) signal; (b) block average
output (white) and impulsive-noise smoothing algorithm output
(solid).

772 Digital Signal Processing Tricks
contaminated with random noise (with a variance of 0.1) and two large
impulsive-noise spike samples.
Afew meaningful issues regarding this noise smoothing process are:
• The block size (N) used in the smoothing algorithm can be any integer,
but for real-time fixed binary-point implementations it’s beneficial to set
N equal to an integer power of two. In that case the compute-intensive
division operations in Eqs. (13–123) and (13–124) can be accomplished
by binary arithmetic right shifts to reduce the computational workload.
• If there’s a possibility that more than two large noise spikes are con-
tained in a block of input samples, then we collect more than N+2 sam-
ples of x(n) and discard the appropriate number of maximum and
minimum samples to eliminate the large impulsive noise samples.
• We could forgo the Eq. (13–124) processing and merely perform Eq.
(13–123) to compute the mean M(q). In that case, for a given N, the stan-
dard deviation of M(q) would be roughly 15 to 20 percent greater than
A(q).
As pointed out by M. Givens, impulsive noise can also be reduced by a
class of filters known as median filters[73]. Median filters, not covered in this
text, are typically used in noise reduction of two-dimensional signals (im-
ages). However, median filters can also be implemented to process one-
dimensional signals, such as our x(n) signal here, and should be considered in
any impulsive-noise reduction application.
13.26 EFFICIENT POLYNOMIAL EVALUATION
On the off chance that you didn’t know, there are two popular tricks used to
speed up polynomial evaluations (computations), known as Horner’s Rule
andEstrin’s Method.We illustrate those two techniques below.
13.26.1 Floating-Point Horner’s Rule
Horner’s Rule uses nested operations to reduce the number of multiply oper-
ations needed to compute polynomials. An example of a polynomial compu-
tation is, for example, using the following expression to compute the
arctangent of x:
arctan(x) = 0.14007x4–0.34241x3–0.01522x2+ 1.00308x–0.00006. (13–125)
To see how the computational workload of polynomial evaluations can
be reduced, consider the following kth-order polynomial:

13.26 Efficient Polynomial Evaluation 773
f (x) = c xk+ . . . + c x3+ c x2+ c x+ c . (13–126)
k k 3 2 1 0
It can be rewritten as
f (x) = f (x) = x(x(x( ... x(c x+ c ) + c ) ... + c ) + c ) + c (13–127)
k Hk k k-1 k-2 2 1 0
where the “H” subscript means Horner. Using this method to compute poly-
nomials
• reduces the number of necessary multiply operations, and
• is straightforward to implement using programmable DSP chips with
their multiplyandaccumulate(MAC) architectures.
For example, consider the 5th-order polynomial
f (x) = c x5+ c x4+ c x3+ c x2+ c x+ c . (13–128)
5 5 4 3 2 1 0
Evaluated in the standard way, Eq. (13–128) would require nine multiplies
and five additions, whereas the Horner version
f (x) = f (x) = x(x(x(x(c x+ c ) + c ) + c ) + c ) + c (13–128’)
5 H5 5 4 3 2 1 0
requires only five multiplies and five adds when the computations begin with
the innermost multiply and add operations (c x + c ).
5 4
Here are a few examples of polynomials in the Horner format:
c x2+ c x+ c = x(c x+ c ) + c . (13–129)
2 1 0 2 1 0
c x3+ c x2+ c x+ c = x(x(c x+ c ) + c ) + c . (13–130)
3 2 1 0 3 2 1 0
c x4+ c x3+ c x2+ c x+ c = x(x(x(c x+ c ) + c ) + c ) + c . (13–131)
4 3 2 1 0 4 3 2 1 0
By the way, the multiplications and additions cannot be performed in
parallel. Because Horner’s Rule is inherently serial, we need the result of the
last multiplication before we can start the next addition, and that addition re-
sult is needed before the follow-on multiplication.
Horner’s Rule is another of those handy computer techniques we use
whose origins are very old. Chinese mathematicians described it in the 1200s.
European mathematicians (including William Horner) rediscovered and pub-
licized it in the early 1800s. However, it seems Sir Isaac Newton also invented
and used it in the 1600s.
13.26.2 Horner’s Rule in Binary Shift Multiplication/Division
The Horner’s Rule method of nested multiplications has special significance
for us when we evaluate polynomials using fixed-point number formats.

774 Digital Signal Processing Tricks
Using Horner’s Rule enables us to minimize the truncation quantization error
when we use binary right shifts to implement fractional multiplications. For
example, if we are using fractional fixed-point numbers in the 1.15 format, as
described in Section 12.1.6, and we want to multiply an x(n) sample by 0.3125,
we can perform that multiplication as
0.3125x(n) = 2–2x(n) + 2–4x(n). (13–132)
Those scaling factors on the right side of Eq. (13–132) can be implemented
using binary right shifts by two and four bits. The larger the right shifts, how-
ever, the greater the truncation quantization errors in this type of fractional
multiplication. Using Horner’s Rule, we can implement Eq. (13–132) as
0.3125x(n) = 2–2[x(n) + 2–2x(n)], (13–132’)
where the maximum binary right shift is by two bits, reducing the resultant
truncation quantization error.
13.26.3 Estrin’s Method
If your computing hardware is able to perform multiple parallel (simultane-
ous) multiplyandaccumulate(MAC) operations, we can increase the computa-
tional speed of Horner’s Rule by using parallel processing in a technique
called Estrin’s Method.
Here’s how Estrin’s Method works: Various kth-order polynomials, such
as that in Eq. (13–126), can be evaluated using
f (x) = (c x+c )
1 1 0
f (x) = [c x2+(c x+c )]
2 2 1 0
f (x) = [(c x+c )x2+(c x+c )]
3 3 2 1 0
f (x) = {c x4 +[(c x+c )x2+(c x+c )]}
4 4 3 2 1 0
f (x) = {(c x+c )x4+[(c x+c )x2+(c x+c )]}
5 5 4 3 2 1 0
f (x) = {[c x2+(c x+c )]x4+[(c x+c )x2+(c x+c )]}
6 6 5 4 3 2 1 0
f (x) = {[(c x+c )x2+(c x+c )]x4+[(c x+c )x2+(c x+c )]}
7 7 6 5 4 3 2 1 0
f (x) = [c x8+{[(c x+c )x2+(c x+c )]x4+[(c x+c )x2+(c x+c )]}]
8 8 7 6 5 4 3 2 1 0
f (x) = [(c x+c ) x8+{[(c x+c )x2+(c x+c )]x4+[(c x+c )x2+(c x+c )]}].
9 9 8 7 6 5 4 3 2 1 0
The above expressions look complicated, but they’re really not. The terms in-
side parentheses, brackets, and curly brackets are nested sub-expressions of
the form axq + b—precisely what we need for MAC operations. For example,
the sub-expressions within parentheses can be computed simultaneously
with a DSPprocessor’s parallel MAC operations.
To illustrate Estrin’s Method, if your processing hardware can perform
four simultaneous MAC operations, and assuming value x2 has been previ-

13.27 Designing Very High-Order FIRFilters 775
ously computed, we can evaluate polynomial f (x) in the following three
7
steps:
1. U= (c x+c ), V= (c x+c ), W= (c x+c ), and X= (c x+c )
7 6 5 4 3 2 1 0
2. Y= (Ux2+V), Z= (Wx2+X), x4= (x2x2+0)
3. f (x) = (Yx4+Z)
7
The four computations in Step 1 are performed simultaneously. Likewise, the
three computations in Step 2 are performed simultaneously. The final Step 3
is a single MAC operation.
Yes, Estrin’s Method requires multiple processing steps, but this method
is able to avoid much of the inherent (slow) serial processing dictated by
Horner’s Rule. The bottom line here is that while Estrin’s Method does not re-
duce the computational workload (number of multiplies and additions) of
Horner’s Rule, it does increase the computational speed of polynomial evalu-
ations by taking advantage of modern-day parallel processing hardware ar-
chitectures.
13.27 DESIGNING VERY HIGH-ORDER FIR FILTERS
There are linear-phase filtering applications wherein we’re interested in de-
signing very high-performance (very narrow passband widths, and/or very
high attenuation) nonrecursive FIR filters. Consider the possibility that
you’ve used Eq. (7–34), or some other algorithm, to determine that you need
to implement a 2000-tap linear-phase FIR filter. Then when you try to design
such a filter using your trusty Parks-McClellan Exchange-based (Remez) fil-
ter design software, you obtain unusable design results. It happens that
some software incarnations of the Parks-McClellan Exchange algorithm have
convergence problems (inaccurate results) when the number of filter taps,
or filter order, exceeds 400 to 500. There’s a slick way around this high-
order FIR filter design problem using a frequency-domain zero-stuffing tech-
nique.†
If our FIR filter design software cannot generate FIR coefficient sets
whose lengths are in the thousands, then we can design a shorter-length set of
coefficients and interpolate those coefficients (time-domain impulse re-
sponse) to whatever length we desire. Rather than use time-domain interpo-
lation schemes and account for their inaccuracies, we can simplify the process
by performing time-domain interpolation by means of frequency-domain
zero stuffing.
† I thank my DSPpal Eric Jacobsen, Minister of Algorithms at Abineau Communications, for
publicizing this technique.

776 Digital Signal Processing Tricks
An example of the process is as follows: Assume that we have a signal
sampled at a rate of f = 1000 Hz. We want a lowpass filter whose cutoff fre-
s
quency is 20 Hz with 60 dB of stopband attenuation. Compounding the prob-
lem are the requirements for linear phase and removal of any DC (zero Hz)
component from the signal. (Those last two requirements preclude using the
DC-removal schemes in Section 13.23.) First, we design a prototype nonrecur-
sive FIR filter having, say, N = 168 coefficients whose desired frequency
response magnitude is shown in Figure 13–72(a); its h (k) coefficients are de-
p
picted in Figure 13–72(b). Next, we compute a 168-point DFT of the coeffi-
cients to obtain the frequency-domain samples H (m) whose magnitudes are
p
shown in Figure 13–72(c).
Under the assumption that our final desired filter requires roughly
1600 taps, we’ll interpolate the h (k) prototype impulse response by a factor
p
of M = 10. We perform the interpolation by inserting (M–1)N zeros in the
center of the H (m) frequency-domain samples, yielding a 1680-point H(m)
p
frequency-domain sequence whose magnitudes are shown in Figure
13–73(a). Finally, we perform a 1680-point inverse DFT on H(m) to obtain
0
Prototype filter
magnitude response
-50
-400 -200 0 200 400
0.4
Prototype filter
0.2 coefficients, h(k)
p
0
-0.2
0 50 100 150
1
DFT magnitude of prototype
coefficients, |H(m)|
p
0.5
0
0 50 100 150
Figure 13–72 Prototype FIR filter: (a) magnitude response; (b) h(k) coefficients;
p
(c) |H(m)| magnitudes of the 168-point DFT of h(k).
p p
Bd
(a)
Frequency (Hz)
(b)
(c)
Frequency (m)

13.27 Designing Very High-Order FIRFilters 777
1
Magnitude of zero-
stuffed DFT, |H(m)|
0.5
0
0 500 1000 1500
Frequency (m)
0.04
0.02
0
-0.02
0 400 800 1200 1600
0
-50
-100
-30 -20 -10 0 10 20 30
Frequency (Hz)
Figure 13–73 Desired FIR filter: (a) magnitude of zero-stuffed H(m); (b) interpo-
p
lated h(k) coefficients; (c) magnitude of desired frequency re-
sponse.
the interpolated h(k) impulse response (coefficients), shown in Figure
13–73(b), for our desired filter. (The ten-fold compression of the H (m) pass-
p
band samples results in a ten-fold expansion of the h (k) impulse response
p
samples.) The frequency magnitude response of our final very high-order
FIR filter, over the frequency range of –30 to 30 Hz, is provided in Figure
13–73(c).
With this process, the prototype filter’s h (k) coefficients are preserved
p
within the interpolated filter’s coefficients if the H (N/2) sample (f/2) is
p s
zero. That condition ensures that H(m) exhibits conjugate symmetry and
forces the h(k) coefficients to be real-only.
The design steps for this high-order filter design scheme are:
• With the desired filter requiring MN taps, set the number of prototype
filter coefficients, N, to an integer value small enough so your FIR filter
design software provides usable results. The integer interpolation factor
Mequals the number of desired taps divided by N.
Bd
(a)
1512 zeros
Interpolated
coefficients, h(k)
(b)
Desired
magnitude
response
(c)

778 Digital Signal Processing Tricks
• Design the N-tap prototype FIR filter accounting for the M-fold fre-
quency compression in the final filter. (That is, cutoff frequencies for the
prototype filter are Mtimes the desired final cutoff frequencies.)
• Perform an N-point DFT on the prototype filter’s h (k) coefficients to ob-
p
tain H (m).
p
• Insert M–1 zero-valued samples just before the H (N/2) sample of H (m)
p p
to obtain the new MN-point H(m) frequency response.
• Compute the MN-point inverse DFT of H(m), yielding an MN-length in-
terpolated h(k) coefficient set. (Due to computational errors, discard the
imaginary part of h(k), making it real-only.)
• Multiply h(k) by M to compensate for the 1/M amplitude loss induced
by interpolation.
• Test the h(k) coefficient set to determine its actual frequency response
using standard filter analysis methods. (One method: append thousands
of zeros to h(k) and perform a very large FFT on the expanded sequence.)
An example application of this filter design is when you’re building a
high-performance lowpass polyphase filter, as discussed in Chapter 10. (The
structures of the high-performance interpolated FIR and frequency sampling
lowpass filters don’t permit their decomposition into polyphase subfilters for
such an application.)
13.28 TIME-DOMAIN INTERPOLATION USING THE FFT
The thoughtful reader may have looked at the above Section 13.27 FIR filter
impulse response interpolation scheme and wondered, “If we can interpolate
time-domain impulse responses, we should be able to interpolate time-
domain signals using the same frequency-domain zero-stuffing method.”
To quote Rocky Balboa, “This is very true.” In fact, the Section 13.27
interpolation-by-M process applied to time signals is sometimes called exact
interpolation because its performance is equivalent to using an ideal, infinite-
stopband attenuation, time-domain interpolation filter. Let’s see how this in-
terpolation scheme works.
To establish our notation, let’s say we compute the FFT of an N-point
x(n) time sequence to produce its X(m) frequency-domain samples. Next we
stuff (M-1)N zeros in the middle of X(m) to yield the MN-length X (m) fre-
int
quency samples, where MN is an integer power of two. Then we perform an
MN-point inverse FFT on X (m) to obtain the interpolated-by-Mx (n) times
int int
samples. Using this frequency-domain zero stuffing to implement time-
domain signal interpolation involves two important issues upon which we
now focus.

13.28 Time-Domain Interpolation Using the FFT 779
13.28.1 Computing Interpolated Real Signals
The first issue: to ensure the interpolated x (n) time sequence is real-only,
int
conjugate symmetry must be maintained in the zero-stuffed X (m) frequency
int
samples. If the X(m) sequence has a nonzero sample at X (N/2), the f/2 fre-
int s
quency component, we must use the following steps in computing X (m) to
int
guarantee conjugate symmetry:
• Perform an N-point FFT on an N-point x(n) time sequence, yielding N
frequency samples, X(m).
• Create an MN-point spectral sequence X (m) initially set to all zeros.
int
• Assign X (m)=X(m), for 0 ≤m≤(N/2)–1.
int
• Assign both X (N/2) and X (MN–N/2) equal to X(N/2)/2. (This step,
int int
to maintain conjugate symmetry and improve interpolation accuracy, is
not so well known[74].)
• Assign X (m) =X(q), where MN–(N/2)+1 ≤ m ≤ MN–1, and
int
(N/2)+1≤q≤N–1.
• Compute the real part of the MN-point inverse FFT of X (m), yielding
int
the desired MN-length interpolated x (n) sequence.
int
• Finally, if desired, multiply x (n) by M to compensate for the 1/M am-
int
plitude loss induced by interpolation.
Whew! Our mathematical notation makes this signal interpolation
scheme look complicated, but it’s really not so bad. Table 13–8 shows the
frequency-domain X (m) sample assignments, where 0 ≤ m ≤ 15, to interpo-
int
late an N=8-point x(n) sequence by a factor of M=2.
Table 13–8 X (m) Assignments for Interpolation by Two
int
m X (m) m X (m)
int int
0 X(0) 8 0
1 X(1) 9 0
2 X(2) 10 0
3 X(3) 11 0
4 X(4)/2 12 X(4)/2
5 0 13 X(5)
6 0 14 X(6)
7 0 15 X(7)

780 Digital Signal Processing Tricks
One of the nice properties of the above algorithm is that every Mth
x (n) sample coincides with the original x(n) samples. In practice, due to our
int
finite-precision computing, the imaginary parts of our final x (n) may have
int
small nonzero values. As such, we take x (n) to be the real part of the inverse
int
FFT of X (m).
int
Here’s the second issue regarding time-domain real signal interpola-
tion. This technique of interpolation using FFT zero stuffing only provides
acceptable results when the original x(n) time sequence has a spectrum hav-
ing negligible spectral energy in the vicinity of ±f/2, as shown in Figure
s
13–74 for lowpass and bandpass signals. By negligible we mean spectral
magnitudes that are, say, below a discrete signal’s quantization noise back-
ground spectral level.
An example of violating the above spectral restriction is when x(n) is a
sinusoidal sequence containing a noninteger number of cycles. That signal’s
positive-frequency spectrum will have nonzero spectral energy extending
from zero Hz to f/2 Hz caused by spectral leakage. Trying to interpolate such
s
an x(n) using this FFT zero-stuffing scheme will yield an interpolated time se-
quence with unacceptably high amplitude errors at the beginning and end of
the interpolated sequence.
With the advent of fast hardware DSP chips and pipelined FFT tech-
niques, the above time-domain interpolation algorithm may be viable for a
number of applications, such as computing selectable sample rate time se-
quences of a test signal that has a fixed spectral envelope shape; providing in-
terpolation, by selectable factors, of signals that were filtered in the frequency
domain using the fast convolution method (Section 13.10); or digital image re-
sampling. One scenario to consider is using the efficient 2N-Point Real FFT
technique, described in Section 13.5.2, to compute the forward FFT of the real-
valued x(n). Of course, the prudent engineer would conduct a literature
search to see what algorithms are available for efficiently performing inverse
FFTs when many of the frequency-domain samples are zeros.
Negligible spectral energy Negligible spectral energy
|X(f)| |X(f)|
–f –f /2 0 f /2 f Freq –f –f /2 0 f /2 f Freq
s s s s s s s s
(a) (b)
Figure 13–74 Spectral restrictions for interpolation using the FFT: (a) lowpass sig-
nal case; (b) bandpass signal case.

13.29 Frequency Translation Using Decimation 781
13.28.2 Computing Interpolated Analytic Signals
We can use the frequency-domain zero-stuffing scheme to generate an
interpolated-by-M analytic (complex-valued) time signal based upon the real
N-point time sequence x(n), if Nis even[75]. The process is as follows:
• Perform an N-point FFT on an N-point real x(n) time sequence, yielding
r
Nfrequency samples, X(m).
r
• Create an MN-point spectral sequence X (m) initially set to all zeros,
int
where MNis an integer power of two.
• Assign X (0)=X(0), and X (N/2)=X(N/2).
int r int r
• Assign X (m) = 2X(m), for 1≤m≤=(N/2)–1.
int r
• Compute the MN-point inverse FFT of X (m), yielding the desired
int
MN-length interpolated analytic (complex) x (n) sequence.
c,int
• Finally, if desired, multiply x (n) by Mto compensate for the 1/Mam-
c,int
plitude loss induced by interpolation.
To minimize the interpolation error in the complex x (n) sequence, the
c,int
original x(n) sequence must have negligible spectral energy in the vicinity of
r
±f/2, as described earlier for real-valued interpolation.
s
13.29 FREQUENCY TRANSLATION USING DECIMATION
In this section we show tricks for implementing multiplierless frequency
translation of both real and complex signals using simple decimation.
13.29.1 Translation of Real Signals Using Decimation
We can frequency translate a real bandpass signal toward zero Hz, converting
it to a lowpass signal, without the need for mixing multipliers. We do this by
performing decimation by an integer factor D as shown in Figure 13–75(a). If
the bandpass filter provides an output signal of bandwidth B Hz, located as
shown in Figures 13–75(b) and 13–75(d) where kis a positive integer, decima-
tion by D will yield lowpass signals whose spectra are shown in Figures
13–75(c) and 13–75(e), depending on whether integer k is odd or even. Take
care to notice the inverted spectra in Figure 13–75(e). To avoid decimated-
output aliasing errors, we must satisfy the Nyquist criterion and ensure that
x (n)’s bandwidth Bis not greater than f/(2D).
BP s
13.29.2 Translation of Complex Signals Using Decimation
It’s possible to frequency translate a complex bandpass signal, without the
need for mixing multipliers, so that a spectral replication is centered at zero

782 Digital Signal Processing Tricks
(a) x(n) Bandpass x BP (n) D x LP (n)
filter
Sample rate =f Sample rate =f/D
s s
k is even k is odd
|X (f)| B |X (f)| B
BP1 BP2
(b)
-kf 0 kf Freq -kf 0 kf Freq
s s s s
2D 2D 2D 2D
Decimate byD Decimate byD
|X (f)| |X (f)|
LP1 LP2
(c)
-f -f 0 f f Freq -f -f 0 f f Freq
s s s s s s s s
D 2D 2D D D 2D 2D D
k is even k is odd
|X (f)| B |X (f)| B
BP1 BP2
(d)
-kf 0 kf Freq -kf 0 kf Freq
s s s s
2D 2D 2D 2D
Decimate byD Decimate byD
|X (f)| |X (f)|
LP1 LP2
(e)
-f -f 0 f f Freq -f -f 0 f f Freq
s s s s s s s s
D 2D 2D D D 2D 2D D
|X (f)| |X' (f)|
CB Decimate byD CB
(f)
-kf 0 kf Freq -2f -f 0 f 2f Freq
s s s s s s
D D D D D D
Figure 13–75 Real and complex bandpass signal translation using decimation
byD.
Hz. The process we’re describing here is called complex down-conversion. The
left side of Figure 13–75(f) shows the spectrum of a complex baseband signal
whose |X (m)| spectral magnitude contains only positive-frequency spec-
CB
tral components.
If we individually decimate the real and imaginary parts of the complex
time sequence x (n), whose spectrum is X (m), by D,the resulting complex
CB CB

13.30 Automatic Gain Control (AGC) 783
sequence will have a spectral image centered exactly at zero Hz as shown by
|X’ (m)| in Figure 13–75(f). The key stipulation here, as you may have
CB
guessed, is that the original pre-decimated |X (m)| spectral energy must be
CB
centered at an integer multiple of f/D.
s
13.30 AUTOMATIC GAIN CONTROL (AGC)
Since the early days of vacuum tube radios, circuits were needed to automati-
cally adjust a receiver’s gain, as an input signal varied in amplitude, to main-
tain a (relatively) constant output signal level. These feedback mechanisms,
called automatic gain control (AGC) circuits, are an important component of
modern analog and digital communications receivers. Figure 13–76(a) illus-
trates a simple digital AGC process[76,77]. Its operation is straightforward:
The output signal power is sampled and compared to a reference level R(the
x(n) y(n)
α R
Gain
(a)
-
z-1
5
Peak = 1
x(n)
(b) 0
-5
0 200 400 600 800 1000 1200
Time (samples)
10
Peak = 1.41
y(n)
5 Fast time constant Slow time constant
(c)
0
-5
0 200 400 600 800 1000 1200
Time (samples)
Figure 13–76 AGC process: (a) linear AGC circuit; (b) example input x(n) with
amplitude fluctuations; (c) y(n) output for α=0.01 and R=1.

784 Digital Signal Processing Tricks
desired output amplitude rms level). If the output signal level is too high
(low), a negative (positive) signal is fed back, reducing (increasing) the gain.
The control parameter αregulates the amplitude of the feedback signal and is
used to control the AGC’s time constant (how rapidly gain changes take
effect).
Given an input signal x(n) in Figure 13–76(b) whose amplitude envelope
is fluctuating, the AGC structure provides the relatively constant amplitude
y(n) output shown in Figure 13–76(c).
We called Figure 13–76(a) a “simple AGC process,” but AGC is not all
that simple. The process is a nonlinear, time-varying, signal-dependent feed-
back system. As such, it’s highly resistant to normal time-domain or z-domain
analysis. This is why AGC analysis is empirical rather than mathematical and
explains why there’s so little discussion of AGC in the DSPliterature.
Depending on the nature of x(n), the feedback signal may fluctuate
rapidly and the feedback loop will attempt to adjust the system gain too
often. This will cause a mild AM modulation effect, inducing low-level har-
monics in the y(n) output. That problem can be minimized by inserting a sim-
ple lowpass filter in the feedback loop just before, or just after, the R adder.
But such filtering does not remedy the circuit’s main drawback. The time con-
stant (attack time) of this AGC scheme is input signal level dependent and is
different depending on whether the x(n) is increasing or decreasing. These
properties drastically reduce our desired control over the system’s time con-
stant. To solve this problem, we follow the lead of venerable radio AGC de-
signs and enter the logarithmic domain.
We can obtain complete control of the AGC’s time constant, and increase
our AGC’s dynamic range, by using logarithms as shown in Figure 13–77(a).
As is typical in practice, this log AGC process has a lowpass filter (LPF) to
eliminate too-rapid gain changes[78]. That filter can be a simple moving aver-
age filter, a cascaded integrator-comb (CIC) filter, or a more traditional low-
pass filter having a sin(x)/ximpulse response.
For the logarithmic AGC scheme, the feedback loop’s time constant is
dependent solely on α and independent of the input signal level, as can be
seen in Figure 13–77(b) when the x(n) input is that in Figure 13–76(b). The Log
and Antilog operations can be implemented as log (x) and 2x, respectively.
2
13.31 APPROXIMATE ENVELOPE DETECTION
In this section, we present a crude (but simple to implement) complex signal
envelope detection scheme. By “envelope detection” we mean estimating the
instantaneous magnitude of a complex signal x (n). The process is straightfor-
c
ward: we sum the absolute values of a complex signal’s real and imaginary
parts and apply that sum to a simple 1st-order lowpass IIR filter to obtain an
envelope signal E(n) as shown in Figure 13–78(a). The filter’s feedback coeffi-

13.31 Approximate Envelope Detection 785
x(n) y(n)
Gain
Antilog 2 log(R)
(a)
-
z-1 Log LPF
10
y(n)
Peak = 1.4
5
(b)
0
-5
0 200 400 600 800 1000 1200
Time (samples)
Figure 13–77 AGC process: (a) logarithmic AGC circuit; (b) y(n) output for
α=0.01 and R=1.
cient αis in the range of 0 to 1. (That lowpass filter is our exponential averager
discussed in Section 11.6, which some DSPfolks call a leaky integrator.) The E(n)
sequence is proportional to the desired instantaneous magnitude of x (n), or
c
E(n) ≈K|x (n)| = K x (n) 2 +x (n) 2 . (13–133)
c r i
To gauge the envelope detector’s performance, consider a sampled ver-
sion of an amplitude-modulated sinusoid such as the x(n) in Figure 9–7(a)
r
from which a sampled analytic (complex) x (n) can be generated. If x (n) is ap-
c c
plied to our envelope detection process, the processing results are shown in
Figures 13–78(b) and 13–78(c), where the solid curves represent E(n) and the
dashed curves are the true magnitude of x (n). Notice how the amount of
c
smoothing of the E(n) fluctuations depends on the value of α.
If the scaling coefficient α/2 can take the form
α 2K −1 1 1
= = − (13–133’)
2 2⋅2K 2 2K+1
where K is a positive integer, then we can eliminate the multipliers in Figure
13–78(a). If we satisfy Eq. (13–133’), the multiply by α/2 can be replaced by
two binary right shifts and a subtract operation, and the multiply by (1–α)
can be replaced by a single binary right-shift operation. This situation gives
us a multiplierless envelope detector.

786 Digital Signal Processing Tricks
x r (n) Absolute |x r (n)| + |x i (n)|
value
E(n)
x(n) =x(n) +jx(n) + +
c r i
(a) x i (n) Absolute 1-(cid:7)
value (cid:7)/2 z-1
Adder output (unfiltered) (cid:7) = 0.4
1.5 1
|x(n)|+|x(n)| |x(n)|
r i c
1
(b)
0.5
0.5
E(n)
0 0
0 0.1 0.2 0.3 0 0.1 0.2 0.3
(cid:7) = 0.2 (cid:7) = 0.05
1 1
|x(n)| |x(n)|
(c) c c
0.5 0.5
E(n) E(n)
0 0
0 0.1 0.2 0.3 0 0.1 0.2 0.3
Figure 13–78 Envelope detection: (a) block diagram; (b) |x(n)|+|x(n)| adder
r i
output, and E(n) for α= 0.4; (c) E(n) for α= 0.2 and α= 0.05.
Sequence x(n) must be used to generate a complex analytic x (n) se-
r c
quence (using one of the methods discussed in Sections 9.4 and 9.5) upon
which this envelope detector scheme can be applied. The advantage of this
envelope detection process is that, of course, no squaring or square root com-
putations in Eq. (13–133), nor the |x(n)| and |x(n)| comparisons in the vec-
r i
tor magnitude approximations in Section 13.2, need be performed.
Whether this envelope approximation technique yields sufficiently accu-
rate results is for the user to decide. Its accuracy may be below the require-
ments of most AM (amplitude modulation) detection requirements, but the
process may well be useful for estimating signal magnitude in automatic gain
control (AGC) or energy detection applications.
13.32 A QUADRATURE OSCILLATOR
Here we present a well-behaved digital quadrature oscillator, whose output is
y(n) + jy (n), having the structure shown in Figure 13–79(a). If you’re new to
i q
digital oscillators, that structure looks a little complicated but it’s really not so
bad. If you look carefully, you see the computations are

13.32 A Quadrature Oscillator 787
y(n) y(n)
cos(θ) i cos(θ) i
z-1 z-1
+ +
y'(n-1)
- y(n-1) i - y(n-1)
i i
G(n)
sin(θ) sin(θ)
y(n-1) y(n-1)
q y'(n-1) q
q
z-1 z-1
cos(θ) cos(θ)
y(n) y(n)
q q
(a) (b)
Figure 13–79 Quadrature oscillators: (a) standard structure; (b) structure with
AGC.
y(n) = y(n–1)cos(θ) – y (n–1)sin(θ) (13–134)
i i q
and
y (n) = y(n–1)sin(θ) + y (n–1)cos(θ). (13–134’)
q i q
Those computations are merely the rectangular form of multiplying the previ-
ous complex output by a complex exponential
ejθ
as
y(n) + jy (n) = [y(n–1) + jy (n–1)][cos(θ) +jsin(θ)]
i q i q
= [y(n–1) + jy
(n–1)]ejθ
. (13–135)
i q
So the theory of operation is simple. Each new complex output sample is
the previous output sample rotated by θradians, where θis 2πf/f with f and f
t s t s
being the oscillator tuning frequency and the sample rate, respectively, in Hz.
To start the oscillator, we set the initial conditions of y(n–1) = 1 and
i
y (n–1) = 0 and repeatedly compute new outputs, as time index n advances,
q
using Eq. (13–134). This oscillator is called a coupled quadrature oscillator be-
cause both of its previous outputs are used to compute each new in-phase
and each new quadrature output. It’s a useful oscillator because the full range
of tuning frequencies is available (from nearly zero Hz up to roughly f/2),
s
and its outputs are equal in amplitude, unlike some other quadrature oscilla-
tor structures[79]. The tough part, however, is making this oscillator stable in
fixed-point arithmetic implementations.
Depending on the binary word widths, and the value θ, the output am-
plitudes can either grow or decay as time increases because it’s not possible
to represent ejθ having a magnitude of exactly one, over the full range of θ,
using fixed-point number formats. The solution to amplitude variations is to

788 Digital Signal Processing Tricks
compute y’(n–1) and y ’(n–1) and multiply those samples by an instanta-
i q
neous gain factor G(n) as shown in Figure 13–79(b). The trick here is how to
compute the gain samples G(n).
We can use a linear automatic gain control (AGC) method, described in
Section 13.30, as shown in Figure 13–80(a) where α is a small value, say,
α = 0.01. The value R is the desired rms value of the oscillator outputs. This
AGC method greatly enhances the stability of our oscillator. However, there’s
a computationally simpler AGC scheme for our oscillator that can be devel-
oped using the Taylor series approximationwe learned in school. Here’s how.
Using an approach similar to reference [80], we can define the desired
gain as
M
G(n) = des. (13–136)
M
act
This is the desired output signal magnitude M over the actual output mag-
des
nitude M . We can also represent the gain using power as
act
P P
des des
G(n) = = (13–137)
P P +E
act des
where the constant P is the desired output signal power and P is the ac-
des act
tual output power. The right side of Eq. (13–137) shows P replaced by the
act
desired power P plus an error component E,and that’s the ratio we’ll com-
des
pute. To avoid square root computations and because the error E will be
small, we’ll approximate that ratio with a two-term Taylor series expansion
about E= 0 using
G(n) ≈a + a (E). (13–138)
0 1
Computing the Taylor series’ coefficients to be a = 1 and a = –1/2P ,
0 1 des
and recalling that E= P –P , we estimate the instantaneous gain as
act des
G(n)≈1 – 1 (P –P ) = 3 – P act = 3 – y i '(n−1) 2 +y q '(n−1) 2 . (13–139)
2P act des 2 2P 2 2P
des des des
If we let the quadrature output peak amplitudes equal 1/ 2, P equals 1/2
des
and we eliminate the division in Eq. (13–139), obtaining
3
G(n)≈ – [y’(n–1)2+ y ’(n–1)2]. (13–140)
2 i q
The simplified structure of the G(n) computation is shown in Figure 13–80(b).

13.33 Specialized Exponential Averaging 789
α
3/2
y'(n-1) y'(n-1)
i i
+
- -
z-1 G(n) G(n)
+
y q '(n-1) R y q '(n-1)
(a) (b)
Figure 13–80 AGC schemes: (a) linear AGC; (b) simplified AGC.
As for practical issues, to avoid gain values greater than one (for those
fixed-point fractional number systems that don’t allow numbers ≥1), we use
the clever recommendation from reference [79] of multiplying by G(n)/2 and
doubling the products in Figure 13–79(b). Reference [80] recommends using
rounding, instead of truncation, for all intermediate computations to improve
output spectral purity. Rounding also provides a slight improvement in tun-
ing frequency control. Because this oscillator is guaranteed stable, and can be
dynamically tuned, it’s definitely worth considering for real-valued as well as
quadrature oscillator applications[79].
13.33 SPECIALIZED EXPONENTIAL AVERAGING
In Chapter 11 we discussed the behavior and utility of using an exponential
averaging lowpass filter, also called a leaky integrator, to reduce noise fluctua-
tions that contaminate constant-amplitude signal measurements. In this sec-
tion we present three specialized exponential averaging techniques in the
form of
• single-multiply averaging,
• multiplier-free averaging, and
• dual-mode averaging.
13.33.1 Single-Multiply Exponential Averaging
This DSPtrick shows how to reduce the computational workload of the stan-
dard exponential averager[81]. An exponential averager’s difference equa-
tionis
y(n) = αx(n) + (1 – α)y(n–1) (13–141)
where α is a constant called the averager’s weighting factor, in the range
0 <α < 1. The process requires two multiplies per y(n) output sample as
shown in Figure 13–81(a).

790 Digital Signal Processing Tricks
x(n) y(n) x(n) y(n)
z1
z 1
y( 1)
(a) (b)
x(n) u(n) w(n) y(n)
BRS,L
(c)
BRS,M
z 1
L = 5, M = 6, L = 0, M = 1,
= 0.0156 = 0.5
L = 1, M = 2, L = 0, M = 6,
= 0.25 = 0.9844
(d)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
x(n) y(n)
(e)
Switch 1/K z 1
Figure 13–81 Exponential averaging: (a) standard network; (b) single-multiply
network; (c) multiplierless network; (d) possible values for α;
(e) dual-mode averaging.
We can rearrange Eq. (13–141) to the form
y(n) = y(n–1) + α[x(n) – y(n–1)], (13–141’)
which eliminates one of the averager’s multiplies, at the expense of an addi-
tional adder, giving us a single-multiply exponential averager shown in Fig-
ure 13–81(b). This neat single-multiply exponential averager maintains the
DC (zero Hz) gain of unity exhibited by the traditional two-multiply expo-
nential averager in Figure 13–81(a).
13.33.2 Multiplier-Free Exponential Averaging
It is possible to eliminate the multiplier in Figure 13–81(b) if we place restric-
tions on the permissible values of α. For example, if α= 0.125 = 1/8, then the

13.33 Specialized Exponential Averaging 791
output of the multiplier is merely the multiplier’s input sample shifted right
by three bits.
On the other hand, if αtakes the form
1 1
α= −
(13–142)
2L 2M
where L = 0, 1, 2, 3, ... , and M = 1, 2, 3, ... , we can replace the multiplication
by α in Figure 13–81(b) with two binary right shifts and a subtract operation
as shown in Figure 13–81(c). In that figure the “BRS,L” block means an arith-
metic, or hardwired, Binary Right Shift by Lbits.
For example, if L = 2 and M = 5, then from Eq. (13–142), α = 0.2188. The
sequence w(n) = 0.2188u(n) = (1/4 – 1/32)u(n) is computed by subtracting
u(n) shifted right by M= 5 bits from u(n) shifted right by L= 2 bits.
The tick marks in Figure 13–81(d) show the possible values for the
weighting factor α over the range of 0 ″ L ″ 5, where for each L, M is in the
range L+1 ″ M ″ 6 in Eq. (13–142). That figure tells us that we have a reason-
able selection of αvalues for our noise-reduction filtering applications.
The point is, for fixed-point implementation of exponential averaging,
check to see if your desired α weighting factor can be represented by the dif-
ference of various reciprocals of integer powers of two. If so, then binary
word shifting enables us to implement a multiplierless exponential averager.
13.33.3 Dual-Mode Averaging
Here’s a clever exponential averaging scheme that blends both the quick time
response of a moving averager and the noise-reduction control of an expo-
nential averager.†The structure of this dual-mode averager is depicted in Fig-
ure 13–81(e). The averager operates as follows: The switch remains open for K
input samples after which the y(n) output is equal to the K-point average of
the x(n) input. Just prior to the arrival of the K+1 input sample the switch
closes, converting the moving average filter to an exponential averager, giv-
ing us control over the filter’s noise-reduction properties as described in Sec-
tion 11.6.
Of course, K does not have to be an integer. In this case we can still im-
plement dual-mode averaging by closing the switch just prior to the arrival of
the x(⎣K⎦) input sample, where ⎣K⎦ means the integer part of K. After the Kth
input sample has arrived, the averager’s gain at zero Hz (DC gain) is unity.
As discussed in the previous section, if the weighting factor 1/Kcan be repre-
sented by the difference of various reciprocals of integer powers of two, then
we can implement a multiplierless dual-mode noise-reduction filter.
†We thank DSPguru Fred Harris for recommending this dual-mode averager.

792 Digital Signal Processing Tricks
13.34 FILTERING NARROWBAND NOISE USING FILTER NULLS
Here we present two filter design tricks that take advantage of the frequency-
domain magnitude nulls of simple FIR filters. These schemes are particularly
useful when used in AM and FM demodulation systems.
The first example uses a simple 3-tap nonrecursive FIR filter. Such a fil-
ter can be both computationally efficient, and useful, for narrowband-noise
reduction. Here’s how. Think about the x(n) time-domain signal, contami-
nated with high-frequency noise, shown in Figure 13–82(a) with its spectrum
provided in Figure 13–82(b). The sample rate of the signal is 10 kHz. Let’s as-
sume we want to recover the low-frequency signal of interest (centered at
zero Hz) without inducing phase distortion, and we need to attenuate the
narrowband high-frequency noise, centered at 4.1 kHz, by at least 50 dB. Our
solution, of course, is to pass our noisy signal through a linear-phase lowpass
FIR filter whose frequency magnitude response is indicated as the dashed
curve in Figure 13–82(b).
Seeking the most computationally efficient filter possible, let’s say we’re
clever and recall the special characteristic of a half-band FIR filter in which
roughly half its coefficients are zero-valued. So we could design a 9-tap half-
band FIR filter, having only five nonzero-valued coefficients, and that solu-
tion would be acceptable. Here’s where our trick comes in; we decide to use
the linear-phase 3-tap FIR filter shown in Figure 13–83(a) with its single non-
unity coefficient h .
1
If |h | ≤2, the 3-tap FIR filter’s transfer function will have two zeros on
1
the z-plane at angles ±ω as shown in Figure 13–83(b). The frequency magni-
n
tude response of the filter is shown in Figure 13–83(c). (Here, the normalized
1
x(n)
0
(a)
–1
0 100 200 300 400 500
n
0
|X(f)|
Noise Noise
–20
(b)
B
d Signal of
–40
interest
–60
–4 –2 0 2 4 5
Freq(kHz)
Figure 13–82 A noisy x(n): (a) time signal; (b) its X(f) spectral magnitude.

13.34 Filtering Narrowband Noise Using Filter Nulls 793
x(n)
z z
h
1
(a)
y(n)
1 |H( )|
0
art
p
y n –20
n
ar 0
B
a
gi
n
d
–40
m
I
–1
–1 0 1 0
n n
Real part f/2) Freq (f/2)
s s
(b) (c)
Figure 13–83 A 3-tap FIR filter: (a) filter structure; (b) pole locations; (c) frequency
magnitude response.
frequency axis value of π corresponds to a continuous-time frequency of half
the sample rate, f/2.) Our goal, then, is to choose the h coefficient such that
s 1
the filter’s positive-frequency magnitude null lands right on the 4.1 kHz cen-
ter frequency of the narrowband noise in Figure 13–82(b).
Our 3-tap filter design problem is easy because we have an expression
for the h coefficient as a function of the desired f null frequency in Hz. The
1 n
h coefficient value is
1
h = –2cos(2πf /f). (13–143)
1 n s
With f = 4.1 kHz and f = 10 kHz, our h coefficient is 1.69. (The derivation of
n s 1
Eq. (13–143) was left as a homework problem in Chapter 5.) The H(f) fre-
quency magnitude response of the h = 1.69 filter is shown as the dotted curve
1
in Figure 13–84(a). The Y(f) spectrum of the filter’s output is shown in Figure
13–84(b) where the narrowband noise has been attenuated by roughly 54 dB.
(Recall that the noise magnitudes in the original X(f) spectrum were approxi-
mately 12 dB above the signal’s peak magnitude in Figure 13–82(b).) The fil-
ter’s time-domain y(n) output signal, our signal of interest, is shown in Figure
13–84(c). It’s instructive to compare that output signal to the filter’s x(n) input
signal in Figure 13–82(a).
So we solved our narrowband noise filtering problem with a linear-
phase FIR filter requiring only two additions and one multiply per filter out-
put sample. Neat, huh?

794 Digital Signal Processing Tricks
0
|X(f)|
–20
B |H(f)|
d
(a) –40
–60
–4 –2 0 2 5
Freq (kHz)
Filter null
0
|Y(f)|
–20
B
d
(b) –40
–60
–4 –2 0Freq (kHz)2 4 5
0.6
y(n)
(c) 0
–0.6
0 100 200 300 400 500
n
Figure 13–84 Three-tap filter performance: (a) |H(f)| response; (b) filter output
spectrum; (c) filter time-domain output signal.
Our second example of this filter design approach that takes advantage of
the frequency-domain magnitude nulls of simple FIR filters involves the attenu-
ation of multiple narrowband noise spectral components whose center frequen-
cies are harmonically related in a frequency shift keying (FSK) demodulation
system[82]. Think about a signal of interest centered at 455 kHz as shown in Fig-
ure 13–85(a). That signal, sampled at f = 10 MHz, is contaminated with un-
s
wanted high-amplitude DC (zero Hz) bias noise and narrowband spectral noise
components at multiples of 455 kHz. Removing the DC bias, whose magnitude
is 0 dB in Figure 13–85(a), and extracting the signal of interest from the noise ap-
pears to require some sort of bandpass filter centered at 455 kHz.
However, the trick is to use a standard FIR comb filter to remove the un-
wanted DC bias and attenuate the harmonic noise components. Acomb filter
is shown in Figure 13–86(a) where the z–N operation is merely a delay of N
samples. The |H(f)| frequency magnitude response of an N= 8, for example,
comb filter is provided in Figure 13–86(b) where f is the sample rate.
s

13.34 Filtering Narrowband Noise Using Filter Nulls 795
DC bias Signal of interest
0
|X(f)|
–20
B
d
–40
(a)
–60
–80
455 kHz 910 kHz
0 1 2 MHz 3 4 5
0
|Y(f)|
–20
B
d –40
(b)
–60
–80
455 kHz 1365 kHz
0 1 2 3 4 5
MHz
Figure 13–85 Harmonic noise example: (a) |X(f)| spectrum; (b) filter output
spectrum.
For N-delay comb filter design purposes, the following two equations
give us the frequency locations of the magnitude nulls (f ) and magnitude
null
peaks(f )in the filter’s |H(f)|,
peak
kf ⎢N⎥
f = s , wherek=0,±1,±2,...,± ⎢ ⎥ (13–144)
null N ⎣ 2 ⎦
( 2k+1 ) f ⎢⎢N−1⎥
f = s , wherek= 0,±1,±2,...,± ⎢ ⎥, (13–144’)
peak 2N ⎣ 2 ⎦
where ⎣X⎦ means the integer part of X. These f and f expressions are
null peak
valid forboth odd and even Nso long asNis larger than one.
|H(f)|
Comb filter 0
x(n) y(n)
+ + –10
B
– d–20
z–N
0 f /8 f /4 3f /8 f /2
s s s s
Freq
(a) (b)
Figure 13–86 Standard N-delay FIR comb filter: (a) filter structure; (b) frequency
magnitude response when N= 8.

796 Digital Signal Processing Tricks
For this noise-reduction problem, we need a comb filter that provides a
magnitude null at zero Hz and a magnitude peak at 455 kHz. Rearranging Eq.
(13–144’) to find a candidate value for the comb delay Nfor k= 0, we have
f 106
N= s = ( )=10.99, (13–145)
2f 2 455⋅103
peak
so we select Nto be 11. The filter’s output spectrum, when N= 11, is shown in
Figure 13–85(b). There we see the dramatic reduction in the unwanted DC
bias as well as the narrowband noise located at 910 kHz. (For reference pur-
poses, we’ve included the N = 11 comb filter’s magnitude response in Figure
13–85(b).)
So in this example we simplified our overall narrowband noise filtering
problem using a linear-phase FIR comb filter requiring only one addition per
filter output sample. In practice the comb filter is followed by a low-order
lowpass filter, whose stopband would begin in the vicinity of 1365 kHz. That
follow-on filter will have a significantly reduced computational workload
compared to the case where the pre-filtering by the simple comb filter was
not performed.
For completeness, we must mention here that an alternatecomb filter can
be built using the network in Figure 13–87(a) where addition is performed as
opposed to the subtraction in Figure 13–86(a).
For the alternate comb filter in Figure 13–87(a) the following two equa-
tions give usthe frequency locations of the magnitude nulls(f ) and mag-
null,alt
nitude peaks(f )in this N-delay comb filter’s |H (f)|,
peak,alt alt
( 2k+1 ) f ⎢NN−1⎥
f = s , wherek= 0,±1,±2,...,± ⎢ ⎥ (13–146)
null,alt 2N ⎣ 2 ⎦
kf ⎢N⎥
f = s , wherek=0,±1,±2,...,± ⎢ ⎥⎥ (13–146’)
peak,alt N ⎣ 2 ⎦
|H (f)|
Alternate alt
0
comb filter
x(n) + y(n) –10
+ B
d
–20
+
z–N
0 f/8 f/4 3f/8 f/2
s s s s
Freq
(a) (b)
Figure 13–87 Alternate N-delay FIR comb filter: (a) filter structure; (b) |H (f)| fre-
alt
quency magnitude response when N= 8.

13.35 Efficient Computation of Signal Variance 797
where ⎣X⎦means the integer part of X. This alternate comb filter gives us a bit
of design flexibility because it passes low-frequency signals due to a fre-
quency magnitude peak at zero Hz (DC).
13.35 EFFICIENT COMPUTATION OF SIGNAL VARIANCE
In this section we show how to reduce the computational workload, and re-
quired data storage, in computing the unbiased and biased variances of a sig-
nal sequence. (Definitions of biased and unbiased variances can be found in
Appendix D.)
We start our discussion with the traditional definition of the unbiased
variance of x(n), a sequence of Nsamples, written as
∑N
[x(n)−x ]2 (13–147)
ave
Var =
n=1
unbiased N−1
where x is the average of the N-length x(n) sequence. Because N is a con-
ave
stant, we can treat the divide by Nneeded to compute x , and the above di-
ave
vide by (N–1), as multiplies by reciprocals, allowing us to say that Eq.
(13–147) requires 3N–2 addition and N+2 multiply operations. As it turns out,
we can obtain an equivalent expression for Var that has a reduced num-
unbiased
ber of arithmetic operations[83]. Here’s how.
First, we square the bracketed term in the summation in Eq. (13–147)
and write
∑N
[x(n)2 −2x(n)x +x 2]
ave ave
Var =
n=1
unbiased N−1
∑N ∑N ∑N
x(n)2 − 2x(n)x + x 2 (13–148)
ave ave
=
n=1 n=1 n=1
.
NN−1
Because the center summation in Eq. (13–148) is
–Σ2x(n)x = –2x ∑x(n) = –2x Nx = –2Nx 2
ave ave ave ave ave
we can rewrite Var as
unbiased

798 Digital Signal Processing Tricks
∑N ∑N
[x(n)2]−2Nx 2 + [x 2]
ave ave
Var =
n=11 n−1
unbiased N−1
∑N
[x(n)2]−2Nx 2 +Nx 2 (13–148´)
ave ave
=
n=1
.
N−1
Next, we arrive at our desired expression by combining terms and write
Var as
unbiased
∑N
[x(n)2] −Nx 2 (13–149)
ave
Var =
n=1
.
unbiased N−1
The efficient Eq. (13–149) requires only 2N–1 addition and N+4 multiply
operations. So at the expense of two extra multiplies, we’ve reduced the num-
ber of additions needed to compute Var by roughly N relative to Eq.
unbiased
(13–147).
There is a second advantage in using Eq. (13–149) instead of Eq. (13–147)
in computing the variance of N incoming x(n) samples. When using Eq.
(13–147) to compute Var ,we first compute x and must retain, in mem-
unbiased ave
ory, the N-length x(n) sequence in order to compute the [x(n) – x ]2sequence.
ave
When using Eq. (13–149) to compute Var , we can simultaneously accu-
unbiased
mulate (sum) the N incoming x(n) samples and accumulate the N computed
x(n)2 samples without having to keep past x(n) samples in memory. Thus Eq.
(13–149) reduces the amount of data storage needed to compute Var .
unbiased
The traditional definition for the biased variance of N x(n) samples is
written as
∑N
[x(n)−x ]2 (13–150)
ave
Var =
n=1
.
biased N
Using a derivation similar to how we arrived at Eq. (13–149), we can write an
efficient expression for computing a biased variance as
∑N
[x(n)2] (13–150’)
Var = n=1 −x 2.
biased N ave

13.36 Real-Time Computation of Signal Averages and Variances 799
Equation (13–150’) requires 2N–1 addition and N+2 multiply operations. Here
again, we’ve reduced the number of additions needed to compute Var by
biased
roughly Nand reduced the necessary data storage, relative to Eq. (13–150). In
the next section we discuss the hardware implementation of variance compu-
tations with a focus on real-time processing.
13.36 REAL-TIME COMPUTATION OF SIGNAL AVERAGES
AND VARIANCES
In this section we present techniques for the efficient computation of real-time
estimates of signal average and variance. By “real-time” we mean a continu-
ing sequence of statistical estimates will be generated, in time, as a continuing
sequence of input signal samples is applied to our processing networks.
13.36.1 Computing Moving Averages and Variances
Figure 13–88(a) shows a 5-point real-time recursive moving averager that we
analyzed in Section 11.5. For the reasons discussed in Section 11.5 the recur-
sive moving averager is the most computationally efficient method for com-
puting moving averages.
Figure 13–88(b) shows an alternate, but equivalent, recursive moving
averager where the integrator now precedes the 1/5 multiplication. In this al-
ternate arrangement the binary register holding the integrator’s accumulation
results must be large enough to accommodate values in the range of five
(number of unit-delay registers) times the true average of the most recent N
x(n) input samples.
In Figure 13–88(c) we redraw the alternate recursive moving averager in
order to show the network of a general N-point recursive moving averager.
There we use a single z–N delay element symbol to represent an N-length
delay line. In that figure we show a network inside the dashed-line box,
which we’ll use later for other statistical computations, called a recursive run-
ning sum(RRS).
Focusing now on the second topic of this section, there is a way to esti-
mate the real-time N-point moving unbiased variance of a signal,
x (n)[84]. (A definition of unbiased variance is provided in Appendix
var,unbiased
D.) To see how, we start with the expression for the unbiased variance of
Ntime samples, Eq. (13–149) from the previous section, rewritten here as
∑n
[x(n)2] (13–151)
N[x (n)]2
x (n)= n−N+1 − aave ,
var,unbiased N−1 N−1

800 Digital Signal Processing Tricks
Recursive 5-point moving averager
x(n) x(n–5)
z–1 z–1 z–1 z–1 z–1
+ –
(a)
x (n)
ave
z–1
1/5
Alternate recursive 5-point moving averager
x(n) x(n–5)
z–1 z–1 z–1 z–1 z–1
+ –
(b)
x (n)
ave
z–1
Integrator 1/5
RecursiveN-point moving averager
x(n) x (n)
ave
–
(c) z–N z–1 1/N
x(n–N) Nx (n–1)
ave
N-point recursive
running sum (RRS)
Figure 13–88 Real-time recursive N-point moving averager: (a) standard N= 5
implementation; (b) alternate implementation; (c) general recur-
sive depiction.
where x (n) is the average of the most recent Ninput x(n) samples. The lim-
ave
its on the summation in Eq. (13–151) are such that we’re summing a sliding-
in-time block of Nsamples of x(n)2.
The implementation of Eq. (13–151) is shown in Figure 13–89(a) where
the process uses two N-point RRS networks from Figure 13–88(c) to compute
the N-point moving unbiased variance x (n) and the x (n) N-point
var,unbiased ave
moving average of x(n)[83,85]. Note that the x (n) and x (n) outputs
var,unbiased ave
are not valid until the N-stage delay lines are filled with input data.
To estimate the real-time N-point moving biased variance of a signal,
x (n), we compute
var,biased

13.36 Real-Time Computation of Signal Averages and Variances 801
N-point moving unbiased variance and
N-point moving average
x(n) N-point x var,unbiased (n)
RRS –
1/N N
(a) 1/(N–1)
N-point
RRS x (n)
ave
N-point moving biased variance and
N-point moving average
x(n) N-point x var,biased (n)
RRS –
(b) 1/N
1/N
N-point
RRS x (n)
ave
Figure 13–89 Real-timeN-point moving variance networks.
∑n
[x(n)2] (13–152)
x (n)= n−N+1 −[x (n))]2
var,biased N ave
using the network shown in Figure 13–89(b).
From a practical standpoint, in fixed-point systems, note that the binary
word width of the upper RRS delay lines, in Figure 13–89, must be twice as
wide as the lower RRS delay lines.
The above real-time fixed-length moving average and moving variance
networks require data memory to implement their N-point delay lines. The
following section describes techniques for estimating cumulative averages
and variances with reduced data memory requirements.
13.36.2 Computing Exponential Moving Average and Variance
An alternate method to generate estimates of both the real-time moving aver-
age and real-time moving variance of a signal is to use the exponential aver-
ager that we discussed in Section 11.6, shown in Figure 13–90(a). The
coefficient α is the exponential averager’s weighting factor that controls the
amount of averaging that takes place at the output of the network.
To generate our desired exponential moving average and exponential
moving variance, we use two independent exponential averaging (EA)

802 Digital Signal Processing Tricks
Exponential averager (EA)
z 1
(a)
Exponential moving variance and
exponential moving average
x(n) EA x var (n)
( )
2
(b)
x (n)
EA ave
( )
1
Figure 13–90 Exponential moving average and exponential moving variance:
(a) standard exponential averaging network; (b) full structure.
networks as shown in Figure 13–90(b). The two weighting factors, α and α ,
1 2
are constants in the range of zero to one.
The process in Figure 13–90(b) has several attractive properties. The α
1
and α coefficients permit control over the averaging behavior of the process;
2
and the Figure 13–90(b) process requires fewer computations per output sam-
ple, and reduced delay-line element (data memory) requirements, relative to
the networks in Figure 13–89.
13.37 BUILDING HILBERT TRANSFORMERS FROM HALF-BAND FILTERS
This section discusses two techniques for obtaining the coefficients of a
Hilbert transformer from the coefficients of an N-point nonrecursive FIR half-
band filter[86,87]. The first scheme is useful for someone who needs to design
a Hilbert transformer when only generic lowpass FIR filter design software is
available. The second scheme is useful for those unfortunate folks who have
no FIR filter design software at hand but have available the coefficients of a
half-band filter.
13.37.1 Half-band Filter Frequency Translation
We can design a Hilbert transformer by first designing an N-tap half-band fil-
ter using our favorite FIR filter design software, with the restriction that N+1
is an integer multiple of four. Let’s call the half-band filter’s coefficients h (k),
hb

13.37 Building Hilbert Transformers from Half-Band Filters 803
where the coefficients’ index variable k is 0, 1, 2, ..., N–1. Next we obtain the
Hilbert transformer’s h (k) coefficients using
hilb
⎡π⎛ N−1⎞⎤
h hilb (k)=2sin ⎣ ⎢ 2 ⎝ ⎜k− 2 ⎠ ⎟ ⎦ ⎥h hb (k). (13–153)
Figure 13–91(a) shows the coefficients of a simple 7-tap half-band filter
whose DC gain is unity. Figure 13–91(b) shows the Hilbert transformer’s
h (k) coefficients obtained from Eq. (13–153). The network using those
hilb
h (k) coefficients to generate a complex (analytic) x (n) = x(n)+jx (n) se-
hilb c I Q
quence from an original real-valued x (n) sequence is shown in Figure
R
13–91(c). (Notice the z–2delay blocks comprising two unit-delay elements.)
Let’s call the network in Figure 13–91(c) a complex bandpass filter and
describe its characteristics a bit further. Figure 13–92(a) shows the |H (f)|
hb
frequency magnitude response of a half-band filter, and Figure 13–92(b)
shows us that the complex bandpass filter’s |H (f)| frequency magnitude
cbp
response is |H (f)| translated up in frequency by f/4. However, notice that
hb s
|H (f)|’s passband gain and ripple, as well as its stopband ripple, are twice
cbp
that of |H (f)|. To make the complex bandpass filter’s gain unity, rather than
hb
two, we decrease its coefficients by a factor of two and multiply the x(n) se-
I
quence in Figure 13–91(c) by 0.5. That 0.5 multiply could, of course, be imple-
mented with an arithmetic right shift of the x(n) samples.
I
0.5 1
h (k) h (k)
hb hilb
0 0
–0.5 –1
0 2 4 k 6 0 2 4 k 6
(a) (b)
x(n)
I
x (n)
R
z–2 z–1 z–1 z–2
h (0) h (2) h (4) h (6)
hilb hilb hilb hilb
+
x (n)
Q
x(n) = x(n) + jx (n)
(c) c I Q
Figure 13–91 Seven-tap half-band FIR transformation: (a) h (k); (b) h (k);
hb hilb
(c) complex bandpass filter structure.

804 Digital Signal Processing Tricks
1
(a) |H hb (f)|
0.5
0
–f/2 –f/4 0 f/4 f/2
s s Freq s s
2
|H (f)|
(b) cbp
1
0
–f/2 –f/4 0 f/4 f/2
s s Freq s s
Figure 13–92 Frequency magnitude responses: (a) half-band filter; (b) complex
bandpass filter.
The nifty part of this complex bandpass filter is as follows: To build a
complex nonrecursive FIR filter having the performance (transition region
width, stopband attenuation, etc.) of a real N-tap lowpass FIR filter, we typi-
cally must implement two real N-tap FIR filters having an overall computa-
tional workload of 2(N–1) additions and 2N multiplications per complex
output sample, as well as provide 2N memory locations to store the complex
coefficients. The complex bandpass filter in Figure 13–91(c) reduces those
computations and the memory requirement by a factor of two.
Here’s another attribute: because the complex filter’s coefficients are an-
tisymmetrical, we can use the folded FIR filter scheme described in Section
13.7 to reduce the number of multipliers by another factor of two!
13.37.2 Half-band Filter Coefficient Modification
This second half-band filter to Hilbert transformer conversion scheme is useful
for those unfortunate folks who have no nonrecursive FIR filter design soft-
ware at hand but do happen to have the coefficients of a half-band filter. We
can obtain the h (k) coefficients of a Hilbert transformer with a straightfor-
hilb
ward modification of the half-band FIR filter’s h (k) coefficients. The modifi-
hb
cation steps are as follows:
1. Identify the center coefficient of h (k); call it h .
hb center
2. Make the signs (polarity) of all nonzero coefficients before h nega-
center
tive.
3. Make the signs (polarity) of all nonzero coefficients after h positive.
center
4. Set the h coefficient equal to zero.
center

13.38 Complex Vector Rotation with Arctangents 805
0.5
h (k)
hb
(a) 0
–0.5
0 2 4 6 8 10
k
0.5
h (k)
hilb
(b) 0
–0.5
0 2 4 6 8 10
k
Figure 13–93 Half-band filter coefficient modification: (a) original h (k) coeffi-
hb
cients; (b) h (k) coefficients.
hilb
An example of this half-band filter coefficient modificationprocess is shown for an
11-tap FIR half-band filter’s h (k) in Figure 13–93. In order to use the Figure
hb
13–93(b) h (k) coefficients in the complex bandpass filter in Figure 13–92(c),
hilb
those h (k) coefficients will need to be multiplied by a factor of two, or the x(n)
hilb I
sequence in Figure 13–91(c) must be multiplied by 0.5. The 0.5 multiply can be
implemented with an arithmetic right shift of the x’(n) samples if desired.
r
13.38 COMPLEX VECTOR ROTATION WITH ARCTANGENTS
It’s often the case in quadrature (I/Q) processing systems that we want to
compute the angle of a complex time-domain sample. That angle computa-
tion for a complex sample C= I+ jQis, of course,
θ =tan −1
⎛
⎜
Q⎞
⎟. (13–154)
⎝ ⎠
I
As it turns out, the arctangent computation in Eq. (13–154) can be performed
in many ways—anywhere from slow but accurate computationally intensive
high-order polynomial evaluation, to high-speed crude-accuracy table look-
up methods. However, regardless of the method used, we can improve the ac-
curacy and speed of an arctangent computation if we limit the angular range
over which it is performed. And that’s where the vector rotation tricks pre-
sented here come into play.
13.38.1 Vector Rotation to the 1st Octant
Plotted on the complex plane, a complex sample C = I + jQ can reside in any
of the eight octants shown in Figure 13–94(a). When performing arctangents,

806 Digital Signal Processing Tricks
Q Q
3rd 2nd
C
1st
4th 1st
1st
5th 8th I I
6th 7th
1st
C
(a) (b)
Figure 13–94 Complex vector rotation: (a) octant definitions; (b) vector rotation
from the 7th octant to the 1st octant.
please know that arctangent algorithms, be they high precision and computa-
tionally expensive or be they computationally simple and lower precision, are
more accurate for small angles. (That is because the problematic arctangent
function is only approximately linear for small angles.) So what does this
mean to us? It means that if we can effectivelyrotate the angle of complex sam-
ple Cinto Figure 13–94(a)’s 1st or 8th octant, a smaller angle, arctangent algo-
rithms will provide more accurate results.
For example, consider the complex number represented by vector C in
Figure 13–94(b) residing in the 7th octant. The angle θwe want to compute is
more negative than –π/4 radians (–45 degrees). Our trick is to rotate C to a
new (and smaller) angle θ , compute θ with an arctangent algorithm, and
1st 1st
add θ to –π/2 to obtain the desired value for θ.
1st
Rotating vector Ccan be implemented as follows:
• If vector C‘s Qcomponent is negative (Cis in the 5th through the 8th oc-
tant), we can rotate Cby 180 degrees by negating both the Iand Qcom-
ponents.
• If vector Cis in the 3rd or 4th octant, we can rotate Cclockwise by 90 de-
grees by setting the new I equal to the old Q value, and setting the new
Q equal to the negative of the old I value. (Note that the negative of the
old Ivalue is equal to the absolute value of the old Ivalue.)
• If vector C is in the 2nd octant, we can rotate C clockwise by 45 degrees
by swapping the Iand Qcomponents.
Using the above rotation operations for our Figure 13–94(b) example, we
can rotate the original 7th-octant C = I + jQ to the 3rd octant by creating vec-
tor C = –I– jQ. Next we rotate C to the 1st octant by creating vector C =
3rd 3rd 1st
–Q+ jI. We compute θ as
1st

13.38 Complex Vector Rotation with Arctangents 807
θ =tan −1 ⎛ ⎜ I ⎞ ⎟ = −tan −1 ⎛ ⎜ I ⎞ ⎟ (13–155)
1st ⎝−Q⎠ ⎝Q⎠
using an arctangent algorithm and finally add θ to –π/2 to obtain our de-
1st
sired value for θ.
OK, here’s the neat part of this trick. We don’t actually have to perform
any of the above vector rotations to obtain angle θ . We merely need to find
1st
the signs of the original I and Q components and determine which compo-
nent has the larger magnitude. With those three pieces of information we de-
termine in which octant vector Cis located by using Table 13–9.
Once we know vector C‘s octant, we take advantage of the following ro-
tational symmetriesof arctangents
tan −1
⎛
⎜−
Q⎞
⎟ = −tan −1
⎛
⎜
Q⎞
⎟ (13–156)
⎝ ⎠ ⎝ ⎠
I I
tan −1 ⎛ ⎜ Q⎞ ⎟ = π −tan −1 ⎛ ⎜ I ⎞ ⎟ (13–156’)
⎝ I ⎠ 2 ⎝Q⎠
and compute our desired θ value using the appropriate expression in Table
13–10.
Given that this arctangent process is implemented with programmable
hardware, we’ll have Table 13–10’s four different arctangent approximation
routines located at four different memory locations to which we’ll jump. The
process to determine the necessary two jump address index bits (b ,b ) based
1 0
on vector C‘s octant is shown in Figure 13–95.
Table 13–9 Octant Identification
Sign(I) Sign(Q) |Q| – |I| Octant
+ + |Q| – |I| < 0 1st
+ + |Q| – |I| ≥0 2nd
– + |Q| – |I| ≥0 3rd
– + |Q| – |I| < 0 4th
– – |Q| – |I| < 0 5th
– – |Q| – |I| ≥0 6th
+ – |Q| – |I| ≥0 7th
+ – |Q| – |I| < 0 8th

808 Digital Signal Processing Tricks
Table 13–10 Arctan Computation
Quadrant Angle θ
1st, or 8th θ= tan–1(Q/I)
2nd, or 3rd θ= π/2 – tan–1(I/Q)
4th, or 5th θ= sign(Q).π+ tan–1(Q/I)
6th, or 7th θ= –π/2 – tan–1(I/Q)
To avoid division by zero when using the algorithms in Table 13–10, it’s
prudent to precede the Figure 13–95 processing with checkingto see if Ior Qis
zero:
• If I= 0, θis set to π/2 or –π/2 depending on the sign of Q.
• If Q= 0, θis set to 0 or πdepending on the sign of I.
• If Iand Qare both zero, set θto 0.
Again, this section does not present any specific arctangent algorithms.
This material shows how to make a given arctangent algorithm more accurate.
Octant Identification Sequence
Set "Jump Address Bits": bb = 00
1 0
Yes
|Q| - |I| < 0
No
Yes
I > 0
Set b = 1 No
1
Yes Set b = 1
Q > 0 0
No
bb = [1,0] bb = [0,0]
(2 1 n 0 d or 3rd (1 1 s 0 t or 8th
Set b 0 = 1 octant) octant)
bb = [1,1] bb = [0,1]
1 0 1 0
(6th or 7th (4th or 5th
octant) octant)
Jump to appropriate arctan routine's starting
memory location specified by bb.
1 0
Figure 13–95 Octant and jump address identification flow.

13.38 Complex Vector Rotation with Arctangents 809
(cid:6)
13.38.2 Vector Rotation by ± /8
While we’re on the subject of vector rotation, if a 1st-octant vector C resides
1st
in the angle range of π/8 ≤ θ ≤ π/4 radians (Range 1 in Figure 13–96(a),
1st
22.5o ≤ θ ≤ 45o), we can rotate that vector by –π/8 radians (–22.5o), forcing
1st
the new vector into Region 2. We may want to perform this rotation because
arctangent algorithms have improved accuracy in Region 2.
We rotate a vector C = I + jQ residing in Range 1 to Range 2 by
1st 1st 1st
multiplying C by the complex number
e–jπ/8=
(A–jB), where
1st
A= 0.923879, and B= 0.382683. (13–157)
We can simplify the complex multiply by dividing Aand Bby 0.923879, yielding
A’ = 1, and B’ = 0.414213. (13–158)
This gives us a new (A’ –jB’) = (1 –j0.414213) multiplier, reducing the number
of necessary real multiplies in this –π/8 rotation process[88]. However, be
aware that this (A’ –jB’) rotation induces a vector magnitude gain of 1.0824
(0.69 dB) in the rotated vector.
Here’s how we decide if the 1st-octant vector C lies in the Range 1 of
1st
π/8 ≤ θ ≤ π/4 radians. If the minimum of I or Q is less than 0.414213
1st 1st 1st
times the maximum of I or Q , then C lies in Region 1, in which case
1st 1st 1st
vector rotation by (A’ –jB’) multiplication is performed. Otherwise the 1st-
octant vector is in Range 2, requiring no rotation.
Imag
Range 1
(a)
Range 2
1st /8 (22.5o)
0
0 Real
Imag
0
8th o) Real
Range 3
(b)
Range 4
Figure 13–96 Angle ranges of the 1st and 8th octants.

810 Digital Signal Processing Tricks
In a similar manner, if an 8th-octant vector C resides in the angle
8th
range of –π/4 ≤θ ≤–π/8 radians (Range 4 in Figure 13–96(b)), we can rotate
8th
that vector by π/8 radians (22.5o), forcing the new vector into Region 3 by
multiplying C by (A’ + jB’).
8th
Again, the angle range reduction schemes in this section allow us to use
arctangent algorithms that are computationally simpler (and thus faster) for a
given accuracy. Of course, this technique forces us to perform additional
angle range checking and to compute products such as (I + jQ )(A’ –jB’).
1st 1st
Perhaps this scheme is most useful when used with an arctangent look-up
table method. You make the call.
13.39 AN EFFICIENT DIFFERENTIATING NETWORK
This section presents a computationally efficient differentiating network that
approximates the process of taking the derivative of a discrete time-domain
sequence. In Chapter 7 we introduced the central-difference differentiator, de-
fined by
y (n) = [x(n) – x(n–2)]/2, (13–159)
Cd
as a simple differentiating network that has desirable high-frequency (noise)
attenuation behavior. The frequency magnitude response of that differentia-
tor is the dashed |H (f)| curve in Figure 13–97(a). (For comparison, we
Cd
show an ideal differentiator’s straight-line |H (f)| magnitude response in
Ideal
Figure 13–97(a). The frequency axis in that figure covers the positive-
frequency range 0≤ω≤π samples/radian, corresponding to a continuous-time
frequency range of 0 to f/2, where f is the input sample rate in Hz.) The
s s
central-difference differentiator’s frequency range of linear operation is from
zero to roughly 0.08f Hz.
s
Here we recommend a computationally efficient differentiator that
maintains the central-difference differentiator’s beneficial high-frequency at-
tenuation behavior but extends its frequency range of linear operation. The
proposeddifferentiator is defined by
−x(n) x(n−6)
y (n)= +x(n−2)−x(n−4)+ . (13–160)
Pr 16 116
The Eq. (13–160) differentiator’s frequency magnitude response is the solid
|H (f)| curve in Figure 13–97(a), where its frequency range of linear opera-
Pr
tion extends from zero to approximately 0.17f Hz, roughly twice the usable
s
frequency range of the central-difference differentiator. The differentiator in
Eq. (13–160) has a gain greater than that of the central-difference differentia-
tor, so the solid curve in Figure 13–97(a) was scaled for easy comparison of
|H (f)| and |H (f)|. The |H (f)| curve is the DFT of 0.6 · y (n).
cd dif dif dif

13.39 An Efficient Differentiating Network 811
2.5
|H (f)|
2 Ideal
1.5
(a) |H (f)|
Pr
1
|H (f)|
0.5 Cd
0
0 0.1f 0.2f 0.3f 0.4f 0.5f
s s s s s
Freq (Hz)
0.08f 0.17f
s s
x(n) x(n–2) x(n–4) x(n–6)
z –2 z –2 z –2
–1/16 1/16
(b) –
y (n)
Pr
x(n–6) x(n–4)
z –2
x(n) x(n–2)
z –2 z –2
(c)
– –
1/16
y (n)
Pr
Figure 13–97 Proposed differentiator: (a) performance; (b) standard structure;
(c) folded structure.
The structure of the proposed differentiator is shown in Figure 13–97(b)
where a delay block comprises two unit-delay elements. The folded-FIR struc-
ture for this differentiator is presented in Figure 13–97(c) where only a single
multiply need be performed per y (n) output sample. The really slick aspect
Pr
of the y (n) differentiator is that its non-unity coefficients (±1/16) are integer
Pr
powers of two. This means that a multiplication in Figure 13–97 can be imple-
mented with an arithmetic right shift by four bits. Happily, such a binary
right-shift implementation is a linear-phase multiplierless differentiator.
Another valuable feature of the y (n) differentiator is that its time delay
Pr
(group delay) is exactly three samples. Such an integer delay makes this
differentiator convenient for use when the y (n) output must be time-
pr
synchronized with other signals within a system. For fairness, we point out
that the disadvantage of this very efficient differentiator is that for proper op-
eration its x(n) input signals must be low frequency, less than one-fifth the
input sample rate.

812 Digital Signal Processing Tricks
In terms of performance and computational efficiency, the only con-
tender to the proposed differentiator is the first narrowband “super Lanczos
low-noise differentiator” discussed in Chapter 7. However, the y (n) differen-
Pr
tiator proposed here has better high-frequency noise attenuation than the
Lanczos differentiator.
13.40 LINEAR-PHASE DC-REMOVAL FILTER
In this section we introduce a linear-phase DC-removal filter useful for re-
moving the DC bias from a time-domain signal. The filter is based on the no-
tion of subtracting an input signal’s moving average (DC bias) from that
signal, as shown in Figure 13–98(a).
In order to reduce the delay line length of a standard tapped-delay line
moving average network, we use the D-point recursive moving averager
(MA), shown in Figure 13–98(b). The bottom path, in Figure 13–98(b), is a
simple delay line having a length equal to the averager’s group delay,
(D–1)/2 samples. This enables us to time-synchronize the averager’s v(n) out-
put with the x(n) input in preparation for the subtraction operation. There are
two delay lines in Figure 13–98(b): the D-length z–Ddelay line in the top path
and the bottom path’s (D–1)/2-length delay line.
x(n) v(n) – y(n)
Moving average
(a)
Delay
Moving averager (MA)
x(n) v(n) – y(n)
–
(b) z–D z–1 1 D
z–(D–1)/2
0
B
d –2.9 dB
–10
(c)
D = 31
–20
0 0.1 0.2 0.3 0.4 0.5
Freq (times f)
s
Figure 13–98 DC-removal filter: (a) filter concept; (b) filter structure; (c) filter fre-
quency response.

13.40 Linear-Phase DC-Removal Filter 813
The D-point recursive moving averager (MA) in Figure 13–98(b) has a
transfer function defined by
1 1−z −D
H(z)= ⋅ . (13–161)
D 1−z −1
This DC-removal network’s passband performance, when D = 31, is
shown in Figure 13–98(c). (The frequency axis value of 0.5 corresponds to a
cyclic frequency of half the input signal’s f sample rate.) While the network
s
has the desired infinite attenuation at zero Hz, its passband peak-peak ripple
is unpleasantly large at 2.9 dB. We can do better, as we shall see.
If D is an integer power of two, the 1/Dscaling in (1) can be performed
using a binary right shift by log (D) bits, making Figure 13–98(b) a multiplier-
2
free network. However, in that scenario the MA’s group delay is not an inte-
ger number of samples, making it difficult to synchronize the delayed x(n)
and the v(n) sequences. To solve this problem we can use two cascaded
D-point MAs as shown in Figure 13–99(a). Because the cascaded MAs have an
integer group delay of D–1 samples, we can be clever and tap off the first
moving averager’s comb delay line, eliminating the bottom-path delay line in
13–98(b). This way we still only need implement two delay lines in Figure
13–99(a), one z–Ddelay line in each MA.
The magnitude response of the Figure 13–99(a) dual-MA DC-removal
network, for D = 32, is shown in Figure 13–99(c). In that figure we show the
DC-removal filter’s passband with its narrower transition region width and
a much improved peak-peak ripple of 0.42 dB. What we’ve created, then, is a
MA
x(n) – y(n)
MA
–
(a) z–D+1 z–1 1 D
z–1
0
B –0.42 dB
d
–2
(b)
D = 32
–4
0 0.1 0.2 0.3 0.4 0.5
Freq (times f)
s
Figure 13–99 Dual-MA filter: (a) filter structure; (b) filter frequency response.

814 Digital Signal Processing Tricks
linear-phase, multiplierless, DC-removal network having a narrow transition
region near zero Hz.
Happily, it’s worth noting that standard tapped-delay line, linear-phase,
highpass FIR filter designs using least-squares error minimization, or the
Parks-McClellan method, require more than 100 taps to approximate our D=
32 DC-removal filter’s performance.
On a practical note, the MAs in Figure 13–99(a) contain integrators that
can experience data overflow. (An integrator’s gain is infinite at DC.) Using
two’s complement fixed-point arithmetic avoids integrator overflow errors if
we ensure that the integrator (accumulator) bit width is at least
accumulator bits = number of bits in q(n) + <log (D)= (13–162)
2
where q(n) is the input sequence to an accumulator, and <k= means that if k is
not an integer, round it up to the next larger integer.
For an even narrower filter transition region width, in the vicinity of zero
Hz, than that shown in Figure 13–99(c), we can set Dto a larger integer power
of two; however, this will not reduce the DC-removal filter’s passband ripple.
At the expense of three additional delay lines, and four new addition
operations per output sample, we can implement the linear-phase DC-
removal filter shown in Figure 13–100(a). That quad-MA implementation,
having a group delay of 2D–2 samples, yields an improved passband peak-
peak ripple of only 0.02 dB, as shown in Figure 13–100(b), as well as a re-
duced-width transition region relative to the dual-MAimplementation.
The DC removal network in Figure 13–100(a) contains four 1/D scaling
operations which, of course, can be combined and implemented as a single
binary right shift by 4log2(D) bits. So the bottom line here is that at the ex-
x(n) – y(n)
MA MA MA
–
z–D+1 z–1 1 D
(a)
z–1 z–D+1
0
B –0.02 dB
d
–0.5
(b)
D = 32
–1.0
0 0.1 0.2 0.3 0.4 0.5
Freq (times f)
s
Figure 13–100 Quad-MA filter: (a) filter structure; (b) filter frequency response.

13.42 Efficient Linear Interpolation 815
pense of multiple delay lines, it is possible to efficiently perform linear-phase
DC removal.
13.41 AVOIDING OVERFLOW IN MAGNITUDE COMPUTATIONS
Here we present a little trick to help avoid a common problem when comput-
ing the magnitude of a complex number using fixed-point binary number for-
mats. Let’s say we have a complex number crepresented by c= R+ jI, and we
want to compute the magnitude cusing the familiar expression
|c|= R2 +I2.
(13–163)
Equation (13–163) is troublesome because the R2 and I2 terms will cause data
word overflow errors if either R or I is greater than the square root of your
fixed-point number format’s largest positive number. For example, in a
signed 16-bit number format, |R| and |I| must be less than 181 to avoid
overflow errors. At the expense of absolute value comparison, branch, and di-
vide operations, Eq. (13–164) alleviates overflow problems[89]:
⎧
⎪|R| 1+(I/R)2 if R ≥|I| (13–164)
|c|=⎨
⎩⎩ ⎪ |I| 1+(R/I)2 if R <|I|.
13.42 EFFICIENT LINEAR INTERPOLATION
In this section we present a computationally efficient linear interpolation trick
that’s useful because it performs linear interpolation requiring at most one
multiply per output sample[90]. For example, given the x(n) time sequence in
Figure 13–101(a), this linear interpolator will generate the y(n) sequence
shown in Figure 13–101(b) when the interpolation factor is L = 3. Notice how
the original x(n) samples are preserved in the y(n) output sequence.
The block diagram of this efficient linear interpolator is that in Figure
13–101(c). That mysterious block labeled “Hold Interpolator, L“ is merely the
operation where each input sample to the block is repeated L–1 times. For ex-
ample, if the input to the Hold Interpolator operation is {1,4,3}, and L= 3, the
output of the Hold Interpolator is {1,1,1,4,4,4,3,3,3}.
In fixed-point binary implementations if we’re able to select L to be an
integer power of two, then, happily, the final 1/Lmultiplication can be imple-
mented with a binary arithmetic right shift by log L bits, yielding a multipli-
2
erless linear interpolator. Of course, if a gain of Lis acceptable, no 1/Lscaling
need be performed at all.

816 Digital Signal Processing Tricks
4
x(n)
2
(a)
0
–2
0 1 2 3 n 4 5 6
4
y(n) [L= 3]
2
(b)
0
–2
0 5 10 n 15 20
x(n) Hold y(n)
Interpolator,L
–
(c)
z–1 z–1 1/L
Figure 13–101 Linear interpolation: (a) input sequence; (b) L= 3 interpolated se-
quence; (c) interpolator structure.
The neat part of this interpolator is that the computational workload, the
number of additions and multiplies per output sample, remains fixed regard-
less of the value of interpolation factor L.
The experienced reader might now say, “Ah, while this network is com-
putationally simple, linear interpolation is certainly not the most accurate
method of interpolation, particularly for large interpolation factors of L.” That
is true, but if interpolation is being done in multiple sections, using this effi-
cient linear interpolation as the final section at the highest data rate (when the
signal samples are already very close together) will introduce only a small in-
terpolation error.
13.43 ALTERNATE COMPLEX DOWN-CONVERSION SCHEMES
Here we present two interesting complex down-conversion and decimation
techniques used to generate an analytic (complex) version, centered at zero
Hz, of a real bandpass signal that was originally centered at ±f/4 (one-fourth
s
the sample rate). Both methods perform signal frequency translation by way
of decimation.

13.43 Alternate Complex Down-Conversion Schemes 817
13.43.1 Half-band Filter Down-conversion
The first complex down-conversion method makes use of computationally ef-
ficient half-band filters[91]. The process is shown in Figure 13–102(a), where
we use indices n, p, and m to clarify the multirate nature of this process. The
real x (n) input signal has the spectrum shown in Figure 13–102(b), and for
R
our example the sample rate is f = 24 kHz. The Delay/Hilbert transform filter
s
combination attenuates the negative-frequency spectral components of X (f)
R
to produce the complex u(n) + ju (n) signal whose spectrum is provided in Fig-
I Q
ure 13–102(c). (The Delay function is a cascade of unit-delay elements, whose
length is the group delay of the Hilbert filter, needed to time-synchronize the
u(n) and u (n) sequences.) The follow-on downsample by two, discard every
I Q
other sample, produces the complex v(p) sequence having the spectrum
shown in Figure 13–102(d) where the new sample rate is 12 kHz.
Next, sequences v(p) and v (p) are applied to two identical real-valued
I Q
highpass half-band filters, each having the frequency magnitude response
u(n) v(p) w(p) x(m)
Delay I 2 I h (k) I 2 I
hp
x (n)
R
x(m)
(a) Hilbert u (n) v (p) w (p) x (m) c
filter Q 2 Q h (k) Q 2 Q
hp
[h (k)]
hilb
x(m) =x(m) +jx (m)
c I Q
|X (f)|
R
(b)
kHz
–12 –8 –4 0 4 8 12 16 20 24 28 32 36
(–f /2) (f /2) (f)
s s s
|U(f)|
(c)
kHz
–12 –8 –4 0 4 8 12 16 20 24 28 32 36
(–f /2) (f /2) (f)
s s s
|V(f)|
(d)
kHz
–12 –8 –4 0 4 8 12 16 20 24 28 32 36
(–f ) (f ) (2f )
s (f/2) s s
s
Figure 13–102 Analytic signal generation and decimation by two.

818 Digital Signal Processing Tricks
|H (f)|
hp
(a)
kHz
–12 –8 –4 0 4 8 12 16 20 24 28 32 36
(–f) (f) (2f)
s (f/2) s s
s
|W(f)|
(b)
kHz
–12 –8 –4 0 4 8 12 16 20 24 28 32 36
(–f) (f) (2f)
s (f/2) s s
s
|X(f)|
c
(c)
kHz
–12 –8 –4 0 4 8 12 16 20 24 28 32 36
(–f s ) (f s ) (2f s ) (4f s )
Figure 13–103 Highpass filtering, down-conversion, and decimation by two.
shown in Figure 13–103(a), yielding the complex w(p) = w(p) and w (p)
I Q
whose spectrum is that in Figure 13–103(b). The final step in this down-
conversion process is another decimation by two, producing the desired x (m)
c
sequence having the spectrum given in Figure 13–103(c) where the output
sample rate is 6 kHz. Due to the nature of half-band filters there will be some
amount of spectral overlap in X (f) as shown in Figure 13–103(c). The amount
c
of spectral overlap is proportional to the transition region width of an h (k)
hp
filter (inversely proportional to the number of filter taps).
There are three useful aspects to this first complex down-conversion
scheme that enhance its computational efficiency:
• If the Hilbert transform filter has an odd number of taps, roughly half of
its coefficients will be zero-valued, and the Delay function is an integer
number of unit-delay elements.
• Roughly half of the coefficients of the highpass half-band filters, with
their transition regions centered at f/4 and 3f/4, will be zero-valued.
s s
• Because the coefficients of the filters in Figure 13–102(a) are either sym-
metrical or antisymmetrical, we can use the folded FIR filter scheme de-
scribed in Section 13.7 to reduce the number of multipliers by another
factor of two.

13.43 Alternate Complex Down-Conversion Schemes 819
13.43.2 Efficient Single-Decimation Down-conversion
Our second complex down-conversion trick is a very computationally efficient
scheme, shown in Figure 13–104(a), that operates on real x (n) signals centered
R
at ±f/4. Just as in Figure 13–102(a), the Delay/Hilbert transform filter combi-
s
nation attenuates the negative-frequency spectral components of x (n) to pro-
R
duce a complex analytic signal whose spectrum is centered at f/4 (6 kHz). The
s
downsample-by-four, retain every fourth sample, operation down-converts
(frequency translates) the desired complex signal originally centered at f/4 Hz
s
down to a center frequency of zero Hz. The compensation filter is used to com-
pensate for the non-flat frequency magnitude response of the simple 2-tap
Hilbert filter in order to widen the down-converter’s usable passband width.
(The Delay function after the downsampling in the top path is needed to time-
synchronize the x(m) and x (m) sequences.) The detailed block diagram of the
I Q
down-converter is shown in Figure 13–104(b), where the compensation filter’s
coefficients are h (0) = –1/32, and h (1) = 1/2 + 1/16.
c c
x(m)
I
Delay 4 Delay
x (n)
R x(m)
(a) Hilbert Compensation x Q (m) c
filter 4
filter [h(k)]
[h (k)] c
hilb x(m) =x(m) +jx (m)
c I Q
x (n) x(m)
R z–1 4 z–1 I
x(m)
z–1 x (m) c
z–1 Q
+
–
(b)
4 z–1
Hilbert filter
h(0) h(1)
h(0) = –1/32 c c
c
h(1) = 1/2 + 1/16
c
Compensation filter
Figure 13–104 High-efficiency complex down-conversion: (a) process; (b) de-
tailed structure.

820 Digital Signal Processing Tricks
If the x (n) input signal’s bandwidth is no greater than f/6, then the
R s
Hilbert filter attenuates x (n)’s undesired negative-frequency spectral compo-
R
nents, at the x (n) output, by approximately 35 dB. That much attenuation
c
may not be something to write home about, but keep in mind that this down-
converter requires no multipliers because the multiplies by the h (0) and h (1)
c c
coefficients can be implemented with binary shifts and adds. At the expense
of two multiplies per output sample, the compensation filter coefficients can
be set to h (0) = –0.02148 and h (1) = 0.54128 to attenuate x (n)’s undesired
c c R
negative-frequency spectral components by roughly 45 dB.
13.44 SIGNAL TRANSITION DETECTION
When we are tasked to build a system that must detect transitions in a pulsed
signal, we generally look to a digital differentiator as the solution to our prob-
lem. However, when a pulsed signal’s transition spans many samples, and
particularly if the signal is noisy, digital differentiators do not provide reliable
signal transition detection. One compelling solution to this problem uses a
standard tapped-delay line (time-domain convolution) filtering scheme de-
veloped by C. Turner[92]. Called time-domain slope filtering, the transition de-
tection tapped-delay line filter uses Ncoefficients defined by
12k−6(N−1)
C = − (13–165)
k N3 −N
where the coefficient index kcovers the range 0 ≤k≤N–1.
For any integer N, the slope filtering C coefficients comprise a linear
k
ramp, making that sequence quite useful for detecting linear transitions in an
input signal. Figure 13–105(a) shows the output of the time-domain slope fil-
tering process, when N = 53. In that figure we see that the slope filter per-
forms well in detecting the transitions of the Input signal. The dotted curve in
Figure 13–105(a) is the output of a traditional tapped-delay line digital differ-
entiator having 53 taps. (The frequency magnitude of the traditional digital
differentiator, specifically designed to attenuate high-frequency noise, is pro-
vided in Figure 13–105(b).)
The superiority of the time-domain slope filtering scheme over tradi-
tional differentiation is further illustrated in Figure 13–105(c) where the
pulsed Input signal is contaminated with high-level noise.
Concerning two practical issues, if the number of samples in a pulsed
input signal’s transition is L, the value for N, found empirically, is generally
in the range of L/4 to L. It’s convenient to set N to be an odd integer, forcing
the filter’s delay to be an integer number, (N–1)/2, of samples. This facilitates
the time synchronization of the filter’s output to other sequences in a system.

13.45 Spectral Flipping around Signal Center Frequency 821
1 Slope filtering output (solid)
Input
0.5
(a)
0
–0.5
Differentiator
output (dotted)
–1
0 200 400 600 800 1000 1200
Time samples
0.4
|H(f)|
(b)
0.2
0
0 0.1f 0.2f 0.3f 0.4f
s s Frequency s s
1 Slope filtering output (solid)
Input
Differentiator
0.5 output (dotted)
(c)
0
–0.5
–1
0 200 400 600 800 1000 1200
Time samples
Figure 13–105 Time-domain slope filtering: (a) pulsed input performance;
(b) digital differentiator magnitude response; (c) high-noise input
performance.
Also, if the C coefficients are to be used in correlation processing (as opposed
k
to the convolution processing discussed above), the correlation’s C coeffi-
k
cients should be the coefficients from Eq. (13–165) reversed in time order.
13.45 SPECTRAL FLIPPING AROUND SIGNAL CENTER FREQUENCY
In Section 2.4, we discussed a super-simple method of spectral flipping (spec-
tral inversion) of a real signal where the center of spectral rotation was f/4. In
s
this section we discuss a different kind of spectral flipping process.

822 Digital Signal Processing Tricks
Consider the situation where we need to process a real-valued x(n) time
signal, whose X(f) spectrum is shown in Figure 13–106(a), to obtain a real-
valued y(n) time signal whose spectrum is the flipped Y(f) spectrum shown in
Figure 13–106(b). Notice that the center of rotation of the desired spectral flip-
ping is not f/4 Hz but is instead the x(n) signal’s f center frequency. The
s c
spectral flipping process described in Section 2.4 does not solve our problem
because that process would result in the undesirable spectrum shown in Fig-
ure 13–106(c), where the original X(f) spectrum is the dashed curve.
There are two methods to solve our f -centered spectral flipping prob-
c
lem. Figure 13–107(a) shows the first method, comprising a multirate process-
ing technique. In considering this spectral flipping method, the user should
keep in mind that:
• The two lowpass filters (LPFs) have passbands that extend from zero Hz
to f Hz. (Note that the sample rate for both filters is 2f Hz.) The second
H s
LFP’s transition region width is less than 2f .
L
• The cosine mixing sequence uses the upsampled-by-two time index
variable n’.
• The multiply-by-four operation compensates for the sequence u(n’) am-
plitude loss by a factor of two caused by interpolation, and the ampli-
tude loss by another factor of two due to the cosine mixing.
Of course, a smart engineer will eliminate the multiply-by-four operation alto-
gether by increasing the DC (zero Hz) gain of one of the lowpass filters by four.
The second method we could use to obtain a signal having the desired
Figure 13–106(b) spectrum, promoted by D. Bell, is the process shown in Fig-
|X(f)|
...
(a)
Hz
0 f/2 f
f s s
c
f f
L H
|Y(f)|
...
(b)
0 f c f s /4 f s /2 f s Hz
...
(c)
0 f s /4 f s /2 3f s /4 f s Hz
Figure 13–106 Spectral flipping, centered at f: (a) original spectrum; (b) desired
c
spectrum; (c) incorrect spectrum.

13.46 Computing Missing Signal Samples 823
Interpolation by two
4
x(n) u(n') y(n)
(a) 2 LPF LPF 2
f 2f f
s s cos( n'(f+f )/f) s
L H s
2
x(n) Complex u(n) Real v(n) y(n)
(b)
LPF Part
j2 nf/f j2 nf/f
e c s e c s
Figure 13–107 Spectral flipping techniques: (a) first method; (b) second method.
ure 13–107(b)[93]. While somewhat more computationally intensive than the
above multirate method, this technique works well and deserves mention
here. The first complex multiplication and the Complex LPF are identical to
the quadrature sampling operations we discussed in Figure 8–18(a). The two
identical lowpass filters, comprising the Complex LPF, have passbands that
extend from zero Hz to (f – f )/2 Hz, and transition region widths of less than
H L
2f . The Real Part operation merely means take the real part of sequence v(n).
L
We can eliminate the multiply-by-two operation by increasing the DC
(zero Hz) gain of the complex filter by two. In this method, as Bell recom-
mends, we can combine the second complex multiply and Real Part extrac-
tion stages by computing only the real part of sequence u(n), yielding
sequence v(n). The multiply-by-two operation compensates for the amplitude
loss by a factor of two caused by the Real Part operation.
13.46 COMPUTING MISSING SIGNAL SAMPLES
Consider the situation where we need to process a time-domain signal that
has been corrupted such that every Qth sample is missing from the desired
signal sequence. This section provides a trick for how to recover periodically
spaced missing samples of a corrupted time sequence[94].
To explain our problem, assume we want to process an x (n) time se-
0
quence, whose sample rate is f Hz, but all we have available to us is a cor-
s
rupted x (n) sequence where:
q
• x (n) is equal to the desired x (n) with every Qth sample of x (n) missing.
q 0 0
The missing samples in x (n), x (pQ) where p= 0, 1, 2, ... are represented
q q
by zero-valued samples.

824 Digital Signal Processing Tricks
• x (n) is band-limited with negligible energy above BHz where
0
Q−1 f
B< ⋅ s (13–166)
Q 2
for some integer Q≥2 where f is the data sample rate in Hz.
s
As an example, when Q = 5, if the desired x (n) is the sequence in Figure
0
13–108(a), then x (n) is the corrupted sequence shown in Figure 13–108(b).
q
Our job, then, is to recover (interpolate) the missing samples in x (n), x (0),
q q
x (5), x (10), ... etc., to reconstruct the desired x (n) sequence.
q q 0
The solution to our problem is to apply the x (n) sequence to the tapped-
q
delay line reconstruction filter shown in Figure 13–109. Describing Figure
13–109’s operation in words: our desired x (n–K) samples are the x (n–K)
o q
samples at the center tap of the filter unless that x (n–K) sample is a zero-
q
valued missing sample, in which case the switches toggle and we compute
the estimated x (n–K) = x (pQ).
o o
The filter’s c(k) coefficients are determined by first evaluating the fol-
lowing expression:
h(k)=
Q−1
⋅sinc
⎛
⎜
Q−1
k
⎞
⎟ ⋅ w(k) (13–167)
⎝ ⎠
Q Q
where integer index kis in the range –K≤k≤K, sinc(x) = sin(πx)/πx, and w(k)
is a time-symmetric window sequence of length 2K+1. Next, we use h(k) to
compute our desired filter coefficients as
2
x(n)
0
(a) 0
–2
0 10 20 30 40 50 60
n
2
x (n)
q
(b) 0
–2
0 10 20 30 40 50 60
n
Figure 13–108 Time sequences: (a) original x(n); (b) corrupted x(n) when Q= 5.
0 q

13.46 Computing Missing Signal Samples 825
x (n)
q
c(–K)
z –1
... c(–K+1)
x(n–K) when
0
z –1 c(–1) n–k pQ
x (n–K) y(n)
q
c(1)
z –1
Whenn–k =pQ,
... c(K–1)
toggle all switches
and compute the
convolution sum
z –1 c(K) y(n)~~x(pQ)
o
Figure 13–109 Reconstruction filter implementation.
⎧ 0, k =0
c(k)= ⎨ (13–168)
⎩h(k)/[1−h(0)], k ≠0.
This missing sample recovery process can also be applied to complex
x (n) signals, in which case the real and imaginary parts of a complex x (n)
q q
must be filtered separately.
There are two practical considerations to keep in mind when using this
missing sample recovery process. The first consideration is to be aware that
the maximum bandwidth Bgiven in Eq. (13–166) is based on the assumption that
the reconstruction filter has an infinite number of taps. As such, for practical-
length filters the B bandwidth requirement must be reduced. To show this,
Figure 13–110 illustrates the missing sample recovery error when Q = 5, B =
0.4f, using a Chebyshev window with –100 dB sidelobes, for various values of
s
K. The input signal is a noiseless sinusoid, with unity peak amplitude, swept
in frequency from a very low frequency up to f /2 (half the sample rate).
s
In that figure we see that a K= 3 filter (7 taps) exhibits low missing sam-
ple recovery error until the input signal’s frequency approaches roughly
0.25f , where the recovery error starts to become large. When K= 5, the recov-
s
ery error doesn’t become large until the input signal’s frequency approaches
roughly 0.3f . (The unlabeled curve in Figure 13–110 is a K= 7 curve.) So what
s
we see is that to minimize our missing sample recovery error for short-length

826 Digital Signal Processing Tricks
B
or| 0
err
x –2
a
m K = 3
| g10 –4
L o K = 5 K = 23
–6
0 0.1f 0.2f 0.3f 0.4f 0.5f
s s s s s
Frequency
Figure 13–110 Recovery error curves, for various K, versus input tone frequency.
filters, the maximum input signal bandwidth must be kept substantially
lower than the BHz specified in Eq. (13–166).
The second practical consideration to consider when using this missing
sample recovery process is the w(k) window sequence in Eq. (13–167). There
seems to be no “best” window sequence that minimizes the recovery error for
all real-world signals that we might encounter. So experimentation, using
various window functions, becomes necessary. Agood place to start is to use
either Kaiser or Chebyshev window sequences whose control parameters are
set such that the windows’ frequency-domain sidelobes are very low relative
to their main lobe levels.
We conclude this section by mentioning that reference [95] describes a
missing sample recovery technique that is applicable when the pattern of
missing samples is more complicated than the simple every Qth sample de-
scribed here.
13.47 COMPUTING LARGE DFTS USING SMALL FFTS
It is possible to compute N-point discrete Fourier transforms (DFTs) using
radix-2 fast Fourier transforms (FFTs) whose sizes are less than N. For exam-
ple, let’s say the largest size FFT software routine we have available is a 1024-
point FFT. With the following trick we can combine the results of multiple
1024-point FFTs to compute DFTs whose sizes are greater than 1024.
The simplest form of this idea is computing an N-point DFT using two
N/2-point FFT operations. Here’s how the trick works for computing a 16-
point DFT, of a 16-sample x(n) input sequence, using two 8-point FFTs. First
we perform an 8-point FFT on the x(n) samples where n = 0, 2, 4, ..., 14. We’ll
call those FFT results X (k). Then we store two copies of X (k) in Memory
0 0
Array 1 as shown in Figure 13–111. Next we compute an 8-point FFT on the

13.47 Computing Large DFTs Using Small FFTs 827
x(n) samples where n= 1, 3, 5, ..., 15. We call those FFT results X (k). We store
1
two copies of X (k) in Memory Array 3 in Figure 13–111.
1
In Memory Array 2 we have stored 16 samples of one cycle of the com-
plex exponential e–j2πm/N, where N = 16, and 0 ≤ m ≤ 15. Finally we compute
our desired 16-point X(m) samples by performing the arithmetic shown in
Figure 13–111 on the horizontal rows of the memory arrays. That is,
X(0) = X (0) +
e–j2π0/16X
(0),
0 1
X(1) = X (1) +
e–j2π1/16X
(1),
0 1
. . .
X(15) = X (7) +
e–j2π15/16X
(7).
0 1
The desired X(m) DFT results are stored in Memory Array 4.
We describe the above process, algebraically, as
X(k) = X (k) +
e–j2πk/16X
(k) (13–169)
0 1
and
X(k+ 8) = X (k) +
e–j2π(k+8)/16X
(k) (13–169’)
0 1
for kin the range 0 ≤k≤7.
Memory Memory Memory Memory
Array 4 Array 1 Array 2 Array 3
X(0) X(0) e–j20/16 X(0)
0 1
X(1) X(1) e–j21/16 X(1)
0 1
X(2) X(2) e–j22/16 X(2)
0 1
. . . . . . . . . . . .
X(6) X(6) e–j26/16 X(6)
0 1
X(7) X(7) e–j27/16 X(7)
= 0 + 1
X(8) X(0) e–j28/16 X(0)
0 1
X(9) X(1) e–j29/16 X(1)
0 1
X(10) X(2) e–j210/16 X(2)
0 1
. . . . . . . . . . . .
X(14) X(6) e–j214/16 X(6)
0 1
X(15) X(7) e–j215/16 X(7)
0 1
Figure 13–111 A 16-point DFT using two 8-point FFTs.

828 Digital Signal Processing Tricks
Notice that we did nothing to reduce the size of Memory Array 2 due to
redundancies in the complex exponential sequence
e–j2πm/N.
As it turns out,
for an N-point DFT, only N/4 complex values need be stored in Memory
Array 2. The reason for this is that
e −j2(m+N/2)/N =−e −j2m/N, (13–170)
which involves a simple sign change on
e–j2πm/N.
In addition,
e −j2(m+N/4)/N =−je −j2m/N, (13–170’)
which is merely swapping the real and imaginary parts of
e–j2πm/Nplus
a sign
change of the resulting imaginary part. So Eqs. (13–170) and (13–170’) tell us
that only the values e–j2πm/N for 0 ≤ m ≤ N/4–1 need be stored in Memory
Array 2. With that reduced storage idea aside, to be clear regarding exactly
what computations are needed for our “multiple-FFTs” technique, we leave
Memory Array 2 unchanged from that in Figure 13–111.
The neat part of this “multiple-FFTs” scheme is that our DFT length, N, is
not restricted to be an integer power of two. We can use computationally effi-
cient radix-2 FFTs to compute DFTs whose lengths are any integer multiple of
an integer power of two. For example, we can compute an N = 24-point DFT
using three 8-point FFTs. To do so, we perform an 8-point FFT on the x(n) sam-
ples, where n= 0, 3, 6, ..., 21, to obtain X (k). Next we compute an 8-point FFT
0
on the x(n) samples, where n = 1, 4, 7, ..., 22, to yield X (k). And then we per-
1
form an 8-point FFT on the x(n) samples, where n= 2, 5, 8, ..., 23, to obtain an
X (k) sequence. Finally, we compute our desired 24-point DFT results using
2
X(k) = X (k) +
e–j2πk/24X
(k) +
e–j2π(2k)/24X
(k) (13–171)
0 1 2
X(k+ 8) = X (k) +
e–j2π(k+8)/24X
(k) +
e–j2π[2(k+8)]/24X
(k) (13–171’)
0 1 2
and
X(k+ 16) = X (k) +
e–j2π(k+16)/24·
X (k) +
e–j2π[2(k+16)]/24·
X (k) (13–171’’)
0 1 2
for k in the range 0 ≤ k ≤ 7. The memory-array depiction of this process is
shown in Figure 13–112, with our final 24-point DFT results residing in Mem-
ory Array 6. Memory Array 2 contains N = 24 samples of one cycle of the
complex exponential e–j2πm/24, where 0 ≤ m ≤ 23. Memory Array 4 contains 24
samples of two cycles of the complex exponential
e–j2π(2m)/24.
To conclude this section, we state that the larger the size of the FFTs, the
more computationally efficient is this “multiple-FFTs” spectrum analysis tech-
nique. This behavior is illustrated in Figure 13–113 where we show the number
of complex multiplies required by the “multiple-FFTs” algorithm versus the de-
sired DFT size (N). The top bold curve is the number of complex multiplies re-
quired by the standard (inefficient) DFT algorithm, and the bottom dashed
curve is the number of complex multiplies required by a single N-point radix-2

13.47 Computing Large DFTs Using Small FFTs 829
Memory Memory Memory Memory Memory Memory
Array 6 Array 1 Array 2 Array 3 Array 4 Array 5
X(0) X(0) e–j20/24 X(0) e–j20/24 X(0)
0 1 2
X(1) X(1) e–j21/24 X(1) e–j22/24 X(1)
0 1 2
. . . . . . . . . . . . . . . . . .
X(7) X(7) e–j27/24 X(7) e–j214/24 X(7)
0 1 2
X(8) X(0) e–j28/24 X(0) e–j216/24 X(0)
0 1 2
X(9) X(1) e–j29/24 X(1) e–j218/24 X(1)
= 0 + 1 + 2
. . . . . . . . . . . . . . . . . .
X(15) X(7) e–j215/24 X(7) e–j230/24 X(7)
0 1 2
X(16) X(0) e–j216/24 X(0) e–j232/24 X(0)
0 1 2
X(17) X(1) e–j217/24 X(1) e–j234/24 X(1)
0 1 2
. . . . . . . . . . . . . . . . . .
X(23) X(7) e–j223/24 X(7) e–j246/24 X(7)
0 1 2
Figure 13–112 A 24-point DFT using three 8-point FFTs.
FFT. The curves in the center of the figure show the number of complex multi-
plies required by the “multiple-FFTs” algorithm when various FFT sizes (P) are
used to compute an N-point DFT. For example, if we must perform a 4096-
point DFT using this “multiple-FFTs” algorithm, it’s better for us to perform
sixteen 256-point FFTs rather than one hundred twenty-eight 32-point FFTs.
109
N2
108
P=4
P=8
e
s 107 P=16
ulti
pli
P=32
x
m
106
P=64
pl e P=128
m
#
of
c o 105 P=256 Sing
F
le
FT
N-pt
104
103
0 2k 4k 6k 8k 10k 12k 14k 16k 18k
N(DFT size)
Figure 13–113 Number of complex multiplies versus N.

830 Digital Signal Processing Tricks
13.48 COMPUTING FILTER GROUP DELAY WITHOUT ARCTANGENTS
Here we present an interesting scheme used to compute the group delay of
digital filters that does not require the phase unwrapping process needed
when computing arctangents in traditional group delay measurement algo-
rithms. The technique is based on the following: Assume we have the
N-sample h(k) impulse response of a digital filter, with k (0″k″N–1) being our
time-domain index, and that we represent the filter’s discrete-time Fourier
transform (DTFT), H(ω), in polar form as
H(ω) = M(ω)ejφ(ω). (13–172)
In Eq. (13–172), M(ω) is the frequency magnitude response of the filter, φ(ω) is
the filter’s phase response, and ω is continuous frequency measured in radi-
ans/second. Taking the derivative of H(ω) with respect to ω, and performing
a variety of algebraic acrobatics, we can write
jd[H(ω)]/dω d[φ(ω)] d[M(ω)]/ddω
=− + j . (13–173)
M(ω)⋅ejφ(ω) dω M(ω)
So what does that puzzling gibberish in Eq. (13–173) tell us? As it turns
out, it tells us a lot if we recall the following items:
• jd[H(ω)]/dω= the DTFT of k· h(k)
• M(ω) · ejφ(ω)= H(ω) = the DTFT of h(k)
• –d[φ(ω)]/dω= group delay of the filter
Now we are able to translate Eq. (13–173) into the meaningful expression
DTFT[k⋅h(k)] d[M(ω)]/ddω
=groupdelay+ j . (13–173’)
DTFT[h(k)] M(ω)
Discretizing expression (13–173’) by replacing the DTFT with the discrete
Fourier transform (DFT), we arrive at our scheme for computing the group
delay of a digital filter, measured in samples:
⎡ DFT[k⋅h(k)] ⎤
Filtergroupdelay = real⎢ ⎥. (13–174)
⎣ DFT[h(k)]] ⎦
So, starting with a filter’s N-sample h(k) impulse response, performing
two N-point DFTs and an N-sample complex division, we can compute the fil-
ter’s passband group delay. (Of course, to improve our group delay granular-
ity we can zero-pad our original h(k) before computing the DFTs). Again, the
advantage of the process in expression (13–174) is that the phase unwrapping

13.49 Computing a Forward and Inverse FFT Using a Single FFT 831
.
0.6 k h(k)
e
d 0.4
u
(a) plit 0.2
m 0
A h(k)
–0.2
0 5 10 15 20 24
k
s) 3
e
pl
m 2
a
(b)
el.
( s 1
d
p. 0
Gr –0.5f
s Frequ
0
ency
0.5f
s
s) 3
e
pl
m 2
(c) a
s
el.
( 1
d
p. 0
Gr –0.5f
s Frequ
0
ency
0.5f
s
Figure 13–114 Group delay computation: (a) 25-sample h(k) and k·h(k); (b) 25-
point group delay; (c) 64-point group delay.
process needed in traditional group delay algorithms is not needed here. Note
that in implementing the process in expression (13–174), we must be prepared
to accommodate the situation where a frequency-domain DFT[h(k)] sample is
zero-valued, which will make a group delay sample unrealistically large.
As an example, the square dots in Figure 13–114(a) show the N = 25-
sample h(k) impulse response of a 2nd-order IIR lowpass filter. A 25-sample
filter group delay estimation, using expression (13–174), is shown in Figure
13–114(b). When we zero-pad the h(k) and k · h(k) sequences to a length of 64
samples (0≤k≤63), expression (13–174) yields the group delay estimate in Fig-
ure 13–114(c).
13.49 COMPUTING A FORWARD AND INVERSE FFT USING
A SINGLE FFT
In Section 13.5 we described the processes of using a single N-point complex
FFT to perform both a 2N-Point Real FFT and two independent N-Point Real
FFTs. This section presents the algorithm for simultaneously computing a for-
ward FFT and an inverse FFT using a single radix-2 FFT[96].

832 Digital Signal Processing Tricks
X(m) y(n) S1: v(0) = 2y(0),
v(p) = y(p) + y(N–p)
+ j[y(p) – y(N–p)]
for p = 1, 2,...,N/2–1,
v(N/2) = 2y(N/2)
S2: v(q) = y(N/2–p)*,
for q = N/2+1,N/2+2...,N–1
v(n)
S3: z(n) = Real{X(n)} – Imag{v(n)} + j[Imag{X(n)} + Real{v(n)}],
for n = 0, 1,...,N–1
z(n)
S4: Z(m) = FFT{z(n)},
for m = n = 0, 1,...,N–1
Z(m)
S6: Y(0) = Imag{Z(0)}/2,
S5: x(0) = Real{Z(0)}, Y(p) = [Imag{Z(p)} + Imag{Z(N–p)}]/4
x(p) = Real{Z(N–p)}, + j[Imag{Z(N–p)} – Imag{Z(p)}]/4
for p = 1, 2,...,N–1 for p = 1, 2,...,N/2–1,
Y(N/2) = Imag{Z(N/2)}/2
S7: Y(q) = Y(N/2–p)*,
for q = N/2+1,N/2+2...,N–1
x(n) Y(m)
Figure 13–115 Simultaneous FFT and inverse FFT algorithm.
Our algorithm is depicted by the seven steps, S1 through S7, shown in
Figure 13–115. In that figure, we compute the x(n) inverse FFT of the N-point
frequency-domain conjugate-symmetric input sequence X(m), as well as com-
pute the Y(m) forward FFT of the N-point time-domain real-valued input se-
quence y(n) using the single complex FFT in Step S4. Sample indices nand m
both range from 0 to N–1 where Nis an integer power of two.
At first glance Figure 13–115 looks more complicated than it actually is,
and here’s why:
• Steps S1 and S2 create a complex sequence that we call v(n).
• Step S1 generates the first N/2+1 samples of v(n) based on the real-
valued input sequence y(n).
• Step S2 extends v(n) to a length of Nsamples and forces v(n) to be conju-
gate symmetric. The “*” symbol in Step S2 means conjugation.

13.50 Improved Narrowband Lowpass IIR Filters 833
• Step S3 combines the conjugate-symmetric sequences X(m) and v(n) to
create a sequence we call z(n). (Sequence z(n) is not conjugate symmetric.)
• Step S4 is the algorithm’s single radix-2 FFT operation, generating com-
plex sequence Z(m).
• Step S5 generates the desired real-valued x(n) time sequence by per-
forming a circular reversalof the real part of Z(m). (That is, other than the
first sample, the real parts of Z(m) samples are reversed in order to pro-
duce x(n). This type of sequence reversal is discussed in Appendix C.)
• Steps S6 and S7 generate the desired frequency-domain Y(m) sequence.
• Step S6 generates the first N/2+1 samples of Y(m).
• Step S7 extends the sequence from Step S6 to a length of Nsamples and
forces conjugate symmetry, to produce Y(m). The “*” symbol in Step S7
means conjugation.
The Figure 13–115 algorithm’s computational workload is one complex
N-point FFT and roughly 2Nadditions/subtractions.
13.50 IMPROVED NARROWBAND LOWPASS IIR FILTERS
Due to their resistance to quantized-coefficient errors, traditional 2nd-order
infinite impulse response (IIR) filters are the fundamental building blocks in
computationally efficient high-order IIR digital filter implementations. How-
ever, when used in fixed-point number systems, the inherent properties of
quantized-coefficient 2nd-order IIR filters do not readily permit their use in
narrowband lowpass filtering applications. Narrowband lowpass IIR filters
have traditionally had a bad reputation—for example, MATLAB’s Signal Pro-
cessing Toolbox documentation warns: “All classical IIR lowpass filters are
ill-conditioned for extremely low cutoff frequencies.”
This section presents a neat trick to overcome the shortcomings of nar-
rowband 2nd-order lowpass IIR filters, with no increase in filter coefficient bit
widths and no increase in the number of filter multiplies per output sample.
13.50.1 The Problem with Narrowband Lowpass IIR Filters
Narrowband lowpass IIR filters are difficult to implement because of intrinsic
limitations on their z-plane pole locations. Let’s examine the restrictions on
the z-plane pole locations of a standard 2nd-order IIR filter whose structure is
shown in Figure 13–116(a).
Such an IIR filter, having a transfer function given by
1 1
H(z)= = , (13–175)
1−2rcos(θ)z −1+r2z −2 (1−rejθ
z
−1)(1−re −−jθ
z
−1)

834 Digital Signal Processing Tricks
X(z)
X(z) Y(z)
rcos( )
z 1
z 1
rsin( )
Y(z)
2rcos((cid:6))
z 1 rcos( )
z 1
r2
rsin( )
(a) (b)
Figure 13–116 Second-order IIR filters: (a) standard form; (b) coupled form.
has a pair of conjugate poles located at radii of r, at angles of ±θradians. (For
filter stability reasons, we always ensure that r< 1.) In fixed-point implemen-
tations, quantizing the 2rcos(θ) and –r2 coefficients restricts the possible pole
locations[97,98]. On the z-plane, a pole can only reside at the intersection of a
vertical line defined by the quantized value of 2rcos(θ) and a concentric circle
whose radius is defined by the square root of the quantized value of r2. For
example, Figure 13–117 shows the first quadrant of possible z-plane pole loca-
tions using five magnitude bits to represent the filter’s two coefficients. No-
tice the irregular spacing of those permissible pole locations. (Due to
trigonometric symmetry, the pole locations in the other three quadrants of the
z-plane are mirror images of those shown in Figure 13–117.)
So here’s the problem we have with standard 2nd-order IIR filters: If we
use floating-point software to design a very narrowband (high-order) low-
pass IIR filter (implemented as cascaded 2nd-order filters) having poles resid-
ing in the shaded area near z= 1, subsequent quantizing of the designed filter
coefficients to five magnitude bits will make the poles shift to one of the loca-
tions shown by the dots on the border of the shaded region in Figure 13–117.
Unfortunately that pole shifting, inherent in the Figure 13–116(a) IIR filter im-
plementation due to coefficient quantization in fixed-point systems, prevents
us from realizing the desired narrowband lowpass filter. We can always re-
duce the size of the shaded forbidden zone near z = 1 in Figure 13–117 by in-
creasing the number of bits used to represent the 2nd-order filters’
coefficients. However, in some filter implementation scenarios increasing co-
efficient binary-word bit widths may not be a viable option.
One solution to the above problem is to use the so-called coupled-form
IIR filter (also called the Gold-Rader filter[99]) structure, shown in Figure
13–116(b), having a transfer function given by

13.50 Improved Narrowband Lowpass IIR Filters 835
1
z-plane
0.8
0.6
art
p
g.
a
m
I 0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
Realpart
Figure 13–117 Possible pole locations for five magnitude bit coefficient
quantization.
rsin(θ)z −1
H (z)= . (13–176)
cf 1−2rcos(θ)z −1+r2z −2
Because the coupled-form filter’s quantized coefficients in Figure 13–116(b)
are linear in rcos(θ) and rsin(θ), its possible pole locations are on a regularly
spaced grid on the z-plane defined by z= rcos(θ) + jrsin(θ). This enables us to
build 2nd-order narrowband lowpass IIR filters with poles in the desired
shaded region of Figure 13–117.
This pole placement behavior is a beautiful example of the difference be-
tween apparently equivalent filter implementations. With infinite-precision
coefficients the standard and coupled-form IIR filters, having identical de-
nominators in their transfer functions, will have identical z-plane pole loca-
tions. But with quantized coefficients the two filters will have different pole
locations.
Back to our problem. While the coupled-form IIR filter gives us in-
creased flexibility in placing z-plane poles for lowpass filtering, unfortu-
nately, this coupled-form implementation requires twice the number of
multiplies needed by the standard 2nd-order IIR filter in Figure 13–116(a).
In the following material we describe a slick narrowband lowpass IIR
filter structure, proposed by Harris and Loudermilk, having poles residing in

836 Digital Signal Processing Tricks
the shaded region of Figure 13–117 with no increase in coefficient bit width
and no additional multiplication operations beyond those needed for a stan-
dard 2nd-order IIR filter[100].
13.50.2 An Improved Narrowband Lowpass IIR Filter
The improved lowpass IIR filter is created by replacing each unit-delay ele-
ment in a standard 2nd-order IIR filter with multiple unit-delay elements as
shown in the left portion of Figure 13–118. This zero-stuffed IIR filterwill retain
its original lowpass passband and have multiple passband images, exactly as
did the interpolated finite impulse response (IFIR) filters that we studied in
Chapter 7. The zero-stuffed IIR filter is followed by a lowpass image-reject fil-
ter that attenuates those unwanted passband images. Given this cascaded
structure, which we’ll demonstrate shortly, we call the filter combination in
Figure 13–118 an interpolated infinite impulse response(interpolated-IIR) filter.
The M-length delay lines, where M is a positive integer, in the zero-
stuffed IIR filter shift a standard IIR filter’s conjugate poles, originally located
at z=
re±jθ
, to the new locations of
z = Mr⋅e ±jθ/M. (13–177)
new
That is, the new conjugate pole locations are at radii of the Mth root of r, at
angles of ±θ /M radians. Happily, those interpolated-IIR filter pole locations
can now reside in the desired shaded region of Figure 13–117 without using
more bits to represent the zero-stuffed IIR filter’s coefficients.
If the original Figure 13–116(a) 2nd-order IIR filter contains feedforward
coefficients, those coefficients are also delayed by M-length delay lines.
Interpolated-IIR filter
X(z) Y(z)
z M z M z 1
2rcos( ) z M
r2
Zero-stuffed IIR filter Image-reject filter
Figure 13–118 Interpolated-IIR filter.

13.50 Improved Narrowband Lowpass IIR Filters 837
13.50.3 Interpolated-IIR Filter Example
Let’s show an example of an interpolated-IIR filter in action. With f repre-
s
senting a filter’s input signal sample rate in Hz, assume we want to imple-
ment a recursive lowpass filter whose one-sided passband width is 0.005f
s
with a stopband attenuation greater than 60 dB. If we choose to set M = 4,
then we start our interpolated-IIR filter design process by designing a stan-
dard IIR filter having a one-sided passband width of M · 0.005f = 0.02f .
s s
Using our favorite IIR filter design software (for an elliptic IIR filter in this ex-
ample), we obtain a 5th-order prototype IIR filter. Partitioning that 5th-order
prototype IIR filter into two 2nd-order and one single-order IIR filter sections,
all in cascade and having coefficients represented by 12-bit words, yields the
frequency magnitude response shown in Figure 13–119(a).
Next, replacing the unit-delay elements in the filter sections with M = 4
unit-delay elements results in the frequency magnitude response shown in Fig-
ure 13–119(b). There we see the multiple narrowband passband images induced
by the M= 4-length delay lines of the interpolated-IIR filter. Our final job is to at-
tenuate those unwanted passband images. We can do so by following the cas-
caded increased-delay IIR filter sections with a cascaded integrator-comb (CIC)
0
One-sided
–20 passband
width = 0.02f
(a) d
B–40 s
–60
–80
–0.5 –0.4 –0.3 –0.2 –0.1 0 0.1 0.2 0.3 0.4
Freq (xf)
s
0
Images Images
–20
CIC
B–40
(b) d
–60
–80
–0.5 –0.4 –0.3 –0.2 –0.1 0 0.1 0.2 0.3 0.4 0.5
Freq (xf)
s
0
One-sided
–20 passband
width = 0.005f
B–40 s
(c) d
–60
–80
–0.5 –0.4 –0.3 –0.2 –0.1 0 0.1 0.2 0.3 0.4 0.5
Freq (xf)
s
Figure 13–119 Frequency magnitude responses: (a) original IIR prototype filter;
(b) zero-stuffed interpolated-IIR filter and CIC filters (dashed);
(c) final narrowband 12-bit coefficient filter.

838 Digital Signal Processing Tricks
filter, whose structure is shown on the right side ofFigure13–118. (The CIC filter
is computationally advantageous because it requires no multiplications.) To sat-
isfy our desired 60 dB stopband attenuation requirement, we use a 2nd-order
CIC filter—two 1st-order CIC filters in cascade—to attenuate the passband im-
ages in Figure 13–119(b). The result of our design is the interpolated-IIR and CIC
filter combination whose composite frequency magnitude response meets our
filter requirements as shown Figure 13–119(c).
In practice, 2nd-order subfilters may have large gains requiring unpleas-
antly large bit-width multipliers and large bit-width registers to store inter-
mediate results. For this reason it may be necessary to scale the IIR subfilters’
coefficients as discussed in Chapter 6, or truncate the subfilters’ output sam-
ples, in order to avoid undesirably large bit-width processing.
To recap this material, we discussed the limitations encountered when
using traditional 2nd-order quantized-coefficient IIR filters to perform nar-
rowband lowpass filtering and mentioned the coupled-form IIR filter that re-
duced those limitations albeit with an increased computational cost of
doubling the number of multiplies per filter output sample. Next we de-
scribed, and then demonstrated, an interpolated-IIR filter that overcomes the
shortcomings of traditional lowpass IIR filters. The interpolated-IIR filter pro-
vides improved lowpass IIR filter performance while requiring no increase in
filter coefficient bit widths and no additional multiply operations beyond a
traditional IIR filter. When it comes to narrowband lowpass IIR filters, there’s
a new sheriff in town.
13.51 A STABLE GOERTZEL ALGORITHM
In Section 13.17.1 we discussed the computational value of the Goertzel algo-
rithm for computing discrete Fourier transform (DFT) spectral components.
However, we also mentioned that the Figure 13–42 complex resonator imple-
mentation of the Goertzel algorithm places resonator z-domain poles on the
z-plane’s unit circle. Having a resonator pole on the unit circle leads to poten-
tial instability problems because we cannot represent the resonator’s coeffi-
cients with infinite precision. We’re forced to represent the coefficients as
accurately as a fixed number of binary bits allows. This means the resonator’s
poles will not lie exactly on the unit circle. If an imprecise binary representa-
tion of the coefficient 2cos(2πm/N) places the poles slightly inside the
z-plane’s unit circle, then the computed X(m) spectral sample will contain a
small error. Even worse, if an imprecise binary representation of 2cos(2πm/N)
places the poles slightly outside the unit circle, then the resonator is unstable.
For this reason, typical applications of the Goertzel algorithm restrict the
transform length Nto be in the hundreds.

13.51 A Stable Goertzel Algorithm 839
One way to avoid those potential stability problems, and let N be any
value we wish, is by way of a heterodyning scheme. That is, instead of build-
ing an imperfect resonator centered at our frequency of interest, 2’m/N radi-
ans/sample, we frequency translate our signal of interest down to zero
frequency where we can build a perfect resonator as shown in Figure
13–120(a). We say “perfect resonator” because that resonator, centered at zero
frequency (frequency index m = 0), has coefficients of two and one, which
can be represented by binary words with perfect precision.
Such a resonator has a z-domain transfer function of
Y(z) 1−z −1
H (z)= =
G,stable W(z) 1−2z −1+z −2 (13–178)
with a single z-domain zero located at z = 1 and two poles at z = 1 as shown
in Figure 13–120(b). One of the poles cancels the zero at z = 1. The advan-
tages of the network in Figure 13–120(a) are that it is guaranteed stable, and it
exhibits no output error due to a pole or zero being slightly inside or outside
the z-plane unit circle.
Now the perceptive reader would factor Eq. (13–178) as
1−z −1 1
H (z)= =
G,stable (1−z −1)(1−z −1) 1−z −1
(13–179)
and redraw Figure 13–120(a) as shown in Figure 13–121(a).
Figure 13–121(a) tells us that our desired X(m) =y (n) spectral sample is
equal to the sum of the Nsamples output by the multiplier in Figure 13–121(a).
(This makes perfect sense because the zero-frequency spectral sample of an
N-point DFT, X(0), is computed by merely summing a DFT’s N input sam-
ples.) So our “stable Goertzel algorithm” now becomes quite simple.
1 z-plane
x(n) w(n) y(n) art
– –
y
p 0.5 Two
o n
p
e
o l
z
e
e
s
r o
and
e–j2πnm/N
z–1 n
ar
0
gi
a
m –0.5
I
2 z–1 –1
–1 0 1
Real part
(a) (b)
Figure 13–120 Stable Goertzel algorithm: (a) resonator implementation;
(b)z-plane poles and zero.

840 Digital Signal Processing Tricks
1 z-plane
art
x(n) y(n) p 0.5
y
ar
n 0
e–j2πnm/N
z–1 a
gi
m –0.5
I
–1
–1 0 1
Real part
(a) (b)
Figure 13–121 Simplified stable Goertzel algorithm: (a) simplified resonator imple-
mentation; (b) z-plane pole.
Ah, but there’s trouble in paradise. The “weak link in the chain” of the
Figure 13–121(a) network is that we’re assuming the heterodyning sequence
e–j2πm/N
is ideal in its precision. If you’ve ever tried to generate a complex
e–j2πm/Nsequence
using binary arithmetic, you know that your sequence must
be quantized to some fixed number of bits, and thus have imperfect precision.
That means the output of your
e–j2πm/Noscillator
will either increase in magni-
tude, or decrease in magnitude, as time index nincreases. However, we solve
that problem by using the guaranteed-stable quadrature oscillator described
in Section 13.32.
It’s fair to copy a slogan from the Aston Martin automobile company
and say that the Figure 13–121(a) Goertzel algorithm, using the stable quadra-
ture oscillator, is “engineered to exceed all expectations.”
REFERENCES
[1] Powell, S. “Design and Implementation Issues of All Digital Broadband Modems,” DSP
World Workshop Proceedings, Toronto, Canada, September 13–16, 1998, pp. 127–142.
[2] Frerking, M. Digital Signal Processing in Communications Systems, Chapman & Hall, New
York, 1994, p. 330.
[3] Jacobsen, E., Minister of Algorithms, Abineau Communications, private communication,
September 11, 2003.
[4] Palacherls, A. “DSP-mPRoutine Computes Magnitude,” EDN, October 26, 1989.
[5] Mikami, N., Kobayashi, M., and Yokoyama, Y. “ANew DSP-Oriented Algorithm for Cal-
culation of the Square Root Using a Nonlinear Digital Filter,” IEEE Trans. on Signal Pro-
cessing, Vol. 40, No. 7, July 1992.
[6] Lyons, R. “Turbocharge Your Graphics Algorithm,” ESD: The Electronic System Design
Magazine, October 1988.

References 841
[7] Adams W., and Brady, J. “Magnitude Approximations for Microprocessor Implementa-
tion,” IEEE Micro, Vol. 3, No. 5, October 1983.
[8] Harris Semiconductor Corp. HSP50110 Digital Quadrature Tuner Data Sheet, File Number
3651, February 1994.
[9] Sabin,W.,andSchoenike,E.,eds.,SingleSidebandSystemsandCircuits,McGraw-Hill,New
York,1987.
[10] Schreiner, C. “Subject: Re: Approximation for Sum of Two Squares,” Usenet group
comp.dsppost, October 4, 1999.
[11] Filip, A. “Linear Approximations to sqrt(x2+y2) Having Equiripple Error Characteristics,”
IEEE Trans. on Audio and Electroacoustics, December 1973, pp. 554–556.
[12] Bingham, C., Godfrey, M., and Tukey, J. “Modern Techniques for Power Spectrum Estima-
tion,” IEEE Trans. on Audio and Electroacoust., Vol. AU-15, No. 2, June 1967.
[13] Bergland, G. “A Guided Tour of the Fast Fourier Transform,” IEEE Spectrum Magazine,
July 1969, p. 47.
[14] Harris, F. “On the Use of Windows for Harmonic Analysis with the Discrete Fourier
Transform,” Proceedings of the IEEE,Vol. 66, No. 1, January 1978.
[15] Nuttall, A. “Some Windows with Very Good Sidelobe Behavior,” IEEE Trans. on Acoust.
Speech, and Signal Proc.,Vol. ASSP-29, No. 1, February 1981.
[16] Cox, R. “Complex-Multiply Code Saves Clocks Cycles,” EDN, June 25, 1987.
[17] Rabiner, L., and Gold, B. Theory and Application of Digital Signal Processing, Prentice Hall,
Englewood Cliffs, New Jersey, 1975.
[18] Sorenson, H., Jones, D., Heideman, M., and Burrus, C. “Real-Valued Fast Fourier Trans-
form Algorithms,” IEEE Trans. on Acoust. Speech, and Signal Proc., Vol. ASSP-35, No. 6,
June 1987.
[19] Cooley, J., Lewis, P., and Welch, P. “The Fast Fourier Transform Algorithm: Programming
Considerations in the Calculation of Sine, Cosine and Laplace Transforms,” Journal Sound
Vib., Vol. 12, July 1970.
[20] Brigham, E. The Fast Fourier Transform and Its Applications, Prentice Hall, Englewood Cliffs,
New Jersey, 1988.
[21] Burrus, C., et al. Computer-Based Exercises for Signal Processing, Prentice Hall, Englewood
Cliffs, New Jersey, 1994, p. 53.
[22] Hewlett-Packard, “The Dynamic Range Benefits of Large-Scale Dithered Analog-to-
Digital Conversion, HP Product Note: 89400-7.
[23] Blesser, B., and Locanthi, B. “The Application of Narrow-Band Dither Operating at the
Nyquist Frequency in Digital Systems to Provide Improved Signal-to-Noise Ratio over
Conventional Dithering,” J. Audio Eng. Soc., Vol. 35, June 1987.

842 Digital Signal Processing Tricks
[24] Coleman, B., et al. “Coherent Sampling Helps When Specifying DSPA/D Converters,”
EDN, October 1987.
[25] Ushani, R. “Classical Tests Are Inadequate for Modern High-Speed Converters,” EDN
Magazine, May 9, 1991.
[26] Meehan, P., and Reidy, J. “FFT Techniques Give Birth to Digital Spectrum Analyzer,” Elec-
tronic Design, August 11, 1988, p. 120.
[27] Beadle, E. “Algorithm Converts Random Variables to Normal,” EDN Magazine, May 11,
1995.
[28] Spiegel, M. Theory and Problems of Statistics, Shaum’s Outline Series, McGraw-Hill, New
York, 1961, p. 142.
[29] Davenport, W., Jr., and Root, W. Random Signals and Noise, McGraw-Hill, New York, 1958.
[30] Salibrici, B. “Fixed-Point DSPChip Can Generate Real-Time Random Noise,” EDN Maga-
zine, April 29, 1993.
[31] Marsaglia, G., and Tsang, W. “The Ziggurat Method for Generating Random Variables,”
Journal of Statistical Software, Vol. 5, No. 8, 2000.
[32] http://finmath.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_ZMGRV.pdf.
[33] http://www.jstatsoft.org/v05/i08/ziggurat.pdf.
[34] Donadio, M. “Lost Knowledge Refound: Sharpened FIR Filters,” IEEE Signal Processing
Magazine, Vol. 20, No. 5, September 2003, pp. 61–63.
[35] Kwentus, A., et al. “Application of Filter Sharpening to Cascaded Integrator-Comb Deci-
mation Filters,” IEEE Transactions on Signal Processing, Vol. 45, February 1997, pp. 457–467.
[36] Gentili, P., et al. “Improved Power-of-Two Sharpening Filter Design by Genetic Algo-
rithm,” 1996 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP‘96),
Atlanta, Georgia, Vol. 3, 1996, p. 1375.
[37] Graychip Inc. “Upconverting Signals with the GC2011 for Easier Digital to Analog Con-
version,” Application Note: GC2011-AN9804, December 20, 1998.
[38] Donadio, M., private communication, September 11, 2003.
[39] Jacobsen, E., and Kootsookos, P. “Fast, Accurate Frequency Estimators,” IEEE Signal Pro-
cessing Magazine, “DSPTips & Tricks” column, Vol. 24, No. 3, May 2007.
[40] Nagai, K. “Pruning the Decimation-in-Time FFT Algorithm with Frequency Shift,” IEEE
Trans. on ASSP, Vol. ASSP-34, August 1986, pp. 1008–1010.
[41] Skinner, D. “Pruning the Decimation-in-Time FFT Algorithm,” IEEE Trans. on ASSP, Vol.
ASSP-24, April 1976, pp. 193–194.

References 843
[42] Markel, J. D. “FFT Pruning,” IEEE Trans on Audio Electroacoust., Vol. AU-19, December
1971, pp.305–311.
[43] Sreenivas, T., and Rao, P. “FFT Algorithm for Both Input and Ouput Pruning,” IEEE Trans.
on ASSP, Vol. ASSP-27, June 1979, pp. 291–292.
[44] Lyons, R. “Program Aids Analysis of FFT Algorithms,”EDN Magazine, August 6, 1987.
[45] Goertzel, G. “An Algorithm for the Evaluation of Finite Trigonometric Series,” American
Math. Monthly,Vol. 65, 1958, pp. 34–35.
[46] Proakis, J., and Manolakis, D. Digital Signal Processing: Principles, Algorithms, and Applica-
tions,3rd ed., Prentice Hall, Upper Saddle River, New Jersey, 1996, pp. 480–481.
[47] Oppenheim, A., Schafer, R., and Buck, J. Discrete-Time Signal Processing, 2nd ed., Prentice
Hall, Upper Saddle River, New Jersey, 1999, pp. 633–634.
[48] Farhang-Boroujeny, B., and Lim, Y. “A Comment on the Computational Complexity of
Sliding FFT,” IEEE Trans. Circuits and Syst. II,Vol. 39, No. 12, December 1992, pp. 875–876.
[49] Farhang-Boroujeny, B., and Gazor, S. “Generalized Sliding FFT and Its Application to Im-
plementation of Block LMS Adaptive Filters,” IEEE Trans. Sig. Proc., Vol. 42, No. 3, March
1994, pp. 532–538.
[50] Douglas, S., and Soh, J. “ANumerically-Stable Sliding-Window Estimator and Its Appli-
cation to Adaptive Filters,” Proc. 31st Annual Asilomar Conf. on Signals, Systems, and Com-
puters, Pacific Grove, California, Vol. 1, November 1997, pp. 111–115.
[51] Crochiere, R., and Rabiner, L. Multirate Digital Signal Processing, Prentice Hall, Englewood
Cliffs, New Jersey, 1983, pp. 315–319.
[52] Zoran Corp. “Vernier Spectral Analysis with the ZR34161 Vector Signal Processor,” Tech.
Note ZAN34003, Santa Clara, California, 1989.
[53] Gumas, C. “Window-Presum FFT Achieves High-Dynamic Range, Resolution,” Personal
Engineering and Instrumentation News, July 1997, pp. 58–64.
[54] Hack, T. “IQ Sampling Yields Flexible Demodulators,” RF Design, April 1991.
[55] Bateman, A. “Quadrature Frequency Discriminator,” GlobalDSP Magazine, October 2002.
[56] http://aulos.calarts.edu/pipermail/test/1998-March/001028.html.
[57] Dick, C., and Harris, F. “FPGASignal Processing Using Sigma-Delta Modulation,” IEEE
Signal Proc. Magazine, Vol. 17, No. 1, January 2000.
[58] Bateman, A. “Implementing a Digital AC Coupling Filter,” GlobalDSP Magazine, February
2003.
[59] Shenoi, K. Digital Signal Processing in Communications Systems, Chapman & Hall, New
York, 1994, p. 275.
[60] Bristow-Johnson, R. “Subject: Fixed-PointDC Blocking Filter with Noise Shaping,” Usenet
group comp.dsppost, June 22, 2000.

844 Digital Signal Processing Tricks
[61] Bristow-Johnson, R. “Subject: Virtues of Noise Shaping,” Usenet group comp.dsppost, Au-
gust21, 2001.
[62] Ascari, L., et al. “Low Power Implementation of a Sigma Delta Decimation Filter for Car-
diac Applications,” IEEE Instrumentation and Measurement Technology Conference, Bu-
dapest, Hungary, May 21–23, 2001, pp. 750–755.
[63] Gao, Y., et al. “Low-Power Implementation of a Fifth-Order Comb Decimation Filter for
Multi-Standard Transceiver Applications,” Int. Conf. on Signal Proc. Applications and
Technology (ICSPAT), Orlando, Florida, 1999.
[64] Gao, Y., et al. “A Comparison Design of Comb Decimators for Sigma-Delta Analog-to-
Digital Converters,” Int. Journal: Analog Integrated Circuits and Signal Processing, Kluwer
Academic Publishers, ISSN: 0925–1030, 1999.
[65] Ballanger, M., et al. “Digital Filtering by Polyphase Network: Application to Sample-Rate
Alteration and Filter Banks,” IEEE Trans. on Acoust. Speech, and Signal Proc., Vol. ASSP-24,
No. 2, April 1976, pp. 109–114.
[66] Brandt, B., and Wooley, B. “ALow-Power Area-Efficient Digital Filter for Decimation and
Interpolation,” IEEE Journ. of Solid-State Circuits, Vol. 29, June 1994, pp. 679–687.
[67] Willson, A., Jr. “AProgrammable Interpolation Filter for Digital Communications Appli-
cations,” Final report for MICRO Project 96–149, UCLA, 1996–1997.
[68] Dumonteix, Y., et al. “Low Power Comb Decimation Filter Using Polyphase Decomposi-
tion for Mono-Bit ΣΔAnalog-to-Digital Converters,” Int. Conf. on Signal Processing Ap-
plications and Technology (ICSPAT), San Jose, California, 2000.
[69] Yang, H., and Snelgrove, W. “High Speed Polyphase CIC Decimation Filters,” IEEE Int.
Symposium on Circuits and Systems, Vol. 2, 1996, pp. 229–232.
[70] Jang, Y., and Yang, S. “Non-Recursive Cascaded Integrator-Comb Decimation Filters with
Integer Multiple Factors,” 44th IEEE Midwest Symposium on Circuits and Systems
(MWSCAS), Dayton, Ohio, August 2001.
[71] Dvorak, R. “Software Filter Boosts Signal-Measurement Stability, Precision,” Electronic
Design, February 3, 2003.
[72] Lynn, P., and Fuerst, W. Introductory Digital Signal Processing, with Computer Applications,
John Wiley and Sons, New York, 1997, pp. 285–297.
[73] Givens, M., private communication, October 12, 2004.
[74] Fraser, D. “Interpolation by the FFT Revisited—An Experimental Investigation,” IEEE
Trans. on Acoust. Speech, and Signal Proc., Vol. ASSP-37, No. 5, May 1989, pp. 665–676.
[75] Marple, S., Jr. “Computing the Discrete-Time ‘Analytic’ Signal via FFT,” IEEE Trans. on
Signal Proc., Vol. 47, No. 9, September 1999, pp. 2600–2603.
[76] Harris, F. “T102: Digital Signal Processing for Digital Modems,” DSP World Spring De-
sign Conf., Santa Clara, California, April 1999.

References 845
[77] Harris, F. “On the Design, Implementation, and Performance of a Microprocessor-
Controlled AGC System for a Digital Receiver,” IEEE Military Communications Conf.,
San Diego, Caliornia, October 1988.
[78] Analog Devices, Inc. “80 MSPS, Dual-Channel WCDMAReceive Signal Processor (RSP)
AD6634,” Data Sheet Rev. 0, 2002, pp. 28–34.
[79] Turner, C. “Recursive Discrete-Time Sinusoidal Oscillators,” IEEE Signal Processing Maga-
zine,Vol. 20, No. 3, May 2003, pp. 103–111.
[80] Paillard, B., and Boudreau, A. “Fast, Continuous, Sinewave Generator,” GlobalDSP On-
line Magazine,December 2003.
[81] Vassilevsky, V. “Efficient Multi-tone Detection,” IEEE Signal Processing Magazine, “DSP
Tips & Tricks” column, Vol. 24 , No. 2, March 2007.
[82] Shiung, D., Ferng, H., and Lyons, R. “Filtering Tricks for FSK Demodulation,” IEEE Signal
Processing Magazine, “DSPTips & Tricks” column, Vol. 22, No. 3, May 2005.
[83] Spiegel, M. Statistics,Shaum’s Outline Series, McGraw-Hill, New York, 1961, p. 77.
[84] Hadstate, J. “Subject: Re: Question about Computing a “Moving Variance,” Usenet group
comp.dsppost, March 1, 2005.
[85] Turner, C. “Subject: Re: Question About Computing a “Moving Variance,” Usenet group
comp.dsppost, February 27, 2005.
[86] Jackson, L. “On the Relationship Between Digital Hilbert Transformers and Certain Low-
Pass Filters,” IEEE Trans. on Acoust. Speech, and Signal Proc., Vol.ASSP-23, No. 4,August
1975.
[87] Harris, F. Multirate Signal Processing for Communication Systems, Prentice Hall, Upper Sad-
dle River, New Jersey, 2004, pp. 210–212.
[88] Turner, C. “Subject: How Do You Rotate a Phasor by pi/8 Radians,” Usenet group
comp.dsppost, May 29, 2002.
[89] Press, W., et al., Numerical Recipes in C: The Art of Scientific Computing, 2nd ed. Cambridge
University Press, New York, 1992, p. 177.
[90] Losada, R., and Lyons, R. “Reducing CIC Filter Complexity,” IEEE Signal Processing Maga-
zine, “DSPTips and Tricks” column, Vol. 23, No. 4, July 2006.
[91] Ohlsson, H., et al. “Design of a Digital Down Converter Using High Speed Digital Fil-
ters,” in Proc. Symp. on Gigahertz Electronics, Gothenburg, Sweden, March 13–14, 2000, pp.
309–312.
[92] Turner, C. “Slope Filtering: An FIR Approach to Linear Regression,” IEEE Signal Process-
ing Magazine, “DSPTips & Tricks” column, Vol. 25, No. 6, November 2008.
[93] Bell, D. “Subject: Re: Frequency Inversion,” Usenet group comp.dsppost, August 30, 2006.
[94] Adams, R. “Nonuniform Sampling of Audio Signals,” J. Audio Eng. Soc., Vol. 40, No. 11,
November 1992, pp. 886–894.

846 Digital Signal Processing Tricks
[95] Bariska, A. “Recovering Periodically-Spaced Missing Samples,” IEEE Signal Processing
Magazine, “DSPTips and Tricks” column, Vol. 24, No. 6, November 2007.
[96] Moshe, S., and Hertz, D. “On Computing DFT of Real N-Point Vector and IDFT of DFT-
Transformed Real N-Point Vector via Single DFT,” IEEE Signal Processing Letters, IEEE,
Vol. 6, No. 6, June 1999, p. 141.
[97] Proakis, J., and Manolakis, D. DigitalSignalProcessing: Principles,Algorithms,andApplica-
tions,3rd ed., Prentice Hall, Upper Saddle River, New Jersey, 1996, pp. 572–576.
[98] Oppenheim, A., and Schafer, R. Discrete-Time Signal Processing, 2nd ed., Prentice Hall,
Englewood Cliffs, New Jersey, 1989, pp. 382–386.
[99] Gold, B., and Rader, C. “Effects of Parameter Quantization on the Poles of a Digital Fil-
ter,” Proceedings of the IEEE, Vol. 55, May 1967, pp. 688–689.
[100] Harris, F., and Lowdermilk, W. “Implementing Recursive Filters with Large Ratio of
Sample Rate to Bandwidth,” in Conference Record of the Forty-first Asilomar Conference on
Signals, Systems and Computers, Pacific Grove, California, November 4–7, 2007, pp.
1149–1153.

APPENDIX A
The Arithmetic of
Complex Numbers
To understand digital signal processing, we have to get comfortable using
complex numbers. The first step toward this goal is learning to manipulate
complex numbers arithmetically. Fortunately, we can take advantage of our
knowledge of real numbers to make this job easier. Although the physical sig-
nificance of complex numbers is discussed in Chapter 8, the following discus-
sion provides the arithmetic rules governing complex numbers.
A.1 GRAPHICAL REPRESENTATION OF REAL AND COMPLEX NUMBERS
To get started, real numbers are those positive or negative numbers we’re
used to thinking about in our daily lives. Examples of real numbers are 0.3,
–2.2, 5.1, etc. Keeping this in mind, we see how a real number can be repre-
sented by a point on a one-dimensional axis, called the real axis, as shown in
Figure A–1.
We can, in fact, consider that all real numbers correspond to all of the
points on the real axis line on a one-to-one basis.
Acomplex number, unlike a real number, has two parts: a real part and
an imaginary part. Just as a real number can be considered to be a point on
This point represents the
real number –2.2
–7 –6 –5 –4 –3 –2 –1 0 1 2 3 4 5 6 7 Real axis
Figure A–1 The representation of a real number as a point on the one-
dimensional real axis.
847

848 The Arithmetic of Complex Numbers
Imaginary axis (j)
I
This point represents the
M
complex number C = R + jI
ø
0 R Real axis
Figure A–2 The phasor representation of the complex number C= R+ jIon the
complex plane.
the one-dimensional real axis, a complex number can be treated as a point on
a complex plane as shown in Figure A–2. We’ll use this geometrical concept to
help us understand the arithmetic of complex numbers.†
A.2 ARITHMETIC REPRESENTATION OF COMPLEX NUMBERS
Acomplex number Cis represented in a number of different ways in the liter-
ature, such as
Rectangular form: → C= R+ jI, (A–1)
Trigonometric form: → C= M[cos(ø) + jsin(ø)], (A–1’)
Exponential form: → C= Mejø, (A–1’’)
Magnitude and angle form: → C= M∠ø. (A–1’’’)
Equations (A–1’’) and (A–1’’’) remind us that the complex number Ccan also
be considered the tip of a phasor on the complex plane, with magnitude M, in
the direction of ø degrees relative to the positive real axis as shown in Figure
A–2. (We’ll avoid calling phasor M a vector because the term vector means
different things in different contexts. In linear algebra, vector is the term used
to signify a one-dimensional matrix. On the other hand, in mechanical engi-
neering and field theory, vectors are used to signify magnitudes and direc-
tions, but there are vector operations (scalar or dot product, and vector or
cross-product) that don’t apply to our definition of a phasor. The relationships
between the variables in this figure follow the standard trigonometry of right
triangles. Keep in mind that Cis a complex number, and the variables R, I, M,
andø are all real numbers. The magnitude of C, sometimes called the modulus
of C, is
† The complex plane representation of a complex number is sometimes called an Argand
diagram—named after the French mathematician Jean Robert Argand (1768–1825).

A.2 Arithmetic Representation of Complex Numbers 849
M=|C|= R2 +I2,
(A–2)
and, by definition, the phase angle, or argument, of Cis the arctangent of I/R,
or
ø=tan −1
⎛
⎝ ⎜
I⎞
⎠ ⎟. (A–3)
R
The variable ø in Eq. (A–3) is a general angle term. It can have dimensions of
degrees or radians. Of course, we can convert back and forth between degrees
and radians using πradians=180°. So, if ø is in radians and ø is in degrees,
r d
then we can convert ø to degrees by the expression
r
180ø
ø = r. (A–4)
d π
Likewise, we can convert ø to radians by the expression
d
πø
¯ = d. (A–5)
r
180
The exponential form of a complex number has an interesting character-
istic that we need to keep in mind. Whereas only a single expression in rec-
tangular form can describe a single complex number, an infinite number of
exponential expressions can describe a single complex number; that is, while,
in the exponential form, a complex number Ccan be represented by C=Mejø,
it can also be represented by
C= Mejø= Mej(ø + 2πn), (A–6)
where n=±1, ±2, ±3, . . . and ø is in radians. When ø is in degrees, Eq. (A–6) is
in the form
C= Mejø = Mej(ø + n360°). (A–7)
Equations (A–6) and (A–7) are almost self-explanatory. They indicate that the
point on the complex plane represented by the tip of the phasor C remains
unchanged if we rotate the phasor some integral multiple of 2πradians or an
integral multiple of 360°. So, for example, if
C=Mej(20°),
then
C=
Mej(20°)
=
Mej(380°)
=
Mej(740°).
(A–8)
The variable ø, the angle of the phasor in Figure A–2, need not be constant.
We’ll often encounter expressions containing a complex sinusoid that takes
the form
C=Mejωt.
(A–9)

850 The Arithmetic of Complex Numbers
Equation (A–9) represents a phasor of magnitude M whose angle in Figure
A–2 is increasing linearly with time at a rate of ω radians each second. If
ω = 2π, the phasor described by Eq. (A–9) is rotating counterclockwise at a
rate of 2π radians per second—one revolution per second—and that’s why ω
is called the radian frequency. In terms of frequency, Eq. (A–9)’s phasor is ro-
tating counterclockwise at ω=2πfradians per second, where fis the cyclic fre-
quency in cycles per second (Hz). If the cyclic frequency is f = 10 Hz, the
phasor is rotating at 20πradians per second. Likewise, the expression
C=Me–jωt
(A–9’)
represents a phasor of magnitude M that rotates in a clockwise direction
about the origin of the complex plane at a negative radian frequency of –ωra-
dians per second.
A.3 ARITHMETIC OPERATIONS OF COMPLEX NUMBERS
A.3.1 Addition and Subtraction of Complex Numbers
Which of the above forms for C in Eq. (A–1) is the best to use? It depends on
the arithmetic operation we want to perform. For example, if we’re adding
two complex numbers, the rectangular form in Eq. (A–1) is the easiest to use.
The addition of two complex numbers, C = R + jI and C = R + jI , is
1 1 1 2 2 2
merely the sum of the real parts plus jtimes the sum of the imaginary parts as
C +C = R + jI + R + jI = R + R +j(I + I ). (A–10)
1 2 1 1 2 2 1 2 1 2
Figure A–3 is a graphical depiction of the sum of two complex numbers using
the concept of phasors. Here the sum phasor C + C in Figure A–3(a) is the
1 2
new phasor from the beginning of phasor C to the end of phasor C in Figure
1 2
A–3(b). Remember, the Rs and the Is can be either positive or negative num-
bers. Subtracting one complex number from the other is straightforward as
long as we find the differences between the two real parts and the two imagi-
nary parts separately. Thus
C –C = (R +jI )– (R +jI ) = R – R +j(I – I ). (A–11)
1 2 1 1 2 2 1 2 1 2
An example of complex number addition is discussed in Section 11.3, where
we covered the topic of averaging fast Fourier transform outputs.
A.3.2 Multiplication of Complex Numbers
We can use the rectangular form to multiply two complex numbers as
C C = (R + jI ) (R +jI ) = (R R – I I )+j(R I + R I ). (A–12)
1 2 1 1 2 2 1 2 1 2 1 2 2 1

A.3 Arithmetic Operations of Complex Numbers 851
Imaginary Imaginary
axis (j) axis (j)
I
1 C
C 2
1
C
1 I –I
1 2
R 2 C 1 + C 2
0 R 1 Real 0 R 1 + R 2 Real
C axis axis
2
–I
2
(a) (b)
Figure A–3 Geometrical representation of the sum of two complex numbers.
However, if we represent the two complex numbers in exponential form, their
product takes the simpler form
C C = M ejø1M ejø2= M M ej(ø1+ø2) (A–13)
1 2 1 2 1 2
because multiplication results in the addition of the exponents. Of some inter-
est is the fact that the product of the magnitudes of two complex numbers is
equal to the magnitude of their product. That is,
|C |·|C | = |C C |. (A–13’)
1 2 1 2
As a special case of multiplication of two complex numbers, scaling is
multiplying a complex number by another complex number whose imagi-
nary part is zero. We can use the rectangular or exponential forms with equal
ease as follows:
kC= k(R+jI) = kR+jkI, (A–14)
or in exponential form,
kC= k(Mejø) = kMejø. (A–15)
A.3.3 Conjugation of a Complex Number
The complex conjugate of a complex number is obtained merely by changing
the sign of the number’s imaginary part. So, if we denote C* as the complex
conjugate of the number C= R+jI=Mejø, then C*is expressed as
C*= R– jI= Me–jø. (A–16)

852 The Arithmetic of Complex Numbers
There are three characteristics of conjugates that occasionally come in handy.
First, the conjugate of a product is equal to the product of the conjugates. That
is, if C= C C , then from Eq. (A–13)
1 2
C* = (C C ) * = (M M ej(ø1+ø2)) * = M M e–j(ø1+ø2)
1 2 1 2 1 2
= M e–jø1M e–jø2= C *C * . (A–17)
1 2 1 2
Second, the sum of conjugates of two complex numbers is equal to the conju-
gate of the sum. We can show this in rectangular form as
(R + jI ) * + (R + jI ) * = (R – jI ) + (R – jI )
1 1 2 2 1 1 2 2
= R + R – j(I + I ) = [R + R + j(I + I )] * . (A–17’)
1 2 1 2 1 2 1 2
Third, the product of a complex number and its conjugate is the complex
number’s magnitude squared. It’s easy to prove this in exponential form as
CC* = MejøMe–jø= M2ej0= M2. (A–18)
(This property is often used in digital signal processing to determine the rela-
tive power of a complex sinusoidal phasor represented by
Mejωt.)
A.3.4 Division of Complex Numbers
The division of two complex numbers is also convenient using the exponen-
tial and magnitude and angle forms, such as
jø
C C 2 1 = M M 2 1 e e jø 1 2 = M M 2 1 e j(ø 1 −ø 2 ) (A–19)
and
C M
C
1 =
M
1∠(ø
1
−ø
2
) . (A–19’)
2 2
Although not nearly so handy, we can perform complex division in rectangu-
lar notation by multiplying the numerator and the denominator by the com-
plex conjugate of the denominator as
C R + jI
1 = 1 1
C R + jI
2 2 2
(A–20)
R + jI ⋅ R − jI
= 1 1 2 2
R + jI R − jI
2 2 2 2
(RR +I I )+ j(R I −RI )
= 1 2 1 2 2 1 1 2 .
R2 + I2
2 2

A.3 Arithmetic Operations of Complex Numbers 853
A.3.5 Inverse of a Complex Number
Aspecial form of division is the inverse, or reciprocal, of a complex number.
If C=Mejø, its inverse is given by
1 = 1 = 1 e −jø. (A–21)
C Mejø M
In rectangular form, the inverse of C=R+jIis given by
1 1 R− jI R− jI C *
= ⋅ = = . (A–22)
C R+ jI R− jI R2 +I2 M2
We obtain Eq. (A–22) by substituting R =1, I =0, R =R, and I =Iin Eq. (A–20).
1 1 2 2
A.3.6 Complex Numbers Raised to a Power
Raising a complex number to some power is easily done in the exponential
form. If C=Mejø, then
Ck= Mk(ejø)k= Mkejkø. (A–23)
For example, if C=3ej125°, then Ccubed is
(C)3= 33(ej3.125° ) = 27ej375° = 27ej15° . (A–24)
We conclude this appendix with four complex arithmetic operations that
are not very common in digital signal processing—but you may need them
sometime.
A.3.7 Roots of a Complex Number
The kth root of a complex number Cis the number that, multiplied by itself k
times, results in C. The exponential form of C is the best way to explore this
process. When a complex number is represented by C= Mejø, remember that
it can also be represented by
C= Mej(ø + n360°). (A–25)
In this case, the variable ø in Eq. (A–25) is in degrees. There are k distinct
roots when we’re finding the kth root of C. By “distinct,” we mean roots
whose exponents are less than 360°. We find those roots by using the fol-
lowing:
k C =k Mej(ø+n360o) =k Mej(ø+n360o)/k. (A–26)

854 The Arithmetic of Complex Numbers
Next, we assign the values 0, 1, 2, 3, . . ., k–1 to n in Eq. (A–26) to get the k
roots of C. OK, we need an example here! Let’s say we’re looking for the cube
(third) root of
C=125ej(75°).
We proceed as follows:
3C =3 125ej(75o) =3 125ej(75o+n360o) =3125ej(75o+n360o)/3. (A–27)
Next we assign the values n=0, n=1, and n=2 to Eq. (A–27) to get the three
roots of C. So the three distinct roots are
1st root: → 3C =5ej(75o+0 ⋅ 360o)/3 =5ej(25o);
2nd root: → 3C =5ej(75o+1 ⋅ 360o)/3 =5ej(435o)/3 =5ej(145o);
and
⋅
3rd root: → 3C =5ej(75o+2360o)/3 =5ej(795o)/3 =5ej(265o).
A.3.8 Natural Logarithms of a Complex Number
Taking the natural logarithm of a complex number C= Mejø is straightfor-
ward using exponential notation; that is,
lnC= ln(Mejø) = lnM+ ln(ejø)= lnM+ jø, (A–28)
where 0 ≤ø < 2π. By way of example, if C=12ejπ/4, the natural logarithm of Cis
lnC= ln(12ejπ/4) = ln(12)+ jπ/4 = 2.485+ j0.785. (A–29)
⋅
This means that e(2.485 +j0.785)= e2.485 ej0.785= 12ejπ/4.
Before leaving this topic of the natural logarithm of complex numbers,
we remind the reader that
ejπ
=–1, which allows us to write
ln(–1) = jπ, (A–30)
showing how the natural logarithm of a negative real number is defined.
As an interesting aside, rearranging the
ejπ
=–1 expression enables us to
write what many mathematicians call “the most beautiful formula in mathe-
matics.” That equation is
ejπ
+ 1 = 0. (A–31)
Equation (A–31) is famous because the natural constants e, π, 0, and 1, along
with the fundamental operations of addition, multiplication, exponentiation,
the “j” operator, and equality, all appear exactly once!

A.3 Arithmetic Operations of Complex Numbers 855
A.3.9 Logarithm to the Base 10 of a Complex Number
We can calculate the base 10 logarithm of the complex number C=Mejøusing
⋅
log C= log (Mejø) = log M+ log (ejø) = log M+ jø log (e).† (A–32)
10 10 10 10 10 10
Of course e is the irrational number, approximately equal to 2.71828, whose
log to the base 10 is approximately 0.43429. Keeping this in mind, we can sim-
plify Eq. (A–32) as
⋅
log C≈log M+ j(0.43429 ø). (A–32’)
10 10
Repeating the above example with C =
12ejπ/4
and using the Eq. (A–32’)
approximation, the base 10 logarithm of Cis
⋅
log C= log (12ejπ/4) = log (12) + j(0.43429 π/4)
10 10 10
⋅
= 1.079 + j(0.43429 0.785) = 1.079+ j0.341. (A–33)
The result from Eq. (A–33) means that
⋅ ⋅
10(1.079 + j0.341)= 101.079 10j0.341= 12 (e2.302)j0.341
⋅
= 12ej(2.302 0.341)= 12ej0.785=12ejπ/4. (A–33’)
A.3.10 Log to the Base 10 of a Complex
Number Using Natural Logarithms
Unfortunately, some software mathematics packages have no base 10 loga-
rithmic function and can calculate only natural logarithms. In this situation,
we just use
ln(x)
log (x)= (A–34)
10 ln(10)
to calculate the base 10 logarithm of x. Using this change of base formula, we
can find the base 10 logarithm of a complex number C=Mejø; that is,
lnC
log C= =(log e)(lnC). (A–35)
10 ln10 10
Because log (e) is approximately equal to 0.43429, we use Eq. (A–35) to state
10
that
⋅ ⋅
log C≈0.43429 (lnC) = 0.43429 (lnM+ jø). (A–36)
10
⋅
†For the second term of the result in Eq. (A–32) we used log (xn) = n log xaccording to the law
a a
of logarithms.

856 The Arithmetic of Complex Numbers
Repeating, again, the example above of
C=12ejπ/4,
the Eq. (A–36) approxima-
tion allows us to take the base 10 logarithm of Cusing natural logs as
⋅
log C= 0.43429 (ln(12) + jπ/4)
10
⋅
= 0.43429 (2.485+ j0.785) = 1.079+ j0.341, (A–37)
giving us the same result as Eq. (A–32).
A.4 SOME PRACTICAL IMPLICATIONS
OF USING COMPLEX NUMBERS
At the beginning of Section A.3, we said that the choice of using the rectangu-
lar versus the polar form of representing complex numbers depends on the
type of arithmetic operations we intend to perform. It’s interesting to note
that the rectangular form has a practical advantage over the polar form when
we consider how numbers are represented in a computer. For example, let’s
say we must represent our complex numbers using a four-bit sign-magnitude
binary number format. This means that we can have integral numbers rang-
ing from –7 to +7, and our range of complex numbers covers a square on the
complex plane as shown in Figure A–4(a) when we use the rectangular form.
On the other hand, if we used four-bit numbers to represent the magnitude of
a complex number in polar form, those numbers must reside on or within a
circle whose radius is 7 as shown in Figure A–4(b). Notice how the four
shaded corners in Figure A–4(b) represent locations of valid complex values
using the rectangular form but are out of bounds if we use the polar form. Put
another way, a complex number calculation, yielding an acceptable result in
rectangular form, could result in an overflow error if we use polar notation in
Imaginary Imaginary
axis (j) axis (j)
Real axis Real axis
(a) (b)
Figure A–4 Complex integral numbers represented as points on the complex
plane using a four-bit sign-magnitude data format: (a) using rectan-
gular notation; (b) using polar notation.

References 857
our computer. We could accommodate the complex value 7 + j7 in rectangu-
lar form but not its polar equivalent, because the magnitude of that polar
number is greater than 7.
Although we avoid any further discussion here of the practical implica-
tions of performing complex arithmetic using standard digital data formats, it
is an intricate and interesting subject. To explore this topic further, the inquisi-
tive reader is encouraged to start with the references.
REFERENCES
[1] Plauger, P. J. “Complex Math Functions,” Embedded Systems Programming,August 1994.
[2] Kahan, W. “Branch Cuts for Complex Elementary Functions, or Much Ado About Noth-
ing’s Sign Bit,” Proceedings of the Joint IMA/SIAM Conference on the State of the Art in Numeri-
cal Analysis,Clarendon Press, 1987.
[3] Plauger, P. J. “Complex Made Simple,” Embedded Systems Programming,July 1994.

This page intentionally left blank

APPENDIX B
Closed Form of a
Geometric Series
In the literature of digital signal processing, we often encounter geometric se-
ries expressions like
N∑ −1 rp −rN
rn = , (B–1)
1−r
n=p
or
N∑ −1
e −j2πnm/N =
1−e −j2πm
. (B–2)
1−e −j2πm/N
n=0
Unfortunately, many authors make a statement like “and we know that” and
drop Eqs. (B–1) or (B–2) on the unsuspecting reader who’s expected to accept
these expressions on faith. Assuming that you don’t have a Ph.D. in mathe-
matics, you may wonder exactly what arithmetic sleight of hand allows us to
arrive at Eqs. (B–1) or (B–2)? To answer this question, let’s consider a general
expression for a geometric series such as
N∑ −1
S= arn =arp +arp+1+arp+2 +...+arN−1, (B–3)
n=p
where n, N, and p are integers and a and r are any constants. Multiplying
Eq.(B–3) by rgives us
N∑ −1
Sr= arn+1 =arp+1+arp+2 +...+arN−1+arN. (B–4)
n=p
859

860 Closed Form of a Geometric Series
Subtracting Eq. (B–4) from Eq. (B–3) gives the expression
S–Sr=S(1 – r) =arp–arN,
or
S=a
⋅rp −rN
. (B–5)
1−r
So here’s what we’re after. The closed formof the series is
Closed form of a general
N∑ −1
arn =a
⋅rp
1
−
−
r
r
N
. (B–6)
geometric series: → n=p
(By “closed form,” we mean taking an infinite series and converting it to a
simpler mathematical form without the summation.) When a = 1, Eq. (B–6)
validates Eq. (B–1). We can quickly verify Eq. (B–6) with an example. Letting
N=5, p=0, a=2, and r=3, for example, we can create the following list:
⋅
n arn= 2 3n
⋅
0 2 30= 2
⋅
1 2 31= 6
⋅
2 2 32= 18
⋅
3 2 33= 54
⋅
4 2 34= 162
The sum of this column is
∑4
⋅
2 3n =242.
n=0
Plugging our example N,p,a, and rvalues into Eq. (B–6),
N∑ −1
arn =a
⋅rp −rN
=2
⋅30 −35
=2
⋅1−243
=242, (B–7)
1−r 1−3 –2
n=p
which equals the sum of the rightmost column in the list above.

Closed Form of a Geometric Series 861
As a final step, the terms of our earlier Eq. (B–2) are in the form of
Eq. (B–6) as p = 0, a = 1, and r =
e–j2πm/N.†
So plugging those terms from
Eq.(B–2) into Eq. (B–6) gives us
N∑ −1 e −j2πnm/N =1 ⋅e −j2πm0/N −e −j2πmN/N = 1−e −j2πm , (B–8)
1−e −j2πm/N 1−e −j2πm/N
n=0
confirming Eq. (B–2).
†From the math identity axy= (ax)y, we can say e–j2πnm/N= (e–j2πm/N)n, so r=e–j2πm/N.

This page intentionally left blank

APPENDIX C
Time Reversal
and the DFT
The notion of time reversal in discrete systems occasionally arises in the study
of the discrete Fourier transform (DFT), the mathematical analysis of digital
filters, and even in practice (straight time reversal is used in a digital filtering
scheme described in Section 13.12). We give the topic of time reversal some
deserved attention here because it illustrates one of the truly profound differ-
ences between the worlds of continuous and discrete systems. In addition, the
spectral effects of reversing a time sequence are (in my opinion) not obvious
and warrant investigation.
Actually, in discrete-time systems there are two forms of time reversal
we need to think about. Consider the 6-point x(n) time-domain sequence
x(n) = x(0),x(1),x(2),x(3),x(4),x(5). (C–1)
Due to the periodicity properties of discrete sampled representations
(discussed in Section 3.17), we can depict the x(n) time sequence as samples
on a circle as shown in Figure C–1(a). There we arbitrarily assign positive
time flow as counterclockwise rotation. (For our UK friends, counterclock-
wise means your anticlockwise.)
Time reversal, as defined here for sequences that are treated as periodic,
means traveling clockwise around the circle (in the negative time direction),
creating a new time sequence
x (n) = x(0),x(5),x(4),x(3),x(2),x(1). (C–2)
c
We call x (n) the circular time reversal of x(n), where the subscript “c”
c
meanscircularreversal, and depict x (n) as in Figure C–1(b).
c
863

864 Time Reversal and the DFT
x(2) x(1) x(4) x(5)
Positive Positive
time time
x(3) x(n) x(0) x(3) x(n) x(0)
c
Negative
time
x(4) x(5) x(2) x(1)
(a) (b)
Figure C–1 Circular representations of periodic sequences: (a) original x(n) se-
quence; (b) circular time reversal of x(n).
The interesting issue here is that for real N-point time sequences, the
DFT of x (n) is the complex conjugate of the DFT of x(n). That is,
c
X (m) = X*(m) (C–3)
c
where the DFT index is 0≤ m ≤ N–1. Due to the conjugate symmetry of DFTs
of real sequences, we should realize that X*(m) is a straight reversal of the
X(m) samples.
Let’s illustrate Eq. (C–3) with an example. With X(m) representing the
DFT of x(n), we can write down X(m)’sm=4 sample X(4) as
X(4)=x(0)e–j2π0/6+x(1)e–j2π4/6+x(2)e–j2π8/6
+x(3)e–j2π12/6+x(4)e–j2π16/6+x(5)e–j2π20/6.
(C–4)
Becausee–j2πk/6has
a period of 6, we can write Eq. (C–4) as
X(4)=x(0)e–j2π0/6+x(1)e–j2π4/6+x(2)e–j2π2/6
+x(3)e–j2π0/6+x(4)e–j2π4/6+x(5)e–j2π2/6.
(C–5)
Next, let’s write down the (circular-reversed) X (m)’sm=4-sample X (4) as
c c
X
(4)=x(0)e–j2π0/6+x(5)e–j2π4/6+x(4)e–j2π8/6
c
+x(3)e–j2π12/6+x(2)e–j2π16/6+x(1)e–j2π20/6
(C–6)
or
X
(4)=x(0)e–j2π0/6+x(5)e–j2π4/6+x(4)e–j2π2/6
c
+x(3)e–j2π0/6+x(2)e–j2π4/6+x(1)e–j2π2/6.
(C–7)
ReplacingX (4)’s negative angles with their positive-angle equivalents yields
c
X
(4)=x(0)ej2π0/6+x(5)ej2π2/6+x(4)ej2π4/6
c
+x(3)ej2π0/6+x(2)ej2π2/6+x(1)ej2π4/6,
(C–8)

Time Reversal and the DFT 865
x(n)
c
... x(1), x(0), x(5), x(4), x(3), x(2), x(1), x(0), x(5), x(4), x(3), x(2), x(1), x(0), ...
x(n)
s
Figure C–2 Periodic sequences x(n) and x(n).
s c
which is the conjugate of Eq. (C–5), demonstrating that X(m) and X (m) are
c
complex conjugates.
An alternate time reversal concept, which we’ll call straight time rever-
sal, is the simple reversal of Eq. (C–1)’s x(n), yielding an x (n) sequence
s
x (n) = x(5),x(4),x(3),x(2),x(1),x(0), (C–9)
s
where the subscript “s” means straight reversal. For real N-point time se-
quences, the DFT of x (n) is
s
X (m) =
X*(m)e–j2πm(N–1)/N.
(C–10)
s
We can demonstrate Eq. (C–10) the same way we did Eq. (C–3), but con-
sider Figure C–2. There we show the samples of repeated revolutions around
the x (n) circle in Figure C–1(b), indicating both the 6-point x (n) and the
c s
6-point x (n) sequences. Notice how x (n) is shifted backward in time by
c s
five samples from x (n).
c
Using the principle of the DFT’s shifting theorem from Section 3.6, we
know that X (m) is equal to X (m) times a linear phase shift of
e–j2πm(5)/6for
our
s c
N=6 example. So, in the general N-point sequence case,
X (m) = X
(m)e–j2πm(N–1)/N=X*(m)e–j2πm(N–1)/N,
(C–11)
s c
which validates Eq. (C–10).

This page intentionally left blank

APPENDIX D
Mean, Variance,
and Standard
Deviation
In our studies, we’re often forced to consider noise functions. These are de-
scriptions of noise signals that we cannot explicitly describe with a time-
domain equation. Noise functions can be quantified, however, in a worthwhile
way using the statistical measures of mean, variance, and standard deviation.
Although here we only touch on the very broad and important field of statis-
tics, we will describe why, how, and when to use these statistical indicators,
so that we can add them to our collection of signal analysis tools. First we’ll
determine how to calculate these statistical values for a series of discrete data
samples, cover an example using a continuous analytical function, and con-
clude this appendix with a discussion of the probability density functions of
several random variables that are common in the field of digital signal pro-
cessing. So let’s proceed by sticking our toes in the chilly waters of the mathe-
matics of statistics to obtain a few definitions.
D.1 STATISTICAL MEASURES
Consider a continuous sinusoid having a frequency of f Hz with a peak am-
o
plitude of A expressed by the equation
p
x(t) =A sin(2πf t). (D–1)
p o
Equation (D–1) completely specifies x(t)—that is, we can determine x(t)’s
exact value at any given instant in time. For example, when time t=1/4f , we
o
867

868 Mean, Variance, and Standard Deviation
x(n)
6.0
x = 5.17
ave
5.5
5.0
4.5
4.0
3.5
n
1 2 3 4 5 6 7 8
Figure D–1 Average of a sequence of eight values.
know that x(t)’s amplitude will beA ,and at the later time t=1/2f , x(t)’s am-
p o
plitude will be zero. On the other hand, we have no definite way to express
the successive values of a random function or of random noise.† There’s no
equation like Eq. (D–1) available to predict future noise-amplitude values, for
example. (That’s why they call it random noise.) Statisticians have, however,
developed powerful mathematical tools to characterize several properties of
random functions. The most important of these properties have been given
the names mean, variance, and standard deviation.
Mathematically, the sample mean, or average, of N separate values of a
sequence x, denoted x , is defined as[1]
ave
1
∑N x(1)+x(2)+x(3)+...+x(N)
x = x(n)= . (D–2)
ave N N
n=1
Equation (D–2), already familiar to most people, merely states that the aver-
age of a sequence of N numbers is the sum of those numbers divided by N.
Graphically, the average can be depicted as that value around which a series
of sample values cluster, or congregate, as shown in Figure D–1. If the eight
values depicted by the dots in Figure D–1 represent some measured quantity
and we applied those values to Eq. (D–2), the average of the series is 5.17, as
shown by the dotted line.
An interesting property of the average (mean value) of an x(n) sequence
is that x is the value that makes the sum of the differences between x(n) and
ave
x equal to zero. That is, the sum of the sequence d (n) =x(n) – x is zero.
ave iff ave
†We define random noise to be unwanted, unpredictable disturbances contaminating a signal or
a data sequence of interest.

D.1 Statistical Measures 869
Now that we’ve defined average, another key definition is the variance of
a sequence, σ2, defined as
1
∑N
σ2 = [x(n)−x ]2
N ave
n=1
(D–3)
[x(1)−x ]2 +[x(2)−x ]2 +[x(3)−x ]2 +...+[x(N)−x ]2
= ave ave ave ave .
N
Sometimes in the literature we’ll see σ2 defined with a 1/(N–1) factor before
the summation instead of the 1/N factor in Eq. (D–3). In a moment we’ll ex-
plain why this is so.
Variance is a very important concept because it’s the yardstick with
which we measure, for example, the effect of quantization errors and the use-
fulness of signal-averaging algorithms. It gives us an idea how the aggregate
values in a sequence fluctuate around the sequence’s average and provides us
with a well-defined quantitative measure of those fluctuations. Mathemati-
cians call those fluctuations the dispersion of the sequence. (Because the posi-
tive square root of the variance, the standard deviation, is typically denoted
as σ in the literature, we’ll use the conventional notation of σ2 for the vari-
ance.)
Equation (D–3) looks a bit perplexing if you haven’t seen it before. Its
meaning becomes clear if we examine it carefully. The x(1) – x value in the
ave
bracket, for example, is the difference between the x(1) value and the se-
quence average x . For any sequence value x(n), the x(n) –x difference,
ave ave
which we denote as Δ(n), can be either positive or negative, as shown in Fig-
ure D–2. Specifically, the differences Δ(1), Δ(2), Δ(3), and Δ(8) are negative be-
cause their corresponding sequence values are below the sequence average
shown by the dotted line. If we replace the x(n) –x difference terms in
ave
Eq.(D–3) with Δ(n) terms, the variance can be expressed as
1
∑N
σ2 =
N
[Δ(n)]2, where Δ(n)=x(n)−x
ave
. (D–4)
n=1
The reader might wonder why the squares of the differences are
summed, instead of just the differences themselves. This is because, by the
very nature of the definition of x , the sum of the Δ(n) difference samples
ave
will always be zero. Because we need an unsigned measure of each differ-
ence, we use the difference-squared terms as indicated by Eq. (D–4). In that
way, individual Δ(n) difference terms will contribute to the overall variance
regardless of whether the difference is positive or negative. Plugging the Δ(n)
values from the example sequence in Figure D–2 into Eq. (D–4), we get a

870 Mean, Variance, and Standard Deviation
x(n)
6.0
5.5
–Δ(1)
Δ(4)
Δ(5)
Δ(6)
Δ(7)
–Δ(8)
5.0 –Δ(2)
4.5 –Δ(3)
x = 5.17
ave
4.0
3.5
n
1 2 3 4 5 6 7 8
Figure D–2 Difference values Δ(n) of the sequence in Figure D–1.
variance value of 0.34. Another useful measure of a signal sequence is the
square root of the variance known as the standard deviation. Taking the square
root of Eq. (D–3) to get the standard deviation σ,
1
∑N
σ= σ2 = [x(n)−x ]2. (D–5)
N ave
n=1
So far, we have three measurements to use in evaluating a sequence of
values: the average x , the variance σ2, and the standard deviation σ.
ave
Where x indicates around what constant level the individual sequence val-
ave
ues vary, σ2 is a measure of the magnitude of the noise fluctuations around
the average x . If the sequence represents a series of random signal samples,
ave
we can say that x specifies the average, or constant, value of the signal. The
ave
variance σ2 is the magnitude squared, or power, of the fluctuating compo-
nent of the signal. The standard deviation, then, is an indication of the magni-
tude of the fluctuating component of the signal.
D.2 STATISTICS OF SHORT SEQUENCES
In this section we discuss a subtle issue regarding the variance of a discrete
sequence. The variance Eq. (D–3) is only exactlycorrect if Nis infinitely large.
When N is a small number and we’re computing an [x(4)–x ] term, for ex-
ave
ample, that [x(4)–x ] value is too highly influenced (biased) by the single
ave
x(4) sample. This results in an [x(4)–x ] value that’s slightly smaller than it
ave
should be[2]. As such, Eq. (D–3) is often called a biased estimate of the true
variance of x(n). Mathematicians have determined that using a 1/(N–1) factor,
called Bessel’s correction, before the summation in Eq. (D–3) yields a more ac-

D.2 Statistics of Short Sequences 871
curate estimation of the true variance of the infinite-length sequence x(n),
when we use only Nsamples of x(n) to estimate the true variance. That is,
1
∑N
σ u 2 nbiased = N−1 [x(n)−x ave ]2. (D–6)
n=1
Equation (D–6) is called an unbiased estimateof the variance of x(n). However,
when N is greater than, say, 100, as it often is in real-world applications, the
difference between Eqs. (D–3) and (D–6) will have little practical significance.
We can justify that claim by showing an example of the percent differ-
ence in using Eqs. (D–3) and (D–6), as a function of the x(n) sequence length
N, as the solid curve in Figure D–3. Considering the unbiased variance to be
correct (zero error), the solid error curve in Figure D–3 shows how much
smaller (negative percent error) the biased variance will be compared to the
unbiased variance when x(n) is Gaussian (to be described later) distributed
random noise of unity variance. For instance, the percent error between the
biased and the unbiased variance estimates is roughly –1 percent when N =
100. The dashed curve in Figure D–3 is equal to –100 percent times the true
x(n) variance divided by N, so we can say that the percent error in using Eq.
(D–3) compared to Eq. (D–6) is roughly
σ2
Biased variancepercenterror=− 100× true. (D–7)
N
The bottom line here is that Eq. (D–6) should be considered for use in com-
puting the variances of discrete sequences when Nis small. Section 13.35 dis-
cusses a computationally efficient, and memory-saving, way to compute
variances.
0
−10
err
or
error ~~ −1%
%
−20
−30
0 20 40 60 80 100
N
Figure D–3 Percent error in Eq. (D–3) relative to Eq. (D–6).

872 Mean, Variance, and Standard Deviation
D.3 STATISTICS OF SUMMED SEQUENCES
Here we discuss the statistical effects of adding two sequences. This material
has great utility in noise-reduction operations. If we add two equal-length in-
dependent (uncorrelated) sequences q(n) and r(n), such that
p(n) = q(n) + r(n), (D–8)
thanks to the good work of dead mathematicians we can say[3]:
• The average (mean) of the p(n) sequence is equal to the sum of the indi-
vidual averages of the q(n) and r(n) sequences.
• The variance of the p(n) sequence is equal to the sum of the individual
variances of the q(n) and r(n) sequences. That is,
σ2 =σ2 +σ2.
p q r
This means that if we consider the variances of two signals as being
measures of their noise powers, then when two noisy signals are added,
the resultant signal’s noise power is the sum of the two individual noise
powers.
• The variance of C · p(n) = C · q(n) + C · r(n), where C is a constant, is C2
times the variance of the p(n) sequence, or
σ2 =C2σ2.
C⋅p p
The above properties are related to a key characteristic of sampled sig-
nals that we can use for noise reduction by way of averaging. Assume we
have an infinitely long x(n) sequence contaminated with uncorrelated noise,
and the variance of x(n) is K. If we extract Nblocks of samples from x(n), with
each block sequence being M samples in length, and average those N se-
quences, the variance of the resultant single M-sample average sequence is
⎛x (n)+x (n)+...+x (n)+x (n)⎞
Varianceof ⎜ 0 1 N−2 N−1 ⎟
⎝ NN ⎠
1 NK K
= ⋅(K+K+...+K+K)= = . (D–9)
N2 N2 N
The square root of Eq. (D–9) yields the standard deviation of the single
M-sample average sequence as

D.3 Statistics of Summed Sequences 873
x (n),N = 4 The
ave
standard
deviation
10
of these
samples
isσ /2 = 2.
x
0
0 10 20 30 40 50 60 n
x (0), the average of the first
ave
group of four x(n) samples.
Figure D–4 x (n) sequence when N= 4.
ave
σ
σ = x
ave (D–10)
N
where σ is the standard deviation of the original x(n) sequence.
x
As an example of Eq. (D–10), say that we have an x(n) sequence and
compute the average of the first N samples of x(n), x(0) through x(N–1), to
produce an x (0) sample. Next we compute the average of the second set of
ave
Nsamples of x(n), x(N) through x(2N–1), to produce an x (1) sample, and so
ave
on. If the standard deviation of an x(n) sequence, having an average value of
10 and standard deviation σ = 4, Figure D–4 shows the N= 4-point averaged
x
x (n) sequence having an average value of 10 and a reduced standard devia-
ave
tion of σ /N = 4/2 = 2. Chapter 11 gives practical examples of using Eq.
x
(D–10) in real-world situations.
On a practical note, if x (n) are signal samples and x (n) are noise sam-
s n
ples, we can think of the x(n) samples in Eqs. (D–9) and (D–10) as being repre-
sented by x(n) = x (n) + x (n). The notion of contaminating noise being
s n
uncorrelated means that all the x (n) noise samples are independent from
n
each other, which implies that no information about any one noise sample can
be determined from knowledge of any of the other noise samples. This as-
sumption is not always valid if a noisy x(n) signal has been filtered. With low-
pass filtering, adjacent noise samples will be correlated (their amplitudes will
be similar); the narrower the lowpass filter’s passband, the more adjacent
noise samples tend to be correlated. If the lowpass filter’s passband is wide
relative to half the sample rate (f/2), then the correlation among noise sam-
s
ples will be low and the noise samples can be considered uncorrelated. If the
lowpass filter’s passband is very narrow relative to f/2, then averaging is not
s
as effective as we might expect from Eqs. (D–9) and (D–10).
We have discussed many statistical measures of real-valued discrete se-
quences, so Table D–1 compiles what we’ve learned so far. The x(n) sequence

874 Mean, Variance, and Standard Deviation
Table D–1 Statistical Measures of Real-Valued Sequences
Signal Statistical Measure Interpretation
1
N∑ −1 Mean of the x(n) signal samples squared. In-
Power= [x(n)]2 terpreted as the average signal power.
N
n=0
Root mean square (rms) of x(n) signal. An
x = 1
N∑ −1
[x(n)]2 equivalent signal amplitude whose square is
rms N the average signal power. rms has the same
n=0
dimensions (units) as x(n).
1
N∑ −1 The average (mean) value of x(n) signal. Inter-
x = x(n) preted as the DC (zero Hz) bias, or DC compo-
ave N n=0 nent, of a signal. x ave has the same dimensions
as x(n).
(x )2 DC power.
ave
The variance of x(n) signal. Interpreted as the
σ2 = 1
∑N
[x(n)−x ]2 power of the fluctuating (alternating, AC) por-
N n=1 ave tion of a signal. When x ave = 0, σ2= average
signal power.
The standard deviation of x(n) signal. An
σ = 1
∑N
[x(n)−x ]2 equivalent signal amplitude of the fluctuating
N ave (alternating, AC) portion of a signal, whose
n=1
square is the signal variance. σhas the same
dimensions as x(n). When x = 0, σ= x .
ave rms
in the table can be an information-carrying signal, a noise-only signal, or a
combination of the two.
D.4 STANDARD DEVIATION (RMS) OF A CONTINUOUS SINEWAVE
In computing the average power in electric circuits, for sinewave signals engi-
neers often use a parameter called the rms value of the sinewave. That pa-
rameter, x , for discrete samples is defined as
rms
1
∑N
x = x(n)2. (D–11)
rms N
n=1
The x(n) in Eq. (D–11) is the square root of the mean (average) of the
rms
squares of the sequence x(n). For a continuous sinusoid x(t) = A sin(2πft) =
p
A sin(ωt) whose average value is zero, x is x defined as
p rms rms-sine

D.5 Estimating Signal-to-Noise Ratios 875
2π
x rms-sine = 2 1 π ⋅ ∫ ⎡ ⎣A p sin(ωt) ⎤ ⎦ 2 d(ωt)
0
= A p 2 ⋅ 2 ∫ π 1 ⎡⎣1−cos(ωt)⎤⎦d(ωt) = A p 2 ⋅ ⎡ ⎢ ωt −− 1 sin(ωt) ⎤ ⎥ 2π
2π 2 2π ⎣ 2 2 ⎦
0 0
A 2 ⎡2π⎤ A
= p ⋅ ⎢ ⎥ = p . (D–12)
2π ⎣ 2 ⎦ 2
This x expression is a lot easier to use for calculating average power dis-
rms-sine
sipation in circuit elements than performing the integral of more complicated
expressions. When a signal’s average value is zero, then its rms value is equal
to the signal’s standard deviation. The variance of a sinewave is, of course,
the square of Eq. (D–12).
We’ve provided the equations for the mean (average) and variance of a
sequence of discrete values, introduced an expression for the standard devia-
tion or rms of a sequence, and given an expression for the rms value of a con-
tinuous sinewave. The next question is “How can we characterize random
functions for which there are no equations to predict their values and we
have no discrete sample values with which to work?” The answer is that we
must use probability density functions. Before we do that, in Section D.6, let’s
first show how to use our statistical measures to estimate the signal-to-noise
ratio of a discrete signal.
D.5 ESTIMATING SIGNAL-TO-NOISE RATIOS
Given the above statistics of sampled signals, we now discuss a widely used
way to quantify the quality of a noise-contaminated signal. By “quality” we
mean the difference between listening to a recording of the Beatles’ song
“Hey Jude” on your iPod in a library and listening to the song while standing
next to a running jet engine. We quantify the quality of a noise-contaminated
signal by measuring, or estimating, its signal-power-to-noise-power ratio
(SNR). The SNR of a signal is the ratio of the power of the noise-free signal
over the power of the noise, or
Signal power
SNR= . (D–13)
Noisepower

876 Mean, Variance, and Standard Deviation
To illustrate the notion of SNR, the following list shows the SNRs (in dB)
of a few common signal processing devices:
Signal type SNR
Intelligible human speech 10 dB
Home video 30 dB
Analog telephone line 32 dB
Studio-quality video 40 dB
Maximum possible for 8-bit data 49.7 dB
AM analog radio 50 dB
Analog cassette tape player 60 dB
FM analog radio 65 dB
Modern long-play (LP) vinyl records 70 dB
Portable CD player 80 dB
Top-quality consumer CD player 95 dB
Maximum possible for 16-bit data 97.7 dB
Highest-quality studio audio 120 dB
The SNR of a signal can be estimated in either the time domain or the
frequency domain. We discuss those operations next.
D.5.1 Estimating SNR in the Time Domain
We can estimate, by way of time-domain measurement, the SNR of a signal
based on time-domain sample values. If x (n) are real-valued signal samples
s
and x (n) are real-valued noise samples, the SNR of a signal x(n) = x (n) +
n s
x (n) is
n
1 N∑ −11
[x (n)]2
Signal power N s
SNR= =
n=0
(D–14)
Noisepower 1 N∑ −1
[x (n)]2
N n
n=0
where the divide-by-N operations are shown for clarity but need not be per-
formed because they cancel in the numerator and denominator. If we know
the variances of x (n) and x (n), we can express the SNR of the fluctuating
s n
(AC) portion of a signal as

D.5 Estimating Signal-to-Noise Ratios 877
Signalvariance
σ2
SNR= = s . (D–15)
Noisevariance σ2
n
In practice signal powers can vary over many orders of magnitude. For
example, military radar systems transmit signals whose power is measured in
megawatts, while the signal received by your cell phone antenna is measured
in microwatts. That’s 12 orders of magnitude! As such, it’s both convenient
and common to describe signal power and noise power logarithmically using
decibels. (Decibels are discussed in Appendix E.) We express signal-to-noise
ratios measured in decibels (dB) as
SNR = 10 · log (SNR) dB (D–16)
dB 10
where the SNR term in Eq. (D–16) is the SNR value from Eqs. (D–14) or
(D–15). If we know the rms values of x (n) and x (n), then we can express a
s n
signal’s SNR in dB as
⎛Signal rms⎞
SNR =20⋅log ⎜ ⎟ dB. (D–17)
dB 10⎝ Noiserms ⎠
Because the ratio in Eq. (D–17) is in terms of amplitudes (voltages or cur-
rents), rather than powers, we’re forced to use the factor of 20 in computing
SNR based on rms values. If we know the standard deviations of x (n) and
dB s
x (n), we can express the SNR of the fluctuating (AC) portion of a signal in dB
n
as
⎛σ ⎞
SNR
dB
=20⋅log
10⎝
⎜σ s
⎠
⎟ dB. (D–18)
n
The values for linear SNR, Eq. (D–14), are always positive, but values for
SNR can be positive or negative. For example, if a signal’s linear SNR is 4,
dB
then its SNR is 10 · log (4) = 6 dB. If a signal’s linear SNR is 1/4, then its
dB 10
SNR is 10 · log (1/4) = –6 dB.
dB 10
D.5.2 Estimating SNR in the Frequency Domain
We can obtain a rough estimate of the SNR of a signal based on its frequency-
domain characteristics. The standard procedure for doing so is as follows: As-
sume we have N= 100 samples of the noisy 986 Hz real-valued x(n) sinusoid,
where the sample rate is f = 8 kHz, as shown in Figure D–5(a). After perform-
s
ing a 100-point DFT, and computing the spectral magnitude-squared sample
values, we obtain the positive-frequency |X(m)|2power spectral samples de-
picted in Figure D–5(b).

878 Mean, Variance, and Standard Deviation
0.5
x(n)
(a) 0
−0.5
0 20 40 60 80 100
n
60
|X(m)|2
40
(b)
20 Threshold
0
0 1 2 3 4
Frequency (kHz)
Figure D–5 SNR estimation example: (a) noisy time-domain sinusoid; (b) 100-point
DFT power samples.
Next we determine a Threshold power value, the dashed line in Figure
D–5(b), above which only signal-related power samples exist and below
which are the noise-only power samples. The estimated SNR of x(n) is then
Sumofthe|X(m)|2 samples aboveThreshoold
SNR= . (D–19)
Sumofthe|X(m)|2 samples below Threshoold
The SNR measured in dB is found using
SNR = 10 · log (SNR) dB. (D–20)
dB 10
There are several practical topics to keep in mind when estimating SNR
by way of frequency-domain samples:
• For computational-efficiency reasons, the length of x(n) should be an in-
teger power of two so that fast Fourier transforms (FFTs) can be used to
obtain an |X(m)|2sequence.
• Due to the spectral symmetry of real-only time samples, we need only
examine the |X(m)|2power samples in the range 0 ≤m≤N/2, i.e., posi-
tive frequency.
• The Threshold value should be set such that as many of the signal
power samples as possible, including any harmonics of the fundamental
signal, are above that Threshold value.
• If we repeat our SNR estimation computation on multiple non-overlap-
ping N-sample x(n) sequences, we’ll see a noticeable variation (variance)

D.6 The Mean and Variance of Random Functions 879
in the various SNR estimation results. To improve the accuracy, and re-
peatability, of our SNR estimation it’s prudent to collect many blocks of
N-sample x(n) sequences and perform many FFTs to compute multiple
|X(m)| magnitude sequences. Then those multiple |X(m)| sequences
are averaged before computing a single |X(m)|2power sequence for use
in Eq. (D–19). The idea is to improve the accuracy (reduce the variance)
of our SNR estimations by way of averaging as indicated by Eq. (D–2).
D.5.3 Controlling Test Signal SNR in Software
For completeness, below are methods for adjusting the SNR of a real-valued
discrete test signal generated in software. Here’s what we mean. Assume we
have generated a noise-contaminated zero-mean signal sequence x(n) = x (n)
s
+ x (n), where x (n) are noise-free signal samples and x (n) are noise-only
n s n
samples. We can adjust the SNR of x(n) to a desired value of SNR , mea-
new
sured in dB, by scaling the x (n) noise samples as
n
x (n)= K⋅x (n) (D–21)
n,new n
where
N∑ −1
[x (n)]2
s
K= n=0 ⋅10 −SNR new /110. (D–22)
N∑ −1
[x (n)]2
n
n=0
So the SNR of the new x (n) = x (n) + x (n) sequence will be SNR dB
new s n,new new
where the original x (n) noise-free samples remain unchanged. Notice that the
s
ratio in Eq. (D–22) is the linear (not dB) SNR of the original x(n) sequence.
In a similar manner, we scale the original x (n) noise-free samples as
s
x (n)=x (n)/ K (D–23)
s,new s
so that the SNR of the new x (n) = x (n) + x (n) sequence will be the de-
new s,new n
sired SNR dB. In this case the original x (n) noise samples remain un-
new n
changed.
D.6 THE MEAN AND VARIANCE OF RANDOM FUNCTIONS
To determine the mean or variance of a random function, we use what’s
called the probability density function. The probability density function (PDF) is
a measure of the likelihood of a particular value occurring in some function.

880 Mean, Variance, and Standard Deviation
We can explain this concept with simple examples of flipping a coin or throw-
ing dice as illustrated in Figures D–6(a) and (b). The result of flipping a coin
can only be one of two possibilities: heads or tails. Figure D–6(a) indicates this
PDF and shows that the probability (likelihood) is equal to one-half for both
heads and tails. That is, we have an equal chance of the coin side facing up
being heads or tails. The sum of those two probability values is one, meaning
that there’s a 100 percent probability that either a head or a tail will occur.
Figure D–6(b) shows the probability of a particular sum of the upper
faces when we throw a pair of dice. This probability function is not uniform
because, for example, we’re six times more likely to have the die faces sum to
seven than sum to two (snake eyes).
We can say that after tossing the dice a large number of times, we should
expect that 6/36 = 16.7 percent of those tosses would result in sevens, and
1/36 = 2.8 percent of the time we’ll get snake eyes. The sum of those 11 proba-
bility values in Figure D–6(b) is also one, telling us that this PDF accounts for
all (100 percent) of the possible outcomes of throwing the dice.
The fact that PDFs must account for all possible result conditions is em-
phasized in an interesting way in Figure D–6(c). Suppose a woman says, “Of
my two children, one is a girl. What’s the probability that my daughter has a
sister?” Be careful now—curiously enough, the answer to this controversial
question is not a 50-50 chance. There are more possibilities to consider than
the girl just having a brother or a sister. We can think of all the possible com-
binations of birth order of two children such that one child is a girl. Because
we don’t know the gender of the first-born child, there are three gender order
Coin face PDF Sum of dice PDF
1/2 6/36
4/36
2/36
Coin 2 3 4 5 6 7 8 9 10 11 12 Sum of
Heads Tails face dice faces
(b)
(a)
Birth order PDF
1/3
(c)
girl boy girl Gender
then then then birth order
boy girl girl
Figure D–6 Simple probability density functions: (a) probability of flipping a single
coin; (b) probability of a particular sum of the upper faces of two
dice; (c) probability of the order of birth of the girl and her sibling.

D.6 The Mean and Variance of Random Functions 881
possibilities: girl, then boy; boy, then girl; and girl, then girl as shown in Figure
D–6(c). So the possibility of the daughter having a sister is 1/3 instead of 1/2!
(Believe it.) Again, the sum of those three 1/3rd probability values is one.
Two important features of PDFs are illustrated by the examples in Fig-
ure D–6: PDFs are always positive and the area under their curves must be
equal to unity. The very concept of PDFs make them a positive likelihoodthat a
particular result will occur, and the fact that some result must occur is equiva-
lent to saying that there’s a probability of one (100 percent chance) that we’ll
have that result. For continuous probability density functions, p(x), we indi-
cate these two characteristics by
PDF values are never negative: →p(x) ≥0, (D–24)
and
(cid:4)
The sum of all the PDF values is one: → ∫ p(x)dx=1. (D–25)
-(cid:4)
In Section D.1 we illustrated how to calculate the average (mean) and
variance of discrete samples. We can also determine these statistical measures
for a random function x if we know the PDF of that function. Using μ to de-
x
note the average of a random function of x, μ is defined as
x
(cid:4)
∫
μ = x⋅p(x)dx (D–26)
x
-(cid:4)
and the variance of xis defined as[4]
(cid:4) (cid:4)
∫ ∫
σ2= (x−μ )2⋅p(x)dx = x2⋅p(x)dx−μ2. (D–27)
x x x
-(cid:4) -(cid:4)
In digital signal processing, we’ll encounter continuous probability den-
sity functions that are uniform in value similar to the examples in Figure D–3.
In these cases it’s easy to use Eqs. (D–26) and (D–27) to determine their aver-
age and variance. Figure D–7 illustrates a uniform continuous PDF indicating
Continuousp(x)
1 1
=
b– (–a) b+a
–a 0 b x
Figure D–7 Continuous uniform probability density function.

882 Mean, Variance, and Standard Deviation
a random function whose values have an equal probability of being any-
where in the range from –ato b.
From Eq. (D–25) we know that the area under the curve must be unity
(i.e., the probability is 100 percent that the value will be somewhere under the
curve). So the amplitude of p(x) must be the area divided by the width, or p(x)
= 1/(b+ a). From Eq. (D–26) the average of this p(x) is
(cid:4) (cid:4) b
∫ ∫ 1 1 ∫∫
μ = x⋅p(x)dx = x⋅ dx = xdx
x b+a b+a
-(cid:4) -(cid:4) -a
1 ⎡ x2⎤b b2 −a2 (b+a)(b−a) b−a
= ⎢ ⎥ = = = , (D–28)
b+a⎣ 2 ⎦ 2(b+a) 22(b+a) 2
−a
which happens to be the midpoint in the range from –a to b. The variance of
the PDF in Figure D–7 is
∫ (cid:4) ∫ b 1 (b−a)2
σ2= x2⋅p(x) dx−μ2 = x2 dx−
x x b+a 4
-(cid:4) -a
1 ⎡ x3⎤b (b−a)2 1 (b−a)2
= ⎢ ⎥ − = ⋅(b3+a3)−−
b+a⎣ 3 ⎦ 4 3(b+a) 4
−a
(b+a)(b2 −ab+a2) b2 −2ab+a2
= −
3(b+a) 4
b2 +2ab+a2 (b+a)2
= = . (D–29)
12 12
We use the results of Eqs. (D–28) and (D–29) in Chapter 9 to analyze the er-
rors induced by quantization from analog-to-digital converters, and the ef-
fects of finite word lengths of hardware registers.
D.7 THE NORMAL PROBABILITY DENSITY FUNCTION
Aprobability density function (PDF) that’s so often encountered in nature de-
serves our attention. This function is so common that it’s actually called the
normalPDF and is also sometimes called the GaussianPDF. (Ascheme for gen-
erating discrete data to fit this function is discussed in Section 13.12.)

References 883
p(x)
0.4 0.3989
0.3
0.2
0.1
μ
x
–3σ
μ–2σ
μ
x
–σ μ
x
μ
x
+σ
μ+2 σ
μ
x
+3σ x
x x
68.27%
95.45%
99.73%
Figure D–8 A normal PDF with mean = μ and standard deviation = σ.
x
This function, whose shape is shown in Figure D–8, is important be-
cause random data having this distribution is very useful in testing both soft-
ware algorithms and hardware processors. The normal PDF is defined
mathematically by
(D–30)
p(x)= 1 e −(x−μ x )2/2σ2 .
σ 2π
The area under the curve is one and the percentages at the bottom of
Figure D–8 tell us that, for random functions having a normal distribution,
there’s a 68.27 percent chance that any particular value of x will differ from
the mean by ≤σ. Likewise, 99.73 percent of all the xdata values will be within
3σof the mean μ .
x
REFERENCES
[1] Papoulis, A. Probability Random Variables, and Stochastic Processes, McGraw-Hill, New York,
1965, pp. 189, pp. 266–268.
[2] Miller, Irwin, and Freund, John. Probability and Statistics for Engineers, 2nd ed., Prentice
Hall, Englewood Cliffs, New Jersey, 1977.
[3] Meyer, B. Introductory Probability and Statistical Applications, Addison-Wesley, Reading,
Massachusetts, 1965, pp. 122–125.
[4] Bendat, Julius, and Piersol, Allen. Measurement and Analysis of Random Data, John Wiley
and Sons, New York, 1972.

This page intentionally left blank

APPENDIX E
Decibels
(dB and dBm)
This appendix introduces the logarithmic function used to improve the mag-
nitude resolution of frequency-domain plots when we evaluate signal spec-
tra, digital filter magnitude responses, and window function magnitude
responses. When we use a logarithmic function to plot signal levels in the fre-
quency domain, the vertical axis unit of measure is decibels.
E.1 USING LOGARITHMS TO DETERMINE
RELATIVE SIGNAL POWER
In discussing decibels, it’s interesting to see how this unit of measure
evolved. When comparing continuous (analog) signal levels, early specialists
in electronic communications found it useful to define a measure of the differ-
ence in powers of two signals. If that difference was treated as the logarithm
of a ratio of powers, it could be used as a simple additive measure to deter-
mine the overall gain or loss of cascaded electronic circuits. The positive loga-
rithms associated with system components having gain could be added to the
negative logarithms of those components having loss quickly to determine
the overall gain or loss of the system. With this in mind, the difference be-
tween two signal power levels (P and P ), measured in bels, was defined as
1 2
the base 10 logarithm of the ratio of those powers, or
⎛ ⎞
P
Power difference=log ⎜ 1⎟ bels. † (E–1)
10⎝P ⎠
2
†The dimensionless unit of measure belwas named in honor of Alexander Graham Bell.
885

886 Decibels (dB and dBm)
The use of Eq. (E–1) led to another evolutionary step because the unit of
bel was soon found to be inconveniently large. For example, it was discov-
ered that the human ear could detect audio power level differences of
one-tenth of a bel. Measured power differences smaller than one bel were so
common that it led to the use of the decibel (bel/10), effectively making the
unit of bel obsolete. The decibel (dB), then, is a unit of measure of the relative
power difference of two signals defined as
⎛ ⎞
Power difference=10 ⋅ log ⎜ P 1⎟ dB. (E–2)
10⎝P ⎠
2
The logarithmic function 10.log (P /P ), plotted in Figure E–1, doesn’t seem
10 1 2
too beneficial at first glance, but its application turns out to be very useful.
Notice the large change in the function’s value when the power ratio (P /P )
1 2
is small, and the gradual change when the ratio is large. The effect of this
nonlinearity is to provide greater resolution when the ratio P /P is small,
1 2
giving us a good way to recognize very small differences in the power levels
of signal spectra, digital filter responses, and window function frequency
responses.
Let’s demonstrate the utility of the logarithmic function’s variable reso-
lution. First, remember that the power of any frequency-domain sequence
representing signal magnitude |X(m)| is proportional to |X(m)| squared.
For convenience, the proportionality constant is assumed to be one, so we say
the power of |X(m)| is
discrete power spectrum of X(m)= |X(m)|2. (E–3)
10 . log(P /P ), in dB
1 2
4
2
0
0 .1 .3 .5 .7 .9 1 1.1 1.3 1.5 1.7 1.9 2.0 P /P
–2 1 2
–4
–6
–8
–10
–12
–14
–16
–18
Figure E–1 Logarithmic decibel function of Eq. (E–2).

E.1 Using Logarithms to Determine Relative Signal Power 887
Although Eq. (E–3) may not actually represent power (in watts) in the classi-
cal sense, it’s the squaring operation that’s important here, because it’s analo-
gous to the traditional magnitude squaring operation used to determine the
power of continuous signals. (Of course, if X(m) is complex, we can calculate
the power spectrum sequence using |X(m)|2 = X (m)2 + X (m)2.) Taking
real imag
ten times the log of Eq. (E–3) allows us to express a power spectrum sequence
X (m) in dB as
dB
.
X (m) = 10 log (|X(m)|2) dB. (E–4)
dB 10
Because log(x2) = log(x) + log(x) = 2log(x), we can eliminate the squaring op-
eration in Eq. (E–4) by doubling the factor of ten and represent the power
spectrum sequence by the expression
.
X (m) = 20 log (|X(m)|) dB. (E–5)
dB 10
Without the need for the squaring operation, Eq. (E–5) is a more convenient
way than Eq. (E–4) to calculate the X (m) power spectrum sequence from the
dB
X(m) sequence.
Equations (E–4) and (E–5), then, are the expressions used to convert a
linear magnitude axis to a logarithmic magnitude-squared, or power, axis
measured in dB. What we most often see in the literature are normalized log
magnitude spectral plots where each value of |X(m)|2 is divided by the first
|X(0)|2power value (for m=0), as
normalized X dB (m)=10 ⋅ log 10 ⎛ ⎝ ⎜ | | X X ( ( m 0) ) | | 2 2⎞ ⎠ ⎟ =20 ⋅ log 10 ⎛ ⎝ ⎜ | | X X ( ( m 0) ) | |⎞ ⎠ ⎟dB. (E–6)
The division by the |X(0)|2 or |X(0)| value always forces the first value in
the normalized log magnitude sequence X (m) equal to 0 dB.† This makes it
dB
easy for us to compare multiple log magnitude spectral plots. To illustrate,
let’s look at the frequency-domain representations of the Hanning and trian-
gular window functions. The magnitudes of those frequency-domain func-
tions are plotted on a linear scale in Figure E–2(a) where we’ve arbitrarily
assigned their peak values to be 2. Comparing the two linear scale magnitude
sequences, W (m) and W (m), we can see some minor differences
Hanning triangular
between their magnitude values. If we’re interested in the power associated
with the two window functions, we square the two magnitude functions and
plot them on a linear scale as in Figure E–2(b). The difference between the two
window functions’ power sequences is impossible to see above the frequency
of, say, m = 8 in Figure E–2(b). Here’s where the dB scale helps us out. If we
plot the normalized log magnitude versions of the two magnitude-squared
†That’s because log (|X(0)|/|X(0)|)=log (1)=0.
10 10

888 Decibels (dB and dBm)
Window function magnitude responses
2.0 on a linear scale
1.8
1.6
1.4 |W H a n n i n g (m)|
1.2
(a) 1.0
0.8 |W (m)|
triangular
0.6
0.4
0.2
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 Frequency
(m)
Window function magnitude-squared
4.0 responses on a linear scale
3.6
3.2
2.8 2
|W H a n n i n g (m)|
2.4
(b) 2.0
1.6
2
|W (m)|
1.2 triangular
0.8
0.4
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 Frequency
(m)
Window function magnitude-squared responses
0 on a normalized logarithmic scale (dB)
–10 W (m), Hanning
H
–20
W (m), triangular
T
–30
(c) –40
–50
–60
–70
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 Frequency
(m)
Figure E–2 Hanning (white squares) and triangular (black squares) window func-
tions in the frequency domain: (a) magnitude responses using a linear
scale; (b) magnitude-squared responses using a linear scale; (c) log
magnitude responses using a normalized dB scale.

E.2 Some Useful Decibel Numbers 889
sequences on a logarithmic dB scale using Eq. (E–6), the difference between
the two functions will become obvious.
Normalization, in the case of the Hanning window, amounts to calculat-
ing the log magnitude sequence normalized over |W (0)| as
Hanning
W (m)=10 ⋅ log
⎛
⎜ |W Hanning
(m)|2⎞
⎟ =20 ⋅ log
⎛
⎜ |W Hanning (m)|
⎞
⎟ dB . (E–7)
H 10 ⎝|W (0)|2 ⎠ 10⎝|W (0)|⎠
Hanning Hanning
The normalized log magnitude sequences are plotted in Figure E–2(c). We can
now clearly see the difference in the magnitude-squared window functions in
Figure E–2(c) as compared to the linear plots in Figure E–2(b). Notice how
normalization forced the peak values for both log magnitude functions in Fig-
ure E–2(c) to be zero dB. (The dots in Figure E–2 are connected by lines to em-
phasize the sidelobe features of the two log magnitude sequences.)
Although we’ve shown the utility of dB plots using window function
frequency responses as examples, the dB scale is equally useful when we’re
plotting signal-power spectra or digital filter frequency responses. We can
further demonstrate the dB scale using a simple digital filter example. Let’s
say we’re designing an 11-tap highpass FIR filter whose coefficients are
shown in Figure E–3(a). If the center coefficient h(5) is –0.48, the filter’s fre-
quency magnitude response |H (m)| can be plotted as the white dots on
–0.48
the linear scale in Figure E–3(b). Should we change h(5) from –0.48 to –0.5, the
new frequency magnitude response |H (m)| would be the black dots in
–0.5
Figure E–3(b). It’s difficult to see much of a difference between |H (m)|
–0.48
and |H (m)| on a linear scale. If we used Eq. (E–6) to calculate two normal-
–0.5
ized log magnitude sequences, they could be plotted as shown in Figure
E–3(c), where the filter sidelobe effects of changing h(5) from –0.48 to –0.5 are
now easy to see.
E.2 SOME USEFUL DECIBEL NUMBERS
If the reader uses dB scales on a regular basis, there are a few constants worth
committing to memory. A power difference of 3 dB corresponds to a power
factor of two; that is, if the magnitude-squared ratio of two different fre-
quency components is 2, then from Eq. (E–2),
power difference=10 ⋅ log ⎛ ⎜ 2⎞ ⎟ =10 ⋅ log (2)=3.01≈3 dB . (E–8)
10⎝ ⎠ 10
1
Likewise, if the magnitude-squared ratio of two different frequency compo-
nents is 1/2, then the relative power difference is –3 dB because

890 Decibels (dB and dBm)
FIR filter coefficients, h(k)
0.4
0.2
2 5 8
(a) 0
0 1 3 4 6 7 9 10 k
–0.2
–0.4
–0.48
–0.6
FIR filter magnitude responses
on a linear scale
1.2
1
|H (m)|
–0.5
0.8 (black)
(b)
0.6
0.4
|H (m)|(white)
0.2 –0.48
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 Frequency
(m)
FIR filter magnitude responses
on a logarithmic scale (dB)
0
|H (m)|
–0.5
–10
|H (m)|
–0.48
–20
(c)
–30
–40
–50
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 Frequency
(m)
Figure E–3 FIR filter magnitude responses: (a) FIR filter time-domain coefficients;
(b) magnitude responses using a linear scale; (c) log magnitude re-
sponses using the dB scale.

E.3 Absolute Power Using Decibels 891
Table E–1 Some Useful Logarithmic Relationships
Magnitude-
Magnitude squared power Relative dB
ratio (P /P ) ratio (approximate)
1 2
10–1/2 10–1 –10 ←P is one-tenth P
1 2
2–1 2–2= 1/4 –6 ←P is one-fourth P
1 2
2–1/2 2–1= 1/2 –3 ←P is one-half P
1 2
20 20= 1 0 ←P is equal to P
1 2
21/2 21= 2 3 ←P is twice P
1 2
21 22= 4 6 ←P is four times P
1 2
101/2 101= 10 10 ←P is ten times P
1 2
101 102= 100 20 ←P is one hundred times P
1 2
103/2 103= 1000 30 ←P is one thousand times P
1 2
power difference=10 ⋅ log ⎛ ⎜ 1⎞ ⎟ =10 ⋅ log (0.5)=−3.01≈−3 dB . (E–9)
10⎝ ⎠ 10
2
Table E–1 lists several magnitude and power ratios versus dB values
that are worth remembering. Keep in mind that decibels indicate only relative
power relationships. For example, if we’re told that signal Ais 6 dB above sig-
nal B, we know that the power of signal Ais four times that of signal B, and
that the magnitude of signal Ais twice the magnitude of signal B. We may not
know the absolute power of signals A and B in watts, but we do know that
the power ratio is P /P =4.
A B
E.3 ABSOLUTE POWER USING DECIBELS
Let’s discuss another use of decibels that the reader may encounter in the lit-
erature. It’s convenient for practitioners in the electronic communications
field to measure continuous signal-power levels referenced to a specific ab-
solute power level. In this way, they can speak of absolute power levels in
watts while taking advantage of the convenience of decibels. The most com-
mon absolute power reference level used is the milliwatt. For example, if P in
2
Eq. (E–2) is a reference power level of one milliwatt, then
absolute power of P =10 ⋅ log ⎛ ⎜ P 1 ⎞ ⎟ =10 ⋅ log ⎛ ⎜ P 1 in watts⎞ ⎟dBm.(E–10)
1 10⎝P ⎠ 10⎝ 1 milliwatt ⎠
2

892 Decibels (dB and dBm)
The dBm unit of measure in Eq. (E–10) is read as “dB relative to a milliwatt.”
Thus, if a continuous signal is specified as having a power of 3 dBm, we
know that the signal’s absolute power level is 2 times one milliwatt, or 2 mil-
liwatts. Likewise, a –10 dBm signal has an absolute power of 0.1 milliwatts.†
The reader should take care not to inadvertently use dB and dBm inter-
changeably. They mean very different things. Again, dB is a relative power
level relationship, and dBm is an absolute power level in milliwatts.
†Other absolute reference power levels can be used. People involved with high-power transmit-
ters sometimes use a single watt as their reference power level. Their unit of power using deci-
bels is the dBW, read as “dB relative to a watt.” In this case, for example, 3 dBW is equal to a
2-watt power level.

APPENDIX F
Digital Filter
Terminology
The first step in becoming familiar with digital filters is to learn to speak the
language used in the filter business. Fortunately, the vocabulary of digital fil-
ters corresponds very well to the mother tongue used for continuous (analog)
filters—so we don’t have to unlearn anything that we already know. This ap-
pendix is an introduction to the terminology of digital filters.
Allpass filter—an IIR filter whose magnitude response is unity over its entire
frequency range, but whose phase response is variable. Allpass filters are
typically appended in a cascade arrangement following a standard IIR fil-
ter, H (z), as shown in Figure F–1.
1
An allpass filter, H (z), can be designed so that its phase response
ap
compensates for, or equalizes, the nonlinear phase response of an original
IIR filter[1–3]. Thus, the phase response of the combined filter, H
combined
(z), is more linear than the original H (z), and this is particularly desirable
1
in communications systems. In this context, an allpass filter is sometimes
called a phase equalizer.
Allpass filters have the property that the numerator polynomial co-
efficients in the filter’s H(z) transfer function are a reverse-order version
of the denominator polynomial coefficients. For example, the following
transfer function describes a 2nd-order allpass filter:
B+Az −1+z −2
H (z)= (F–1)
allpass 1+Az −1+Bz −2
where the numerator polynomial coefficients are [B, A, 1] and the denom-
inator polynomial coefficients are [1, A, B].
893

894 Digital Filter Terminology
x(n) Original IIR filter: Allpass IIR filter: y(n)
H 1 (z) H a p (z)
Combined IIR filter: H c o m b i n e d (z)
Figure F–1 Typical use of an allpass filter.
Attenuation—an amplitude loss, usually measured in dB, incurred by a sig-
nal after passing through a digital filter. Filter attenuation is the ratio, at a
given frequency, of the signal amplitude at the output of the filter divided
by the signal amplitude at the input of the filter, defined as
⎛ ⎞
attenuation=20 ⋅ log ⎜ a out⎟ dB . (F–2)
10⎝ a ⎠
in
For a given frequency, if the output amplitude of the filter is smaller
than the input amplitude, the ratio in Eq. (F–2) is less than one, and the at-
tenuation is a negative number.
Band reject filter—a filter that rejects (attenuates) one frequency band and
passes both a lower- and a higher-frequency band. Figure F–2(a) depicts
the frequency response of an ideal band reject filter. This filter type is
sometimes called a notch filter.
x(n) y(n) x(n) y(n)
Band reject Bandpass filter
filter response response
1 1
-f /2 0 f /2 Freq -f /2 0 f /2 Freq
s s s s
(a) (b)
Figure F–2 Filter symbols and frequency responses: (a) band reject filter;
(b)bandpass filter.

Digital Filter Terminology 895
Bandpass filter—a filter, as shown in Figure F–2(b), that passes one fre-
quency band and attenuates frequencies above and below that band.
Bandwidth—the frequency width of the passband of a filter. For a lowpass
filter, the bandwidth is equal to the cutoff frequency. For a bandpass filter,
the bandwidth is typically defined as the frequency difference between
the upper and lower 3 dB points.
Bessel function—a mathematical function used to produce the most linear
phase response of all IIR filters with no consideration of the frequency
magnitude response. Specifically, filter designs based on Bessel functions
have maximally constant group delay.
Butterworth function—a mathematical function used to produce maximally
flat filter magnitude responses with no consideration of phase linearity or
group delay variations. Filter designs based on a Butterworth function
have no amplitude ripple in either the passband or the stopband. Unfor-
tunately, for a given filter order, Butterworth designs have the widest
transition region of the most popular filter design functions.
Cascaded filters—a filtering systemwhere multiple individual filters are con-
nected in series; that is, the output of one filter drives the input of the fol-
lowing filter as illustrated in Figures F–1 and 6–37(a).
Center frequency (f )—the frequency lying at the midpoint of a bandpass fil-
0
ter. Figure F–2(b) shows the f center frequency of a bandpass filter.
o
Chebyshev function—a mathematical function used to produce passband or
stopband ripples constrained within fixed bounds. There are families of
Chebyshev functions based on the amount of ripple, such as 1 dB, 2 dB,
and 3 dB of ripple. Chebyshev filters can be designed to have a frequency
response with ripples in the passband and a flat stopband (Chebyshev
Type I), or flat passbands and ripples in the stopband (Chebyshev Type
II). Chebyshev filters cannot have ripples in both the passband and the
stopband. Digital filters based upon Chebyshev functions have steeper
transition region roll-off but more nonlinear-phase response characteris-
tics than, say, Butterworth filters.
CIC filter—cascaded integrator-comb filter. CIC filters are computationally
efficient, linear-phase, recursive, FIR, lowpass filters used in sample rate
change applications. Those filters are discussed in Chapter 10.
Coefficients—seefilter coefficients.

896 Digital Filter Terminology
Frequency magnitude
response (dB)
0
Passband ripple
Stopband
attenuation
f f/2 Freq
0 c s
Passband Transition Stopband
region
Figure F–3 A lowpass digital filter frequency response. The stopband relative am-
plitude is –20 dB.
Cutoff frequency—the highest passband frequency for lowpass filters (and
the lower passband frequency for highpass filters) where the magnitude
response is within the peak-peak passband ripple region. Figure F–3 illus-
trates the f cutoff frequency of a lowpass filter.
c
Decibels (dB)—a unit of attenuation, or gain, used to express the relative
voltage or power between two signals. For filters, we use decibels to indi-
cate cutoff frequencies (–3 dB) and stopband signal levels (–20 dB) as il-
lustrated in Figure F–3. Appendix E discusses decibels in more detail.
Decimation filter—a lowpass digital FIR filter where the output sample rate
is less than the filter’s input sample rate. As discussed in Section 10.1, to
avoid aliasing problems, the output sample rate must not violate the
Nyquist criterion.
Digital filter—computational process, or algorithm, transforming a discrete
sequence of numbers (the input) into another discrete sequence of num-
bers (the output) having a modified frequency-domain spectrum. Digital
filtering can be in the form of a software routine operating on data stored
in computer memory or can be implemented with dedicated hardware.
Elliptic function—a mathematical function used to produce the sharpest roll-
off for a given number of filter taps. However, filters designed by using
elliptic functions, also called Cauer filters, have the poorest phase linearity
of the most common IIR filter design functions. The ripples in the pass-
band and stopband are equal with elliptic filters.

Digital Filter Terminology 897
Envelope delay—seegroup delay.
Filter coefficients—the set of constants, also called tap weights, used to multi-
ply against delayed signal sample values within a digital filter structure.
Digital filter design is an exercise in determining the filter coefficients that
will yield the desired filter frequency response. For an FIR filter, by defin-
ition, the filter coefficients are the impulse response of the filter.
Filter order—a number describing the highest exponent in either the numera-
tor or denominator of the z-domain transfer function of a digital filter. For
tapped-delay line FIR filters, there is no denominator in the transfer func-
tion and the filter order is merely the number of delay elements used in
the filter structure. Generally, the larger the filter order, the better the
frequency-domain performance, and the higher the computational work-
load, of the filter.
Finite impulse response (FIR) filter—defines a class of digital filters that
have only zeros on the z-plane. The key implications of this are: (1) FIR fil-
ter impulse responses have finite time durations, (2) FIR filters are always
stable, and (3) FIR filters can have exactly linear phase responses (so long
as the filters’ impulse response samples are symmetrical, or antisymmet-
rical). For a given filter order, digital FIR filters have a much more grad-
ual transition region roll-off (poorer performance) than digital IIR filters.
FIR filters can be implemented with both nonrecursive (tapped-delay
line) and recursive (CIC filters, for example) structures.
Frequency magnitude response—a frequency-domain description of how a
filter interacts with input signals. The frequency magnitude response in
Figure F–3 is a curve of filter attenuation (in dB) versus frequency. Associ-
ated with a filter’s magnitude response is a phase response.
Group delay—the negative of the derivative of a filter’s frequency-domain
phase response with respect to frequency, G(ω) = –d(H (ω))/d(ω). If a fil-
ø
ter’s complex frequency response is represented in polar form as
H(ω) = |H(ω)|ejHø(ω) (F–3)
where digital frequency ω is continuous and ranges from –π to π radi-
ans/sample, corresponding to a cyclic frequency range of –f/2 to f/2 Hz,
s s
then the filter’s group delay is defined as
−d(H (ω))
G(ω)= φ . (F–4)
d(ω)

898 Digital Filter Terminology
Because the dimensions of H (ω) are radians, and the dimensions of ωare
ø
radians/sample, the dimensions of group delay G(ω) are time measured
in samples.
If a filter’s complex frequency response is expressed in terms of a
normalized frequency variable as
H(f) = |H(f)|ejHø(f) (F–5)
where frequency f is continuous and is in the range of –0.5 ≤ f ≤ 0.5, then
the group delay G(f) is defined as
G(f)=
−1
⋅
d(H
φ
(f))
. (F–6)
2π d(f)
The concept of group delay deserves additional explanation beyond
a simple algebraic definition. For an ideal lowpass filter, for example, the
frequency-domain phase response will be linear and the group delay
would be constant. Group delay can also be thought of as the propagation
time delay of the envelope (the information) of an amplitude-modulated
(AM) signal as it passes through a digital filter. (In this context, group
delay is often called envelope delay.) If a filter’s passband group delay is
not constant (a nonlinear-phase filter), then group delay distortion occurs
because signals at different frequencies take different amounts of time (a
different number of sample time intervals) to pass through the filter.
Half-band filter—a type of FIR filter whose transition region is centered at
one-quarter of the sampling rate, or f/4. Specifically, the end of the pass-
s
band and the beginning of the stopband are equally spaced about f/4.
s
Due to their frequency-domain symmetry, half-band filters are often used
in decimation filtering schemes because half of their time-domain coeffi-
cients are zero. This reduces the number of necessary filter multiplica-
tions, as described in Section 5.7.
Highpass filter—a filter that passes high frequencies and attenuates low fre-
quencies, as shown in Figure F–4(a). We’ve all experienced a kind of high-
pass filtering in our living rooms. Notice what happens when we turn up
the treble control (or turn down the bass control) on our home stereo sys-
tems. The audio amplifier’s normally flat frequency response changes to a
kind of analog highpass filter, giving us that sharp and tinny sound as the
high-frequency components of the music are being accentuated.
Impulse response—a digital filter’s time-domain output sequence when the
input is a single unity-valued sample (impulse) preceded and followed
by zero-valued samples. Adigital filter’s frequency-domain response can

Digital Filter Terminology 899
x(n) y(n) x(n) y(n)
Highpass filter Lowpass filter
response response
1 1
–f/2 0 f/2 Freq -f /2 0 f/2 Freq
s s s s
(a) (b)
Figure F–4 Filter symbols and frequency responses: (a) highpass filter; (b) low-
pass filter.
be calculated by taking the discrete Fourier transform of the filter’s time-
domain impulse response[4].
Infinite impulse response (IIR) filter—a class of digital filters that may have
both zeros and poles on the z-plane. As such, IIR filters are not guaran-
teed to be stable and almost always have nonlinear phase responses. For a
given filter order (number of IIR feedback taps), IIR filters have a much
steeper transition region roll-off than digital FIR filters.
Linear-phase filter—a filter that exhibits a constant change in phase angle (de-
grees) as a function of frequency. The resultant filter phase plot versus fre-
quency is a straight line. As such, a linear-phase filter’s group delay is a
constant. To preserve the integrity of their information-carrying signals, lin-
ear phase is an important criterion for filters used in communications sys-
tems.
Lowpass filter—a filter that passes low frequencies and attenuates high fre-
quencies as shown in Figure F–4(b). By way of example, we experience
lowpass filtering when we turn up the bass control (or turn down the treble
control) on our home stereo systems, giving us that dull, muffled sound as
the high-frequency components of the music are being attenuated.
Nonrecursive filter—a digital filter implementation where no filter output
sample is ever retained for later use in computing a future filter output
sample. Such filters have no “feedback” signal paths.
Notch filter—seeband reject filter.

900 Digital Filter Terminology
Order—seefilter order.
Passband—that frequency range over which a filter passes signal energy with
minimum attenuation, usually defined as the frequency range where the
magnitude response is within the peak-peak passband ripple region, as
depicted in Figure F–3.
Passband ripple—peak-peak fluctuations, or variations, in the frequency
magnitude response within the passband of a filter as illustrated in Fig-
ureF–3.
Phase response—the difference in phase, at a particular frequency, be-
tween an input sinewave and the output sinewave at that frequency.
The phase response, sometimes called phase delay, is usually depicted
by a curve showing the filter’s phase shift versus frequency. Section
5.8 discusses digital filter phase response in more detail.
Phase wrapping—an artifact of arctangent software routines, used to cal-
culate phase angles, that causes apparent phase discontinuities. When
a true phase angle is in the range of –180o to –360o, some software
routines automatically convert those angles to their equivalent posi-
tive angles in the range of 0o to +180o. Section 5.8 illustrates an exam-
ple of phase wrapping when the phase of an FIR filter is calculated.
Quadrature filter—a dual-path digital filter operating on complex sig-
nals, as shown in Figure F–5. One filter operates on the in-phase i(n)
data, and the other filter processes the quadrature-phase q(n) signal
data. Quadrature filtering is normally performed on complex signals,
whose spectra are centered at zero Hz, using lowpass digital filters.
Recursive filter—a digital filter implementation where current filter output
samples are retained for later use in computing future filter output sam-
ples. Such filters have “feedback” signal paths.
Relative attenuation—attenuation measured relative to the largest magni-
tude value. The largest signal level (minimum attenuation) is typically as-
signed the reference level of zero dB, as depicted in Figure F–3, making all
other magnitude points on a frequency-response curve negative dB
values.
Ripple—refers to fluctuations (measured in dB) in the passband, or stopband,
of a filter’s frequency-response curve. Elliptic and Chebyshev-based fil-

Digital Filter Terminology 901
Quadrature
filter
x i (n) Lowpass i(n)
filter
x (n)
bp
cos(2 fnt)
c s Lowpass q(n)
x (n) filter
q
-sin(2 fnt)
c s
Figure F–5 Two lowpass filters used to implement quadrature filtering.
ters have equiripple characteristics in that their ripple is constant across
their passbands. Bessel- and Butterworth-derived filters have no ripple in
their passband responses. Ripples in the stopband response are some-
times called out-of-band ripple.
Roll-off—a term used to describe the steepness, or slope, of the filter re-
sponse in the transition region from the passband to the stopband. Apar-
ticular digital filter may be said to have a roll-off of 12 dB/octave,
meaning that the second-octave frequency would be attenuated by 24 dB,
and the third-octave frequency would be attenuated by 36 dB, and so on.
Shape factor—a term used to indicate the steepness of a filter’s roll-off. Shape
factor is normally defined as the ratio of a filter’s passband width divided
by the passband width plus the transition region width. The smaller the
shape factor value, the steeper the filter’s roll-off. For an ideal filter with a
transition region of zero width, the shape factor is unity. The term shape
factoris also used to describe analog filters.
Stopband—that band of frequencies attenuated by a digital filter. Figure F–3
shows the stopband of a lowpass filter.
Structure—a fancy term meaning the block diagram, the signal-flow imple-
mentation, of a digital filter. For example, lowpass moving average filters
may be built (implemented) with both nonrecursive structures and recur-
sive structures.
Tap—a multiplication operation inside a digital filter that computes the prod-
uct of a single data value times a single filter coefficient.
Tap weights—seefilter coefficients.

902 Digital Filter Terminology
Tchebyschev function—seeChebyshev function.
Transfer function—a mathematical expression of the ratio of the output of a
digital filter divided by the input of the filter as expressed in a transform
domain (e.g., z-domain, Laplace, frequency). Given the transfer function,
we can determine the filter’s frequency magnitude and phase responses.
Transition region—the frequency range over which a filter transitions from
the passband to the stopband. Figure F–3 illustrates the transition region
of a lowpass filter. The transition region is sometimes called the transition
band.
Transversal filter—in the field of digital filtering, transversal filter is another
name for FIR filters implemented with the nonrecursive structures de-
scribed in Chapter 5.
Zero-phase filter—an off-line (because it operates on a block of filter input
samples) filtering method which cancels the nonlinear phase response of
an IIR filter. Section 13.12 details this non-real-time filtering technique.
REFERENCES
[1] Rabiner, L. R., and Gold, B. The Theory and Application of Digital Signal Processing, Prentice
Hall, Englewood Cliffs, New Jersey, 1975, pp. 206, 273, and 288.
[2] Oppenheim, A. V., and Schafer, R. W. Discrete-Time Signal Processing, Prentice Hall, Engle-
wood Cliffs, New Jersey, 1989, pp. 236 and 441.
[3] Laakso, T. I., et al. “Splitting the Unit Delay,” IEEE Signal Processing Magazine, January
1996, p. 46.
[4] Pickerd, J. “Impulse-Response Testing Lets a Single Test Do the Work of Thousands,” EDN,
April 27, 1995.

APPENDIX G
Frequency
Sampling Filter
Derivations
While much of the algebra related to frequency sampling filters is justifiably
omitted in the literature, several derivations are included here for two rea-
sons: first, to validate the equations used in Section 7.5; and second, to show
the various algebraic acrobatics that may be useful in your future digital sig-
nal processing analysis efforts.
G.1 FREQUENCY RESPONSE OF A COMB FILTER
The frequency response of a comb filter is H (z) evaluated on the unit cir-
comb
cle. We start by substituting
ejω
for z in H (z) from Eq. (7–37), because
comb
z=ejω
defines the unit circle, giving
H
comb
(ejω
) = H
comb
(z)| z=ejω= (1 –
e–jNω
). (G–1)
Factoring out the half-angled exponential
e–jωN/2,
we have
H
(ejω
) =
e–jωN/2(ejωN/2–e–jωN/2).
(G–2)
comb
Using Euler’s identity,
2jsin(α)=ejα –e–jα
, we arrive at
H
(ejω
) =
e–jωN/2[2jsin(ωN/2)].
(G–3)
comb
Replacingjwithejπ/2,
we have
H
(ejω
) =
e–j(ωN–π)/22sin(ωN/2).
(G–4)
comb
903

904 Frequency Sampling Filter Derivations
Determining the maximum magnitude response of a filter is useful in
DSP. Ignoring the phase shift term (complex exponential) in Eq. (G–4), the
frequency-domain magnitude response of a comb filter is
|H (ejω )| = 2|sin(ωN/2)|, (G–5)
comb
with the maximum magnitude being 2.
G.2 SINGLE COMPLEX FSF FREQUENCY RESPONSE
The frequency response of a single-section complex FSF is H (z) evaluated on
ss
the unit circle. We start by substituting
ejω
for z in H (z), because z =
ejω
de-
ss
fines the unit circle. Given an H (z) of
ss
H(k)
H (z) = (1 – z–N) , (G–6)
ss 1−[e j2πk/N ]z −1
we replace the zterms with
ejω
, giving
H(k)
H
ss
(ejω
) = H
ss
(z)| z=ejω= (1 –
e–jNω
)
1−[e j2πk/N ]e −jω
1−e −jNω
=H(k) . (G–7)
1−e −j(ω−2πk/N)
Factoring out the half-angled exponentials
e–jωN/2ande–j(ω/2–πk/N),
we have
H
(ejω
) = H(k)
e −jωN/2 (e jωN/2 −e −jωN/2 )
. (G–8)
ss
e
−j(ω/2−πk/N)
(e
j(ω/2−πk/N)−e −j(ω/2−πk/N)
)
Using Euler’s identity,
2jsin(α)=ejα –e–jα
, we arrive at
H
(ejω
) = H(k)
e −jωN/2 [2jsin(ωN/2)]
. (G–9)
ss e −jω/2 e jπk/N [2jsin(ω/2−πk/N)]
Canceling common factors and rearranging terms in preparation for our final
form, we have the desired frequency response of a single-section complex FSF:
H
(ejω
) =
e–jω(N–1)/2e–jπk/NH(k)
sin(ωN/2)
. (G–10)
ss sin(ω/2−πk/N)
Next we derive the maximum amplitude response of a single-section
FSF when its pole is on the unit circle and H(k)=1. Ignoring those phase shift
factors (complex exponentials) in Eq. (G–10), the amplitude response of a
single-section FSF is

G.3 Multisection Complex FSF Phase 905
sin(ωN/2)
H
(ejω
) = . (G–11)
ss,amp sin(ω/2−πk/N)
We want to know the value of Eq. (G–11) when ω=2πk/N, because that’s
the value of ωat the pole locations, but |H (ejω )| is indeterminate as
ss ω=2πk/N
sin(πk) sin(πk) 0
H
(ejω
)| = = = . (G–12)
ss,amp ω=2πk/N sin(πk/N−πk/N) sin(0) 0
Applying the Marquis de L’Hopital’s Rule to Eq. (G–11) yields
d[sin(ωN/2)]/dω
|
H
(ejω
)| =
ss,amp ω→2πk/N d[sin(ω/2−πk/N)]/dω ω→2πk/N
N/2 cos(ωN/2) | Ncos(πk)
= = = N(–1)k. (G–13)
1/2 cos(ω/2−πk/N) ω→2πk/N cos(πk/N−πk/N)
The phase factors in Eq. (G–10), when ω= 2πk/N, are
|
e–jω(N–1)/2e–jπk/N =e–jπkejπk/Ne–jπk/N= (–1)k. (G–14)
ω=2πk/N
Combining the result of Eqs. (G–13) and (G–14) with Eq. (G–10), we have
|H (ejω )| = |H(k)|N(–1)2k= |H(k)|N. (G–15)
ss,amp ω=2πk/N
So the maximum magnitude response of a single-section complex FSF at
resonance is |H(k)|N, independent of k.
G.3 MULTISECTION COMPLEX FSF PHASE
This appendix shows how the (–1)k factors arise in Eq. (7–48) for an even-N
multisection linear-phase complex FSF. Substituting the positive-frequency,
0 ≤ k ≤ (N/2)–1, |H(k)|ejφ(k) gain factors, with φ(k) phase values from
Eq.(7–46), into Eq. (7–45) gives
(N
∑
/2)–1
|H(k)|e
−jkπ(N−1)/N
e
−jkπ/N
H
(ejω
) =
e–jω(N–1)/2sin(ωN/2)
, (G–16)
cplx,lp,pf sin(ω/2−πk/N)
k=0
where the subscript “pf” means positive frequency. Focusing only on the nu-
merator inside the summation in Eq. (G–16), it is

906 Frequency Sampling Filter Derivations
Numerator =
|H(k)|e–jkπ(N–1)/Ne–jkπ/N= |H(k)|e–jkπ/N(N–1+1)
pf
= |H(k)|e–jkπ = |H(k)|(e–jπ )k= |H(k)|(–1)k, (G–17)
showing how the (–1)kfactors occur within the first summation of Eq. (7–48).
Next we substitute the negative-frequency
|H(k)|ejφ(k)
gain factors,
(N/2)+1 ≤ k ≤ N–1, with φ(k) phase values from Eq. (7–46’’), into Eq. (7–45),
giving
H
(ejω
) =
cplx,lp,nf
N
∑
–1
|H(k)|e
jπ(N−k)(N−1)/N
e
−jkπ/N
e–jω(N–1)/2sin(ωN/2)
(G–18)
sin(ω/2−πk/N)
k=(N/2)+1
where the subscript “nf” means negative frequency. Again, looking only at
the numerator inside the summation in Eq. (G–18), it is
Numerator = |H(k)|ejπ(N–k)(N–1)/Ne–jkπ/N= |H(k)|e–jπ[k– (N–k)(N–1)]/N
nf
= |H(k)|e–jπ[N+kN–N2] /N= |H(k)|e–jπ[1 + k–N]= |H(k)|(e–jπ )(e–jkπ )(ejπN). (G–19)
ThatejπNfactor
in Eq. (G–19) is equal to 1 when Nis even, so we write
Numerator =
|H(k)|(–1)(e–jkπ
)(1) =
–|H(k)|(e–jkπ
)
nf
= –|H(k)|(e–jπ )k= –|H(k)|(–1)k, (G–20)
establishing both the negative sign before, and the (–1)kfactor within, the sec-
ond summation of Eq. (7–48). To account for the single-section for the k=N/2
term (this is the Nyquist, or f/2, frequency, where ω = π), we plug the
s
|H(N/2)|ej0gain factor, and k=N/2, into Eq. (7–43), giving
sin(ωN/2)
H
(ejω
)|
=e–jω(N–1)/2e–jπ(N/2)/N|H(N/2)|e–j0
cplx,lp,pf ω=π sin(ω/2−π(N/2)/N)
sin(ωN/2)
=e–jω(N–1)/2e–jπ/2|H(N/2)|
. (G–21)
sin(ω/2−π/2)
G.4 MULTISECTION COMPLEX FSF FREQUENCY RESPONSE
The frequency response of a guaranteed-stable complex N-section FSF, when
r<1, is H (z) with the zvariable in Eq. (7–53) replaced by
ejω
, giving
gs,cplx
N–1
∑ H(k)
H gs,cplx
(ejω
) = H gs,cplx (z)| z=ej ω= (1 –
rNe–jNω
) 1−[re j2πk/N ]e −jω . (G–22)
k=0
To temporarily simplify our expressions, we let θ=ω–2πk/N, giving

G.4 Multisection Complex FSF Frequency Response 907
N–1
∑ H(k)
H gs,cplx
(ejω
) = (1 –
rNe–jNω
) 1−re −jθ . (G–23)
k=0
Factoring out the half-angled exponentials, and accounting for the r factors,
we have
H
(ejω
) =
rN/2e–jNω/2(r–N/2ejNω/2–rN/2e–jNω/2)
gs,cplx
N–1
× ∑ H(k) . (G–24)
r 1/2 e −jθ/2 (r −1/2 e jθ/2 −r 1/2 e −jθ/2 )
k=0
Converting all the terms inside parentheses to exponentials (we’ll see why in
a moment), we have
H
(ejω
) =
rN/2e–jNω/2{e-[Nln(r)/2 –jNω/2]–e[Nln(r)/2 –jNω/2]}
gs,cplx
N–1
∑ H(k)
× . (G–25)
r
1/2
e
−jθ/2
{e
−[ln(r)/2−jθ/2]−e −[ln(r)/2−jθ/2]
}
k=0
The algebra gets a little messy here because our exponents have both
real and imaginary parts. However, hyperbolic functions to the rescue. Recall-
ing when αis a complex number, sinh(α)=(e α –e–α )/2, we have
H (ejω ) = rN/2e–jNω/2{–2sinh[Nln(r)/2 –jNω/2]}
gs,cplx
N–1
× ∑ H(k) . (G–26)
r 1/2 e −jθ/2 {−2sinh[ln(r)/2− jθ/2]}
k=0
Replacing angle θwithω–2πk/N, canceling the –2 factors, we have
H (ejω ) = rN/2e–jNω/2sinh[Nln(r)/2 – jNω/2]
gs,cplx
N–1
∑ H(k)
× . (G–27)
r 1/2 e −j(ω−2πk/N)/2 sinh[ln(r)/2− j(ω−2πk/N)/2]
k=0
Rearranging and combining terms, we conclude with
H
(ejω
)
gs,cplx
N ∑ –1 H(k)e −jπk/N sinh[Nln(r)/2− jNω/2]
= N−1 e–jω(N–1)/2 . (G–28)
r sinh[ln(r)/2− j(ω−2πk/N)/2]
k=0
(Whew! Now we see why this frequency response expression is not usually
found in the literature.)

908 Frequency Sampling Filter Derivations
G.5 REAL FSF TRANSFER FUNCTION
The transfer function equation for the real-valued multisection FSF looks a bit
strange at first glance, so rather than leaving its derivation as an exercise for
the reader, we show the algebraic acrobatics necessary in its development. To
preview our approach, we’ll start with the transfer function of a multisection
complex FSF and define the H(k) gain factors such that all filter poles are in
conjugate pairs. This will lead us to real-FSF structures with real-valued coef-
ficients. With that said, we begin with Eq. (7–53)’s transfer function of a
guaranteed-stableN-section complex FSF of
N–1
∑ H(k)
H gs,cplx (z) = (1 – rNz–N) 1−[re j2πk/N ]z −1 . (G–29)
k=0
AssumingNis even, and breaking Eq. (G–29)’s summation into parts, we can
write
H (z) =
gs,cplx
⎡
H(0) H(N/2)
⎢
(1 – rNz-N) + .
⎣⎢1−rz −1 1+rz −1
⎤
N∑/2–1
H(k)
N∑–1
H(k)
⎥
+ + . (G–30)
1−[re j2πk/N ]z −1 1−[re j2πk/N ]z −1⎦⎥
k=1 k=N/2+1
The first two ratios inside the brackets account for thek=0 andk=N/2
frequency samples. The first summation is associated with the positive-
frequency range, which is the upper half of the z-plane’s unit circle. The sec-
ond summation is associated with the negative-frequency range, the lower
half of the unit circle.
To reduce the clutter of our derivation, let’s identify the two summations
as
N/2–1 N–1
∑ H(k) ∑ H(k)
Summations = + .
1−[re j2πk/N ]z −1 1−[re j2πk/N ]z −1
k=1 k=N/2+1 (G–31)
We then combine the summations by changing the indexing of the second
summation as
N ∑ /2–1 H(k) H(N−k)
Summations = [ + ]. (G–32)
1−[re j2πk/N ]z −1 1−[re j2π[N−k]/N ]z −1
k=1
Putting those ratios over a common denominator and multiplying the
denominator factors, and then forcing the H(N–k) gain factors to be complex
conjugates of the H(k) gain factors, we write

G.5 Real FSF Transfer Function 909
Summations
N ∑ /2–1 H(k)(1−re j2π[N−k]/N z −1 )+H*(k)(1−re j2πk/N z −1 )
= ,
1−re j2π[N−k]/N z −1−re j2πk/N z −1+r 2 e j2π[k+N−k]/N z −2
k=1 (G–33)
where the “*” symbol means conjugation. Defining H(N-k) =H*(k) mandates
that all poles will be conjugate pairs and, as we’ll see, this condition converts
our complex FSF into a real FSF with real-valued coefficients. Plowing for-
ward, because
ej2π[N–k]/N
=
e–j2πN/Ne–j2πk/N
=
e–j2πk/N,
we make that substitution
in Eq. (G–33), rearrange the numerator, and combine the factors of z-1 in the
denominator to arrive at
Summations
N ∑ /2–1 H(k)+H*(k)−[H(k)re −j2πk/N +H*(k)re j2πk/N ]z −1
= .
1−r[e −j2πk/N +e j2πk/N ]z −1+r 2 z −2
k=1 (G–34)
Next we define each complex H(k) in rectangular form with an angle φ ,
k
or H(k) = |H(k)|[cos(φ ) +jsin(φ )], and H*(k) = |H(k)|[cos(φ ) –jsin(φ )]. Real-
k k k k
izing that the imaginary parts of the sum cancel so that H(k) +H*(k) =
2|H(k)|cos(φ ) allows us to write
k
Summations
N ∑ /2–1 2|H(k)|cos(ϕ )−|H(k)|[re j[ϕ k −2πk/N]+re −j[ϕ k −2πk/N] ]z −1
= k .
1−r[e −j2πk/N +e j2πk/N ]z −1+r 2 z −2
k=1 (G–35)
Recalling Euler’s identity, 2cos(α) =e jα + e–jα , and combining the |H(k)| fac-
tors leads to the final form of our summation:
Summations
N ∑ /2–1 2|H(k)|[cos(ϕ )−rcos(ϕ −2πk/N)z −1 ]
= 1−[2rcos k (2πk/N)]z k −1+r 2 z −2 . (G–36)
k=1
Substituting Eq. (G–36) for the two summations in Eq. (G–30), we con-
clude with the desired transfer function
⎡ H(0) H(N/2)
H gs,real (z) = (1 – rNz–N) ⎣ ⎢ 1−rz −1 + 1+rz −1
N ∑ /2–1 2|H(k)|[cos(ϕ )−rcos(ϕ −2πk/N)z −1 ] ⎤
+ k k ⎥ , (G–37)
1−[2rcos(2πk/N)]z −1+r 2 z −2 ⎦ ⎥
k=1
where the subscript “real” means a real-valued multisection FSF.

910 Frequency Sampling Filter Derivations
G.6 TYPE-IV FSF FREQUENCY RESPONSE
The frequency response of a single-section even-N Type-IV FSF is its transfer
function evaluated on the unit circle. To begin that evaluation, we set Eq.
(7–58)’s |H(k)| = 1, and denote a Type-IV FSF’s single-section transfer func-
tion as
1−r 2 z −2
H Type-IV,ss (z) = (1 – rNz–N) 1−2rcos(2πk/N)z −1+r 2 z −2 (G–38)
where the “ss” subscript means single-section. Under the assumption that the
damping factor r is so close to unity that it can be replaced with 1, we have
the simplified FSF transfer function
1−z −2
H (z) = (1 – z–N) . (G–39)
Type-IV,ss 1−2cos(2πk/N)z −1+z −2
Letting ω = 2πk/N to simplify the notation and factoring H (z)’s
r Type-IV,ss
denominator gives
H (z)
Type-IV,ss
1−z −2 (1−z −N )(1−z −2 )
= (1 – z–N) = (G–40)
1−2cos(ω )z −1+z −2 (1−e jω rz −1 )(1−e −jω rz −1 )
r
in which we replace each zterm with
ejω
, as
H
(ejω
)
Type-IV,ss
(1−e −jωN )(1−e −j2ω ) (1−e −jωN )(1−e −j2ω )
= = .(G–41)
(1−e jω re −jω )(1−e −jω re −jω ) (1−e −j(ω−ω r ) )(1−e −j(ω+ω r ) )
Factoring out the half-angled exponentials, we have
H
(ejω
)
Type-IV,ss
e
−jωN/2
(e
jωN/2
−e
−jωN/2
)e
−jω
(e
jω
−e
−jω
)
= . (G–42)
e −j(ω−ω r )/2 (e j(ω−ω r )/2 −e −j(ω−ω r )/2 )e −j(ω+ω r )/2 (e j(ω+ω r )/2 −e −j(ω+ω r )/2 )
Using Euler’s identity,
2jsin(α)=ejα –e–jα
, we obtain
H
(ejω
)
Type-IV,ss
e −jωN/2 [2jsin(ωN/2)]e −jω [2jsin(ω)]
= . (G–43)
e −j(ω−ω r )/2 [2jsin([ω−ω ]/2)]e −j(ω+ω r )/2 [2jsin([ω+ω ]/2)]
r r
Canceling common factors, and adding like terms, we have

G.6 Type-IV FSF Frequency Response 911
H
(ejω
) =
e −jωN/2 e −jω sin(ωN/2)sin(ω)
Type-IV,ss e −j(ω−ω r )/2 e −j(ω+ω r )/2 sin([ω−ω r ]/2)sin([ω+ω r ]/2)
cos(ωN/2−ω)−cos(ωN/2+ω)
=e–jωN/2
. (G–44)
cos(ω )−cos(ω)
r
Plugging 2πk/Nback in for ω,the single-section frequency response is
r
cos(ωN/2−ω)−cos(ωN/2+ω)
H
(ejω
) =
e–jωN/2
. (G–45)
Type-IV,ss cos(2πk/N)−cos(ω)
Based on Eq. (G–45), the frequency response of a multisection even-N
Type-IV FSF is
H
(ejω
)
Type-IV
N ∑ /2 (−1) k |H(k)|[cos(ωN/2−ω)−cos(ωN/2+ω)]
=e–jωN/2
. (G–46)
cos(2πk/N)−cos(ω)
k=0
To determine the amplitude response of a single section, we ignore the phase
shift terms (complex exponentials) in Eq. (G–45) to yield
cos(ωN/2−ω)−cos(ωN/2+ω)
H
(ejω
) = . (G–47)
Type-IV,amp cos(2πk/N)−cos(ω)
To find the maximum amplitude response at resonance we evaluate
Eq.(G–47) when ω=2πk/N, because that’s the value of ωat the FSF’s pole lo-
cations. However, that ω causes the denominator to go to zero, causing the
ratio to go to infinity. We move on with one application of L’Hopital’s Rule to
Eq. (G–47) to obtain
d[cos(ωN/2−ω)−cos(ωN/2+ω)]/dω
|
H
(ejω
)| =
Type-IV,amp ω→2πk/N d[cos(2πk/N)−cos(ω)]/dω ω→2πk/N
−sin(ωN/2−ω)(N/2−1)+sin(ωN/2+ω)(N/2+1)|
=
sin(ω) ω→2πk/N
−[(N−2)/2]sin(πk−2πk/N)+[(N+2)/2]sin(πk+2πk/N)
= .(G–48)
sin(2πk/N)
Eliminating the πk terms by using trigonometric reduction formulas
sin(πk–α) = (–1)k[-sin(α)] and sin(πk+α) = (–1)k[sin(α)], we have a maximum
amplitude response of

912 Frequency Sampling Filter Derivations
H
(ejω
)|
Type-IV,amp ω=2πk/N
[(N−2)/2](−1) k sin(2πk/N)+[(N+2)/2](−1) k sin(2πk/N)
=
2sin(2πk/N)
N(−1) k sin(2πk/N)
= = N(–1)k, k= 1, 2, ..., N –1. (G–49)
sin(2πk/N)
2
Equation (G–49) is only valid for 1 ≤ k ≤ (N/2)–1. Disregarding the (–1)k fac-
tors, we have a magnitude response at resonance, as a function of k, of
|H
(ejω
)| =N, k= 1, 2, ...,
N
–1. (G–50)
Type-IV ω=2πk/N
2
To find the resonant gain at 0 Hz (DC) we set k = 0 in Eq. (G–47), apply
L’Hopital’s Rule (the derivative with respect to ω) twice, and set ω=0, giving
|H
(ejω
)| = 2N. (G–51)
Type-IV ω=0
To obtain the resonant gain at f/2 Hz we set k = N/2 in Eq. (G–47), again
s
apply L’Hopital’s Rule twice, and set ω=π, yielding
|H
(ejω
)| = 2N. (G–52)
Type-IV ω=π

APPENDIX H
Frequency
Sampling Filter
Design Tables
In Section 7.5 we described the so-called Type-IV frequency sampling filter
(FSF). The tables in this appendix provide a list of optimum transition coeffi-
cient values for the Case I (see Figure 7–44) Type-IV lowpass FSFs of various
passband bandwidths, over a range of values of N. Table H–1 provides the
H(k) single transition coefficient and two transition coefficients for even val-
ues of N. Table H–2 provides the H(k) three transition coefficients for even N.
Table H–3 provides the H(k) single transition coefficient and two transition
coefficients for odd values of N, while Table H–4 provides the H(k) three tran-
sition coefficients for odd N.
The passband bandwidth in these tables, signified by the BW parame-
ter, is the number of FSF sections having unity-valued H(k) gain factors. For
example, an N = 32 lowpass FSF using six passband sections and a single
transition region coefficient (T ) would have the H(k) gain values shown in
1
Figure H–1(a). In this case, the T coefficient would be found in Table H–1 for
1
N = 32 at a bandwidth BW = 6. An N = 23 lowpass FSF with five passband
sections and two transition region coefficients (T and T ) would have the
1 2
H(k) gain values shown in Figure H–1(b). In this case, the T and T coeffi-
1 2
cients are found in Table H–2 for N = 23 at a bandwidth BW = 5. An addi-
tional parameter in the tables is the maximum stopband sidelobe attenuation
levels (Atten).
913

914 Frequency Sampling Filter Design Tables
1
0.75
0.5
0.25
0
0 0.2 0.4 0.6 0.8 1
Figure H–1 Transition coefficient examples: (a) one coefficient for N= 32 and
BW=6; (b) two coefficients for N=23 and BW=5.
edutingaM
1
|H(5)|
0.75
|H(6)| = T 1 0.5
= 0.37172559
0.25
0
0 0.2 0.4 0.6 0.8 1
Frequency
edutingaM
|H(4)|
|H(5)| = T
1 = 0.54245814
|H(6)| = T
2
= 0.07981527
Frequency
(a) (b)
Table H–1 Lowpass Type-IV FSF for Even N(One and Two Coefficients)
BW Atten T1 BW Atten T1 T2
N= 16 N= 16
1 –44.9 0.41924081 1 –76.5 0.56626687 0.07922718
2 –45.8 0.38969818 2 –77.2 0.55487263 0.08012238
3 –47.3 0.36942214 3 –81.2 0.53095099 0.07087993
4 –49.6 0.34918551 4 –87.7 0.49927622 0.05813368
N= 24 N= 24
1 –44 0.42452816 1 –73.6 0.57734042 0.08641861
2 –44.1 0.40042889 2 –72.5 0.57708274 0.09305238
3 –44.9 0.38622106 3 –72.9 0.56983709 0.09177956
4 –45.7 0.37556064 4 –73.8 0.55958351 0.08770698
5 –46.5 0.36663149 5 –75.6 0.54689579 0.08202772
N= 32 N= 32
1 –43.6 0.42638815 1 –72.6 0.58109341 0.08892320
2 –43.6 0.40407598 2 –71 0.58466392 0.09771906
3 –44 0.39197681 3 –70.8 0.58101218 0.09823378
4 –44.5 0.38377786 5 –71.6 0.57002693 0.09442029
6 –45.5 0.37172559 7 –73.2 0.55593774 0.0879846

Frequency Sampling Filter Design Tables 915
Table H–1 Lowpass Type-IV FSF for Even N(One and Two Coefficients) (continued)
BW Atten T1 BW Atten T1 T2
7 –45.8 0.38912443 9 –76.2 0.53661082 0.07884406
8 –46.6 0.36087271 13 –106 0.43242657 0.03643965
N= 48 N= 48
1 –43.4 0.42772741 1 –72 0.58385966 0.09079945
2 –43.2 0.40654471 2 –70 0.58999731 0.10107095
3 –43.5 0.39569517 3 –69.5 0.58898579 0.1030238
4 –43.8 0.38879556 5 –69.6 0.58356869 0.10204926
6 –44.4 0.37994174 7 –70 0.57749269 0.09959325
8 –44.9 0.37394938 9 –70.6 0.57143299 0.09683486
10 –45.3 0.3690437
N= 64 N= 64
1 –43.3 0.42815077 1 –71.8 0.58480329 0.09144087
2 –43.1 0.40742967 2 –69.7 0.59178518 0.10221701
3 –43.4 0.39690507 3 –69.2 0.59168291 0.10467406
4 –43.6 0.39043339 4 –68.9 0.5899207 0.10496398
5 –43.9 0.38583162 5 –68.9 0.58788109 0.10457886
6 –44.1 0.38244173 9 –69.4 0.58010661 0.10157302
10 –44.7 0.37382147 13 –70.2 0.57272483 0.09810828
14 –45.3 0.36813961 17 –71.1 0.56417336 0.09386963
N= 96 N= 96
1 –43.2 0.42856954 1 –71.6 0.585424 0.09185794
2 –43 0.40790815 2 –69.5 0.59305878 0.10302346
3 –43.3 0.3977603 3 –68.8 0.59332978 0.10571202
4 –43.5 0.39154291 4 –68.4 0.59202942 0.10623854
5 –43.7 0.38719365 5 –68.3 0.59062246 0.10623287
6 –43.9 0.38409245 9 –68.5 0.58514671 0.10445521
10 –44.4 0.37686993 13 –68.9 0.58087019 0.10253761
14 –44.7 0.37270333 17 –69.3 0.57751103 0.10097157
18 –45 0.36984146 21 –65.3 0.59419612 0.11180709
(continues)

916 Frequency Sampling Filter Design Tables
Table H–1 Lowpass Type-IV FSF for Even N(One and Two Coefficients) (continued)
BW Atten T1 BW Atten T1 T2
N= 128 N= 128
1 –43.2 0.42864273 1 –71.6 0.58574352 0.09208128
2 –43 0.40823844 2 –69.4 0.59357535 0.10335703
3 –43.2 0.39811024 3 –68.6 0.59383385 0.10601544
4 –43.4 0.39188378 4 –68.3 0.59279342 0.10671242
5 –43.7 0.38774353 6 –68.1 0.59024028 0.10655014
7 –44 0.38236389 9 –68.2 0.5868017 0.10542034
10 –44.3 0.37771128 17 –68.7 0.58071332 0.10271656
18 –44.8 0.3717883 25 –69.2 0.57652818 0.1006787
26 –45.1 0.36862161 33 –69.8 0.57265344 0.09871098
N= 192 N= 192
1 –43.2 0.42881027 1 –71.5 0.58589507 0.09218797
2 –43 0.40831822 2 –69.4 0.59383365 0.1035231
3 –43.2 0.39830476 3 –68.6 0.59433749 0.10635017
4 –43.4 0.39219024 4 –68.2 0.5933049 0.10702144
5 –43.6 0.38797095 6 –67.9 0.59098287 0.10700026
7 –43.9 0.3828125 9 –68 0.58790082 0.10604544
10 –44.2 0.37845008 17 –68.4 0.58295021 0.10399738
18 –44.7 0.37302517 25 –68.7 0.57978921 0.10242432
26 –44.9 0.37049755 33 –69 0.57773363 0.10143253
34 –45 0.36908526 41 –69.2 0.57597277 0.10055134
42 –45.2 0.36764286 49 –69.5 0.57407637 0.09954949
N= 224 N= 224
1 –43.2 0.42874481 1 –71.5 0.58591935 0.09220841
2 –43 0.40836093 2 –69.4 0.593894 0.10355622
3 –43.2 0.39831237 3 –68.5 0.59435536 0.10634325
4 –43.4 0.3921651 4 –68.1 0.59354863 0.10719315
5 –43.6 0.38807204 6 –67.9 0.59106856 0.1070268
7 –43.9 0.38281226 9 –67.9 0.58816185 0.10619806

Frequency Sampling Filter Design Tables 917
Table H–1 Lowpass Type-IV FSF for Even N(One and Two Coefficients) (continued)
BW Atten T1 BW Atten T1 T2
10 –44.2 0.37847605 10 –67.9 0.58726527 0.10582934
11 –44.3 0.37742038 17 –68.3 0.58317185 0.10407347
18 –44.6 0.37324982 33 –68.8 0.57860121 0.10189345
34 –45 0.36946431 49 –69.2 0.5757377 0.10043439
50 –45.2 0.36753866 57 –69.4 0.57440527 0.09975201
N= 256 N= 256
1 –43.2 0.42874481 1 –71.5 0.58599793 0.09225917
2 –43 0.40844072 2 –69.4 0.59395199 0.10360852
3 –43.2 0.39839019 3 –68.5 0.59445009 0.10641045
4 –43.4 0.39231838 4 –68.1 0.59358715 0.10721801
5 –43.6 0.38814788 6 –67.9 0.59121865 0.10712227
7 –43.9 0.38296192 9 –67.9 0.58824601 0.10622746
10 –44.2 0.37855003 10 –67.9 0.58727091 0.10580213
11 –44.3 0.37756795 17 –68.2 0.58347729 0.10424412
18 –44.6 0.3733958 33 –68.7 0.57913354 0.1021842
34 –44.9 0.36982575 49 –69 0.57667852 0.10093887
50 –45.1 0.36804233 57 –69.2 0.57568486 0.10043603
58 –45.2 0.3673716 65 –69.3 0.57469205 0.09993526
Table H–2 Lowpass Type-IV FSF for Odd N(One and Two Coefficients)
BW Atten T BW Atten T T
1 1 2
N= 15 N= 15
1 –45.1 0.41802444 1 –77.3 0.56378193 0.07767395
2 –46.2 0.38716426 2 –78.9 0.54831544 0.07646082
3 –48 0.36461603 3 –83.4 0.52139051 0.06627593
4 –51.4 0.34005655 4 –96.3 0.47732772 0.0487037
(continues)

918 Frequency Sampling Filter Design Tables
Table H–2 Lowpass Type-IV FSF for Odd N(One and Two Coefficients) (continued)
BW Atten T BW Atten T T
1 1 2
N= 23 N= 23
1 –44 0.42412451 1 –73.8 0.57648809 0.08585489
2 –44.3 0.3995718 2 –72.7 0.57569102 0.09221862
3 –45.1 0.38497937 3 –73.2 0.56732401 0.09032595
4 –46 0.37367551 4 –74.6 0.55575797 0.08563268
5 –46.8 0.36445788 5 –76.5 0.54245814 0.07981527
N= 33 N= 33
1 –43.6 0.42659097 1 –72.6 0.58141103 0.08913503
2 –43.5 0.40433257 2 –70.9 0.58529197 0.09811292
3 –44 0.39239983 3 –70.7 0.58187597 0.09874619
4 –44.4 0.3843389 5 –71.3 0.57154421 0.09525674
6 –45.3 0.37272147 7 –72.9 0.55826179 0.08918146
8 –46.4 0.36256798 9 –75.3 0.54128598 0.08116809
N= 47 N= 47
1 –43.4 0.42768264 1 –72 0.58376507 0.09073592
2 –43.2 0.40649692 2 –70.1 0.58975381 0.1009182
3 –43.5 0.39553589 3 –69.6 0.5886934 0.10284475
4 –43.8 0.38859729 5 –69.6 0.58311582 0.10178496
6 –44.4 0.37968179 7 –70.1 0.57687412 0.09925266
8 –44.9 0.37362421 9 –70.7 0.57045477 0.0963056
10 –45.4 0.36853968 11 –71.6 0.56319305 0.09280396
N= 65 N= 65
1 –43.3 0.4281872 1 –71.7 0.58480896 0.09144011
–43.1 0.40743541 2 –69.7 0.59188395 0.10227409
3 –43.4 0.39694541 3 –69.1 0.59158717 0.10459977
4 –43.6 0.39043991 4 –68.9 0.59001405 0.10501496
5 –43.9 0.3859325 5 –68.8 0.58797401 0.10462176
6 –44.1 0.38249194 9 –69.4 0.58031079 0.10167681
10 –44.7 0.37399454 13 –70.1 0.57312044 0.09830111

Frequency Sampling Filter Design Tables 919
Table H–2 Lowpass Type-IV FSF for Odd N(One and Two Coefficients) (continued)
BW Atten T BW Atten T T
1 1 2
14 –45.2 0.36841874 17 –71 0.56506463 0.0943328
18 –45.8 0.36324429 21 –72.5 0.55369626 0.08867447
N= 95 N= 95
1 –43.2 0.42852251 1 –71.6 0.58559589 0.09199196
2 –43 0.40799464 2 –69.5 0.59306419 0.10302347
3 –43.3 0.39772511 3 –68.8 0.59323668 0.10563966
4 –43.5 0.39143867 4 –68.5 0.59207559 0.10628422
5 –43.7 0.38722579 5 –68.3 0.5905046 0.10614086
6 –43.9 0.38400287 9 –68.5 0.58506537 0.10440925
10 –44.4 0.3766755 13 –68.9 0.58094477 0.10262083
14 –44.8 0.37268027 17 –69.3 0.57725045 0.10081244
18 –45 0.369842 21 –69.8 0.57370707 0.09903691
N= 125 N= 125
1 –43.2 0.42857276 1 –71.5 0.5856764 0.0920373
2 –43 0.40819419 2 –69.5 0.59346193 0.10328234
3 –43.2 0.39806479 3 –68.7 0.59389103 0.1060686
4 –43.5 0.39191493 5 –68.1 0.59144981 0.1067222
6 –43.8 0.38458541 7 –68.1 0.58887274 0.10611287
8 –44.1 0.38046599 9 –68.2 0.58670301 0.10535739
10 –44.3 0.37761366 17 –68.7 0.5805297 0.10262003
18 –44.8 0.37166577 25 –69.3 0.57610034 0.10041636
26 –45.1 0.36836415 33 –69.9 0.57206269 0.0984002
N= 191 N= 191
1 –43.2 0.42865655 1 –71.5 0.5859143 0.09220684
2 –43 0.40839373 2 –69.4 0.5937664 0.10346999
3 –43.2 0.39822053 3 –68.5 0.59423193 0.1062604
4 –43.4 0.39214502 5 –68 0.59213475 0.1071269
6 –43.8 0.38503751 7 –67.9 0.589875 0.10671096
8 –44 0.38095089 9 –68 0.58788996 0.10603382
(continues)

920 Frequency Sampling Filter Design Tables
Table H–2 Lowpass Type-IV FSF for Odd N(One and Two Coefficients) (continued)
BW Atten T BW Atten T T
1 1 2
10 –44.2 0.37828083 17 –68.4 0.58282022 0.10390276
18 –44.7 0.37305805 25 –68.7 0.57971044 0.10236319
26 –44.9 0.37048161 33 –69 0.57760031 0.10133385
34 –45 0.36904678 41 –69.2 0.57578133 0.10042123
42 –45.1 0.3676021 49 –69.5 0.57404729 0.09954845
N= 223 N= 223
1 –43.2 0.42874481 1 –71.5 0.58589267 0.0921919
2 –43 0.40836093 2 –69.4 0.59379277 0.10347913
3 –43.2 0.39835128 3 –68.5 0.59435536 0.10634325
4 –43.4 0.3921651 4 –68.1 0.59354863 0.10719315
5 –43.6 0.38807204 6 –67.9 0.59114264 0.10708575
7 –43.9 0.38288709 9 –67.9 0.58797899 0.10606475
10 –44.2 0.37851304 10 –67.9 0.58726527 0.10582934
11 –44.3 0.37742038 17 –68.3 0.58329541 0.10416575
18 –44.6 0.37324982 33 –68.8 0.57849661 0.10182631
34 –45 0.36946431 49 –69.2 0.57568307 0.10040604
50 –45.2 0.3674667 57 –69.4 0.5744086 0.09975962
N= 255 N= 255
1 –43.2 0.42874481 1 –71.5 0.58590294 0.09218854
2 –43 0.40836093 2 –69.3 0.59392035 0.10356223
3 –43.2 0.39831237 3 –68.5 0.59440493 0.10637653
4 –43.4 0.39224174 4 –68.1 0.59363182 0.10725379
5 –43.6 0.38814788 6 –67.9 0.59121586 0.10712951
7 –43.9 0.38296192 9 –67.9 0.58822523 0.10621751
10 –44.2 0.37855003 10 –67.9 0.58747402 0.10596604
11 –44.3 0.37756795 17 –68.2 0.58351595 0.10427857
18 –44.6 0.3733958 33 –68.7 0.57906429 0.10213183
34 –44.9 0.36982575 49 –69 0.57660259 0.10088411
50 –45.1 0.36804233 57 –69.2 0.57562728 0.10040259
58 –45.2 0.3673716 65 –69.4 0.57461162 0.09987875

Frequency Sampling Filter Design Tables 921
Table H–3 Lowpass Type-IV FSF for Even N(Three Coefficients)
BW Atten T T T
1 2 3
N= 16
1 –112.6 0.64272445 0.15442399 0.00880089
2 –95.9 0.70487291 0.22419597 0.01947599
3 –100.7 0.70063089 0.21872748 0.01757096
4 –115.9 0.68531929 0.19831357 0.01270197
N= 24
1 –103.8 0.6599002 0.17315143 0.01189889
2 –101.8 0.67718249 0.19461557 0.01429673
3 –95.5 0.69609682 0.21773826 0.01860944
4 –104.1 0.66830223 0.18959572 0.01339907
N= 32
1 –99.6 0.6642114 0.17798254 0.01278046
2 –98.5 0.68804961 0.20639825 0.01646338
4 –87.4 0.73378289 0.26142233 0.02762054
6 –100.5 0.67913658 0.20169658 0.01554611
8 –105.3 0.65936975 0.18380663 0.01270743
N= 48
1 –93.8 0.68361144 0.2010237 0.01735969
2 –96 0.69534463 0.21480253 0.01812435
4 –87.2 0.73314865 0.26098449 0.02762804
6 –86.4 0.73802064 0.26732823 0.02900775
8 –95 0.69703503 0.2211425 0.01909109
10 –90 0.71746809 0.24474881 0.02420421
N= 64
1 –96.6 0.67620503 0.19208214 0.01551621
2 –94.9 0.69693984 0.21653685 0.01842226
3 –89.7 0.72079468 0.24569738 0.02432222
4 –92.3 0.7068141 0.22927121 0.02042893
(continues)

922 Frequency Sampling Filter Design Tables
Table H–3 Lowpass Type-IV FSF for Even N(Three Coefficients) (continued)
BW Atten T T T
1 2 3
8 –91.4 0.70957119 0.23498487 0.02215407
12 –93.8 0.7026052 0.22772953 0.02059288
16 –85.3 0.74439511 0.27543213 0.03085705
N= 96
1 –98.5 0.6720933 0.18712559 0.01449609
2 –92.9 0.70471821 0.22591053 0.02048075
3 –93 0.70905096 0.23165702 0.02121954
4 –88.7 0.72625477 0.25269331 0.02574193
8 –90.8 0.71369108 0.23929089 0.02281527
12 –90.8 0.71110318 0.23715671 0.02248568
16 –85.2 0.74356072 0.27478153 0.03080406
20 –85.8 0.74022029 0.27104418 0.02999046
N= 128
1 –98.3 0.67221636 0.18725564 0.01451885
2 –94.4 0.70015724 0.22042278 0.01929075
3 –92.6 0.70981704 0.23257905 0.02143209
5 –90.6 0.71933148 0.24480839 0.02391897
8 –89.8 0.72190475 0.24869701 0.02481883
16 –88.5 0.72569265 0.25405918 0.02615712
24 –87.4 0.7301942 0.25964746 0.02748522
N= 192
1 –98.1 0.67216994 0.1871603 0.01447431
2 –94.3 0.70064573 0.22097713 0.01939796
3 –92.6 0.71046628 0.23329177 0.02156244
5 –90.6 0.71933299 0.24477507 0.0238993
8 –89.8 0.72185688 0.24857861 0.02477626
16 –88.5 0.72617255 0.2545026 0.02622728
24 –87.7 0.72957884 0.25880678 0.02726692
32 –90.9 0.71321929 0.24041037 0.02328586

Frequency Sampling Filter Design Tables 923
Table H–3 Lowpass Type-IV FSF for Even N(Three Coefficients) (continued)
BW Atten T T T
1 2 3
40 –91.2 0.71133926 0.23853571 0.02293979
48 –91.4 0.70862489 0.2357226 0.0224067
N= 224
1 –98.2 0.67256687 0.18767169 0.01459779
2 –94.2 0.70077254 0.22112728 0.01942992
3 –90.4 0.7026477 0.22304697 0.01885735
5 –91.1 0.71677647 0.24176238 0.0232377
8 –89.9 0.72168089 0.24837531 0.02473386
9 –89.8 0.71675825 0.24253218 0.02331464
16 –90.3 0.71805244 0.24514888 0.0241623
32 –90.6 0.71429115 0.24150812 0.0234885
48 –91.1 0.71133746 0.23857357 0.02295657
N= 256
1 –95.7 0.67780153 0.19398356 0.01590119
2 –94 0.70138048 0.22187281 0.01959708
3 –90.3 0.70235664 0.22265441 0.01875372
5 –91 0.71654134 0.24139758 0.02311255
8 –89.9 0.72167623 0.24835995 0.02472548
9 –89.7 0.71676546 0.24249377 0.02328724
16 –90.2 0.71841628 0.24555225 0.02424786
32 –90.5 0.71523646 0.24257287 0.02372755
48 –90.8 0.71282545 0.2402303 0.02331467
56 –90.7 0.71353605 0.24104134 0.02347778

924 Frequency Sampling Filter Design Tables
Table H–4 Lowpass Type-IV FSF for Odd N(Three Coefficients)
BW Atten T T T
1 2 3
N= 15
1 –99.1 0.67276446 0.18765467 0.01448603
2 –109 0.65109591 0.16879848 0.01032845
3 –103 0.64743501 0.16597437 0.00887322
4 –129.8 0.58430569 0.11830427 0.0041067
N= 23
1 –98.6 0.67021235 0.18516808 0.01420518
2 –96.9 0.6596023 0.17408826 0.00996838
3 –95.3 0.64635467 0.16260027 0.0077623
4 –86.1 0.60390729 0.12509768 0.00296913
N= 33
1 –98.4 0.67150869 0.18654613 0.01442309
2 –97.9 0.68975409 0.20844142 0.01686088
4 –93.8 0.70392025 0.22717012 0.02035858
6 –92.1 0.70836197 0.23374423 0.02185812
8 –92.9 0.70271751 0.22868478 0.02098636
N= 47
1 –99.7 0.66933083 0.18386234 0.01384901
2 –94.7 0.69037782 0.20845236 0.01654889
4 –94.4 0.70435781 0.22714301 0.02019897
6 –94.5 0.70200706 0.22582668 0.01999782
8 –95.4 0.69662478 0.22082819 0.01907788
10 –97 0.69029654 0.21493063 0.01807728
12 –88.8 0.64107819 0.16254219 0.0076571
N= 65
1 –98.8 0.67071168 0.18547676 0.01416929
2 –94.8 0.69743725 0.21725 0.01864255
3 –93.6 0.70659336 0.22882367 0.02062804
4 –93 0.70962871 0.23307976 0.02141978

Frequency Sampling Filter Design Tables 925
Table H–4 Lowpass Type-IV FSF for Odd N(Three Coefficients) (continued)
BW Atten T T T
1 2 3
8 –92.6 0.70884359 0.23403175 0.02173011
12 –88.7 0.72402053 0.25220517 0.02579854
16 –89.9 0.71679306 0.24461807 0.02427071
N= 95
1 –98.6 0.67204252 0.18706788 0.0144861
2 –94.5 0.69934889 0.21945399 0.01908355
3 –93 0.70879388 0.23134381 0.02114855
4 –92.2 0.71277193 0.23666779 0.0221755
8 –89.2 0.72434568 0.25160197 0.02547937
12 –88.8 0.72479877 0.25277864 0.02583655
16 –90.1 0.71754976 0.24492389 0.02419794
20 –92.6 0.70606236 0.23250776 0.02165584
N= 125
1 –98.3 0.67220923 0.18724753 0.01451708
2 –94.4 0.7001615 0.22041952 0.01928486
4 –90.8 0.71816438 0.24299337 0.02355056
6 –90.5 0.71950797 0.24538851 0.02405304
8 –90.5 0.71546288 0.2410237 0.02303409
16 –89.5 0.72139717 0.24910324 0.02505485
24 –89.9 0.71817491 0.24597097 0.02448715
32 –88.9 0.72170356 0.25030101 0.02550214
N= 191
1 –98.1 0.67223344 0.1872747 0.01452257
2 –94.3 0.70075621 0.22112031 0.01943466
4 –90.5 0.7196321 0.24471727 0.02393159
6 –90.9 0.71816017 0.24379217 0.02370239
8 –90.4 0.71833134 0.24451461 0.02390559
16 –90.4 0.71734228 0.244362 0.02399541
24 –90.1 0.71838476 0.24599127 0.02444213
(continues)

926 Frequency Sampling Filter Design Tables
Table H–4 Lowpass Type-IV FSF for Odd N(Three Coefficients) (continued)
BW Atten T T T
1 2 3
32 –89 0.72239379 0.25075541 0.02551853
40 –89.7 0.7188511 0.24690437 0.0247143
48 –91.6 0.70888027 0.23598149 0.02243978
N= 223
1 –98.2 0.67257273 0.18768014 0.01460038
2 –94.2 0.70072996 0.22108243 0.01942305
3 –90.4 0.70262204 0.22302806 0.0188601
5 –91 0.71641456 0.24126979 0.02309323
8 –89.9 0.72166528 0.24835901 0.02473006
9 –89.8 0.71674758 0.24252389 0.02331481
16 –90.3 0.7179882 0.24507651 0.02414698
32 –90.6 0.71403002 0.24119297 0.02341017
48 –91.2 0.71143573 0.2386827 0.02297579
56 –90.7 0.71338506 0.24097504 0.02348867
N= 255
1 –97.6 0.67176623 0.18670931 0.01441482
2 –94.2 0.70070534 0.22105163 0.01941457
3 –90.3 0.70233551 0.22263226 0.01874791
5 –91 0.71654995 0.24140965 0.02311617
8 –89.9 0.72164196 0.24831241 0.02471143
9 –89.7 0.71673449 0.2424607 0.02328277
16 –90.1 0.71885073 0.24604614 0.02435372
32 –89.6 0.71549778 0.24342724 0.02411907
48 –90.9 0.71268842 0.23999548 0.02323405
56 –90.7 0.71349454 0.240994 0.02346651
64 –91.3 0.71035623 0.23761455 0.02277658

APPENDIX I
Computing
Chebyshev
Window
Sequences
Because detailed methods for computing Chebyshev window functions are
not readily available in the literature of DSP, here we provide the steps for
computing these useful window sequences.
Below we provide methods for computing two types of Chebyshev win-
dow sequences. The first window type yields symmetric window sequences,
in which their first and last samples are equal. That type of window is used in
the Window Design Method of tapped-delay line FIR filter design.
The second Chebyshev window computation method produces non-
symmetric window sequences, in which their first and last samples are not
equal. That type of window is used for spectral leakage reduction in spectrum
analysis applications. (This nonsymmetric type of window has a Fourier
transform that is real-only.) I thank DSP guru Prof. Fredric J. Harris, San
Diego State University, for his personal guidance enabling the creation of the
following procedures.
I.1 CHEBYSHEV WINDOWS FOR FIR FILTER DESIGN
Symmetric Chebyshev window sequences, used in the Window Design
Method of tapped-delay line FIR filters, are computed as follows:
1. Given a desired Chebyshev window sequence length of N, where N is
an odd integer, define integer M=N–1.
2. Define the window’s sidelobe-level control parameter as γ. The win-
dow’s sidelobe peak levels will be –20γ dB below the main lobe’s peak
level. (For example, if we desire frequency-domain sidelobe levels to be
40 dB below the main lobe’s peak level, then we set γ= 2.)
927

928 Computing Chebyshev Window Sequences
3. Compute parameter αas
α= cosh[cosh–1(10 γ )/M]. (I–1)
4. Compute the M-length sequence A(m) using
A(m) = |α.cos(πm/M)| (I–2)
where the index mis 0 ≤m≤(M–1).
5. For each m, evaluate the Mth-degree Chebyshev polynomial whose ar-
gument is A(m) to generate a frequency-domain sequence W(m). There
are many ways to evaluate Chebyshev polynomials. Due to its simplic-
ity of notation, we suggest the following:
W(m) = (–1)m.cosh{M.cosh–1[A(m)]}, when |A(m)| > 1, (I–3)
or
W(m) = (–1)m.cos{M.cos–1[A(m)]}, when |A(m)|≤1, (I–4)
depending on whether or not an individual |A(m)| value is greater than
unity. In theory the resultant W(m) sequence is real-only, but our soft-
ware’s computational numerical errors may produce a complex-valued
W(m) with very small imaginary parts. Those imaginary parts, if they
exist, should be ignored. The above (–1)m factors are necessary because
the frequency-domain index mis never less than zero. Note: If your soft-
ware does not accommodate complex values, then you can avoid prob-
lems by replacing A(m) with |A(m)| in this step.
6. Compute a preliminary time-domain window sequence, w(m), using
w(m) = real partof the M-point inverse DFT of W(m).
7. Replacew(0), the first w(m) time sample, with w(0)/2.
8. Append that new w(0) sample value to the end of the M-point w(m) se-
quence, w(N–1) = w(0), creating the desired N-length w(k) window se-
quence where the time index kis 0 ≤k≤(N–1).
9. Normalize the amplitude of w(k), to obtain a unity peak amplitude, by
dividing each sample of w(k) from Step 8 by the maximum sample value
inw(k).
The above procedure seems a bit involved but it’s not really so bad, as
the following Chebyshev window design example will show. Assume we
need an N= 9-sample Chebyshev window function whose frequency-domain
sidelobes are 60 dB below the window’s main lobe level. Given those require-
ments,N= 9, M= 8, γ= 3, and from Eq. (I–1)
α= cosh[cosh–1(103)/8] = 1.4863.

Chebyshev Windows for Spectrum Analysis 929
After the inverse DFT operation in the above Step 6, w(m= 0)/2 = 11.91, thus
we set w(k= 0) = w(k = 8) = 11.91. The maximum value of w(k) is 229.6323, so
we divide the entire w(k) sequence by that value, yielding our final normal-
ized 9-sample symmetric Chebyshev window sequence listed in the right-
most column of Table I–1.
Table I–1 Nine-Point Symmetric Chebyshev Window Computations
m A(m) W(m) w(m) k w(k) w(k) norm.
0 1.4863 1000.00 23.8214 0 11.911 0.0519
1 1.3732 –411.49 52.1550 1 52.1550 0.2271
2 1.0510 6.4074 123.5232 2 123.5232 0.5379
3 0.5688 –0.1276 197.5950 3 197.5950 0.8605
4 0.0000 1.000 229.6323 4 229.6323 1.0000
5 –0.5688 –0.1276 197.5950 5 197.5950 0.8605
6 –1.0510 6.4074 123.5232 6 123.5232 0.5379
7 –1.3732 –411.49 52.1550 7 52.1550 0.2271
8 11.911 0.0519
I.2 CHEBYSHEV WINDOWS FOR SPECTRUM ANALYSIS
Nonsymmetric Chebyshev window sequences, used for spectral leakage re-
duction in spectrum analysis applications, are computed using the above
steps with the following changes:
• For a Q-length nonsymmetric Chebyshev window sequence, where Qis
an even integer, in the above Step 1 set M=Q.
• Skip the above Step 8, retaining the Q-length nonsymmetric w(k) se-
quence, where the time index kis 0 ≤k≤(Q–1). Normalize the amplitude
of the w(k) sequence as described in the above Step 9.
IfQ= 8, for example, our final γ= 3 normalized 8-sample nonsymmetric
Chebyshev window sequence would be the samples listed in the rightmost
column of Table I–2.

930 Computing Chebyshev Window Sequences
Table I–2 Eight-Point Nonsymmetric Chebyshev Window Computations
m A(m) W(m) w(m) k w(k) w(k) norm.
0 1.4863 1000.00 23.8214 0 11.911 0.0519
1 1.3732 –411.49 52.1550 1 52.1550 0.2271
2 1.0510 6.4074 123.5232 2 123.5232 0.5379
3 0.5688 –0.1276 197.5950 3 197.5950 0.8605
4 0.0000 1.000 229.6323 4 229.6323 1.0000
5 –0.5688 –0.1276 197.5950 5 197.5950 0.8605
6 –1.0510 6.4074 123.5232 6 123.5232 0.5379
7 –1.3732 –411.49 52.1550 7 52.1550 0.2271

Index
A AGC (automatic gain control), 783–784
Aliasing
Absolute value, 9. See alsoMagnitude. definition, 36
A/D converters, quantization noise frequency-domain ambiguity, 33–38
clipping, 706 in IIR filters, 304–305
crest factor, 640 All-ones rectangular functions
dithering, 706–709 DFT for, 115–118
effective bits, 641 Dirichlet kernel, 115–118, 120
fixed-point binary word length, effects Allpass filters, definition, 893
of, 634–642 AM demodulation
oversampling, 704–706 filtering narrowband noise, 792–797
reducing, 704–709 Hilbert transforms, 484–485
SNR (signal-to-noise ratio), 637–642, Amplitude
711–714 definition, 8
triangular dither, 708 loss.SeeAttenuation.
A/D converters, testing techniques Amplitude response, DFT
A/D dynamic range, estimating, complex input, 73
714–715 real cosine input, 83–84
histogram testing, 711 Analog, definition, 2
missing codes, detecting, 715–716 Analog filters
quantization noise, estimating with the approximating, 302
FFT, 709–714 vs.digital, 169
SFDR (spurious free dynamic range), Analog signal processing, 2
714–715 Analog-to-digital (A/D) converters.
SINAD (signal-to-noise-and-distortion), SeeA/D converters.
711–714 Analytic signals
SNR (signal-to-noise ratio), 711–714 bandpass quadrature, 455
Adaptive filters, 184 definition, 483
Addition generation methods, comparing,
block diagram symbol, 10 497–498
complex numbers, 850 half-band FIR filters, 497
Additive white noise (AWN), 380 time-domain, generating, 495–497
931

932 Index
Anti-aliasing filters, 42, 555–558 Bell, Alexander Graham, 885
Anti-imaging filters, 555–558 Bels, definition, 885
Arctangent Bessel functions
approximation, 756–758 definition, 895
vector rotation. SeeVector rotation with Bessel-derived filters, ripples, 901
arctangents. Bessel’s correction, 870–871
Argand, Jean Robert, 848 Bias
Argand diagrams of complex numbers, DC, sources and removal, 761
848 in estimates, 870–871
Argand plane, 440–441 fixed-point binary formats, 628
Attenuation in signal variance, computing, 797–799
CIC filters, improving, 557–558 Bilateral Laplace transforms, 258
definition, 894 Bilinear transform method, designing IIR
Automatic gain control (AGC), 783–784 filters
Average, statistical measures of noise, analytical methods, 302
868–870 definition, 257
Average power in electrical circuits, example, 326–330
calculating, 874–875 frequency warping, 319, 321–325,
Averaging signals. SeeSignal averaging. 328–330
AWN (additive white noise), 380 mapping complex variables, 320–324
process description, 324–326
Bin centers, calculating absolute
B
frequency, 139–140
Band reject filters, 894 Binary points, 629
Band-limited signals, 38 Binary shift multiplication/division,
Bandpass design, for FIR filters, 201–203 polynomial evaluation, 773–774
Bandpass filters Biquad filters, 299
comb filters, 400 Bit normalization, 653
definition, 895 Bit reversals
from half-band FIR filters, 497 avoiding, 158
multisection complex FSFs, 398–403 fast Fourier transform input/output
Bandpass sampling data index, 149–151
1st-order sampling, 46 Bits, definition, 623
definition, 43 Blackman windows
optimum sampling frequency, 46 in FIR filter design, 195–201
positioning sampled spectra, 48 spectral leakage reduction, 686
real signals, 46 Blackman windows (exact), 686, 733
sampling translation, 44 Blackman-Harris windows, 686, 733
SNR (signal-to-noise) ratio, 48–49 Block averaging, SNR (signal-to-noise
spectral inversion, 46–47 ratio), 770
spectral replication, 44–45 Block convolution. SeeFast convolution.
Bandpass signals Block diagrams
in the frequency-domain, 454–455 filter structure, 172–174
interpolating, 728–730 quadrature sampling, 459–462
Bandwidth, definition, 895 symbols, 10–11
Bartlett windows. SeeTriangular uses for, 10
windows. Block floating point, 656–657
Base 8 (octal) numbers, 624–625 Boxcar windows. SeeRectangular
Base 16 (hexadecimal) numbers, 625 windows.

Index 933
Butterfly patterns in FFTs Comb filters. See alsoDifferentiators.
description, 145–149 alternate FSF structures, 416–418
optimized, 156 bandpass FIR filtering, 400
radix-2 structures, 151–154 cascaded-comb subfilters, 412–413
single butterfly structures, 154–158 with complex resonators, 392–398
wingless, 156 frequency response, 903–904
Butterworth function second-order comb filters, 412–413
definition, 895 Comb section. CIC filters, 553
derived filters, ripples, 901 Commutative property, LTI, 18–19
Commutator model, polyphase filters, 524
Compensation FIR filters, CIC filters,
C
563–566
Cardano, Girolamo, 439 Complex conjugate, DFT symmetry, 73
Carrier frequency, 44 Complex down-conversion
Cartesian form, quadrature signals, 442 decimation, in frequency translation,
Cascaded filters, 295–299, 895 782
Cascaded integrators, 563 quadrature signals, 455, 456–462
Cascaded-comb subfilters, 412–413 Complex exponentials, quadrature
Cascade/parallel filter combinations, signals, 447
295–297 Complex frequency, Laplace variable, 258
Cauer filters, 896 Complex frequency response, filters, 277
Causal systems, 258 Complex mixing, quadrature signals, 455
Center frequency, definition, 895 Complex multipliers, down-converting
Central Limit Theory, 723 quadrature signals, 458
Central-difference differentiators, 363–366 Complex number notation, quadrature
CFT (continuous Fourier transform), 59, signals, 440–446
98–102 Complex numbers. See alsoQuadrature
Chebyshev function, definition, 895 signals.
Chebyshev windows, 197–201, 927–930 Argand diagrams, 848
Chebyshev-derived filters, ripples, 900 arithmetic of, 848–858
CIC (cascaded integrator-comb) filters definition, 439
cascaded integrators, 563 as a function of time, 446–450
comb section, 553 graphical representation of, 847–848
compensation FIR filters, 563–566 rectangular form, definition, 848–850
definition, 895 rectangular form, vs.polar, 856–857
implementation issues, 558–563 roots of, 853–854
nonrecursive, 765–768 trigonometric form, 848–850
recursive running sum filters, 551–552 Complex phasors, quadrature signals,
structures, 553–557 446–450
substructure sharing, 765–770 Complex plane, quadrature signals,
transposed structures, 765–770 440–441, 446
two’s complement overflow, 559–563 Complex resonators
Circular buffers, IFIR filters, 388–389 with comb filters, 392–398
Clipping A/D converter quantization FSF (frequency sampling filters),
noise, 706 394–398
Coefficients. SeeFilter coefficients. Complex signals. SeeQuadrature signals.
Coherent sampling, 711 Conditional stability, Laplace transform,
Coherent signal averaging. SeeSignal 268
averaging, coherent. Conjugation, complex numbers, 851–852

934 Index
Constant-coefficient transversal FIR precision, 632–634
filters, 184 representing negative values, 625–626
Continuous Fourier transform (CFT), 59, Data overflow. SeeOverflow.
98–102 dB (decibels), definition, 886, 896
Continuous lowpass filters, 41 dBm (decibels), definition, 892
Continuous signal processing DC
definition, 2 bias, sources of, 761
frequency in, 5–6 block-data DC removal, 762
Continuous signals, definition, 2 defined, 62
Continuous systems, time representation, from a time-domain signal, 812–815
5 DC removal, real-time
Continuous time-domain, Laplace using filters, 761–763
transform, 258–259 noise shaping property, 765
Converting analog to digital. SeeA/D with quantization, 763–765
converters. Deadband effects, 293
Convolution.See alsoFIR (finite impulse DEC (Digital Equipment Corp.), floating-
response) filters, convolution. point binary formats, 654–655
fast, 716–722 Decibels
LTI, 19 bels, definition, 885
overlap-and-add, 720–722 common constants, 889–891
overlap-and-save, 718–720 dB, definition, 886, 896
Cooley, J., 135 dBm, definition, 892
CORDIC (COordinate Rotation DIgital Decimation.See alsoInterpolation.
Computer), 756–758 combining with interpolation, 521–522
Coupled quadrature oscillator, 787 definition, 508
Coupled-form IIR filter, 834–836 to implement down-conversion, 676–679
Crest factor, 640 multirate filters, 521–522
Critical Nyquist, 37 sample rate converters, 521–522
Cutoff frequencies drawing downsampled spectra,
definition, 896 515–516
designing FIR filters, 186 frequency properties, 514–515
magnitude loss in the frequency-
domain, 515
overview, 508–510
D
time invariance, 514
Data formats time properties, 514–515
base systems, 624 example, 512–513
definition, 623 overview, 510–511
place value system, 624 polyphase decomposition, 514
Data formats, binary numbers. See also Decimation filters
Fixed-point binary formats; choosing, 510
Floating-point binary formats. definition, 896
1.15 fixed-point, 630–632 Decimation-in-frequency algorithms, FFTs
block floating point, 656–657 radix-2 butterfly structures, 151–154,
converting to hexadecimal, 625 734–735
converting to octal, 624–625 Decimation-in-time algorithms, FFTs
definition, 623 index bit reversal, 149–151
dynamic range, 632–634 radix-2 butterfly structures, 151–154

Index 935
single butterfly structures, 154–158, DFT leakage, minimizing
735–737 Chebyshev windows, 96
Demodulation Hamming windows, 89–93
AM, 484–485 Hanning windows, 89–97
FM, 486 Kaiser windows, 96
quadrature signals, 453–455, 456–462 rectangular windows, 89–97
Descartes, René, 439 triangular windows, 89–93
Detection windowing, 89–97
envelope, 784–786 DFT processing gain
peak threshold, with matched filters, average output noise-power level,
377, 379–380 103–104
quadrature signals, 453–454 inherent gain, 102–105
signal transition, 820–821 integration gain, 105
single tone. SeeSingle tone detection. multiple DFTs, 105
DFT (discrete Fourier transform). See also output signal-power level, 103–104
DTFT (discrete-time Fourier trans- single DFT, 102–105
form); SDFT (sliding DFT). SNR (signal-to-noise ratio), 103–104
analyzing FIR filters, 228–230 DIF (decimation-in-frequency), 734–735
computing large DFTs from small FFTs, Difference equations
826–829 example, 5
definition, 60 IIR filters, 255–256
examples, 63–73, 78–80 Differentiators
versusFFT, 136–137 central-difference, 363–366
frequency axis, 77 differentiating filters, 364
frequency granularity, improving. See first-difference, 363–366
Zero padding. narrowband, 366–367
frequency spacing, 77 optimized wideband, 369–370
frequency-domain sampling, 98–102 overview, 361–363
inverse, 80–81 performance improvement, 810–812
linearity, 75 wideband, 367–369
magnitudes, 75–76 Digital differencer. SeeDifferentiators.
picket fence effect, 97 Digital Equipment Corp. (DEC), floating-
rectangular functions, 105–112 point binary formats, 654–655
resolution, 77, 98–102 Digital filters. See alsospecific filters.
scalloping loss, 96–97 vs.analog, 169
shifting theorem, 77–78 definition, 896
spectral estimation, improving. SeeZero Digital signal processing, 2
padding. Direct Form I filters, 275–278, 289
time reversal, 863–865 Direct Form II filters, 289–292
zero padding, 97–102 Direct Form implementations, IIR filters,
DFT leakage. See alsoSpectral leakage, 292–293
FFTs. Dirichlet, Peter, 108
cause, 82–84 Dirichlet kernel
definition, 81 all-ones rectangular functions, 115–118,
description, 81–82 120
predicting, 82–84 general rectangular functions, 108–112
sinc functions, 83, 89 symmetrical rectangular functions,
wraparound, 86–88 113–114

936 Index
Discrete convolution in FIR filters. Downsampling, decimation
See alsoFIR (finite impulse response) drawing downsampled spectra,
filters, convolution. 515–516
description, 214–215 frequency properties, 514–515
in the time domain, 215–219 magnitude loss in the frequency-
Discrete Fourier transform (DFT). domain, 515
SeeDFT (discrete Fourier transform). overview, 508–510
Discrete Hilbert transforms. SeeHilbert time invariance, 514
transforms. time properties, 514–515
Discrete linear systems, 12–16 DTFT (discrete-time Fourier transform),
Discrete systems 101, 120–123. See alsoDFT (discrete
definition, 4 Fourier transform).
example, 4–5 Dynamic range
time representation, 5 binary numbers, 632–634
Discrete-time expression, 4 floating-point binary formats,
Discrete-time Fourier transform (DTFT), 656–658
101, 120–123 SFDR (spurious free dynamic range),
Discrete-time signals 714–715
example of, 2
frequency in, 5–6
E
sampling, frequency-domain
ambiguity, 33–38 Elliptic functions, definition, 896
use of term, 2 Elliptic-derived filters, ripples, 900
Discrete-time waveforms, describing, 8 Envelope delay. SeeGroup delay.
Dispersion, statistical measures of noise, Envelope detection
869 approximate, 784–786
DIT (decimation-in-time), 735–737 Hilbert transforms, 483–495
Dithering Equiripple filters, 418, 901
A/D converter quantization noise, Estrin’s Method, polynomial evaluation,
706–709 774–775
with filters, 294 Euler, Leonhard, 442, 444
triangular, 708 Euler’s equation
Dolph-Chebyshev windows in FIR filter bilinear transform design of IIR filters,
design, 197 322
Down-conversion DFT equations, 60, 108
Delay/Hilbert transform filter, 817–818, impulse invariance design of IIR filters,
819–820 315
filtering and decimation, 676–679 quadrature signals, 442–443, 449, 453
folded FIR filters, 818 Exact Blackman windows, 686
frequency translation, without Exact interpolation, 778–781
multiplication, 676–679 Exponent, floating-point binary format,
half-band filters, 817–818 652
single-decimation technique, 819–820 Exponential averagers, 608–612
Down-conversion, quadrature signals Exponential moving averages, 801–802
complex, 455, 456–462 Exponential signal averaging. SeeSignal
complex multipliers, 458 averaging, exponential.
sampling with digital mixing, Exponential variance computation,
462–464 801–802

Index 937
F Filter order, 897
Filter taps, estimating, 234–235, 386–387
Fast convolution, 716–722 Filters.See alsoFIR (finite impulse re-
FFT (fast Fourier transform) sponse) filters; IIR (infinite impulse
averaging multiple, 139 response) filters; Matched filters;
constant-geometry algorithms, 158 specific filters.
convolution.SeeFast convolution. adaptive filters, 184
decimation-in-frequency algorithms, allpass, 893
151–154 analogvs.digital, 169
decimation-in-time algorithms, 149–158 band reject, 894
versusDFT, 136–137 bandpass, 895
exact interpolation, 778–781 cascaded, 895
fast FIR filtering, 716–722 Cauer, 896
hints for using, 137–141 CIC, 895
history of, 135 DC-removal, 762–763
interpolated analytic signals, decimation, 896
computing, 781 differentiating, 364. See also
interpolated real signals, interpolating, Differentiators.
779–780 digital, 896
interpreting results, 139–141 down-conversion, 676–679
inverse, computing, 699–702, 831–833 equiripple, 418
in place algorithm, 157 highpass, 898
radix-2 algorithm, 141–149 linear phase, 899
radix-2 butterfly structures, 151–158 lowpass, 899
signal averaging, 600–603 narrowband noise, 792–797
single tone detection, 737–738, 740–741 nonrecursive, 226–230, 290–291, 899
vs.single tone detection, 740–741 optimal FIR, 418
software programs, 141 overview, 169–170
time-domain interpolation, 778–781 parallel, 295–297
Zoom FFT, 749–753 passband, 900
FFT (fast Fourier transform), real process description, 169–170
sequences prototype, 303
a 2N-point real FFT, 695–699 quadrature, 900
twoN-point real FFTs, 687–694 real-time DC removal, 762–763
FFT (fast Fourier transform), twiddle recursive, 290–291, 900
factors recursive running sum, 551–552
derivation of the radix-2 FFT algorithm, Remez Exchange, 418
143–149 sharpening, 726–728
DIF (decimation-in-frequency), 734–735 structure, diagramming, 172–174
DIT (decimation-in-time), 735–737 time-domain slope detection, 820–821
Fibonacci, 450–451 transposed structure, 291–292
Filter coefficients transversal, 173–174. See alsoFIR
definition, 897 (finite impulse response) filters.
for FIRs. SeeImpulse response. zero-phase, 725, 902
flipping, 493–494 Filters, analytic signals
for FSF (frequency sampling filters), half-band FIR filters, 497
913–926 I-channel filters, 496
quantization, 293–295 in-phase filters, 496

938 Index
Filters, analytic signals (con’t) FIR (finite impulse response) filters,
Q-channel filters, 496 convolution
quadrature phase filters, 496 description, 175–186
time-domain FIR filter implementation, discrete, description, 214–215
489–494 discrete, in the time domain, 215–219
Finite-word-length errors, 293–295 fast convolution, 716–722
FIR (finite impulse response) filters. See impulse response, 177–178
alsoFSF (frequency sampling filters); inputs, time order reversal, 176
IFIR (interpolated FIR) filters; IIR signal averaging, 175–176
(infinite impulse response) filters. theorem, applying, 222–226
coefficients. SeeImpulse response. theorem, description, 219–222
constant coefficients, 184 time-domain aliasing, avoiding,
definition, 897 718–722
fast FIR filtering using the FFT, 716–722 time-domain convolution vs.fre-
folded structure. SeeFolded FIR filters. quency-domain multiplication,
frequency magnitude response, deter- 191–194
mining, 179 FIR (finite impulse response) filters,
frequency-domain response, designing
determining, 179 bandpass method, 201–203
group delay, 211–212 cutoff frequencies, 186
half-band.SeeHalf-band FIR filters. with forward FFT software routines,
vs.IIR filters, 332–333 189
impulse response, 177–179 Fourier series design method. See
narrowband lowpass. SeeIFIR Window design method, FIR filters.
(interpolated FIR) filters. Gibbs’s phenomenon, 193
nonrecursive, analyzing, 226–230 highpass method, 203–204
phase response in, 209–214 low-pass design, 186–201
phase unwrapping, 210 magnitude fluctuations, 190–194
phase wrapping, 209, 900 Optimal design method, 204–207
polyphase filters, 522–527 Parks-McClellan Exchange method,
sharpening, 726–728 204–207
signal averaging. SeeSignal averaging, passband ripples, minimizing, 190–194,
with FIR filters. 204–207. See alsoWindows.
signal averaging with, 178, 180–184 Remez method, 204–207
stopband attenuation, improving, stopband ripples, minimizing, 204–207
726–728 time-domain coefficients, determining,
tapped delay, 181–182 186–194
transient response, 181–182 time-domain convolution vs.fre-
z-transform of, 288–289 quency-domain multiplication,
FIR (finite impulse response) filters, 191–194
analyzing very high performance filters, 775–778
with DFTs, 228–230 window design method, 186–194
estimating number of, 234–235 windows used in, 194–201
fractional delay, 233 1st-order IIR filters, signal averaging,
group delay, 230–233 612–614
passband gain, 233–234 1st-order sampling, 46
stopband attenuation, 234–235 First-difference differentiators, 363–366
symmetrical-coefficient FIR filters, Fixed-point binary formats. See also
232–233 Floating-point binary formats.

Index 939
1.15 format, 630–632 MIL-STD 1750A, 654–655
bias, 628 min/max values, determining,
binary points, 629 656–657
decimal numbers, converting to 1.5 unnormalized fractions, 656
binary, 632 word lengths, 655
fractional binary numbers, 629–632 FM demodulation
hexadecimal (base 16) numbers, 625 algorithms for, 758–761
integer plus fraction, 629 filtering narrowband noise, 792–797
lsb (least significant bit), 624 Hilbert transforms, 486
msb (most significant bit), 624 Folded FIR filters
octal (base 8) numbers, 624–625 designing Hilbert transforms, 493
offset, 627–628 down-conversion, 818
overflow, 629 frequency translation, without
Q30 format, 629 multiplication, 678
radix points, 629 half-band filters, sample rate
representing negative values, 625–626 conversion, 548
sign extend operations, 627 Hilbert transforms, designing, 493
sign-magnitude, 625–626 multipliers, reducing, 702–704
two’s complement, 626–627, 629 nonrecursive, 419–420
Fixed-point binary formats, finite word tapped-delay line, 389
lengths Folding frequencies, 40
A/D converter best estimate values, 635 Forward FFT
A/D converter quantization noise, computing, 831–833
634–642 software routines for designing FIR
A/D converter vs.SNR, 640–642 filters, 189
convergent rounding, 651 Fourier series design FIR filters. See
crest factor, 640 Window design method, FIR filters.
data overflow, 642–646 Fourier transform pairs, FIR filters,
data rounding, 649–652 178–179
effective bits, 641 Fractional binary numbers, 629–632
round off noise, 636–637 Fractional delay, FIR filters, 233
round to even method, 651 Frequency
round-to-nearest method, 650–651 continuousvs.discrete systems, 5
truncation, 646–649 of discrete signals, determining. See
Floating-point binary formats. See also DFT (discrete Fourier transform).
Fixed-point binary formats. discrete-time signals, 5–6
bit normalization, 653 properties, interpolation, 519
common formats, 654–655 resolution, improving with FIR filters,
DEC (Digital Equipment Corp.), 654–655 228–230
description, 652 units of measure, 2–3
dynamic range, 656–658 Frequency attenuation, FIR filters, 182
evaluating, 652 Frequency axis
exponent, 652 definition, 77
fractions, 653 DFT, 77
gradual underflow, 656 in Hz, 118
hidden bits, 653 normalized angle variable, 118
IBM, 654–655 in radians/seconds, 118–119
IEEE Standard P754, 654–655 rectangular functions, 118–120
mantissa, 652 with zero padding, 100

940 Index
Frequency domain multisection complex, 398–403
definition, 6 multisection real-valued, 406–409
Hamming windows, 683–686 vs.Parks-McClellan filters, 392
Hanning windows, 683–686 real FSF transfer function, 908–909
listing sequences, 7 stability, 403–406
performance. IIR filters, 282–289 stopband attenuation, increasing,
quadrature signals, 451–454 414–416
spectral leak reduction, 683–686 stopband sidelobe level suppression,
windowing in, 683–686 416
windows, 683–686 transition band coefficients, 414–416
Frequency magnitude response Type IV example, 419–420, 423–426
definition, 897
determining with FIR filters, 179
G
Frequency response
LTI, determining, 19 Gain.See alsoDFT processing gain.
for Mth-order IIR filter, 275–276 AGC (automatic gain control), 783–784
Frequency response, FIR filters IIR filters, scaling, 300–302
determining, 179–186 integration, signal averaging, 600–603
factors affecting, 174 passband, 233–234
modifying, 184–186 windows, 92
Frequency sampling design method vs. Gauss, Karl, 439, 444
FSF, 393–394 Gaussian PDFs, 882–883
Frequency sampling filters. SeeFSF General numbers, 446. See alsoComplex
(frequency sampling filters). numbers.
Frequency translation, bandpass Geometric series, closed form, 107,
sampling, 44 859–861
Frequency translation, with decimation Gibbs’s phenomenon, 193
complex down-conversion, 782 Goertzel algorithm, single tone detection
complex signals, 781–783 advantages of, 739
real signals, 781 description, 738–740
Frequency translation, without example, 740
multiplication vs.the FFT, 740–741
by 1/2 the sampling rate, 671–673 stability, 838–840
by 1/4 the sampling rate, 674–676 Gold-Rader filter, 834–836
down-conversion, 676–679 Gradual underflow, floating-point binary
inverting the output spectrum, formats, 656
678–679 Gregory, James, 23
Frequency warping, 319, 321–325, 328–330 Group delay
FSF (frequency sampling filters). See also definition, 897–898
FIR (finite impulse response) filters. differentiators, 365
complex resonators, 394–398 filters, computing, 830–831
designing, 423–426 FIR filters, 211–212, 230–233
frequency response, single complex
FSF, 904–905
H
history of, 392–394
linear-phase multisection real-valued, Half Nyquist, 37
409–410 Half-band FIR filters
modeling, 413–414 analytic signals, 497

Index 941
as complex bandpass filters, 497 Hilbert transforms
definition, 898 AM demodulation, 484–485
description, 207–209 definition, 480
down-conversion, 817–818 envelope detection, 483–495
frequency translation, 802–804 example, 481–482
Half-band FIR filters, sample rate FM demodulation, 486
conversion impulse response, 487–489
fundamentals, 544–546 one-sided spectrum, 483
implementation, 546–548 signal envelope, 483–495
overview, 543 Hilbert transforms, analytic signals
Hamming, Richard, 366 definition, 483
Hamming windows generation methods, comparing,
in the frequency domain, 683–686 497–498
spectral peak location, 733 half-band FIR filters, 497
Hann windows. SeeHanning windows. time-domain, generating, 495–497
Hanning windows Histogram testing, A/D converter
description, 89–97 techniques, 711
DFT leakage, minimizing, 89–97 Homogeneity property, 12
in the frequency domain, 683–686 Horner, William, 773
spectral peak location, 733 Horner’s Rule, 772–774
Harmonic sampling. SeeBandpass Human ear, sensitivity to decibels, 886
sampling.
Harmonics of discrete signals,
I
determining.SeeDFT (discrete
Fourier transform). IBM, floating-point binary formats,
Harris, Fred, 791 654–655
Heaviside, Oliver, 257 I-channel filters, analytic signals, 496
Hertz, 3 IDFT (inverse discrete Fourier transform),
Hertz, Heinrich, 3 80–81
Hexadecimal (base 16) numbers, 625 IEEE Standard P754, floating-point binary
Hidden bits, floating-point binary formats, 654–655
formats, 653 IF sampling. SeeBandpass sampling.
Highpass filters, definition, 898 IFIR (interpolated FIR) filters. See alsoFIR
Highpass method, designing FIR filters, (finite impulse response) filters.
203–204 computational advantage, 384–385,
Hilbert, David, 479 391
Hilbert transformers, designing definition, 381
common mistake, 493–494 expansion factor M, 381, 385–386
even-tap transformers, 493 filter taps, estimating, 386–387
frequency-domain transformers, 494–495 image-reject subfilter, 382–384, 390
half-band filter coefficient modification, implementation issues, 388–389
804–805 interpolated, definition, 384
half-band filter frequency translation, interpolators.SeeImage-reject subfilter.
802–804 lowpass design example, 389–391
odd-tap transformers, 493 optimum expansion factor, 386
time-domain FIR filter implementation, performance modeling, 387–388
489–494 prototype filters, 382
time-domain transformers, 489–494 shaping subfilters, 382, 385

942 Index
IIR (infinite impulse response) filters. See parallel filter properties, 295–297
alsoFIR (finite impulse response) fil- transposed, 291–292
ters; FSF (frequency sampling filters). transposed Direct Form II, 289–290
allpass, 893 transposition theorem, 291–292
analytical design methods, 302 Imaginary numbers, 439, 446
coupled-form, 834–836 Imaginary part, quadrature signals, 440,
definition, 899 454–455
design techniques, 257. See alsospecific Impulse invariance method, designing IIR
techniques. filters
difference equations, 255–256 aliasing, 304–305
vs.FIR filters, 253, 332–333 analytical methods, 302
frequency domain performance, definition, 257
282–289 Method 1, description, 305–307
infinite impulse response, definition, Method 1, example, 310–313
280 Method 2, description, 307–310
interpolated, example, 837–838 Method 2, example, 313–319
phase equalizers. SeeAllpass filters. preferred method, 317
poles, 284–289 process description, 303–310
recursive filters, 290–291 prototype filters, 303
scaling the gain, 300–302 Impulse response
SNR (signal-to-noise ratio), 302 convolution in FIR filters, 177–178
stability, 263–270 definition, 898–899
z-domain transfer function, 282–289 FIR filters, 177–179
zeros, 284–289 Hilbert transforms, 487–489
z-plane pole / zero properties, 288–289 Incoherent signal averaging. SeeSignal
z-transform, 270–282 averaging, incoherent.
IIR (infinite impulse response) filters, Infinite impulse response (IIR) filters. See
pitfalls in building IIR (infinite impulse response) filters.
coefficient quantization, 293–295 Integer plus fraction fixed-point binary
deadband effects, 293 formats, 629
Direct Form implementations, 292–293 Integration gain, signal averaging,
dither sequences, 294 600–603
finite word length errors, 293–295 Integrators
limit cycles, 293 CIC filters, 553
limited-precision coefficients, 293 overview, 370
overflow, 293–295 performance comparison, 373–376
overflow oscillations, 293 rectangular rule, 371–372
overview, 292–293 Simpson’s rule, 372, 373–376
rounding off, 293 Tick’s rule, 373–376
IIR (infinite impulse response) filters, trapezoidal rule, 372
structures Intermodulation distortion, 16
biquad filters, 299 Interpolated analytic signals, computing,
cascade filter properties, 295–297 781
cascaded, 295–299 Interpolated FIR (IFIR) filters. SeeIFIR
cascade/parallel combinations, 295–297 (interpolated FIR) filters.
changing, 291–292 Interpolated real signals, interpolating,
Direct Form 1, 275–278, 289 779–780
Direct Form II, 289–292 Interpolation.See alsoDecimation.
optimizing partitioning, 297–299 accuracy, 519

Index 943
bandpass signals, 728–730 determining system stability, 263–264,
combining with decimation, 521–522 268
definition, 384, 508 impulse invariance design, Method 1,
drawing upsampled spectra, 520–521 305–307, 310–313
exact, 778–781 impulse invariance design, Method 2,
frequency properties, 519 307–310, 313–319
history of, 519 in parallel filters, 295–297
linear, 815–816 second order, 265–268
multirate filters, 521–522 Laplace transform. See alsoZ-transform.
overview, 516–518 bilateral transform, 258
sample rate converters, 521–522 causal systems, 258
time properties, 519 conditional stability, 268
time-domain, 778–781 for continuous time-domain, 258–259
unwanted spectral images, 519 description, 257–263
upsampling, 517–518, 520–521 development of, 257
zero stuffing, 518 one-sided transform, 258
Interpolation filters, 518 one-sided/causal, 258
Inverse DFT, 80–81 poles on the s-plane, 263–270
Inverse discrete Fourier transform (IDFT), stability, 263–270
80–81 two-sided transform, 258
Inverse FFT, 699–702, 831–833 zeros on the s-plane, 263–270
Inverse of complex numbers, 853 Laplace variable, complex frequency, 258
Inverse sinc filters, 563–566 Leakage.SeeDFT leakage.
I/Q demodulation, quadrature signals, Leaky integrator, 614
459–462 Least significant bit (lsb), 624
l’Hopital’s Rule, 110
Limit cycles, 293
J
Linear, definition, 12
Jacobsen, Eric, 775 Linear differential equations, solving.
j-operator, quadrature signals, 439, SeeLaplace transform.
444–450 Linear interpolation, 815–816
Linear phase filters, 899
Linear systems, example, 13–14
K
Linear time-invariant (LTI) systems. See
Kaiser, James, 270 LTI (linear time-invariant) systems.
Kaiser windows, in FIR filter design, Linearity, DFT, 75
197–201 Linear-phase filters
Kaiser-Bessel windows, in FIR filter DC removal, 812–815
design, 197 definition, 899
Kelvin, Lord, 60 Logarithms
Kootsookos, Peter, 603, 724 and complex numbers, 854–856
Kotelnikov, V., 42 measuring signal power, 191
Lowpass design
designing FIR filters, 186–201
L
IFIR filters, example, 389–391
Lanczos differentiators, 366–367 Lowpass filters, definition, 899
Laplace transfer function Lowpass signals
conditional stability, 268 definition, 38
description, 262–263 sampling, 38–42

944 Index
lsb (least significant bit), 624 Magnitude-angle form, quadrature
LTI (linear time-invariant) systems signals, 442
analyzing, 19–21 Mantissa, floating-point binary formats,
commutative property, 18–19 652
convolution, 19 Matched filters
DFT (discrete Fourier transform), 19 definition, 376
discrete linear systems, 12–16 example, 378–380
frequency response, determining, 19 implementation considerations, 380
homogeneity property, 12 peak detection threshold, 377, 379–380
intermodulation distortion, 16 properties, 376–378
internally generated sinusoids, 16 purpose, 376
linear, definition, 12 SNR (signal-power-to-noise-power
linear system, example, 13–14 ratio), maximizing, 376
nonlinear system, example, 14–16 McClellan, James, 206. See alsoParks-
output sequence, determining, 19 McClellan algorithm.
overview, 12 Mean (statistical measure of noise)
proportionality characteristic, 12 definition, 868–869
rearranging sequential order, 18–19 PDF (probability density function),
time-invariant systems, 17–18 879–882
unit impulse response, 19–20 of random functions, 879–882
Mean (statistical average), of random
functions, 879–882
M
Mehrnia, A., 386
MAC (multiply and accumulate) MIL-STD 1750A, floating-point binary
architecture formats, 654–655
polynomial evaluation, 773 Missing
programmable DSPchips, 333 A/D conversion codes, checking,
Magnitude 715–716
approximation (vector), 679–683 sample data, recovering, 823–826.
of complex numbers, 848 See alsoInterpolation.
definition, 8–9 Mixing to baseband, quadrature signals,
DFT, 75–76 455
Magnitude and angle form of complex Modeling FSF (frequency sampling
numbers, 848–850 filters), 413–414
Magnitude response of DFTs Modulation, quadrature signals, 453–454
aliased sinc function, 108 Modulus of complex numbers, 848
all-ones rectangular functions, 115–118 Most significant bit (msb), 624
fluctuations.SeeScalloping. Moving averages
general rectangular functions, 106–112 CIC filters, 551–552
overview, 105–106 as digital lowpass filters, 20–21
sidelobe magnitudes, 110–111 sample rate conversion, CIC filters,
symmetrical rectangular functions, 551–552
112–115 Moving averages, coherent signal
Magnitude response of DFTs, Dirichlet averaging
kernel exponential moving averages,
all-ones rectangular functions, 115–118, computing, 801–802
120 exponential signal averaging, 801–802
general rectangular functions, 108–112 moving averages, computing, 799–801
symmetrical rectangular functions, nonrecursive moving averagers,
113–114 606–608

Index 945
recursive moving averagers, 606–608 O
time-domain averaging, 604–608
Octal (base 8) numbers, 624–625
msb (most significant bit), 624
Offset fixed-point binary formats, 627–628
Multiplication
1.15 fixed-point binary format, 630–632
block diagram symbol, 10
Optimal design method, designing FIR
CIC filters, simplified, 765–770
filters, 204–207
complex numbers, 850–851
Optimal FIR filters, 418
Multirate filters
Optimization method, designing IIR
decimation, 521–522
filters
interpolation, 521–522
definition, 257
Multirate systems, sample rate conversion
description, 302
filter mathematical notation, 534–535
iterative optimization, 330
signal mathematical notation, 533–534
process description, 330–332
z-transform analysis, 533–535
Optimized butterflies, 156
Multirate systems, two-stage decimation,
Optimized wideband differentiators,
511
369–370
Optimum sampling frequency, 46
N
Order
of filters, 897
Narrowband differentiators, 366–367
polyphase filters, swapping, 536–537
Narrowband noise filters, 792–797
Orthogonality, quadrature signals, 448
Natural logarithms of complex numbers,
Oscillation, quadrature signals, 459–462
854
Oscillator, quadrature
Negative frequency, quadrature signals,
coupled, 787
450–451
overview, 786–789
Negative values in binary numbers,
Taylor series approximation, 788
625–626
Overflow
Newton, Isaac, 773
computing the magnitude of complex
Newton’s method, 372
numbers, 815
Noble identities, polyphase filters, 536
fixed-point binary formats, 629,
Noise
642–646
definition, 589
two’s complement, 559–563
measuring.SeeStatistical measures
Overflow errors, 293–295
of noise.
Overflow oscillations, 293
random, 868
Oversampling A/D converter
Noise shaping property, 765
quantization noise, 704–706
Nonlinear systems, example, 14–16
Nonrecursive CIC filters
description, 765–768
P
prime-factor-R technique, 768–770
Nonrecursive filters. SeeFIR filters Parallel filters, Laplace transfer function,
Nonrecursive moving averagers, 606–608 295–297
Normal distribution of random data, Parks-McClellan algorithm
generating, 722–724 designing FIR filters, 204–207
Normal PDFs, 882–883 vs.FSF (frequency sampling filters), 392
Normalized angle variable, 118–119 optimized wideband differentiators,
Notch filters. SeeBand reject filters. 369–370
Nyquist, H., 42 Parzen windows. SeeTriangular
Nyquist criterion, sampling lowpass windows.
signals, 40 Passband, definition, 900

946 Index
Passband filters, definition, 900 Poles
Passband gain, FIR filters, 233–234 IIR filters, 284–289
Passband ripples on the s-plane, Laplace transform,
cascaded filters, estimating, 296–297 263–270
definition, 296, 900 Polynomial curve fitting, 372
IFIR filters, 390 Polynomial evaluation
minimizing, 190–194, 204–207 binary shift multiplication/division,
PDF (probability density function) 773–774
Gaussian, 882–883 Estrin’s Method, 774–775
mean, calculating, 879–882 Horner’s Rule, 772–774
mean and variance of random func- MAC (multiply and accumulate)
tions, 879–882 architecture, 773
normal, 882–883 Polynomial factoring, CIC filters, 765–770
variance, calculating, 879–882 Polynomials, finding the roots of, 372
Peak correlation, matched filters, 379 Polyphase decomposition
Peak detection threshold, matched filters, CIC filters, 765–770
377, 379–380 definition, 526
Periodic sampling diagrams, 538–539
aliasing, 33–38 two-stage decimation, 514
frequency-domain ambiguity, 33–38 Polyphase filters
Periodic sampling benefits of, 539
1st-order sampling, 46 commutator model, 524
anti-aliasing filters, 42 implementing, 535–540
bandpass, 43–49 issues with, 526
coherent sampling, 711 noble identities, 536
definition, 43 order, swapping, 536–537
folding frequencies, 40 overview, 522–528
Nyquist criterion, 40 polyphase decomposition, 526, 538–539
optimum sampling frequency, 46 prototype FIR filters, 522
real signals, 46 uses for, 522
sampling translation, 44 Power, signal. See alsoDecibels.
SNR (signal-to-noise) ratio, 48–49 absolute, 891–892
spectral inversion, 46–47 definition, 9
undersampling, 40 relative, 885–889
Phase angles, signal averaging, 603–604 Power spectrum, 63, 140–141
Phase delay. SeePhase response. Preconditioning FIR filters, 563–566
Phase response Prewarp, 329
definition, 900 Prime decomposition, CIC filters,
in FIR filters, 209–214 768–770
Phase unwrapping, FIR filters, 210 Prime factorization, CIC filters, 768–770
Phase wrapping, FIR filters, 209, 900 Probability density function (PDF). See
Pi, calculating, 23 PDF (probability density function).
Picket fence effect, 97 Processing gain or loss. SeeDFT
Pisa, Leonardo da, 450–451 processing gain; Gain; Loss.
Polar form Prototype filters
complex numbers, vs.rectangular, analog, 303
856–857 FIR polyphase filters, 522
quadrature signals, 442, 443–444 IFIR filters, 382

Index 947
Q mixing to baseband, 455
modulation, 453–454
Q30 fixed-point binary formats, 629
negative frequency, 450–451
Q-channel filters, analytic signals, 496
orthogonality, 448
Quadratic factorization formula, 266, 282
polar form, 442, 443–444
Quadrature component, 454–455
positive frequency, 451
Quadrature demodulation, 455, 456–462
real axis, 440
Quadrature filters, definition, 900
real part, 440, 454–455
Quadrature mixing, 455
rectangular form, 442
Quadrature oscillation, 459–462
representing real signals, 446–450
Quadrature oscillator
sampling scheme, advantages of,
coupled, 787
459–462
overview, 786–789
simplifying mathematical analysis,
Taylor series approximation, 788
443–444
Quadrature phase, 440
three-dimensional frequency-domain
Quadrature processing, 440
representation, 451–454
Quadrature sampling block diagram,
trigonometric form, 442, 444
459–462
uses for, 439–440
Quadrature signals. See alsoComplex
Quantization
numbers.
coefficient/errors, 293–295
analytic, 455
noise.SeeA/D converters, quantization
Argand plane, 440–441
noise.
bandpass signals in the frequency-
real-time DC removal, 763–765
domain, 454–455
Cartesian form, 442
complex exponentials, 447 R
complex mixing, 455
complex number notation, 440–446 Radix points, fixed-point binary formats,
complex phasors, 446–450 629
complex plane, 440–441, 446 Radix-2 algorithm, FFT
decimation, in frequency translation, butterfly structures, 151–154
781–783 computing large DFTs, 826–829
definition, 439 decimation-in-frequency algorithms,
demodulation, 453–454 151–154
detection, 453–454 decimation-in-time algorithms, 151–154
down-conversion.SeeDown- derivation of, 141–149
conversion, quadrature signals. FFT (fast Fourier transform), 151–158
Euler’s identity, 442–443, 449, 453 twiddle factors, 143–149
exponential form, 442 Raised cosine windows. SeeHanning
in the frequency domain, 451–454 windows.
generating from real signals. SeeHilbert Random data
transforms. Central Limit Theory, 723
generation, 453–454 generating a normal distribution of,
imaginary part, 440, 454–455 722–724
in-phase component, 440, 454–455 Random functions, mean and variance,
I/Q demodulation, 459–462 879–882
j-operator, 439, 444–450 Random noise, 868. See alsoSNR
magnitude-angle form, 442 (signal-to-noise ratio).

948 Index
Real numbers Rosetta Stone, 450
definition, 440 Rounding fixed-point binary numbers
graphical representation of, 847–848 convergent rounding, 651
Real sampling, 46 data rounding, 649–652
Real signals effective bits, 641
bandpass sampling, 46 round off noise, 636–637
decimation, in frequency translation, 781 round to even method, 651
generating complex signals from. See round-to-nearest method, 650–651
Hilbert transforms. Roundoff errors, 293
representing with quadrature signals,
446–450
S
Rectangular form of complex numbers
definition, 848–850 Sample rate conversion. See also
vs.polar form, 856–857 Polyphase filters.
Rectangular form of quadrature signals, decreasing. SeeDecimation.
442 definition, 507
Rectangular functions with IFIR filters, 548–550
all ones, 115–118 increasing. SeeInterpolation.
DFT, 105–112 missing data, recovering, 823–826.
frequency axis, 118–120 See alsoInterpolation.
general, 106–112 by rational factors, 540–543
overview, 105–106 Sample rate conversion, multirate
symmetrical, 112–115 systems
time axis, 118–120 filter mathematical notation, 534–535
Rectangular windows, 89–97, 686 signal mathematical notation, 533–534
Recursive filters. SeeIIR filters z-transform analysis, 533–535
Recursive moving averagers, 606–608 Sample rate conversion, with half-band
Recursive running sum filters, 551–552 filters
Remez Exchange, 204–207, 418 folded FIR filters, 548
Replications, spectral. SeeSpectral fundamentals, 544–546
replications. implementation, 546–548
Resolution, DFT, 77, 98–102 overview, 543
Ripples Sample rate converters, 521–522
in Bessel-derived filters, 901 Sampling, periodic. SeePeriodic
in Butterworth-derived filters, 901 sampling.
in Chebyshev-derived filters, 900 Sampling translation, 44
definition, 900–901 Sampling with digital mixing, 462–464
designing FIR filters, 190–194 Scaling IIR filter gain, 300–302
in Elliptic-derived filters, 900 Scalloping loss, 96–97
equiripple, 418, 901 SDFT (sliding DFT)
out-of-band, 901 algorithm, 742–746
in the passband, 900 overview, 741
in the stopband, 901 stability, 746–747
rms value of continuous sinewaves, SFDR (spurious free dynamic range),
874–875 714–715
Roll-off, definition, 901 Shannon, Claude, 42
Roots of Shape factor, 901
complex numbers, 853–854 Sharpened FIR filters, 726–728
polynomials, 372 Shifting theorem, DFT, 77–78

Index 949
Shift-invariant systems. SeeTime- dual-mode technique, 791
invariant systems. example, 614
Sidelobe magnitudes, 110–111 exponential smoothing, 608
Sidelobes frequency-domain filters, 612–614
Blackman window and, 194–197 moving average, computing, 801–802
DFT leakage, 83, 89 multiplier-free technique, 790–791
FIR (finite impulse response) filters, 184 overview, 608
ripples, in low-pass FIR filters, 193–194 single-multiply technique, 789–790
Sign extend operations, 627 Signal averaging, incoherent
Signal averaging. See alsoSNR (signal- 1st-order IIR filters, 612–614
to-noise ratio). example, 614
equation, 589 frequency-domain filters, 612–614
frequency-domain. SeeSignal overview, 597–599
averaging, incoherent. Signal averaging, with FIR filters
integration gain, 600–603 convolution, 175–176
mathematics, 592–594, 599 example, 170–174, 183–184
multiple FFTs, 600–603 as a lowpass filter, 180–182
phase angles, 603–604 performance improvement, 178
postdetection.SeeSignal averaging, Signal envelope, Hilbert transforms,
incoherent. 483–495
quantifying noise reduction, 594–597 Signal power. See alsoDecibels.
rms.SeeSignal averaging, incoherent. absolute, 891–892
scalar. SeeSignal averaging, incoherent. relative, 885–889
standard deviation, 590 Signal processing
time-domain.SeeSignal averaging, analog, 2. See alsoContinuous signals.
coherent. definition, 2
time-synchronous. SeeSignal averag- digital, 2
ing, coherent. operational symbols, 10–11
variance, 589–590 Signal transition detection, 820–821
video.SeeSignal averaging, incoherent. Signal variance
Signal averaging, coherent biased and unbiased, computing,
exponential averagers, 608–612 797–799, 799–801
exponential moving averages, definition, 868–870
computing, 801–802 exponential, computing, 801–802
exponential smoothing, 608 PDF (probability density function),
filtering aspects, 604–608 879–882
moving averagers, 604–608 of random functions, 879–882
moving averages, computing, 799–801 signal averaging, 589–590
nonrecursive moving averagers, Signal-power-to-noise-power ratio (SNR),
606–608 maximizing, 376
overview, 590–597 Signal-to-noise ratio (SNR). SeeSNR
recursive moving averagers, 606–608 (signal-to-noise ratio).
reducing measurement uncertainty, Sign-magnitude, fixed-point binary
593, 604–608 formats, 625–626
time-domain filters, 609–612 Simpson, Thomas, 372
true signal level, 604–608 SINAD (signal-to-noise-and-distortion),
weighting factors, 608, 789 711–714
Signal averaging, exponential Sinc filters. SeeCIC (cascaded
1st-order IIR filters, 612–614 integrator-comb) filters.

950 Index
Sinc functions, 83, 89, 116 Spectral leakage reduction
Single tone detection, FFT method A/D converter testing techniques,
drawbacks, 737–738 710–711
vs.Goertzel algorithm, 740–741 Blackman windows, 686
Single tone detection, Goertzel algorithm frequency domain, 683–686
advantages of, 739 Spectral peak location
description, 738–740 estimating, algorithm for, 730–734
example, 740 Hamming windows, 733
vs.the FFT, 740–741 Hanning windows, 733
stability, 838–840 Spectral replications
Single tone detection, spectrum analysis, bandpass sampling, 44–45
737–741 sampling lowpass signals, 39–40
Single-decimation down-conversion, Spectral vernier. SeeZoom FFT.
819–820 Spectrum analysis. See alsoSDFT
Single-multiply technique, exponential (sliding DFT); Zoom FFT.
signal averaging, 789–790 center frequencies, expanding, 748–749
Single-stage decimation, vs.two-stage, with SDFT (sliding DFT), 748–749
514 single tone detection, 737–741
Single-stage interpolation, vs.two-stage, weighted overlap-add, 755
532 windowed-presum FFT, 755
Sliding DFT (SDFT). SeeSDFT Zoom FFT, 749–753
(sliding DFT). Spectrum analyzer, 753–756
Slope detection, 820-821 Spurious free dynamic range (SFDR),
Smoothing impulsive noise, 770–772 714–715
SNDR.SeeSINAD (signal-to-noise-and- Stability
distortion). comb filters, 403–404
SNR (signal-to-noise ratio) conditional, 268
vs.A/D converter, fixed-point binary FSF (frequency sampling filters),
finite word lengths, 640–642 403–406
A/D converters, 711–714 IIR filters, 263–270
bandpass sampling, 48–49 Laplace transfer function, 263–264, 268
block averaging, 770 Laplace transform, 263–270
corrected mean, 771 SDFT (sliding DFT), 746–747
DFT processing gain, 103–104 single tone detection, 838–840
IIR filters, 302 z-transform and, 272–274, 277
measuring.SeeStatistical measures of Stair-step effect, A/D converter
noise. quantization noise, 637
reducing. SeeSignal averaging. Standard deviation
smoothing impulsive noise, 770–772 of continuous sinewaves, 874–875
SNR (signal-power-to-noise-power ratio), definition, 870
maximizing, 376 signal averaging, 590
Software programs, fast Fourier trans- Statistical measures of noise
form, 141 average, 868–870
Someya, I., 42 average power in electrical circuits,
Spectral inversion 874–875
around signal center frequency, 821–823 Bessel’s correction, 870–871
bandpass sampling, 46–47 biased estimates, 870–871
Spectral leakage, FFTs, 138–139, 683–686. dispersion, 869
See alsoDFT leakage. fluctuations around the average, 869

Index 951
overview, 867–870. See alsoSNR transposed Direct Form II, 289–290
(signal-to-noise ratio). transposition theorem, 291–292
of real-valued sequences, 874 Sub-Nyquist sampling. SeeBandpass
rms value of continuous sinewaves, sampling.
874–875 Substructure sharing, 765–770
of short sequences, 870–871 Subtraction
standard deviation, definition, 870 block diagram symbol, 10
standard deviation, of continuous complex numbers, 850
sinewaves, 874–875 Summation
summed sequences, 872–874 block diagram symbol, 10
unbiased estimates, 871 description, 11
Statistical measures of noise, estimating equation, 10
SNR notation, 11
for common devices, 876 Symbols
controlling SNR test signals, 879 block diagram, 10–11
in the frequency domain, 877–879 signal processing, 10–11
overview, 875–876 Symmetrical rectangular functions,
in the time domain, 876–877 112–115
Statistical measures of noise, mean Symmetrical-coefficient FIR filters,
definition, 868–869 232–233
PDF (probability density function), Symmetry, DFT, 73–75
879–882
of random functions, 879–882
T
Statistical measures of noise, variance.
See alsoSignal variance. Tacoma Narrows Bridge collapse, 263
definition, 868–870 Tap, definition, 901
PDF (probability density function), Tap weights. SeeFilter coefficients.
879–882 Tapped delay, FIR filters, 181–182
of random functions, 879–882 Taylor series approximation, 788
Steinmetz, Charles P., 446 Tchebyschev function, definition, 902
Stockham, Thomas, 716 Tchebyschev windows, in FIR filter
Stopband, definition, 901 design, 197
Stopband ripples Time data, manipulating in FFTs, 138–139
definition, 901 Time invariance, decimation, 514
minimizing, 204–207 Time properties
Stopband sidelobe level suppression, decimation, 514–515
416 interpolation, 519
Structure, definition, 901 Time representation, continuous vs.
Structures, IIR filters discrete systems, 5
biquad filters, 299 Time reversal, 863–865
cascade filter properties, 295–297 Time sequences, notation syntax, 7
cascaded, 295–299 Time-domain
cascade/parallel combinations, 295–297 aliasing, avoiding, 718–722
changing, 291–292 analytic signals, generating, 495–497
Direct Form 1, 275–278, 289 coefficients, determining, 186–194
Direct Form II, 289–292 convolution, matched filters, 380
optimizing partitioning, 297–299 convolutionvs.frequency-domain
parallel filter properties, 295–297 multiplication, 191–194
transposed, 291–292 equations, example, 7

952 Index
Time-domain (cont.) Truncation, fixed-point binary numbers,
FIR filter implementation, 489–494 646–649
Hilbert transforms, designing, 489–494 Tukey, J., 135
interpolation, 778–781 Two’s complement
slope filters, 820–821 fixed-point binary formats, 626–627,
Time-domain data, converting 629
from frequency-domain data. SeeIDFT overflow, 559–563
(inverse discrete Fourier transform). Two-sided Laplace transform, 258
to frequency-domain data. SeeDFT Type-IV FSF
(discrete Fourier transform). examples, 419–420, 423–426
Time-domain filters frequency response, 910–912
coherent signal averaging, 609–612 optimum transition coefficients,
exponential signal averaging, 609–612 913–926
Time-domain signals
amplitude, determining, 140
U
continuous, Laplace transform for, 258
DC removal, 812–815 Unbiased estimates, 871
definition, 4 Unbiased signal variance, computing,
vs.frequency-domain, 120–123 797–799, 799–801
Time-invariant systems. See alsoLTI Undersampling lowpass signals, 40.
(linear time-invariant) systems. See alsoBandpass sampling.
analyzing, 19–21 Uniform windows. SeeRectangular
commutative property, 18–19 windows.
definition, 17–18 Unit circles
example of, 17–18 definition, 271
Tone detection. SeeSingle tone detection. z-transform, 271
Transfer functions. See alsoLaplace Unit circles, FSF
transfer function. forcing poles and zeros inside, 405
definition, 902 pole / zero cancellation, 395–398
real FSF, 908–909 Unit delay
z-domain, 282–289 block diagram symbol, 10
Transient response, FIR filters, 181–182 description, 11
Transition region, definition, 902 Unit impulse response, LTI, 19–20
Translation, sampling, 44 Unnormalized fractions, floating-point
Transposed Direct Form II filters, 289–290 binary formats, 656
Transposed Direct Form II structure, Unwrapping, phase, 210
289–290 Upsampling, interpolation, 517–518,
Transposed filters, 291–292 520–521
Transposed structures, 765–770
Transposition theorem, 291–292
V
Transversal filters, 173–174. See alsoFIR
(finite impulse response) filters. Variance. SeeSignal variance.
Triangular dither, 708 Vector, definition, 848
Triangular windows, 89–93 Vector rotation with arctangents
Trigonometric form, quadrature signals, to the 1st octant, 805–808
442, 444 division by zero, avoiding, 808
Trigonometric form of complex numbers, jump address index bits, 807
848–850 overview, 805

Index 953
by ±?/8, 809–810 Blackman, 195–201
rotational symmetries, 807 Chebyshev, 197–201, 927–930
Vector-magnitude approximation, choosing, 199–201
679–683 Dolph-Chebyshev, 197
von Hann windows. SeeHanning Kaiser, 197–201
windows. Kaiser-Bessel, 197
Tchebyschev, 197
Wingless butterflies, 156
W
Wraparound leakage, 86–88
Warping, frequency, 319, 321–325, Wrapping, phase, 209, 900
328–330
Weighted overlap-add spectrum analysis,
Z
755
Weighting factors, coherent signal z-domain expression for Mth-order IIR
averaging, 608, 789 filter, 275–276
Wideband compensation, 564 z-domain transfer function, IIR filters,
Wideband differentiators, 367–370 282–289
Willson, A., 386 Zero padding
Window design method, FIR filters, alleviating scalloping loss, 97–102
186–194 FFTs, 138–139
Windowed-presum FFT spectrum FIR filters, 228–230
analysis, 755 improving DFT frequency granularity,
Windows 97–102
Blackman, 195–201, 686, 733 spectral peak location, 731
Blackman-Harris, 686, 733 Zero stuffing
exact Blackman, 686 interpolation, 518
FFTs, 139 narrowband lowpass filters, 834–836
in the frequency domain, 683–686 Zero-overhead looping
magnitude response, 92–93 DSPchips, 333
mathematical expressions of, 91 FSF (frequency sampling filters),
minimizing DFT leakage, 89–97 422–423
processing gain or loss, 92 IFIR filters, 389
purpose of, 96 Zero-phase filters
rectangular, 89–97, 686 definition, 902
selecting, 96 techniques, 725
triangular, 89–93 Zeros
Windows, Hamming IIR filters, 284–289
description, 89–93 on the s-plane, Laplace transform,
DFT leakage reduction, 89–93 263–270
in the frequency domain, 683–686 Zoom FFT, 749–753
spectral peak location, 733 z-plane pole / zero properties, IIR filters,
Windows, Hanning 288–289
description, 89–97 z-transform.See alsoLaplace transform.
DFT leakage, minimizing, 89–97 definition, 270
in the frequency domain, 683–686 description of, 270–272
spectral peak location, 733 FIR filters, 288–289
Windows used in FIR filter design IIR filters, 270–282
Bessel functions, 198–199 infinite impulse response, definition, 280

954 Index
z-transform (cont.) Direct Form 1 structure, 275–278
polar form, 271 example, 278–282
poles, 272–274 frequency response, 277–278
unit circles, 271 overview, 274–275
zeros, 272–274 time delay, 274–278
z-transform, analyzing IIR filters z-domain transfer function, 275–278,
digital filter stability, 272–274, 277 279–280

